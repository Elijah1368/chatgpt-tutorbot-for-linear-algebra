[[0,{"pageContent":"David Poole \nA M    0 'D     E \nR N \nI N T    R    0 D    U, C T I     0     N \n4th edition \n\nFourth edition \nDavid Poole \nTrent University \n� .. � CENGAGE \n•-Learning· \nAustralia\n• \nBrazil\n• \nMexico\n• \nSingapore\n• \nUnited l<ingdom •\nUnited States","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1,"to":21}}}}],[1,{"pageContent":"�,.,, # CENGAGE \n• • Learning· \nLinear Algebra \nA Modern Introduction, 4th Edition \nDavid Poole \nProduct Director: Liz Covello \nProduct Team Manager: Richard Stratton \nContent Developer: Laura Wheel \nProduct Assistant: Danielle Hallock \nMedia Developer: Andrew Coppola \nContent Project Manager: Alison Eigel Zade \nSenior Art Director: Linda May \nManufacturing Planner: Doug Bertke \nRights Acquisition Specialist: Shalice \nShah-Caldwell \nProduction Service & Compositor: \nMPS Limited \nText Designer: Leonard Massiglia \nCover Designer: Chris Miller \nCover & Interior design Image: Image \nSource/Getty Images \nPrinted in the United States of America \n1  2 3 4 5 6 7 17 16 15 14 13 \n© 2015, 2011, 2006 Cengage Learning \nWCN: 02-200-201 \nALL RIGHTS RESERVED. No part of this work covered by the copyright \nherein may be reproduced, transmitted, stored, or used in any form \nor by any means graphic, electronic, or mechanical, including but not \nlimited to photocopying, recording, scanning, digitizing, taping, web","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":23,"to":51}}}}],[2,{"pageContent":"or by any means graphic, electronic, or mechanical, including but not \nlimited to photocopying, recording, scanning, digitizing, taping, web \ndistribution, information networks, or information storage and retrieval \nsystems, except as permitted under Section 107 or 108 of the 1976 \nUnited States Copyright Act, without the prior written permission of \nthe publisher. \nFor product information and technology assistance,  contact us at \nCengage Learning Customer & Sales Support, 1-800-354-9706 \nFor permission to use material from this text or product, \nsubmit all requests on line at www-cengage.com/permissions­\nFurther permissions questions can be emailed to \npermissionrequest@cengage.com. \nLibrary of Congress Control Number: 2013944173 \nISBN-13: 978-1-285-46324-7 \nISBN-10: 1-285-46324-2 \nCengage Learning \n200 First Stamford Place, 4th Floor \nStamford, CT 06902 \nUSA \nCengage Learning is a leading provider of customized learning solutions","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":51,"to":70}}}}],[3,{"pageContent":"ISBN-13: 978-1-285-46324-7 \nISBN-10: 1-285-46324-2 \nCengage Learning \n200 First Stamford Place, 4th Floor \nStamford, CT 06902 \nUSA \nCengage Learning is a leading provider of customized learning solutions \nwith office locations around the globe, including Singapore, the United \nKingdom, Australia, Mexico, Brazil and japan. Locate your local office at \ninternational.cengage.com/region. \nCengage Learning products are represented in Canada by Nelson \nEducation, Ltd. \nFor your course and learning solutions, visit www.cengage.com. \nPurchase any of our products at your local college store or at our \npreferred on line store www.cengagebrain.com. \nInstructors: Please visit login.cengage.com and log in to access \ninstructor-specific resources.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":70,"to":86}}}}],[4,{"pageContent":"Dedicated to the memory of \nPeter Hilton, who was an \nexemplary mathematician, \neducator, and citizen-a unit \nvector in every sense.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":88,"to":92}}}}],[5,{"pageContent":"Chapter 1 \nChapter 2 \niv \nContents \nPreface \nvii \nTo the Instructor xvii \nTo the Student xxiii \nVectors \n1 \n1.0 \n1.1 \n1.2 \n1.3 \nIntroduction: The Racetrack Game \nThe Geometry and Algebra of Vectors \nLength and Angle: The Dot Product \nExploration: Vectors and Geometry \nLines and Planes 34 \nExploration: The Cross Product 48 \n3 \n18 \n32 \nWriting Project: The Origins of th e Dot Product and Cross Product \n1.4 \nApplications \n50 \nForce Vectors \nChapter Review \n50 \n55 \nSystems of Linear Equations \n57 \n2.0 \nIntroduction: Triviality \n57 \n2.1 \nIntroduction to Systems of Linear Equations 58 \n2.2 \nDirect Methods for Solving Linear Systems 64 \nWriting Project: A History of Gaussian Elimination 82 \nExplorations: Lies My Computer Told Me 83 \nPartial Pivoting \n84 \nCounting Operations: An Introduction to th e \nAnalysis of Algorithms 85 \n2.3 \nSpanning Sets and Linear Independence \n88 \n2.4 \nApplications \n99 \nAllocation of Resources 99 \nBalancing Chemical Equations \n101 \nNetwork Analysis \n102 \nElectrical Networks \n104 \nLinear Economic Models 107","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":94,"to":153}}}}],[6,{"pageContent":"2.3 \nSpanning Sets and Linear Independence \n88 \n2.4 \nApplications \n99 \nAllocation of Resources 99 \nBalancing Chemical Equations \n101 \nNetwork Analysis \n102 \nElectrical Networks \n104 \nLinear Economic Models 107 \nFinite Linear Games 109 \nVignette: The Global Positioning System \n121 \n2.5 \nIterative Methods for Solving Linear Systems \n124 \nChapter Review \n134 \n49","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":153,"to":175}}}}],[7,{"pageContent":"Chapter 3 \nChapter 4 \nChapter 5 \nContents \nV \nMatrices \n136 \n3.0 \n3.1 \n3.2 \n3.3 \n3.4 \n3.5 \n3.6 \n3.7 \nIntroduction: Matrices in Action \n136 \nMatrix Operations \n138 \nMatrix Algebra \n154 \nThe Inverse of a Matrix \n163 \nThe LU Factorization \n180 \nSubspaces, Basis, Dimension, and Rank \nIntroduction to Linear Transformations \nVignette: Robotics 226 \nApplications 230 \nMarkov Chains \n230 \nLinear Economic Models 235 \nPopulation Growth \n239 \nGraphs and Digraphs \n241 \nChapter Review \n251 \n191 \n211 \nEigenvalues and Eigenvectors \n253 \n4.0 \n4.1 \n4.2 \n4.3 \n4.4 \n4.5 \n4.6 \nIntroduction: A Dynamical System on Graphs    253 \nIntroduction to Eigenvalues and Eigenvectors \n254 \nDeterminants \n263 \nWriting Project: Which Came First:  The Matrix or th e Determinant? \nVignette: Lewis Carroll's Condensation Method 284 \nExploration: Geometric Applications of Determinants 286 \nEigenvalues and Eigenvectors of n X n Matrices 292 \nWriting Project: The History of Eigenvalues 301 \nSimilarity and Diagonalization \n301 \nIterative Methods for Computing Eigenvalues \n311","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":177,"to":239}}}}],[8,{"pageContent":"Eigenvalues and Eigenvectors of n X n Matrices 292 \nWriting Project: The History of Eigenvalues 301 \nSimilarity and Diagonalization \n301 \nIterative Methods for Computing Eigenvalues \n311 \nApplications and the Perron-Frobenius Theorem 325 \nMarkov Chains \n325 \nPopulation Growth \n330 \nThe Perron-Frobenius Theorem 332 \nLinear Recurrence Relations 335 \nSystems of Linear Differential Equations 340 \nDiscrete Linear Dynamical Systems 348 \nVignette: Ranking Sports Teams and Searching th e Internet \n356 \nChapter Review \n364 \nOrthogonality \n366 \n5.0 Introduction: Shadows on a Wall 366 \n5.1 Orthogonality in IR\n\" \n368 \n5.2 \nOrthogonal Complements and Orthogonal Projections \n378 \n5.3 \nThe Gram-Schmidt Process and the QR Factorization \n388 \nExplorations: The Modified QR Factorization 396 \nApproximating Eigenvalues with the QR Algorithm 398 \n5.4 \nOrthogonal Diagonalization of Symmetric Matrices 400 \n5.5 \nApplications \n408 \nQuadratic Forms \n408 \nGraphing Quadratic Equations \n415 \nChapter Review \n425 \n283","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":239,"to":283}}}}],[9,{"pageContent":"Vi \nContents \nChapter 6 \nChapter 7 \nChapter 8 \nVector Spaces \n427 \n6.0 \n6.1 \n6.2 \n6.3 \n6.4 \n6.5 \n6.6 \nIntroduction: Fibonacci in (Vector) Space \n427 \nVector Spaces and Subspaces 429 \nWriting Project: The Rise of Vector Spaces \n443 \nLinear Independence, Basis, and Dimension 443 \nExploration: Magic Squares \n460 \nChange of Basis 463 \nLinear Transformations \n472 \nThe Kernel and Range of a Linear Transformation \n481 \nThe Matrix of a Linear Transformation 497 \nExploration: Tilings, Lattices, and th e Crystallographic Restriction \n6.7 \nApplications \n518 \nHomogeneous Linear Differential Equations \n518 \nChapter Review \n527 \nDistance and Approximation \n529 \n7.0 Introduction: Taxicab Geometry \n529 \n7.1 \nInner Product Spaces \n531 \nExplorations: Vectors a nd Matrices with Complex Entries \n543 \nGeometric Inequalities and Optimization Problems \n547 \n7.2 \nNorms and Distance Functions \n552 \n7.3 \nLeast Squares Approximation \n568 \n7.4 The Singular Value Decomposition \n590 \nVignette: Digital Image Compression \n607 \n7.5 \nApplications 610","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":285,"to":343}}}}],[10,{"pageContent":"547 \n7.2 \nNorms and Distance Functions \n552 \n7.3 \nLeast Squares Approximation \n568 \n7.4 The Singular Value Decomposition \n590 \nVignette: Digital Image Compression \n607 \n7.5 \nApplications 610 \nApproximation of Functions 610 \nChapter Review \n618 \nCodes Online only \n620 \n8.1 \nCode Vectors \n620 \nVignette: The Codabar System \n626 \n8.2 \nError-Correcting Codes \n627 \n8.3 \nDual Codes \n632 \n8.4 \nLinear Codes \n639 \n8.5 \nThe Minimum Distance of a Code \n644 \nAPPENDIX A \nAPPENDIXB \nAPPENDIXC \nAPPENDIXD \nAPPENDIX£ \nMathematical Notation and Methods of Proof \nAl \nMathematical Induction B 1 \nComplex Numbers \nCl \nPolynomials \nD 1 \nTechnology Bytes \nOnline only \nAnswers to Selected Odd-Numbered Exercises \nANSI \nIndex \nII \n515","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":343,"to":396}}}}],[11,{"pageContent":"For more on the recommendations \nof the Linear Algebra Curriculum \nStudy Group, see 1he College \nMathematics Jo urnal 24 (1993), \n41-46. \nPreface \n1he  last  thing one knows when writing a \nbook is what to put first. \n- Blaise Pascal \nPensees, 1670 \nThe fourth edition of Linear Algebra: A Modern Introduction preserves the approach \nand features that users found to be strengths of the previous editions. However, I have \nstreamlined the text somewhat, added numerous clarifications, and freshened up the \nexercises. \nI want students to see linear algebra as an exciting subject and to appreciate its \ntremendous usefulness. At the same time, I want to help them master the basic con -\ncepts and techniques of linear algebra that they will need in other courses, both in \nmathematics and in other disciplines. I also want students to appreciate the interplay \nof theoretical, applied, and numerical mathematics that pervades the subject. \nThis book is designed for use in an introductory one- or two-semester course","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":398,"to":417}}}}],[12,{"pageContent":"of theoretical, applied, and numerical mathematics that pervades the subject. \nThis book is designed for use in an introductory one- or two-semester course \nsequence in linear algebra. First and foremost, it is intended for students, and I have \ntried my best to write the book so that students not only will find it readable but also \nwill want to read it. As in   the first three editions, I have taken into account the reality \nthat students taking introductory linear algebra are likely to come from a variety of \ndisciplines. In addition to  mathematics majors, there  are  apt  to  be majors from \nengineering, physics, chemistry,  computer science, biology, environmental science, \ngeography, economics, psychology, business, and education, as well as other students \ntaking the course as an elective or to fulfill degree requirements. Accordingly, the book \nbalances theory and applications, is written in a conversational style yet is fully rigorous,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":417,"to":427}}}}],[13,{"pageContent":"taking the course as an elective or to fulfill degree requirements. Accordingly, the book \nbalances theory and applications, is written in a conversational style yet is fully rigorous, \nand combines a traditional presentation with concern for student-centered learning. \nThere is no such thing as a universally best learning style. In any class, there will be \nsome students who work well independently and others who work best in groups; \nsome who prefer lecture-based learning and others who thrive in a workshop setting, \ndoing explorations; some who enjoy algebraic manipulations, some who are adept at \nnumerical calculations (with and without a computer), and some who exhibit strong \ngeometric intuition. In this edition, I continue to present material in a variety of \nw\na\nys-algebraically, geometrically, numerically, and verbally-so that all types oflearn­\ners can find a    path to follow. I have also attempted to present the theoretical, computa­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":427,"to":439}}}}],[14,{"pageContent":"w\na\nys-algebraically, geometrically, numerically, and verbally-so that all types oflearn­\ners can find a    path to follow. I have also attempted to present the theoretical, computa­\ntional, and applied topics in a flexible yet integrated way. In doing so, it is my hope that \nall students will be exposed to the many sides of linear algebra. \nThis book is compatible with the recommendations of the Linear Algebra Curriculum \nStudy Group. From a pedagogical point of view, there is no doubt that for most students \nVii","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":439,"to":447}}}}],[15,{"pageContent":"Viii \nPreface \nSee pages 49, 82, 283, 301, 443 \nconcrete examples should precede abstraction. I have taken this approach here. I also \nbelieve strongly that linear algebra is essentially about vectors and that students need to \nsee vectors first (in a  concrete setting) in order to gain some geometric insight. Moreover, \nintroducing vectors early allows students to see how systems of linear equations arise \nnaturally from geometric problems. Matrices then arise equally naturally as coefficient \nmatrices oflinear systems and as agents of change (linear transformations). This sets the \nstage for eigenvectors and orthogonal projections, both of which are best understood \ngeometrically. The dart that appears on the cover of this book symbolizes a vector and \nreflects my conviction that geometric understanding should precede computational \ntechniques. \nI have tried to limit the number of theorems in the text. For the most part, results","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":449,"to":462}}}}],[16,{"pageContent":"reflects my conviction that geometric understanding should precede computational \ntechniques. \nI have tried to limit the number of theorems in the text. For the most part, results \nlabeled as theorems either will be used later in the text or summarize preceding work. \nInteresting results that are not central to the book have been included as exercises or \nexplorations. For example, the cross product of vectors is discussed only in explo­\nrations (in Chapters 1 and 4). Unlike most linear algebra textbooks, this book has no \nchapter on determinants.  The essential results are all in Section 4.2, with other inter­\nesting material contained in an exploration. The book is, however, comprehensive for \nan introductory text. Wherever possible, I have included elementary and accessible \nproofs of theorems in order to avoid having to say, \"The proof of this result is beyond \nthe scope of this text:' The result is, I hope, a work that is self-contained.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":462,"to":473}}}}],[17,{"pageContent":"proofs of theorems in order to avoid having to say, \"The proof of this result is beyond \nthe scope of this text:' The result is, I hope, a work that is self-contained. \nI have not been stingy with the applications: There are many more in the book than \ncan be covered in a single course. However, it is important that students see the impressive \nrange of problems to which linear algebra can be applied. I have included some modern \nmaterial on finite linear algebra and coding theory that is not normally found in an  intro­\nductory linear algebra text. There are also several impressive real-world applications of \nlinear algebra and one item of historical, if not practical, interest; these applications are \npresented as self-contained \"vignettes:' \nI hope that instructors will enjoy teaching from this book. More important, I hope \nthat students using the book will come away with an appreciation of the beauty, power,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":473,"to":483}}}}],[18,{"pageContent":"I hope that instructors will enjoy teaching from this book. More important, I hope \nthat students using the book will come away with an appreciation of the beauty, power, \nand tremendous utility of linear algebra and that they will have fun along the way. \nWhat's  New in the Fourth Edition \nThe overall structure and style of Linear Algebra: A Modern Introduction remain the \nsame in the fourth edition. \nHere is a summary of what is new: \n• \nThe applications to coding theory have been moved to the new online Chapter 8. \n• \nTo further engage students, five writing projects have been added to the exer­\ncise sets. These projects give students a chance to research and write about aspects of \nthe history and development oflinear algebra. The explorations, vignettes, and many \nof the applications provide additional material for student projects. \n• \nThere are over 200 new or revised exercises. In response to reviewers' com­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":483,"to":498}}}}],[19,{"pageContent":"of the applications provide additional material for student projects. \n• \nThere are over 200 new or revised exercises. In response to reviewers' com­\nments, there is now a full proof of the Cauchy-Schwarz Inequality in Chapter 1 in the \nform of a guided exercise. \n• \nI have made numerous small changes in wording to improve the clarity or \naccuracy of the exposition. Also, several definitions have been made more explicit by \ngiving them their own definition boxes and a few results have been highlighted by \nlabeling them as theorems. \n• \nAll existing ancillaries have been updated.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":498,"to":509}}}}],[20,{"pageContent":"Features \nClear Writing Stvle \nPreface \nix \nThe text is written is a simple, direct, conversational style. As much as possible, I have \nused \"mathematical English\" rather tha  n relying excessively on mathematical nota­\ntion. However, all proofs that are given are fully rigorous, and Appendix A  contains \nan introduction to mathematical notation for those who wish to streamline their own \nwriting. Concrete examples almost always precede theorems, which are then followed \nby further examples and applications. This flow-from specific to general and back \nagain-is consistent throughout the book. \nKev concepts Introduced Early \nMany students encounter difficulty in linear algebra when the course moves from the \ncomputational (solving systems of linear equations, manipulating vectors and matri­\nces) to the theoretical (spanning sets, linear independence, subspaces, basis, and \ndimension). This book introduces all of the key concepts of linear algebra early, in a","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":511,"to":526}}}}],[21,{"pageContent":"ces) to the theoretical (spanning sets, linear independence, subspaces, basis, and \ndimension). This book introduces all of the key concepts of linear algebra early, in a \nconcrete setting, before revisiting them in full generality. Vector concepts such as dot \nproduct, length, orthogonality, and projection are first discussed in Chapter 1 in the \nconcrete setting of IR\n2 \nand IR\n3 \nbefore the more general notions of inner product, norm, \nand orthogonal projection appear in Chapters 5 and 7. Similarly, spanning sets and \nlinear independence are given a concrete treatment in Chapter 2 prior to their gener­\nalization to vector spaces in Chapter 6. The fundamental concepts of subspace, basis, \nand dimension appear first in Chapter 3 when the row, column, and null spaces of a \nmatrix are introduced; it is not until Chapter 6 that these ideas are given a general \ntreatment. In Chapter 4, eigenvalues and eigenvectors are introduced and explored","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":526,"to":540}}}}],[22,{"pageContent":"matrix are introduced; it is not until Chapter 6 that these ideas are given a general \ntreatment. In Chapter 4, eigenvalues and eigenvectors are introduced and explored \nfor 2 X 2 matrices before their n X n counterparts appear. By the beginning of Chap­\nter 4, all of the key concepts of linear algebra have been introduced, with concrete, \ncomputational examples to support them. When these ideas appear in full generality \nlater in the book, students have had time to get used to them and, hence, are not so \nintimidated by them. \nEmphasis on Vectors and Geometry \nIn keeping with the philosophy that linear algebra is primarily about vectors, this \nbook stresses geometric intuition. Accordingly, the first chapter is about vectors, and \nit develops many concepts that will appear repeatedly throughout the text. Concepts \nsuch as orthogonality, projection, and linear combination are all found in Chapter 1, \nas is   a comprehensive treatment of lines and planes in IR\n3 \nthat provides essential","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":540,"to":554}}}}],[23,{"pageContent":"such as orthogonality, projection, and linear combination are all found in Chapter 1, \nas is   a comprehensive treatment of lines and planes in IR\n3 \nthat provides essential \ninsight into the solution of systems of linear equations. This emphasis on vectors, \ngeometry, and visualization is found throughout the text. Linear transformations are \nintroduced as matr  ix transformations in Chapter 3, with many geometric examples, \nbefore general linear transformations are covered in Chapter 6. In Chapter 4, eigen­\nvalues are introduced with \"eigenpictures\" as a visual aid. The proof of Perron's \nTheorem is given first heuristically and then formally, in both cases using a geometric \nargument. The geometry of linear dynamical systems reinforces and summarizes the \nmaterial on eigenvalues and eigenvectors. In Chapter 5, orthogonal projections, or­\nthogonal complements of subspaces, and the Gram-Schmidt Process are all presented \nin the concrete setting of IR\n3 \nbefore being generalized to IR\n\"","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":554,"to":570}}}}],[24,{"pageContent":"thogonal complements of subspaces, and the Gram-Schmidt Process are all presented \nin the concrete setting of IR\n3 \nbefore being generalized to IR\n\" \nand, in Chapter 7, to inner","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":570,"to":575}}}}],[25,{"pageContent":"X \nPreface \nSee pages 1, 136, 427, 529 \nSee pages 32, 286, 460, 515, 543, 547 \nSee pages 83, 84, 85, 396, 398 \nSee pages 623, 641 \nSee pages 121, 226, 356, 607, 626 \nSee pages 248, 359, 526, 588 \nproduct spaces. The nature of the singular value decomposition is also explained in­\nformally in Chapter 7 via a geometric argument. Of the more than 300 figures in the \ntext, over 200 are devoted to fostering a geometric understanding of linear algebra. \nExploralions \nThe introduction to each chapter is a guided exploration (Section O) in which stu­\ndents are invited to discover, individually or in groups, some aspect of the upcoming \nchapter. For example, \"The Racetrack Game\" introduces vectors, \"Matrices in Action\" \nintroduces matrix multiplication and linear transformations, \"Fibonacci in (Vector) \nSpace\" touches on vector space concepts, and \"Taxicab Geometry\" sets up general­\nized norms and distance functions. Additional explorations found throughout the","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":577,"to":594}}}}],[26,{"pageContent":"Space\" touches on vector space concepts, and \"Taxicab Geometry\" sets up general­\nized norms and distance functions. Additional explorations found throughout the \nbook include applications of vectors and determinants to geometry, an investigation \nof 3 X 3 magic squares, a study of symmetry via the tilings of M. C. Escher, an intro­\nduction to  complex linear algebra, and optimization  problems using geometric \ninequalities. There are also explorations that introduce important numerical consid­\nerations and the analysis of algorithms. Having students do some of these explo­\nrations is one way of encouraging them to become active learners and to give them \n\"ownership\" over a small part of the course. \nAPPlicalions \nThe book contains an abundant selection of applications chosen from a broad range \nof disciplines, including mathematics, computer science, physics, chemistry, engi­\nneering, biology, business, economics, psychology, geography, and sociology. Note­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":594,"to":606}}}}],[27,{"pageContent":"of disciplines, including mathematics, computer science, physics, chemistry, engi­\nneering, biology, business, economics, psychology, geography, and sociology. Note­\nworthy among these is a strong treatment of coding theory, from error-detecting \ncodes  (such  as  International Standard Book Numbers)  to  sophisticated error­\ncorrecting codes (such as the Reed-Muller code that was used to transmit satellite \nphotos from space). Additionally, there are five \"vignettes\" that briefly showcase some \nvery modern applications oflinear algebra: the Global Positioning System (GPS), ro­\nbotics, Internet search engines, digital image compression, and the Codabar System. \nExamples and Exercises \nThere are over 400 examples in this book, most worked in greater detail than is cus­\ntomary in   an introductory linear algebra textbook. This level of detail is in  keeping \nwith the philosophy that students should want (and be able) to read a textbook.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":606,"to":617}}}}],[28,{"pageContent":"tomary in   an introductory linear algebra textbook. This level of detail is in  keeping \nwith the philosophy that students should want (and be able) to read a textbook. \nAccordingly, it is not intended that all of these examples be covered in class; many can \nbe assigned for individual or group study, possibly as part of a project. Most examples \nhave at least one counterpart exercise so that students can try out the skills covered in \nthe example before exploring generalizations. \nThere are over 2000 exercises, more than in most textbooks at a similar level. \nAnswers to most of the computational odd-numbered exercises can be found in the \nback of the book. Instructors will find an abundance of exercises from which to select \nhomework assignments. The exercises in each section are graduated, progressing from \nthe routine to the challenging. Exercises range from those intended for hand computa­\ntion to those requiring the use of a calculator or computer algebra system, and from","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":617,"to":628}}}}],[29,{"pageContent":"the routine to the challenging. Exercises range from those intended for hand computa­\ntion to those requiring the use of a calculator or computer algebra system, and from \ntheoretical and numerical exercises to conceptual exercises. Many of the examples and \nexercises use actual data compiled from real-world situations. For example, there are \nproblems on modeling the growth of caribou and seal populations, radiocarbon dating","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":628,"to":632}}}}],[30,{"pageContent":"See page 34 \nPreface \nXi \nof the Stonehenge monument, and predicting major league baseball players' salaries. \nWorking such problems reinforces the fact that linear algebra is a valuable tool for mod­\neling real-life problems. \nAdditional exercises appear in the form of a review after each chapter. In each set, \nthere are 10 true/false questions designed to test conceptual understanding, followed \nby 19 computational and theoretical exercises that summarize the main concepts and \ntechniques of that chapter. \nBiographical Sketches and Etvmological Notes \nIt is important that students learn something about the history of mathematics and \ncome to see it as a social and cultural endeavor as well as a scientific one. Accord­\ningly, the text contains short biographical sketches about many of the mathemati­\ncians who contributed to the development of linear algebra. I hope that these will \nhelp to put a human face on the subject and give students another way of relating to \nthe material.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":634,"to":650}}}}],[31,{"pageContent":"cians who contributed to the development of linear algebra. I hope that these will \nhelp to put a human face on the subject and give students another way of relating to \nthe material. \nI have found that many students feel alienated from mathematics because the \nterminology makes no sense to them-it is simply a collection of words to be learned. \nTo  help overcome this problem, I have included short etymological notes that give \nthe origins of many of the terms used in linear algebra. (For example, why do we \nuse the word normal to refer to a vector that is perpendicular to a plane?) \nMargin Icons \nThe margins of the book contain several icons whose purpose is to alert the reader in \nvarious ways. Calculus is not a prerequisite for this book, but linear algebra has many \ninteresting and important applications to calculus. The\n� \nicon denotes an example or \nexercise that requires calculus. (This material can be omitted if not everyone in the","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":650,"to":664}}}}],[32,{"pageContent":"interesting and important applications to calculus. The\n� \nicon denotes an example or \nexercise that requires calculus. (This material can be omitted if not everyone in the \nclass has had at least one semester of calculus. Alternatively, this material can be as­\nsigned as projects.) The\n� \nicon denotes an example or exercise involving complex \nnumbers. (For students unfamiliar with complex numbers, Appendix C contains all \nthe background material that is needed.) The \ncAs \nicon indicates that a computer algebra \nsystem (such as Maple, Mathematica, or MATLAB) or a calculator with matrix capa­\nbilities (such as almost any graphing calculator) is required-or at least very useful­\nfor solving the example or exercise. \nIn an effort to he  lp students learn how to read and use this textbook most ef­\nfectively, I have noted various places where the reader is advised to pause. These \nmay be places where a calculation is needed, part of a proof must be supplied, a","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":664,"to":681}}}}],[33,{"pageContent":"fectively, I have noted various places where the reader is advised to pause. These \nmay be places where a calculation is needed, part of a proof must be supplied, a \nclaim should be ve  rified, or some extra thought is required. The _.... icon appears \nin the margin at such places; the message is \"Slow down.  Get out your pencil. \nThink about this:' \nTechnology \nThis book can be used successfully whether or not students have access to technol­\nogy. However, calculators with matrix capabilities and computer algebra systems \nare now commonplace and, properly used, can enrich the learning experience as \nwell as help with tedious calculations. In this text, I take the point of view that stu­\ndents need to master all of the basic techniques of linear algebra by solving by hand \nexamples that are not too computationally difficult. Technology may then be used","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":681,"to":692}}}}],[34,{"pageContent":"Xii \nPreface \nSee pages 83, 84, 124, 180, 311, 392, \n555, 561, 568, 590 \nSee pages 319, 563, 600 \n(in whole or in part) to solve subsequent examples and applications and to apply \ntechniques that rely on earlier ones. For example, when systems of linear equa tions \nare first introduced, detailed solutions are provided; later,  solutions are simply \ngiven, and the reader is expected to verify them. This is a good place to use some \nform of technology. Likewise, when applications use data that make hand calcula­\ntion impractical, use technology. All of the numerical methods that are discussed \ndepend on the use of technology. \nWith the aid of technology, students can explore linear algebra in some exciting \nways and discover much for themselves. For example, if one of the coefficients of a \nlinear system is replaced by a   parameter, how much variability is there in the solu­\ntions? How does changing a single entry of a matrix affect its eigenvalues? This book","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":694,"to":709}}}}],[35,{"pageContent":"linear system is replaced by a   parameter, how much variability is there in the solu­\ntions? How does changing a single entry of a matrix affect its eigenvalues? This book \nis not a tutorial on technology, and in places where technology can be used, I have not \nspecified a  particular type  of technology. The  student companion website  that \naccompanies this book offers an online appendix called Technology Bytes that gives \ninstructions for solving a selection of examples from each chapter using Maple, Math­\nematica, and MATLAB. By im  itating these examples, students can do further calcula­\ntions and explorations using whichever CAS they have and exploit the power of these \nsystems to help with the exercises throughout the book, particularly those marked \nwith the \ncAs \nicon. The website also contains data sets and computer code in Maple, \nMathematica, and MATLAB formats keyed to many exercises and examples in the","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":709,"to":721}}}}],[36,{"pageContent":"with the \ncAs \nicon. The website also contains data sets and computer code in Maple, \nMathematica, and MATLAB formats keyed to many exercises and examples in the \ntext. Students and instructors can import these directly into their CAS to save typing \nand eliminate errors. \nFinite and Numerical linear Algebra \nThe text covers two aspects of linear algebra that are scarcely ever mentioned to­\ngether: finite linear algebra and numerical linear algebra. By introducing modular \narithmetic early, I have been able to make finite linear algebra (more properly, \"linear \nalgebra over finite fields;' although I do not use that phrase) a recurring theme \nthroughout the book. This approach provides access to the material on coding theory \nin Chapter 8 (online). There is also an application to finite linear games in Section 2.4 \nthat students really enjoy. In addition to being exposed to the applications of finite \nlinear algebra, mathematics majors will benefit from seeing the material on finite","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":721,"to":735}}}}],[37,{"pageContent":"that students really enjoy. In addition to being exposed to the applications of finite \nlinear algebra, mathematics majors will benefit from seeing the material on finite \nfields, because they are likely to encounter it in such courses as discrete mathematics, \nabstract algebra, and number theory. \nAll students should be aware that in practice, it is impossible to arrive at exact \nsolutions oflarge-scale problems in linear algebra. Exposure to some of the tech­\nniques of numerical linear algebra will provide an indication of how to obtain \nhighly accurate approximate solutions. Some of the numerical topics included in \nthe book are roundoff error and partial pivoting, iterative methods for solving \nlinear systems and computing eigenvalues, the LU and QR factorizations, matrix \nnorms and  condition numbers, least squares approximation, and  the singular \nvalue decomposition. The inclusion of numerical linear algebra also brings up","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":735,"to":746}}}}],[38,{"pageContent":"norms and  condition numbers, least squares approximation, and  the singular \nvalue decomposition. The inclusion of numerical linear algebra also brings up \nsome interesting and important issues that are completely absent from the theory \nof linear algebra, such as pivoting strategies, the condition of a linear system, and \nthe convergence of iterative methods. This book not only raises these questions \nbut also shows how one might approach them. Gerschgorin disks, matrix norms, \nand the singular values of a matrix, discussed in Chapters 4 and 7, are useful in \nthis regard.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":746,"to":753}}}}],[39,{"pageContent":"Appendices \nPreface \nXiii \nAppendix A contains an overview of mathematical notation and methods of proof, \nand Appendix B discusses mathematical induction. All students will benefit from \nthese sections, but those with a mathematically oriented major may wish to pay \nparticular attention to them. Some of the examples in these appendices are uncom­\nmon (for instance, Example B.6 in Appendix B) and underscore the power of the \nmethods. Appendix C is an introduction to complex numbers. For students familiar \nwith these results, this appendix can serve as a useful reference; for others, this sec­\ntion contains everything they need to know for those parts of the text that use com­\nplex numbers. Appendix D  is about polynomials. I have found that many students \nrequire a refresher about these facts. Most students will be unfamiliar with Descartes's \nRule of Signs; it is used in Chapter 4 to explain the behavior of the eigenvalues of","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":755,"to":768}}}}],[40,{"pageContent":"require a refresher about these facts. Most students will be unfamiliar with Descartes's \nRule of Signs; it is used in Chapter 4 to explain the behavior of the eigenvalues of \nLeslie matrices. Exercises to accompany the four appendices can be found on the \nbook's website. \nShort answers to most of the odd-numbered computational exercises are given at \nthe end of the book. Exercise sets to accompany Appendixes A, B, C, and D are avail­\nable on the companion website, along with their odd-numbered answers. \nAncillaries \nFor 1ns1ruc1ors \nEnhanced Web Assign® �ebAssign \nPrinted Access Card: 978-1-285-85829-6 \nOnline Access Code: 978-1-285-85827-2 \nExclusively from Cengage Learning, Enhanced WebAssign combines the exceptional \nmathematics content that you know and love with the most powerful online home­\nwork solution, WebAssign. Enhanced WebAssign engages students with immediate \nfeedback, rich tutorial content, and interactive, fully customizable eBooks (YouBook),","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":768,"to":783}}}}],[41,{"pageContent":"work solution, WebAssign. Enhanced WebAssign engages students with immediate \nfeedback, rich tutorial content, and interactive, fully customizable eBooks (YouBook), \nhelping students to develop a  deeper conceptual understanding of their subject \nmatter. Flexible assignment options give instructors the ability to release assignments \nconditionally based on students' prerequisite assignment scores. Visit us at www. \ncen\ng\na\ng\ne.com/ewa to learn more. \nCengage Learning Testing Powered by Cognero \nCengage Learning Testing Powered by Cognero is a flexible, online system that allows \nyou to author, edit, and manage test bank content from multiple Cengage Learning \nsolutions; create multiple test versions in an instant; and deliver tests from your LMS, \nyour classroom, or wherever you want. \nComplete Solutions Manual \nThe Complete Solutions Manual provides detailed solutions to all exercises in the \ntext, including Exploration and Chapter Review exercises. The Complete Solutions \nManual is available online.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":783,"to":801}}}}],[42,{"pageContent":"The Complete Solutions Manual provides detailed solutions to all exercises in the \ntext, including Exploration and Chapter Review exercises. The Complete Solutions \nManual is available online. \nInstructor's Guide \nThis online guide enhances the text with valuable teaching resources such as group \nwork projects,  teaching tips, interesting exam questions, examples and  extra","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":801,"to":806}}}}],[43,{"pageContent":"xiv \nPreface \nmaterial for lectures, and other items designed to reduce the instructor's prepara­\ntion time and make linear algebra class an exciting and interactive experience. For \neach section of the text, the Instructor's Guide includes suggested time and empha­\nsis, points to stress, questions for discussion, lecture materials and examples, tech­\nnology tips, student projects, group work with solutions, sample assignments, and \nsuggested test questions. \nSolution Builder \nwww.cengage.com/solutionbuilder \nSolution Builder provides full instructor solutions to all exercises in the text, includ­\ning those in the explorations and chapter reviews, in a convenient online format. \nSolution Builder allows instructors to create customized, secure PDF printouts of \nsolutions matched exactly to the exercises assigned for class. \n*Access Cognero and additional instructor resources online at lo\ng\nin.cen\ng\na\ng\ne.com. \nFor Students \nStudent Solutions Manual (ISBN-13: 978-1-285-84195-3)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":808,"to":830}}}}],[44,{"pageContent":"*Access Cognero and additional instructor resources online at lo\ng\nin.cen\ng\na\ng\ne.com. \nFor Students \nStudent Solutions Manual (ISBN-13: 978-1-285-84195-3) \nThe Student Solutions Manual and Study Guide includes detailed solutions to all odd­\nnumbered exercises and selected even-numbered exercises; section and chapter \nsummaries of symbols, definitions, and theorems; and study tips and hints. Complex \nexercises are explored through a question-and-answer format designed to deepen \nunderstanding. Challenging and entertaining problems that further explore selected \nexercises are also included. \n� \nEnhanced Web Assign® WebAssign \nPrinted Access Card: 978-1-285-85829-6 \nOnline Access Code: 978-1-285-85827-2 \nEnhanced Web Assign (assigned by the instructor) provides you with instant feedback \non homework assignments. This online homework system is easy to use and includes \nhelpful links to textbook sections, video examples, and problem-specific tutorials. \nCengageBrain.com","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":830,"to":852}}}}],[45,{"pageContent":"on homework assignments. This online homework system is easy to use and includes \nhelpful links to textbook sections, video examples, and problem-specific tutorials. \nCengageBrain.com \nTo  access additional course materials and companion resources, please visit www. \ncengagebrain.com. At the CengageBrain.com home page, search for the ISBN of your \ntitle (from the back cover of your book) using the search box at  the top of the page. \nThis will take you to the product page where free companion resources can be found. \nAcknowledgments \nThe reviewers of the previous edition of this text contributed valuable and often in­\nsightful comments about the book. I am grateful for the time each of them took to do \nthis. Their judgement and helpful suggestions have contributed greatly to the devel­\nopment and  success of this book, and I would like to thank them personally: \nJamey Bass, City College of San Francisco; Olga Brezhneva, Miami University; Karen","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":852,"to":864}}}}],[46,{"pageContent":"opment and  success of this book, and I would like to thank them personally: \nJamey Bass, City College of San Francisco; Olga Brezhneva, Miami University; Karen \nClark, The College of New Jersey; Marek Elzanowski, Portland State University; \nChristopher Francisco, Oklahoma State University;  Brian Jue,  California State \nUniversity,  Stanislaus; Alexander  Kheyfits,  Bronx Community  College/CUNY; \nHenry Krieger,  Harvey  Mudd College; Rosanna Pearlstein, Michigan State","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":864,"to":869}}}}],[47,{"pageContent":"See page 284 \nSee page 286 \nSee pages 325, 330 \nSee page 348 \nSee page 366 \nSee pages 396, 398 \nSee pages 408, 415 \nSee page 427 \nTo the Instructor \nxix \ncourse\" in determinants contains all the essential material students need, including \nan optional but elementary proof of the Laplace Expansion Theorem. The vignette \n\"Lewis Carroll's Condensation Method\" presents a historically interesting, alternative \nmethod of calculating determinants that students may find appealing. The explo­\nration \"Geometric Applications of Determinants\" makes a nice project that contains \nseveral interesting and useful results. (Alternatively,  instructors who wish to give \nmore detailed coverage to determinants may choose to cover some of this exploration \nin class.) The basic theory of eigenvalues and eigenvectors is found in Section 4.3, and \nSection 4.4 deals with the important topic of diagonalization. Example 4.29 on powers \nof matrices is worth covering in class. The power method and its variants, discussed","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":871,"to":890}}}}],[48,{"pageContent":"Section 4.4 deals with the important topic of diagonalization. Example 4.29 on powers \nof matrices is worth covering in class. The power method and its variants, discussed \nin Section 4.5, are optional, but all students should be aware of the method, and an \napplied course should cover it in detail. Gerschgorin's Disk Theorem can be covered \nindependently of the rest of Section 4.5. Markov chains and the Leslie model of pop­\nulation growth reappear in Section 4.6. Although the proof of Perron's Theorem is \noptional, the theorem itself (like the stronger Perron-Frobenius Theorem) should at \nleast be mentioned because it explains why we should expect a unique positive eigen­\nvalue with a corresponding positive eigenvector in these applications. The applica­\ntions on recurrence relations and differential equations connect linear algebra to dis­\ncrete mathematics and calculus, respectively. The matrix exp onential can be covered","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":890,"to":900}}}}],[49,{"pageContent":"tions on recurrence relations and differential equations connect linear algebra to dis­\ncrete mathematics and calculus, respectively. The matrix exp onential can be covered \nif your class has a good calculus background. The final topic of discrete linear dynam -\nical systems revisits and summarizes many of the ideas in Chapter 4, looking at them \nin a new, geometric light. Students will enjoy reading how eigenvectors can be used to \nhelp rank sports teams and websites. This vignette can easily be extended to a project \nor enrichment activity. \nChapter 5: onhouonalilv \nThe introductory exploration, \"Shadows on a Wall;' is mathematics at its best: it takes \na known concept (projection of a vector onto another vector) and generalizes it in a \nuseful way (projection of a vector onto a subspace-a plane), while uncovering some \npreviously unobserved properties. Se  ction 5.1 contains the basic results about or­\nthogonal and orthonormal sets of vectors that will be used repeatedly from here on.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":900,"to":912}}}}],[50,{"pageContent":"previously unobserved properties. Se  ction 5.1 contains the basic results about or­\nthogonal and orthonormal sets of vectors that will be used repeatedly from here on. \nIn particular, orthogonal matrices should be stressed. In Section 5.2, two concepts \nfrom Chapter 1 are generalized: the orthogonal complement of a subspace and the \northogonal projection of a vector onto a subspace. The Orthogonal Decomposition \nTheorem is important here and helps to set up the Gram-Schmidt Process. Also note \nthe quick proof of the Rank Theorem. The Gram-Schmidt Process is detailed in \nSection 5.3, along with the extremely important QR factorization. The two explo­\nrations that follow outline how the QR factorization is computed in practice and how \nit can be used to approximate eigenvalues. Section 5.4 on orthogonal diagonalization \nof (real) symmetric matrices is needed for the applications that follow. It also contains \nthe Spectral Theorem, one of the highlights of the theory oflinear algebra. The appli­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":912,"to":923}}}}],[51,{"pageContent":"of (real) symmetric matrices is needed for the applications that follow. It also contains \nthe Spectral Theorem, one of the highlights of the theory oflinear algebra. The appli­\ncations in Section 5.5 are quadratic forms and graphing quadratic equations. I  always \ninclude at least the second of these in my course because it  extends what students al­\nready know about conic sections. \nChapter 6: vector Spaces \nThe Fibonacci sequence reappears in Section 6.0, although it is not important that \nstudents have seen it before (Section 4.6). The purpose of this exploration is to show","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":923,"to":930}}}}],[52,{"pageContent":"Preface \nXV \nUniversity; William Sullivan, Portland State University; Matthias Weber, Indiana \nUniversity. \nI am indebted to a great many people who have, over the years, influenced my \nviews about linear algebra and the teaching of mathematics in general. First, I would \nlike to thank collectively the participants in the education and special linear algebra \nsessions at meetings of the Mathematical Association of America and the Canadian \nMathematical Society. I have also learned much from participation in the Canadian \nMathematics Education Study Group  and the Canadian Mathematics Education \nForum. \nI especially want to thank Ed Barbeau, Bill Higginson, Richard Hoshino, John \nGrant McLaughlin, Eric Muller, Morris Orzech, Bill Ral ph, Pat Rogers, Peter Taylor, \nand  Walter Whiteley,  whose advice  and  inspiration  contributed greatly  to  the \nphilosophy and style of this book. My gratitude as well to Robert Rogers, who devel­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":932,"to":946}}}}],[53,{"pageContent":"and  Walter Whiteley,  whose advice  and  inspiration  contributed greatly  to  the \nphilosophy and style of this book. My gratitude as well to Robert Rogers, who devel­\noped the student and instructor solutions, as well as the excellent study guide content. \nSpecial thanks go to   Jim Stewart for his ongoing support and advice. Joe Rotman and \nhis lovely book A First Course in Abstract Algebra inspired the etymological notes in \nthis book, and I relied heavily on Steven Schwartzman's The Words of Mathematics \nwhen compiling these notes. I  thank Art Benjamin for introducing me to the Codabar \nsystem and Joe Grear for clarifying aspects of the history of Gaussian elimination. My \ncolleagues Marcus Pivato and Reem Yassawi provided useful information about dy­\nnamical systems. As always, I am grateful to my students for asking good questions \nand providing me with the feedback necessary to becoming a better teacher. \nI sincerely thank all of the people who have been involved in the production of","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":946,"to":957}}}}],[54,{"pageContent":"and providing me with the feedback necessary to becoming a better teacher. \nI sincerely thank all of the people who have been involved in the production of \nthis book. Jitendra Kumar and the team at MPS Limited did an amazing job produc­\ning the fourth edition. I thank Christine Sabooni for doing a thorough copyedit. Most \nof all, it has been a delight to work with the entire editorial, marketing, and produc­\ntion teams at Cengage Learning: Richard Stratton, Molly Taylor, Laura Wheel, Cynthia \nAshton, Danielle Hallock, Andrew Coppola, Alison Eigel Zade, and Janay Pryor. They \noffered sound advice about changes and additions, provided assistance when I needed \nit, but let me write the book I  wanted to write. I am fortunate to have worked with \nthem, as well as the staffs on the first through third editions. \nAs always, I thank my family for their love, support, and understanding. Without \nthem, this book would not have been possible. \nDavid Poole \ndpoole@trentu.ca","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":957,"to":970}}}}],[55,{"pageContent":"See page 1 \nSee page 32 \nSee page 48 \nSee page 57 \nTo the \nInstructor \n\"Would you tell me, please, \nwhich way I ought to go from here?\" \n\"That depends a good deal on where \nyou want to get to,\" said th e Cat. \n- Lewis Carroll \nAlice's Adventures in \nWonderland, 1865 \nThis text was written with flexibility in mind. It is in  tended for use in a one- or \ntwo-semester course with 36 lectures per semester. The range of topics and applica­\ntions makes it suitable for a variety of audiences and types of courses. However, \nthere is more material in the book than can be covered in cla  ss, even in a two­\nsemester course. After the following overview of the text are some brief suggestions \nfor ways to use the book. \nAn overview of  the Text \nChanler 1: vec1ors \nThe racetrack game in Section 1.0 serves to introduce vectors in an  informal way. (It's also \nquite a lot of fun to play!) Vectors are then formally introduced from both algebraic and","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":974,"to":996}}}}],[56,{"pageContent":"The racetrack game in Section 1.0 serves to introduce vectors in an  informal way. (It's also \nquite a lot of fun to play!) Vectors are then formally introduced from both algebraic and \ngeometric points of view. The operations of addition and scalar multiplication and their \nproperties are first developed in the concrete settings of!R\n2 \nand IR3 before being general­\nized to !R\nn\n. Modular arithmetic and finite linear algebra are also introduced. Section 1.2 \ndefines the dot product of vectors and the related notions oflength, angle, and orthogo­\nnality. The very important concept of (orthogonal) projection is developed here; it will \nreappear in Chapters 5 and 7. The exploration \"Vectors and Geometry\" shows how vec­\ntor methods can be used to prove certain results in Euclidean geometry. Section 1.3 is a \nbasic but thorough introduction to lines and planes in IR\n2 \nand IR3. This section is crucial \nfor understanding the geometric significance of the solution of linear systems in Chap­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":996,"to":1012}}}}],[57,{"pageContent":"basic but thorough introduction to lines and planes in IR\n2 \nand IR3. This section is crucial \nfor understanding the geometric significance of the solution of linear systems in Chap­\nter 2. Note that the cross product of vectors in IR3 is left as an  exploration. The chapter \nconcludes with an application to force vectors. \nChapter 2: svstems of linear Equations \nThe introduction to this chapter ser  ves to illustrate that there is more than one way to \nthink of the solution to a system oflinear equations. Sections 2.1 and 2.2 develop the \nxvii","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1012,"to":1021}}}}],[58,{"pageContent":"xviii \nTo the Instructor \nSee pages 72, 205, 386, 486 \nSee page 121 \nSee pages 83, 84, 85 \nSee page 136 \nSee pages 172, 206, 296, 512, 605 \nSee page 226 \nSee pages 230, 239 \nSee page 253 \nmain computational tool for solving linear systems: row reduction of matrices (Gaus­\nsian and Gauss-Jordan elimination). Nearly all subsequent computational methods in \nthe book depend on this. The Rank Theorem appears here for the first time; it shows \nup again, in more generality, in Chapters 3, 5, and 6. Se  ction 2.3 is very important; it \nintroduces the fundamental notions of spanning sets and linear independence of vec­\ntors. Do not rush through this material. Section 2.4 contains six applications from \nwhich instructors can choose depending on the time available and the interests of the \nclass. The vignette on the Global Positioning System provides another application \nthat students will enjoy. The iterative methods in Section 2.5 will be optional for many","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1023,"to":1041}}}}],[59,{"pageContent":"class. The vignette on the Global Positioning System provides another application \nthat students will enjoy. The iterative methods in Section 2.5 will be optional for many \ncourses but are essential for a course with an applied/numerical focus. The three ex­\nplorations in this chapter are related in that they all deal with aspects of the use of \ncomputers to solve linear systems. All students should at least be made aware of these \nissues. \nChanler 3: Malrices \nThis chapter contains some of the most important ideas in the book. It is a long \nchapter, but the early material can be covered fairly quickly, with extra time allowed \nfor the crucial material in Section 3.5. Section 3.0 is an ex  ploration that introduces \nthe notion of a linear transformation: the idea that matrices are not just static objects \nbut rather a type of function, transforming vectors into other vectors. All of the basic \nfacts about matrices, matrix operations, and their properties are found in the first two","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1041,"to":1053}}}}],[60,{"pageContent":"but rather a type of function, transforming vectors into other vectors. All of the basic \nfacts about matrices, matrix operations, and their properties are found in the first two \nsections. The material on partitioned matrices and the multiple representations of the \nmatrix prod uct is worth stressing, because it is used repeatedly in subsequent sections. \nThe Fundamental Theorem of Invertible Matrices in Section 3.3 is very important \nand will appear several more times as new characterizations of invertibility are pre­\nsented. Section 3.4 discusses the very important LU factorization of a matrix. If this \ntopic is not covered in class, it is worth assigning as a project or discussing in a work­\nshop. The point of Section 3.5 is to present many of the key concepts of linear algebra \n(subspace, basis, dimension, and rank) in the concrete setting of matrices before stu -\ndents see them in full generality. Although the examples in this section are all famil­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1053,"to":1063}}}}],[61,{"pageContent":"(subspace, basis, dimension, and rank) in the concrete setting of matrices before stu -\ndents see them in full generality. Although the examples in this section are all famil­\niar, it is important that students get used to the new terminology and, in particular, \nunderstand what the notion of a basis means. The geometric treatment of linear \ntransformations in Section 3.6 is intended to smooth the transition to general linear \ntransformations in Chapter 6. The example of a projection is particularly important \nbecause it will reappear in Chapter 5. The vignette on robotic arms is a concrete \ndemonstration of composition of linear (and affine) transformations. There are four \napplications from which to choose in Section 3.7. Either Markov chains or the Leslie \nmodel of population growth should be covered so that they can be used again in \nChapter 4, where their behavior will be explained. \nChanler 4: Eigenvalues and Eigenveclors","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1063,"to":1074}}}}],[62,{"pageContent":"model of population growth should be covered so that they can be used again in \nChapter 4, where their behavior will be explained. \nChanler 4: Eigenvalues and Eigenveclors \nThe introduction Section 4.0 presents an interesting dynamical system involving \ngraphs. This exploration introduces the notion of an eigenvector and foreshadows the \npower method in Section 4.5. In keeping with the geometric emphasis of the book, \nSection 4.1 contains the novel feature of \"eigenpictures\" as a way of visualizing the \neigenvectors of 2  X  2 matrices. Determinants appear in Section 4.2, motivated by \ntheir use in finding the characteristic polynomials of small matrices. This \"crash","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1074,"to":1082}}}}],[63,{"pageContent":"XX \nTo the Instructor \nSee page 515 \nSee page 529 \nSee page 543 \nSee page 547 \nSee page 607 \nthat familiar vector space concepts (Section 3.5) can be used fruitfully in a new \nsetting. Because all of the main ideas of vector spaces have already been introduced in \nChapters 1-3, students should find Sections 6.1 and 6.2 fairly familiar. The emphasis \nhere should be on using the vector space axioms to prove properties rather than rely­\ning on computational techniques. When discussing change of basis in Section 6.3, it \nis helpful to show students how to use the notation to remember how the construc­\ntion works.  Ultimately, the Gauss-Jordan method is the most efficient here. Sec­\ntions 6.4 and 6.5 on linear transformations are important. The examples are related to \nprevious results on matrices (and matrix transformations). In particular, it is impor­\ntant to stress that the kernel and range of a linear transformation generalize the null","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1084,"to":1100}}}}],[64,{"pageContent":"previous results on matrices (and matrix transformations). In particular, it is impor­\ntant to stress that the kernel and range of a linear transformation generalize the null \nspace and column space of a matrix. Section 6.6 puts forth the notion that (almost) all \nlinear transformations are essentially matr  ix transformations. This  builds on the \ninformation in Section 3.6, so students should not find it terribly surprising. However, \nthe examples should be worked carefully. The connection between change of basis \nand similarity of matrices is noteworthy. The exploration \"Tilings, Lattices, and the \nCrystallographic Restriction\" is an impressive application of change of basis. The con­\nnection with the artwork ofM. C. Escher makes it all the more interesting. The appli­\ncations in Section 6.7 build on previous ones and can be included as time and interest \npermit. \nChapter 1: Distance and Approximation \nSection 7 .0  opens with  the entertaining \"Taxicab Geometry\"  exploration.  Its","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1100,"to":1112}}}}],[65,{"pageContent":"permit. \nChapter 1: Distance and Approximation \nSection 7 .0  opens with  the entertaining \"Taxicab Geometry\"  exploration.  Its \npurpose is to set up the  material on generalized norms and  distance functions \n(metrics) that follows. Inner product spaces are discussed in Section 7 .1; the em­\nphasis here should be on the examples and using the axioms. The explo  ration \"Vec­\ntors and Matrices with Complex Entries\" shows how the concepts of dot product, \nsymmetric matrix, orthogonal matrix, and orthogonal diagonalization can be ex­\ntended from real to complex vector spaces. The following exploration, \"Geometric \nInequalities and Optimization Problems:' is one that students typically enjoy. (They \nwill have fun seeing how many \"calculus\" problems can be solved without using \ncalculus at all!) Section 7.2 covers generalized vector and matrix norms and shows \nhow the condition number of a matrix is related to the notion of ill-conditioned","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1112,"to":1124}}}}],[66,{"pageContent":"calculus at all!) Section 7.2 covers generalized vector and matrix norms and shows \nhow the condition number of a matrix is related to the notion of ill-conditioned \nlinear systems explored in Chapter 2. Least squares approximation (Section 7.3) is \nan important application of linear algebra in many other disciplines. The Best Ap­\nproximation Theorem and  the Least Squares Theorem are  important, but their \nproofs are intuitively clear. Spend time here on the examples-a few should suffice. \nSection 7.4 presents the singular value decomposition, one of the most impressive \napplications of linear algebra. If your course gets this far, you will be amply re­\nwarded. Not only does the SVD tie together many notions discussed previously; it \nalso affords some new (and quite powerful) applications. If a CAS is available, the \nvignette on digital image compression is worth presenting; it is a visually impres­\nsive display of the power of linear algebra and a fitting culmination to the course.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1124,"to":1135}}}}],[67,{"pageContent":"vignette on digital image compression is worth presenting; it is a visually impres­\nsive display of the power of linear algebra and a fitting culmination to the course. \nThe further applications in Section 7.5 can be chosen according to the time avail­\nable and the interests of the class. \nChapter 8: Codes \nThis online chapter contains applications of linear algebra to the theory of codes. \nSection 8.1 begins with a  discussion of how vectors can be used to design","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1135,"to":1141}}}}],[68,{"pageContent":"See page 626 \nTo the Instructor \nXXi \nerror-detecting codes such as the familiar Universal Product Code (UPC) and \nInternational Standard Book Number (ISBN). This topic only requires knowl­\nedge of Chapter 1. The vignette on the Codabar system used in credit and bank \ncards is an excellent classroom presentation that can even be used to introduce \nSection 8.1. Once students are familiar with matrix operations, Section 8.2 de­\nscribes how codes can be designed to correct as well as detect errors.  The \nHamming codes introduced here are perhaps the most famous examples of such \nerror-correcting codes. Dual codes, discussed in Section 8.3, are an important \nway of constructing new codes from old ones. The notion of orthogonal comple­\nment, introduced in  Chapter  5,  is  the prerequisite concept here. The most \nimportant, and most widely used, class of codes is the class of linear codes that is \ndefined in Section 8.4. The notions of subspace, basis, and dimension are key","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1143,"to":1157}}}}],[69,{"pageContent":"important, and most widely used, class of codes is the class of linear codes that is \ndefined in Section 8.4. The notions of subspace, basis, and dimension are key \nhere. The powerful Reed-Muller codes used by NASA spacecraft are important \nexamples of linear codes. Our discussion of codes concludes in Section 8.5 with \nthe definition of the minimum distance of a code and the role it plays in deter­\nmining the error-correcting capability of the code. \nHow to use the Book \nStudents find the book easy to read, so I usually have them read a section before I \ncover the material in class. That way, I can spend class time highlighting the most \nimportant concepts, dealing with topics students find difficult, working examples, \nand discussing applications. I do not attempt to cover all of the material from the \nassigned reading in class. This approach enables me to keep the pace of the course \nfairly  brisk,  slowing  down for those sections  that  students typically  find \nchallenging.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1157,"to":1170}}}}],[70,{"pageContent":"assigned reading in class. This approach enables me to keep the pace of the course \nfairly  brisk,  slowing  down for those sections  that  students typically  find \nchallenging. \nIn a two-semester course, it is possible to cover the entire book, including a rea­\nsonable selection of applications. For extra flexibility, you might omit some of the \ntopics (for example, give only a  brief treatment of numerical linear algebra), thereby \nfreeing up time for more in-depth coverage of the remaining topics, more applica­\ntions, or some of the explorations. In an honors mathematics course that emphasizes \nproofs, much of the material in Chapters 1-3 can be covered quickly. Chapter 6 can \nthen be covered in conjunction with Sections 3.5 and 3.6, and Chapter 7 can be in­\ntegrated into Chapter 5. I would be sure to assign the explorations in Chapters 1, 4, \n6, and 7 for such a class. \nFor a one-semester course, the nature of the course and the audience will deter­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1170,"to":1182}}}}],[71,{"pageContent":"tegrated into Chapter 5. I would be sure to assign the explorations in Chapters 1, 4, \n6, and 7 for such a class. \nFor a one-semester course, the nature of the course and the audience will deter­\nmine which topics to include. Three possible courses are described below and on the \nfollowing page. The basic course, described first, has fewer than 36 hours suggested, \nallowing time for extra topics, in-class review, and tests. The other two courses build \non the basic course but are still quite flexible. \nA Basic Course \nA course designed for mathematics majors and students from other disciplines is \noutlined on the next page. This course does not mention general vector spaces at all \n(all concepts are treated in a concrete setting) and is very light on proofs. Still, it is a \nthorough introduction to linear algebra.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1182,"to":1193}}}}],[72,{"pageContent":"XXii \nTo the Instructor \nSection Number of Lectures Section Number of Lectures \n1.1 \n3.6 \n1-2 \n1.2 \n1-1.5 \n4.1 \n1 \n1.3 \n1-1.5 \n4.2 \n2 \n2.1 \n0.5-1 \n4.3 \n1 \n2.2 \n1-2 \n4.4 \n1-2 \n2.3 \n1-2 \n5.1 \n1-1.5 \n3.1 \n1-2 \n5.2 \n1-1.5 \n3.2 \n5.3 \n0.5 \n3.3 \n2 \n5.4 \n1 \n3.5 \n2 \n7.3 \n2 \nTotal: 23-30 lectures \nBecause the students in a course such as this one represent a wide variety of dis­\nciplines, I would suggest using much of the remaining lecture time for applications. \nIn my course, I do code vectors in Section 8.1, which students really seem to like, and \nat least one application from each of Chapters 2-5. Other applications can be as­\nsigned as projects, along with as many of the explorations as desired. There is also \nsufficient lecture time available to cover some of the theory in detail. \nA Course with a Comoulalional Emphasis \nFor a course with a computational emphasis, the basic course outlined on the previous \npage can be supplemented with the sections of the text dealing with numerical linear","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1195,"to":1245}}}}],[73,{"pageContent":"For a course with a computational emphasis, the basic course outlined on the previous \npage can be supplemented with the sections of the text dealing with numerical linear \nalgebra. In such a course, I would cover part or all of Sections 2.5, 3.4, 4.5, 5.3, 7.2, and \n7.4, ending with the singular value decomposition. The explorations in Chapters 2 \nand 5 are particularly well suited to such a course, as are almost any of the applications. \nA course tor Sludenls Who Have Already \nSIUdied Some linear Algebra \nSome courses will be aimed at students who have already encountered the basic prin­\nciples of linear algebra in other courses. For example, a college algebra course will \noften include an introduction to systems of linear equations, matrices, and deter­\nminants; a  multivariable calculus course will almost certainly contain material on \nvectors, lines, and planes. For students who have seen such topics already, much early","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1245,"to":1256}}}}],[74,{"pageContent":"minants; a  multivariable calculus course will almost certainly contain material on \nvectors, lines, and planes. For students who have seen such topics already, much early \nmaterial can be omitted and replaced with a quick review. Depending on the back­\nground of the class, it may be possible to skim over the material in the basic course up \nto Section 3.3 in about six lectures. If the class has a   significant number of mathemat­\nics majors (and especially if this is the only linear algebra course they will take), \nI would be sure to cover Sections 6.1-6.5, 7.1, and 7.4 and as many applications as time \npermits. If the course has science majors (but not mathematics majors), I would cover \nSections 6.1 and 7.1 and a broader selection of applications, being sure to include the \nmaterial on differential equations and approximation of functions. If computer sci­\nence students or engineers are prominently represented, I would try to do as much of","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1256,"to":1266}}}}],[75,{"pageContent":"material on differential equations and approximation of functions. If computer sci­\nence students or engineers are prominently represented, I would try to do as much of \nthe material on codes and numerical linear algebra as I could. \nThere are many other types of courses that can successfully use this text. I  hope \nthat you find it useful for your course and that you enjoy using it.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1266,"to":1270}}}}],[76,{"pageContent":"To the \nStud\nent \n\"Where shall I begin, please your \nMajesty?\" he asked. \n\"Begin at the beginning,\" the King \nsaid, gravely,  \"a nd go on  till you come \nto the end: then stop.\" \n-Lewis Carroll \nAlice's Adventures in \nWonderland, 1865 \nLinear algebra is an exciting subject. It is full of interesting results, applications to \nother disciplines, and connections to other areas of mathematics. The Student Solu­\ntions Manual and Study Guide contains detailed advice on how best to use this book; \nfollowing are some general suggestions. \nLinear algebra has several sides: There are computational techniques, concepts, and \napplications. One of the goals of this book is to  help you master all of these facets of \nthe subject and to see the interplay among them. Consequently, it is important that \nyou read and understand each section of the text before you attempt the exercises in \nthat section. If you read only examples that are related to exercises that have been","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1272,"to":1291}}}}],[77,{"pageContent":"you read and understand each section of the text before you attempt the exercises in \nthat section. If you read only examples that are related to exercises that have been \nassigned as homework, you will miss much. Make sure you understand the defini­\ntions of terms and the meaning of theorems. Don't worry if you have to read some­\nthing more than once before you understand it. Have a pencil and calculator with you \nas you read. Stop to work out examples for yourself or to fill in missing calculations. \nThe � icon in the margin indicates a place where you should pause and think over \nwhat you have read so far. \nAnswers to most odd-numbered computational exercises are in the back of the \nbook. Resist the temptation to look up an answer before you have completed a qu es­\ntion. And remember that even if your answer differs from the one in the back, you \nmay still be right; there is more than one correct way to express some of the solutions.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1291,"to":1302}}}}],[78,{"pageContent":"tion. And remember that even if your answer differs from the one in the back, you \nmay still be right; there is more than one correct way to express some of the solutions. \nFor example, a value of l/v2 can also be  expressed as v2/2 and the set of all scalar \nmultiples of the vector \n[ \n1\n�\n2 \n] is the same as the set of all scalar multiples of \n[ \n� ] . \nAs you encounter new concepts, try to relate them to examples that you know. \nWrite out proofs and solutions to exercises in a logical, connected way, using com -\nplete sentences. Read back what you have written to see whether it makes sense. \nBetter yet, if you can, have a friend in the class read what you have written. If it doesn't \nmake sense to another person, chances are that it doesn't make sense, period. \nYou will find that a calculator with matrix capabilities or a computer algebra sys­\ntem is useful. These tools can help you to check your own hand calculations and are","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1302,"to":1319}}}}],[79,{"pageContent":"You will find that a calculator with matrix capabilities or a computer algebra sys­\ntem is useful. These tools can help you to check your own hand calculations and are \nindispensable for some problems involving tedious computations. Technology also \nxx iii","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1319,"to":1322}}}}],[80,{"pageContent":"XXiV \nTo the Student \nenables you to explore aspects of linear algebra on your own. You can play \"what if?\" \ngames: What if I change one of the entries in this vector? What if this matrix is of a \ndifferent size? Can I  force the solution to be what I would like it to be by changing \nsomething? To signal places in the text or exercises where the use of technology is \nrecommended, I have placed the icon \ncAs \nin the margin. The companion website that \naccompanies this book contains computer code working out selected exercises from \nthe book using Maple, Mathematica, and MATLAB, as well as Technology Bytes, an \nappendix providing much additional advice about the use of technology in linear \nalgebra. \nYou are about to embark on a journey through linear algebra. Think of this book \nas your travel guide. Are you ready? Let's go!","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1324,"to":1338}}}}],[81,{"pageContent":"Here they come pouring out of  the blue. \nLittle arrows fo r me and fo r you. \n-Albert Hammond and \nMike Hazelwood \nLittle Arrows \nDutchess Music/BM!, 1968 \nVectors \n1.0 \nIntroduction: The Racetrack Game \nMany measurable quantities, such as length, area, volume, mass, and temperature, \ncan be completely described by specifying their magnitude. Other quantities, such \nas velocity, force, and acceleration, require both a magnitude and a direction for \ntheir description. These quantities are vectors. For example, wind velocity is a vector \nconsisting of wind speed and direction, such as 10 km/h southwest. Geometrically, \nvectors are often represented as arrows or directed line segments. \nAlthough the idea of a vector was introduced in the 19th century, its usefulness \nin applications, particularly those in the physical sciences, was not realized until the \n20th century. More recently, vectors have found applications in computer science,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1340,"to":1357}}}}],[82,{"pageContent":"in applications, particularly those in the physical sciences, was not realized until the \n20th century. More recently, vectors have found applications in computer science, \nstatistics, economics, and the life and social sciences. We will consider some of these \nmany applications throughout this book. \nThis chapter introduces vectors and begins to consider some of their geometric \nand algebraic properties. We begin, though, with a simple game that introduces some \nof the key ideas. [You may even wish to play it with a friend during those (very rare!) \ndull moments in linear algebra class.] \nThe game is played on graph paper. A  track, with a starting line and a finish line, \nis drawn on the paper. The track can be of any length and shape, so long as it is wide \nenough to accommodate all of the players. For this example, we will have two players \n(let's call them Ann and Bert) who use different colored pens to represent their cars","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1357,"to":1368}}}}],[83,{"pageContent":"enough to accommodate all of the players. For this example, we will have two players \n(let's call them Ann and Bert) who use different colored pens to represent their cars \nor bicycles or whatever they are going to race around the track. (Let's think of Ann \nand Bert as cyclists.) \nAnn and Bert each begin by drawing a dot on the starting line at a grid point on \nthe graph paper. They take turns moving to a new grid point, subject to the following \nrules: \n1. Each new grid point and the line segment connecting it to the previous grid point \nmust lie entirely within the track. \n2. No two players may occupy the same grid point on the same turn. (This is the \n\"no collisions\" rule.) \n3. Each new move is related to the previous move as follows: If a player moves \na units horizontally and b units vertically on one move, then on the next move \nhe or she must move between a -1 and a + 1 units horizontally and between","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1368,"to":1381}}}}],[84,{"pageContent":":r: \nr§ \n2 \nChapter 1 Vectors \nThe Irish mathematician William \nRowan Hamilton (1805-1865) \nused vector concepts in his study \nof complex numbers and their \ngeneralization, the quaternions. \nb  -   1 and b  +  1 units vertically. In other words, if the second move is c units \nhorizontally and d units vertically, then \nl\na  -cl :::::: 1 and \nl\nb -  d\ni \n:::::: 1. (This is the \n\"acceleration/deceleration\" rule.) Note that this rule forces the first move to be \n1 unit vertically and/or 1 unit horizontally. \nA player who collides with another player or leaves the track is eliminated. The \nwinner is the first player to cross the finish line. If more than one player crosses \nthe finish line on the same turn, the one who goes farthest past the finish line is the \nwinner. \nIn the sample game shown in Figure 1.1, Ann was the winner. Bert accelerated too \nquickly and had difficulty negotiating the turn at the top of the track. \nTo understand rule 3, consider Ann's third and fourth moves. On her third move,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1383,"to":1408}}}}],[85,{"pageContent":"quickly and had difficulty negotiating the turn at the top of the track. \nTo understand rule 3, consider Ann's third and fourth moves. On her third move, \nshe went 1 unit horizontally and 3 units vertically. On her fourth move, her options \nwere to move 0 to 2 units horizontally and 2 to 4 units vertically. (Notice that some \nof these combinations would have placed her outside the track.) She chose to move \n2 units in each direction. \nI \nI \nr \nr \nA   B \nl l \nFigure 1.1 \nA sample game of racetrack \nProblem 1 Play a few games of racetrack. \nProblem 2 Is it possible for Bert to win this race by choosing a different sequence \nof moves? \nProblem 3 Use the notation [a, b] to denote a move that is a units horizontally \nand b units vertically. (Either a or b or both may be negative.) If move [3, 4] has just \nbeen made, draw on graph paper all the grid points that could possibly be reached \non the next move. \nProblem 4 What is the net effect of two successive moves? In other words, if you","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1408,"to":1429}}}}],[86,{"pageContent":"been made, draw on graph paper all the grid points that could possibly be reached \non the next move. \nProblem 4 What is the net effect of two successive moves? In other words, if you \nmove [a, b] and then [ c, d], how far horizontally and vertically will you have moved \naltogether?","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1429,"to":1433}}}}],[87,{"pageContent":"The Cartesian plane is named \nafter the French philosopher and \nmathematician Rene Descartes \n(1596-1650), whose introduction \nof coordinates allowed geometric \nproblems to be handled using \nalgebraic techniques. \nThe word vector comes from the \nLatin root meaning \"to carrY:' A \nvector is formed when a point is \ndisplaced-or \"carried off\" -a given \ndistance in a given direction. Viewed \nanother way, vectors \"carry\" two \npieces of information: their length \nand their direction. \nWhen writing vectors by hand, \nit is difficult to indicate boldface. \nSome people prefer to write v for \nthe vector denoted in print by v, \nbut in most cases it is fine to use an \nordinary lowercase v. It will usu­\nally be clear from the context when \nthe letter denotes a vector. \nThe word component is derived \nfrom the Latin words co, meaning \n\"together with;' and ponere, mean­\ning \"to put:' Thus, a vector is \"put \ntogether\" out of its components. \nSection 1.1 \nThe Geometry and Algebra of Vectors \n3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1435,"to":1465}}}}],[88,{"pageContent":"from the Latin words co, meaning \n\"together with;' and ponere, mean­\ning \"to put:' Thus, a vector is \"put \ntogether\" out of its components. \nSection 1.1 \nThe Geometry and Algebra of Vectors \n3 \nProblem 5 Write out Ann's sequence of moves using the [a, b] notation. Suppose \nshe begins at the origin (O, 0) on the coordinate axes. Explain how you can find the \ncoordinates of the grid point corresponding to each of her moves without looking at \nth e graph paper. If the axes were drawn differently, so that Ann's starting point was \nnot the origin but the point \n(\n2, 3\n)\n, what would the coordinates of her final point be? \nAlthough simple, this game introduces several ideas that will be useful in our \nstudy of vectors. The next three sections consider vectors from geometric and alge­\nbraic viewpoints, beginning, as in the racetrack game, in the plane. \nThe Geometrv and Algebra  of vectors \nVectors  in the Plane \nWe  begin by considering the Cartesian plane with the familiar x-and y-axes.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1465,"to":1486}}}}],[89,{"pageContent":"The Geometrv and Algebra  of vectors \nVectors  in the Plane \nWe  begin by considering the Cartesian plane with the familiar x-and y-axes. \nA vector is a directed line segment that corresponds to a displacement from one point \nA to another point B; see Figure 1.2. \nThe vector from A to Bis denoted by \nAB\n; the point A is called its initial point, \nor tail, and the point B is called its terminal point, or head. Often, a vector is simply \ndenoted by a single boldface, lowercase letter such as v. \nThe set of all points in the plane corresponds to the set of all vectors whose tails \n----> \nare at the origin 0. To each point A, there corresponds the vector a  = OA; to each \nvector a with tail at 0, there corresponds its head A. (Vectors of this form are some­\ntimes called position vectors.\n) \nIt  is  natural to  represent  such  vectors using coordinates.  For  example,  in \n----> \nFigure 1.3, A = \n(\n3, 2\n) \nand we write the vector a = OA = [3, 2] using square brackets.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1486,"to":1508}}}}],[90,{"pageContent":") \nIt  is  natural to  represent  such  vectors using coordinates.  For  example,  in \n----> \nFigure 1.3, A = \n(\n3, 2\n) \nand we write the vector a = OA = [3, 2] using square brackets. \nSimilarly, the other vectors in Figure 1.3 are \nb  = [-1,  3] and c  = [2, -1] \nThe individual coordinates \n(\n3 and 2 in the case of a) are called the components of the \nvector. A  vector is sometimes said to be an ordered pair of real numbers. The order is \nimportant since, for example, [ 3, 2] � [2, 3 ]. In general, two vectors are equal if and \nonly if their corresponding components are equal. Thus, [x, y] = [l, 5] implies that \nx = 1 andy = 5. \nIt is frequently convenient to use column vectors instead of (or in addition to) \nrow vectors. Another representation of [ 3, 2] is \n[\n�]. (The important point is that the \ny \ny \nA\n�\nB \nFigure 1.2 \nFigure 1.3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1508,"to":1535}}}}],[91,{"pageContent":"4 \nChapter 1 Vectors \nIR2 is pronounced \"r two:' \nWhen vectors are referred to by \ntheir coordinates, they are being \nconsidered analytically. \nExample 1.1 \ncomponents are ordered.\n) \nIn later chapters, you will see that column vectors are some­\nwhat better from a computational point of view; for now, try to get used to both \nrepresentations. \nIt may occur to you that we  cannot really draw the vector [ O, O\nJ \n= 00 from the \norigin to itself. Nevertheless, it is a perfectly good vector and has a special name: the \nzero vector. The zero vector is denoted by 0. \nThe set of all vectors with two components is denoted by IR\n2 \n(where IR denotes \nthe set of real numbers from which the components of vectors in IR\n2 \nare chosen). \nThus, [ -1, 3.5], [ \n\\/2\n, 7f ], and rn, 4] are all in IR\n2\n• \nThinking back to the racetrack game, let's try to connect all of these ideas to vec­\ntors whose tails are not at the origin. The etymological origin of the word vector in","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1537,"to":1566}}}}],[92,{"pageContent":"2\n• \nThinking back to the racetrack game, let's try to connect all of these ideas to vec­\ntors whose tails are not at the origin. The etymological origin of the word vector in \nthe verb \"to carry\" provides a clue. The vector [3, 2] may be interpreted as follows: \nStarting at the origin 0, travel 3 units to the right, then 2 units up, finishing at P. The \nsame displacement may be applied with other init�oin�igure 1.4 shows two \nequivalent displacements, represented by the vectors AB and CD. \ny \nc \nFigure 1.4 \nWe define two vectors as equal if they have the same length and the same direc­\ntion. Thus, \nAB \n= CD in Figure 1.4. (Even though they have different initial and ter­\nminal points, they represent the same displacement.) Geometrically, two vectors are \nequal if one can be obtained by sliding (or translating\n) \nthe other parallel to itself until \nthe two vectors coincide. In terms of components, in Figure 1.4 we have A = \n(\n3, 1\n) \nand B = (6, 3\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1566,"to":1590}}}}],[93,{"pageContent":"equal if one can be obtained by sliding (or translating\n) \nthe other parallel to itself until \nthe two vectors coincide. In terms of components, in Figure 1.4 we have A = \n(\n3, 1\n) \nand B = (6, 3\n)\n. Notice that the  vector [ 3, 2] that records the displacement is just the \ndifference of the respective components: \nSimilarly, \n-----> \n----> \n-----> \nAB = [ 3, 2] = [ 6 -3, 3 -l ] \nCD= r-1 -   (-4\n)\n, 1  -   (-1\n)\nJ \n[3, 2] \nand thus AB = CD, as expected. \nA vector such as oP with its tail at the origin is said to be in standard position. \nThe foregoing discussion shows that every vector can be drawn as a vector in stan­\ndard position. Conversely, a vector in standard position can be redrawn (by transla­\ntion) so that its tail is at any point in the plane. \nIf A = \n(\n-1, 2\n) \nand B = \n(\n3, 4\n)\n, find \nAB \nand redraw it (a) in standard position and \n(b) with its tail at the point C = \n(\n2, -1\n)\n. \n-----> -----> \nSolulion We compute AB = [3  -\n(\n-1\n)\n, 4  -   2] = [4, 2]. If AB is then translated \nto CD, where C = \n(\n2, -1\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1590,"to":1642}}}}],[94,{"pageContent":"(b) with its tail at the point C = \n(\n2, -1\n)\n. \n-----> -----> \nSolulion We compute AB = [3  -\n(\n-1\n)\n, 4  -   2] = [4, 2]. If AB is then translated \nto CD, where C = \n(\n2, -1\n)\n, then we must have D = \n(\n2  + 4, -1 +  2\n) \n= (6, 1\n)\n. (See \nFigure 1.5.\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1642,"to":1665}}}}],[95,{"pageContent":"Section 1.1 \nThe Geometry and Algebra of Vectors \n5 \ny \nD(6, 1) \nFigure 1.5 \nNew Veclors from Old \nAs in   the racetrack game, we often want to \"follow\" one vector by another. This leads \nto the notion of vector addition, the first basic vector operation. \nIf we follow u by v, we can visualize the total displacement as a third vector, \ndenoted by u + v. In Figure 1.6, u  = [ 1, 2] and v = [ 2, 2], so the net effect of follow­\ning u by vis \n[\n1  +  2, 2  +  2] = [3, 4] \nwhich gives u + v. In general, if u  = [u\n1\n, u\n2\n] and v = [ v\n1\n, v\n2\n], then their sum u + v \nis the vector \nIt is helpful to visualize u + v geometrically. The following rule is the geometric \nversion of the foregoing discussion. \ny \n/ \nFigure 1.6 \nVector addition \n/l \n�\n( \n__ \nj2 \nI \n2 \nU \nI \n:2 \n/_\n_\n) \n4 \n_________ _n \n3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1667,"to":1712}}}}],[96,{"pageContent":"6 \nChapter 1 Vectors \nThe Head-to-Tail Rule \nFigure 1.8 \nThe parallelogram \ndetermined by u and v \nThe Parallelogram Rule \nExample 1.2 \nGiven vectors u and v in IR\n2\n, translate v so that its tail coincides with the head \nof u. The sum u + v of u and v is the vector from the tail of u to the head of v. \n(See Figure 1.7.\n) \nFigure 1.1 \nThe head-to-tail rule \nBy translating u  and v parallel to themselves, we obtain a parallelogram, as \nshown in Figure 1.8. This parallelogram is called the para llelogram determined by u \nand v. It leads to an equivalent version of the head-to-tail rule for vectors in standard \nposition. \nGiven vectors u and v in IR\n2 \n(in standard position), their sum u + vis the vector \nin standard position along the diagonal of the parallelogram determined by u and \nv. (See Figure 1.9.\n) \ny \nFigure 1.9 \nThe parallelogram rule \nIfu = [3, -1] and v = [l, 4], compute and draw u + v. \nSolulion We compute u + v = [3 + 1, -1 + 4] = [4, 3]. This vector is drawn","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1714,"to":1744}}}}],[97,{"pageContent":"v. (See Figure 1.9.\n) \ny \nFigure 1.9 \nThe parallelogram rule \nIfu = [3, -1] and v = [l, 4], compute and draw u + v. \nSolulion We compute u + v = [3 + 1, -1 + 4] = [4, 3]. This vector is drawn \nusing the head-to-tail rule in Figure l.lO(a) and using the parallelogram rule in \nFigure l.lO(b).","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1744,"to":1752}}}}],[98,{"pageContent":"Example 1.3 \nSection 1.1 \nThe Geometry and Algebra of Vectors \n1 \ny \n(a) \nFigure 1.10 \ny \nu \n(b) \nThe second basic vector operation is scalar multiplication. Given a vector v and \na real number c,   the scalar multiple cv is the vector obtained by multiplying each \ncomponent ofv by  c. For example, 3[-2, 4] = [-6, 12]. In general, \nGeometrically, cv is a \"scaled\" version of v. \nIfv = [ -2, 4], compute and draw 2v, \nt\nv, and -2v. \nSolution \nWe calculate as follows: \n2v = [2\n( \n-2\n)\n, 2\n(\n4\n)\n] = [ -4, 8 \nJ \nt\nv = \n[t\n(\n-2\n)\n, \nt\n(\n4\n)\n] = [-1, 2] \n-2v = [-2\n(\n-2\n)\n, -2\n(\n4\n)\n] = [4, -8] \nThese vectors are shown in Figure 1.11. \ny \n2v \n-\n2v \nFigure 1.11","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1754,"to":1809}}}}],[99,{"pageContent":"8 \nChapter 1 Vectors \n/./ \n2v \nFigure 1.12 \nThe term scalar comes from the \nLatin word scala, meaning \"lad­\ndd' The equally spaced rungs on \na ladder suggest a scale, and in vec­\ntor arithmetic, multiplication by a \nconstant changes only the scale (or \nlength) of a vector. Thus, constants \nbecame known as scalars. \nExample 1.4 \ny \n-\n2v \nu \nu \n+ ( -  v) \nFigure 1.13 \nVector subtraction \nObserve that cv has the same direction as v if c > 0   and the opposite direction if \nc < 0. We also see that cv is le I times as long as v. For this reason, in the context of \nvectors, constants (i.e., real numbers) are referred to as scalars. As Figure 1.12 shows, \nwhen translation of vectors is taken into account, two vectors are scalar multiples of \neach other if and only if they are parallel. \nA special case of a scalar multiple is ( - l)v, which is written as -v and is called \nthe negative of v. We can use it to define vector subtraction: The difference of u and \nv is the vector u -  v defined by \nu -  v =  u  + (-v)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1811,"to":1841}}}}],[100,{"pageContent":"the negative of v. We can use it to define vector subtraction: The difference of u and \nv is the vector u -  v defined by \nu -  v =  u  + (-v) \nFigure 1.13 shows that u -  v corresponds to the \"other\" diagonal of the parallelo­\ngram determined by u and v. \nIf u  = [1, 2] and v = \n[ -3, 1], then u -  v \n= [1 -( -3), 2 -  1] = [ 4, l]. \nThe definition of subtraction in Example 1.4 also agrees with the way we cal­\nculate a vector such as \nAB\n. If the points A and B correspond to the vectors a and b \n-----> \nA \n� \nin standard position, then AB \n= \nb -a, as sh  own in Figure 1.14. [Observe that the \nFigure 1.14 \nhead-to-tail rule applied to this diagram gives the equation a+ (b -a) =  b. Ifwe \nhad accidentally drawn b  - a with its head at A instead of at B, the diagram would \nhave read b  + (b - a) = a, which is clearly wrong! More will be said about algebraic \nexpressions involving vectors later in this section.] \nvec1ors  in �\n3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1841,"to":1865}}}}],[101,{"pageContent":"have read b  + (b - a) = a, which is clearly wrong! More will be said about algebraic \nexpressions involving vectors later in this section.] \nvec1ors  in �\n3 \nEverything we have just done extends easily to three dimensions. The set of all or­\ndered triples of real numbers is denoted by IR\n3\n. Points and vectors are located using \nthree mutually perpendicular coordinate axes that meet at the origin 0. A point such \nas A = (1, 2, 3)   can be located as follows: First travel 1 unit along the x-axis, then \nmove 2 units parallel to the y-axis, and finally move 3 units parallel to the z-axis. The \ncorresponding vector a= [l, 2, 3] is then OA, as shown in Figure 1.15. \nAnother way to visualize vector a in IR\n3 \nis to  construct a box whose six sides are de­\ntermined by the three coordinate planes (the xy-, xz-, and yz-planes) and by three planes \nthrough the point ( 1, 2, 3) parallel to the  coordinate planes. The vector [ 1, 2, 3]  then corre­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1865,"to":1881}}}}],[102,{"pageContent":"termined by the three coordinate planes (the xy-, xz-, and yz-planes) and by three planes \nthrough the point ( 1, 2, 3) parallel to the  coordinate planes. The vector [ 1, 2, 3]  then corre­\nsponds to the diagonal from the origin to the opposite corner of the box (see Figure 1.16).","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1881,"to":1883}}}}],[103,{"pageContent":"Figure 1.11 \nu +v\n=\nv+u \nx \nFigure 1.15 \nz \nSection 1.1 \nThe Geometry and Algebra of Vectors \n9 \nz \nA\n(l, \n2, \n3\n) \nI \n• \n3 I \nI \nI \n:\n3 \nI \nI \n-\nI \n2\n-\n-\n-\n1\n� \n,. \ny \nx \nFigure 1.16 \nThe \"componentwise\" definitions of vector addition and scalar multiplication are \nextended to IR\n3 \nin an obvious way. \nVectors  in � n \nIn general, we define !R\nn \nas the set of all ordered n-tuples of real numbers written as \nrow or column vectors. Thus, a vector v in !R\nn \nis of the form \nThe individual entries of v are its components; V; is called the ith component. \nWe  extend the definitions of vector addition and scalar multiplication to !R\nn \nin \nthe obvious way: If u  = [u\n1\n, u\n2\n, ••• , u\nn\nl \nand v = \n[ v\n1\n, v\n2\n, ... , v\nn\n], the ith component of \nu + vis U; + V; and the ith component of cv is just CV;. \nSince in !R\nn \nwe can no longer draw pictures of vectors, it is important to be able to \ncalculate with vectors. We must be careful not to assume that vector arithmetic will be","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1885,"to":1956}}}}],[104,{"pageContent":"Since in !R\nn \nwe can no longer draw pictures of vectors, it is important to be able to \ncalculate with vectors. We must be careful not to assume that vector arithmetic will be \nsimilar to the arithmetic of real numbers. Often it is, and the algebraic calculations we \ndo with vectors are similar to those we would do with scalars. But, in later sections, \nwe will encounter situations where vector algebra is quite unlike our previous experi­\nence with real numbers. So it is important to verify any algebraic properties before \nattempting to use them. \nOne such property is commutativity of addition: u + v = v + u for vectors u and \nv. This is ce  rtainly true in IR\n2\n• Geometrically, the head-to-tail rule shows that both \nu + v and v + u are the main diagonals of the parallelogram determined by u and v. \n(The parallelogram rule also reflects this symmetry; see Figure 1.17.) \nNote \nthat Figure 1.17 is simply an illustration of the property u + v = v + u. It","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1956,"to":1972}}}}],[105,{"pageContent":"(The parallelogram rule also reflects this symmetry; see Figure 1.17.) \nNote \nthat Figure 1.17 is simply an illustration of the property u + v = v + u. It \nis not a proof, since it does not cover every possible case. For example, we must also \n� \ninclude the cases where u  = v, u  = -v, and u  = 0. (What would diagrams for these \ncases look like?) For this reason, an algebraic proof is needed. However, it is just as \neasy to give a proof that is valid in !R\nn \nas to give one that is valid in IR\n2\n. \nThe following theorem summarizes the algebraic properties of vector addition \nand scalar multiplication in !R\nn\n. The proofs follow from the corresponding properties \nof real numbers.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1972,"to":1988}}}}],[106,{"pageContent":"10 \nChapter 1 Vectors \nTheorem 1.1 \nThe word theorem is derived from \nthe Greek word theorema, which \nin turn comes from a word mean­\ning \"to look af' Thus, a theorem \nis based on the insights we have \nwhen we look at examples and \nextract from them properties that \nwe try to prove hold in general. \nSimilarly, when we understand \nsomething in mathematics-the \nproof of a theorem, for example­\nwe often say, \"I see:' \nu \nFigure 1.18 \nAlgebraic Properties of Vectors in !R\nn \nLet u, v, and w be vectors in !R\nn \nand let c and d be scalars. Then \na. u + v \n= \nv + u \nb. \n( u + v)  + w \n= u + \n(v + w) \nCommutativity \nAssociativity \nc. u + 0 = u \nd. \nu +  (-u) \n= 0 \ne. c\n( u + v) =cu+ cv \nDistributivity \nDistributivity \nf. \n(\nc + d)u = cu+ du \ng. c\n(\ndu) \n= \n(\ncd\n) u \nh. lu = u \nRemarks \n• \nProperties (c) and (d) together with the commutativity property (a) imply \nthat 0 + u \n= \nu and - u + u \n= 0 as well. \n• \nIfwe read the distributivity properties (e) and (f) from right to left, they say","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":1990,"to":2048}}}}],[107,{"pageContent":"• \nProperties (c) and (d) together with the commutativity property (a) imply \nthat 0 + u \n= \nu and - u + u \n= 0 as well. \n• \nIfwe read the distributivity properties (e) and (f) from right to left, they say \nthat we  can factor a common scalar or a   common vector from a sum. \nProof We  prove properties (a)  and  (b)  and  leave  the proofs  of the  remain­\ning properties as exercises. Let u  =  [ u\n1\n, u\n2\n, ... , u\nn\n], v \n= \n[ v\n1\n, v\n2\n, ... , v\nn\n], and w = \n[\nW\n1\n, \nWz\n, ... , \nW\nn\n]. \n(a) \nU + \nV = \n[\nU1\n, \nU\nz\n, ... , \nU\nn\n]  +  [\nV1\n, \nV\nz\n, ... , \nV\nn\n] \n= \n[\nU1 \n+ \nV1\n, \nU\nz \n+ \nV\nz\n, ... , \nU\nn \n+ \nV\nn\n] \n=  [ v\n1 \n+  u\n1\n, v\n2 \n+ \nu\n2\n, \n•.. \n, v\nn \n+  u\nn\n] \n= [v\n1\n, \nV\nz\n, ... , \nV\nn\n]  +  [\nu\n1\n, \nU\nz\n, ... , \nU\nn\n] \n=\nv+u \nThe second and fourth equalities are by the definition of vector addition, and the \nthird equality is by the commutativity of addition of real numbers. \n(b) Fi  gure 1.18 illustrates associativity in IR\n2\n. Algebraically, we have \n[\n(u\n1 \n+  v\n1\n) \n+  w\n1\n, (u\n2 \n+  v\nz\n) \n+  w\n2\n, ... , (u\nn \n+  v\nn\n) \n+  w\nn\n] \n[u\n1 \n+ \n( v\nl \n+ \nW\n1\n)\n,\nU\n2 \n+ \n(\nV\nz \n+ \nWz\n)\n, ... ,\nU\nn \n+ \n(\nv\nn \n+ \nW\nn\n)\nJ \n[\nu\n1\n,u\n2\n, ... ,u\nn","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":2048,"to":2221}}}}],[108,{"pageContent":"2\n. Algebraically, we have \n[\n(u\n1 \n+  v\n1\n) \n+  w\n1\n, (u\n2 \n+  v\nz\n) \n+  w\n2\n, ... , (u\nn \n+  v\nn\n) \n+  w\nn\n] \n[u\n1 \n+ \n( v\nl \n+ \nW\n1\n)\n,\nU\n2 \n+ \n(\nV\nz \n+ \nWz\n)\n, ... ,\nU\nn \n+ \n(\nv\nn \n+ \nW\nn\n)\nJ \n[\nu\n1\n,u\n2\n, ... ,u\nn\n]  + \n(\n[\nv\n1\n,v\n2\n, ... ,v\nn\n]  + [w\n1\n,w\n2\n, ... ,w\nn\n]\n) \n= \nu  + \n(\nv + w) \nThe fourth equality is by the associativity of addition of real numbers. Note the care­\nful use of parentheses.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":2221,"to":2306}}}}],[109,{"pageContent":"Example 1.5 \nSection 1.1 \nThe Geometry and Algebra of Vectors \n11 \nBy property (b) of Theorem 1.1, we may unambiguously write u + v + wwithout \nparentheses, since we may group the summands in whichever way we please. By (a), \nwe may also rearrange the summands-for example, as w + u + v-if we choose. \nLikewise, sums of four or more vectors can be calculated without regard to order or \ngrouping. In general, ifv\n1\n, v\n2\n, ••. , vk \nare vectors in !R\nn\n, we will write such sums with­\nout parentheses: \nThe next example illustrates the use of Theorem 1.1 in performing algebraic \ncalculations with vectors. \nLet a, b, and x denote vectors in !R\nn\n. \n(a) \nSimplify 3a +  (\nSb -2a)  + 2\n(\nb -a). \n(b) If Sx -  a = 2\n(\na + 2x\n)\n, solve for x in terms of a. \nSolution We will give both solutions in detail, with reference to all of the properties \nin Theorem 1.1 that we use. It is good practice to justify all steps the first few times \nyou do this type of calculation. Once you are comfortable with the vector properties,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":2308,"to":2342}}}}],[110,{"pageContent":"in Theorem 1.1 that we use. It is good practice to justify all steps the first few times \nyou do this type of calculation. Once you are comfortable with the vector properties, \nthough, it is acceptable to leave out some of the intermediate steps to save time and \nspace. \n(a) We begin by inserting parentheses. \n3a + (Sb -2a\n) \n+ 2\n(\nb -  a\n) =  (3a \n+ \n( Sb -2a\n)) \n+ \n2 ( b  -  a\n) \n(\n3a \n+ \n(\n-2a \n+ Sb\n)) \n+ \n(\n2b -2a\n) \n((3a \n+ \n( -2a\n)) \n+ Sb\n) \n+ \n( 2b -2a\n) \n((\n3  + \n(\n-2\n))\na  + Sb\n) \n+ \n( 2b -2a\n) \n(\nla + Sb\n) \n+ \n( 2b -2a\n) \n((\na  + Sb\n) \n+ 2b\n) \n-2a \n( a \n+ \n(\nSb + 2b\n)) \n-2a \n( a \n+ \n(\nS  +  2 ) b\n) \n-2a \n(7b \n+  a\n) \n-2a \n= \n7b + \n( a  -2a) \n= \n7b + \n(\n1 -  2\n) a \n= 7b + (-l) a \n= 7b -  a \n(a), (e) \n(b) \n(f) \n(b ), (h) \n(b) \n(f) \n(a) \n(\nb\n) \n(f), \n(h) \nYou can see why we will agree to omit some of these steps! In practice, it is acceptable to \nsimplify this sequence of steps as \n3a + (Sb -2a) \n+ \n2(b  -a) = 3a + Sb -2a + 2b -2a \n(3a -2a -2a)  + (Sb + 2b\n) \n=\n-a+  7b \nor even to do   most of the calculation mentally.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":2342,"to":2448}}}}],[111,{"pageContent":"12 Chapter 1 Vectors \nExample 1.6 \nExample 1.1 \n(b) In detail, we have \nSx - a = 2\n(\na +   2x\n) \nSx - a =  2a + 2\n(\n2x\n) \n(e) \nSx - a = 2a + \n(\n2 \n· \n2\n)\nx \n(g) \nSx - a= 2a + 4x \n(\nSx - a\n) \n- 4x = \n(\n2a + 4x\n) \n- 4x \n(\n-a +  Sx\n) \n- 4x =  2a + \n(\n4x - 4x\n) \n-a + \n(\nSx - 4x\n) \n=  2a + 0 \n-a + \n(\n5 - 4\n)\nx =   2a \n-a + \n(\nl\n)\nx =   2a \na + \n(\n-a + x\n) \n= a + 2a \n(\na + \n(\n-a\n)) \n+ x = \n(\n1  + 2\n)\na \n0 + x = 3a \nx =   3a \n(a), (b) \n(b) , (d) \n(f), \n(c) \n(h) \n(b ), \n(f) \n( d) \n(c) \nAgain, we will usually omit most of these steps. \nLinear Combinalions and Coordina1es \nA vector that is a sum of scalar multiples of other vectors is said to be a linear combi­\nnation of those vectors. The formal definition follows. \nDefiniliOD \nA vector v is a linear combination of vectors v\n1\n, v\n2\n, ... , vk if \nthere are scalars c\n1\n, c\n2\n, ... , c\nk \nsuch that v = c\n1\nv\n1 \n+ c\n2\nv\n2 \n+ ·   ·   · + c\nk\nv\nk\n. The scalars \nc\n1\n, c\n2\n, ... , c\nk \nare called the coefficients of the linear combination. \nRemark \nDetermining whether a given vector is a linear combination of other","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":2450,"to":2565}}}}],[112,{"pageContent":"1\nv\n1 \n+ c\n2\nv\n2 \n+ ·   ·   · + c\nk\nv\nk\n. The scalars \nc\n1\n, c\n2\n, ... , c\nk \nare called the coefficients of the linear combination. \nRemark \nDetermining whether a given vector is a linear combination of other \nvectors is a problem we will address in Chapter 2. \nIn IR\n2\n, it is possible to depict linear combinations of two (nonparallel) vectors \nquite conveniently. \nLet u = \n[ \n�\n] \nand v = \n[ �\n]\n. We can use u and v to lo  cate a new set of   axes (in the same \nway that e\n1 \n= \n[ \n�\n] \nand e\n2 \n= \n[ \n�\n] \nlocate the standard coordinate axes). We  can use","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":2565,"to":2611}}}}],[113,{"pageContent":"I \nSection 1.1 \nThe Geometry and Algebra of Vectors \n13 \ny \nI \nu \nI-\nI \nI \nFigure 1.19 \nthese new axes to determine a coordinate grid that will let us easily locate linear \ncombinations of u and v. \nAs Figure 1.19 shows, w  can be located by starting at the origin and traveling \n-u followed by 2v. That is, \nw =  -u + 2v \nWe say that the coordinates of w with respect to u and v are -1 and 2. (Note that \nthis is just another way of thinking of the coefficients of the linear combination.) \nIt follows that \n(Observe that -1and 3 are the coordinates ofw with respect to e1 and e\n2\n.) \nSwitching from the standard coordinate axes to alternative ones is a useful idea. It \nhas applications in chemistry and geology, since molecular and crystalline structures \noften do not fall onto a rectangular grid. It is an idea that we will encounter repeatedly \nin this book. \nBinarv  vec1ors and Modular Arilhmelic \nWe will also encounter a type of vector that has no geometric interpretation-at least","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":2613,"to":2640}}}}],[114,{"pageContent":"in this book. \nBinarv  vec1ors and Modular Arilhmelic \nWe will also encounter a type of vector that has no geometric interpretation-at least \nnot using Euclidean geometry. Computers represent data in terms of Os and ls (which \ncan be interpreted as off/on, closed/open, false/true, or no/yes). Binary vectors are \nvectors each of whose components is a 0  or a 1. As we will see in Chapter 8, such \nvectors arise naturally in the study of many types of codes. \nIn this setting, the usual rules of arithmetic must be modified, since the result of \neach calculation involving scalars must be a 0  or a 1. The modified rules for addition \nand multiplication are given below. \nThe only curiosity here is the rule that 1 + 1 = 0. This is not as strange as it appears; \nif we replace 0 with the word \"even\" and 1 with the word \"odd,\" these tables simply","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":2640,"to":2651}}}}],[115,{"pageContent":"14 Chapter 1 Vectors \nExample 1.8 \nWe are using the term length dif­\nfe rently from the way we used it in \n!FR\". This should not be confusing, \nsince there is no geometric notion \nof length for binary vectors. \nExample 1.9 \nExample 1.10 \nExample 1.11 \nsummarize the familiar parity rules for the addition and multiplication of even and \nodd integers. For example, 1 + 1 = 0 exp resses the fact that the sum of two odd inte­\ngers is an even integer. With these rules, our set of sca  lars {O, l} is denoted by 22 and \nis called the set of integers modulo 2. \nIn 22, 1 + 1 + 0 + 1 = 1 and 1 + 1 + 1 + 1 = 0. (These calculations illustrate \nthe parity rules again: The sum of three odds and an even is odd; the sum of four \nodds is even.) \n.+ \nWith 22 as our set of scalars, we now extend the above rules to vectors. The set of \nall n-tuples of Os and ls (with all arithmetic performed modulo 2) is denoted by 2�. \nThe vectors in 2� are called binary vectors of length n.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":2653,"to":2673}}}}],[116,{"pageContent":"all n-tuples of Os and ls (with all arithmetic performed modulo 2) is denoted by 2�. \nThe vectors in 2� are called binary vectors of length n. \nThe vectors in 2� are [ O, O J, [ O, l], [l, O J, and [l, l J. (How many vectors does 2� \ncontain, in general?) \nLet u = [l, 1, 0, 1, O\nJ \nand v = [ O, 1, 1, 1, O J be two binary vectors oflength 5. Find u + v. \nSolulion The calculation of u + v takes place over 22, so we have \nu + v = [1, i, o\n, i\n, o] + [o, I, I, i\n,\no] \n= \n[1 + o, I  + I, o + I, I  + I, o + o] \n= [1,0, 1,0,0\n] \nIt is  possible to generalize what we have just done for binary vectors to vectors whose \ncomponents are taken from a finite set {O, 1, 2, ... , k} fork 2: 2. To do so, we must \nfirst extend the idea of binary arithmetic. \nThe integers modulo 3 is the set 2\n3 \n= {O, 1, 2} with addition and multiplication given \nby the following tables: \n+ 0 \n2 \n0  0  1  2 \n1  2  0 \n2  2  0 \n0  1  2 \n0  0  0  0 \n1  0 \n2 \n2  0  2  1 \nObserve that the result of each addition and multiplication belongs to the set","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":2673,"to":2707}}}}],[117,{"pageContent":"by the following tables: \n+ 0 \n2 \n0  0  1  2 \n1  2  0 \n2  2  0 \n0  1  2 \n0  0  0  0 \n1  0 \n2 \n2  0  2  1 \nObserve that the result of each addition and multiplication belongs to the set \n{O, 1, 2}; we say that 2\n3 \nis closed with respect to the opera  tions of addition and multi­\nplication. It is perhaps easiest to think of this set in terms of a 3-hour clock with 0, 1, \nand 2 on its face, as shown in Figure 1.20. \nThe calculation 1 + 2 = 0 translates as follows:  2  hours after  1  o'clock, it is \n0 o' clock. Just as 24:00 and 12:00 are the same on a 12-hour clock, so 3 and 0 are \nequivalent on this 3-hour clock. Likewise, all multiples of3-positive and negative­\nare equivalent to 0 here; 1 is equivalent to any number that is 1 more than a multiple \nof 3 (such as -2, 4, and 7); and 2 is equivalent to any number that is 2 more than a","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":2707,"to":2728}}}}],[118,{"pageContent":"Example 1.12 \nExample 1.13 \nSection 1.1 \nThe Geometry and Algebra of Vectors \n15 \nmultiple of 3 (such as -1, 5, and 8). We can visualize the number line as wrapping \naround a circle, as shown in Figure 1.21. \n0 \n2 \nFigure 1.20 \nArithmetic modulo 3 \nTo what is 3548 equivalent in Z/ \n... , \n-\n3, 0, 3,  ... \n... ,  1, 2, 5, ... \n.  .. , \n-\n2, 1, 4, ... \nFigure 1.21 \nSolution This is the same as asking where 3548 lies on our 3-hour clock. The key is \nto calculate how far this number is from the nearest  (smaller) multiple of 3; that is, \nwe need to know the remainder when 3548 is divided by 3. By  long division, we find that \n3548 = 3 · 1182 + 2,   so the remainder is 2. Therefore, 3548 is equivalent to 2 in l'..\n3\n• 4 \nIn courses in abstract algebra and number theory, which explore this concept in \ngreater detail, the above equivalence is often written as 3548 = 2   (mod 3) or 3548 \n= \n2 \n(mod 3), where\n= \nis read \"is congruent to.\" We will not use this notation or termi­\nnology here. \nIn l'..\n3\n, calculate 2 + 2 + 1 + 2.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":2730,"to":2766}}}}],[119,{"pageContent":"= \n2 \n(mod 3), where\n= \nis read \"is congruent to.\" We will not use this notation or termi­\nnology here. \nIn l'..\n3\n, calculate 2 + 2 + 1 + 2. \nSolution 1 We use the same ideas as in  Example 1.12. The ordinary sum is 2 + 2 + \n1 + 2 = 7, which is 1 more than 6, so division by 3 leaves a remainder of 1. Thus, 2 + \n2 + 1 + 2 = 1 in l'..\n3\n. \nSolution 2 \nA better way to perform this calculation is to do it step by step entirely in l'..\n3\n• \n2 + 2 +  1  + 2 = \n(\n2  + 2\n) \n+  1  + 2 \n=1+1+2 \n= \n(\n1  +  1\n) \n+ 2 \n= 2 + 2 \n=  1 \nHere we have used parentheses to group the terms we have chosen to combine. We could \nspeed things up by simultaneously combining the first two and the last two terms: \n(\n2 + 2\n) \n+ \n(\n1  + 2\n) \n=  1  + 0 \n=  1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":2766,"to":2807}}}}],[120,{"pageContent":"16 \nChapter 1 Vectors \nRepeated multiplication can be ha  ndled similarly. The idea is to use the addition and \nmultiplication tables to reduce the result of each calculation to 0, 1, or 2. \n4 \nExtending these ideas to vectors is straightforward. \nExample 1.14 In Z�, let u = [2, 2, 0, 1, 2] and v = [l, 2, 2, 2, l]. Then \nm -1 \n0 \n_.,---.\n_ \nm -2 \n2 \n3 \nu + v = [ 2, 2, 0, 1,2] + \n[ 1,2,2,2, l] \n[2 + 1,2  + 2,0+2,1 + 2,2  + l] \n[0,  1,2 ,0,0] \nVectors in Z� are referred to as ternary vectors of length 5. \nFigure 1.22 \nArithmetic modulo m \nIn general, we have the set Z\nm \n= {O, 1, 2, ... , m  -l} of integers modulo m (cor­\nresponding to an m-hour clock, as shown in Figure 1.22\n)\n. A vector oflength n whose \nentries are in Z\nm \nis called an m-ary vector of length n. The set of all m-ary vectors of \nlength n is denoted by z::i. \n.. \nI \nExercises 1.1 \n1. Draw the following vectors in standard position \nin IR\n2\n: \n(a)  a= \n[\n3\n0\n] \n(b)  b = \n[\n2\n3\n] \n(d)  d = \n[ \n3\n] \n-2 \n2. Draw the vectors in Exercise 1 with their tails at the \npoint \n(\n2, -","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":2809,"to":2865}}}}],[121,{"pageContent":".. \nI \nExercises 1.1 \n1. Draw the following vectors in standard position \nin IR\n2\n: \n(a)  a= \n[\n3\n0\n] \n(b)  b = \n[\n2\n3\n] \n(d)  d = \n[ \n3\n] \n-2 \n2. Draw the vectors in Exercise 1 with their tails at the \npoint \n(\n2, -\n3). \n3. Draw the following vectors in standard position in IR\n3\n: \n(a)  a= [ O, 2, O J \n(b)  b = [3, 2, l] \n(c)  c = [l, -2, l] \n(d) d = [-1, -1, -2] \n4. If the vectors in Exercise 3 are translated so that their \nheads are at the point (3, 2, 1\n)\n, find the points that \ncorrespond to their tails. \n5. For each of the following pairs of points, draw the \n----> ----> \nvector AB. Then compute and redraw AB as a vector \nin standard position. \n(a) A= \n(\n1, -1\n)\n, B =   (4, 2\n) \n(b) A= \n(\nO, -2\n)\n, B = \n(\n2, -1\n) \n(c) A = (2, \nf) , \nB = \n(\nt, \n3\n) \n(d) A = \n(t\n, \nt\n), \nB = \n(i, \nt\n) \n6. A hiker walks 4 km north and then 5 km  northeast. \nDraw displacement vectors representing the hiker's \ntrip and draw a vector that represents the hiker's net \ndisplacement from the starting point. \nExercises 7-10 refer to the vectors in Exercise 1. Compute","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":2865,"to":2942}}}}],[122,{"pageContent":"trip and draw a vector that represents the hiker's net \ndisplacement from the starting point. \nExercises 7-10 refer to the vectors in Exercise 1. Compute \nthe indicated vectors and also show how th e resu lts can be \nobtained geometrically. \n7. a+ b \n8. b -  c \n9. d -c \n10. a + d \nExercises 11 and 12 refer to the vectors in Exercise 3. \nCompute the indicated vectors. \n11. 2a + 3c \n12. 3b -2c + d \n13. Find the components of the vectors u, v, u + v, and \nu -v, where u and v are as shown in Figure 1.23. \n14. In Figure 1.24, A, B, C, D, E, and Fare the vertices of a \nregular hexagon centered at the origin. \nExpress each of the following vectors in terms of \n----> ----> \na= OA  and b = OB: \n(a) Ai \n(c) Ai5 \n(e)  xc \n(b) BC \n(d) a \n(f) BC + ill \n+ \nPX","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":2942,"to":2969}}}}],[123,{"pageContent":"Figure 1.23 \nc \nE \nFigure 1.24 \ny \ny \nB \nF \nIn Exercises 15 and 16, simplify the given vector expression. \nIndicate which properties in Theorem 1.1 you use. \n15. \n2(a -  3b) + 3(2b +a) \n16. \n-\n3(a - c) + 2(a + 2b) + \n3(c -\nb) \nIn Exercises 17 and 18, solve for th e vector x in terms of the \nvectors a and b. \n17. x -  a= 2(x - 2a) \n18. x +  2a - b = 3(x +a) -  2(2a - b) \nIn Exercises 19 and 20, draw the coordinate axes relative to \nu and v and locate w. \n19.\nu  = \n[ \n_\n�\n]\n,v \n= \n[\n�\nl\nw \n= \n2\nu \n+ \n3\nv \n20. \nu  = \n[ \n-\n� \nl \nv = \n[ \n_ \n� \nl \nw = \n- u  -\n2\nv \nIn Exercises 21 and 22, draw the standard coordinate axes \non th e same diagram as th e axes relative to u and v.  Use \nthese to find was a linear combination of u and v. \nSection 1.1 \nThe Geometry and Algebra of Vectors \n11 \n21. u \n= \n[ \n-\n�\n]\n, \nv \n= \n[ \n�\n]\n, \nw \n= \n[ \n�\n] \n22. \nu \n= \n[ \n-\n�\n]\n, \nv \n= \n[ \n�\n]\n, \nw \n= \n[ \n�\n] \n23. Draw diagrams to illustrate properties (d) and (e) of \nTheorem 1.1. \n24. Give algebraic proofs of properties ( d) through (g) of \nTheorem 1.1.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":2971,"to":3073}}}}],[124,{"pageContent":"]\n, \nv \n= \n[ \n�\n]\n, \nw \n= \n[ \n�\n] \n22. \nu \n= \n[ \n-\n�\n]\n, \nv \n= \n[ \n�\n]\n, \nw \n= \n[ \n�\n] \n23. Draw diagrams to illustrate properties (d) and (e) of \nTheorem 1.1. \n24. Give algebraic proofs of properties ( d) through (g) of \nTheorem 1.1. \nIn Exercises 25-28, u and v are binary vectors. Find u + v \nin each case. \n25.u  = \n[\n�\n]\n,v = \n[\n�\n] \n27.u\n= \n[\n1,0 ,1,1],v\n= \n[\n1,1,1 ,1] \n28. u \n= \n[ 1, 1,0, 1,0],v \n= \n[ O, 1, 1, 1,0] \n29. Write out the addition and multiplication tables for Z\n4\n. \n30. Write out the addition and multiplication tables for Zs. \nIn Exercises 31-43, perform th e indicated calculations. \n31. 2  + 2 + 2 in Z\n3 \n32. 2 · 2 · 2 in Z\n3 \n33. 2\n(\n2  + 1 + 2\n) \nin Z\n3 \n34. 3 + 1 + 2 + 3 in Z\n4 \n35. 2 \n· \n3 \n· \n2 in Z\n4 \n36. 3\n(\n3 + 3 + 2\n) \nin Z\n4 \n37. 2 + 1 + 2 + 2 + 1 in Z\n3\n, Z\n4\n, and  Zs \n38. \n(\n3 + 4\n)(\n3 + 2 + 4 + 2\n) \nin Zs \n39. 8\n(\n6 + 4 +  3\n) \nin Z\n9 \n40. \n2\n100 \nin Z\n11 \n41. [2, 1,2] + [2, 0, 1] in Z� \n42. 2[2, 2, 1] in Z� \n43. 2\n(\n[3, l, 1,2] +  [3, 3, 2, l]\n)\nin Z!and Z� \nIn Exercises 44-55, solve the given equation or indicate that \nthere is no solution. \n44. x + 3 = 2 in Zs","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":3073,"to":3192}}}}],[125,{"pageContent":"42. 2[2, 2, 1] in Z� \n43. 2\n(\n[3, l, 1,2] +  [3, 3, 2, l]\n)\nin Z!and Z� \nIn Exercises 44-55, solve the given equation or indicate that \nthere is no solution. \n44. x + 3 = 2 in Zs \n46.  2x =  1 in Z\n3 \n48. 2x = 1 in Zs \n50. 3x = 4 in Z\n6 \n52. Bx = 9 in Z\n11 \n54. 4x + 5 = 2 in Z\n6 \n45. \nx +  5 =  1 in Z\n6 \n47. 2x = 1 in Z\n4 \n49. 3x = 4 in Zs \n51. 6x = 5 in Zs \n53. 2x + 3 = 2 in Z\n5 \n55. \n6x + 3 =  1 in Zs \n56. (a)  For which values of a does x + a = 0 have a solu­\ntion in Zs? \n(b) For which values of a and b does x + a =  b have a \nsolution in Z6? \n(c) For which values of a, b, and m does x + a =  b \nhave a solution in Z\nm\n? \n57. (a)  For which values of a does ax =  1 have a solution \nin Zs? \n(b) \nFor which values of a does ax =  1 have a solution \nin Z6? \n(c) For which values of a and m does ax =  1 have a \nsolution in z\nm\n?","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":3192,"to":3237}}}}],[126,{"pageContent":"18 Chapter 1 Vectors \nExample 1.15 \nLength and Angle: The Dot Product \nIt is quite  easy to reformulate the familiar geometric concepts of length, distance, \nand angle in terms of vectors. Doing so will allow us to use these important and \npowerful ideas in settings more general than IR\n2 \nand IR\n3\n• In subsequent chapters, \nthese simple geometric tools will be used to solve a wide variety of problems arising \nin applications-even when there is no geometry apparent at all! \nThe Doi Producl \nThe vector versions of length, distance, and angle can all be described using the \nnotion of the dot product of two vectors. \nDefinilion If \nthen the dot product u · v of u and v is defined by \nIn words, u · v is the sum of the products of the corresponding components of u \nand v. It is important to note a couple of things about this \"product\" that we have just \ndefined: First, u and v must have the same number of components. Second, the dot","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":3239,"to":3258}}}}],[127,{"pageContent":"and v. It is important to note a couple of things about this \"product\" that we have just \ndefined: First, u and v must have the same number of components. Second, the dot \nproduct u · v is a number, not another vector. (This is why u · v is sometimes called \nthe scalar product of u and v.\n) \nThe dot product of vectors in !R\nn \nis a special and im­\nportant case of the more general notion of inner product, which we will explore in \nChapter 7. \nSolution \nu·v= l·(-3) \n+ \n2·5\n+\n(-3)  ·2=1 \nNotice that if we had calculated v · u in Example 1.15, we would have computed \nv·u = (-3)  ·1+5·2 + \n2·(-3)  =  1 \nThat u · v \n= \nv · u in general is clear, since the individual products of the components \ncommute. This commutativity property is one of the properties of the dot product \nthat we will use repeatedly. The main properties of the dot product are summarized \nin Theorem 1.2.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":3258,"to":3282}}}}],[128,{"pageContent":"Theorem 1.2 \nSection 1.2 \nLength and Angle: The Dot Product \n19 \nLet u, v, and w be vectors in �\nn \nand let c be a scalar. Then \na. u·v = v·u \nb. u\n. ( \nv + w\n)  =  u\n. \nv + u\n. \nw \nCommutativity \nDistributivity \nc. ( cu\n)  . \nv =  c\n( \nu \n. \nv\n) \nd. u · u 2:  0 and u · u  =  0 if and only if u  = 0 \nProof We prove (a) and  (c) and leave proof of the remaining properties for the \nexercises. \n(a)  Applying the definition of dot product to u · v and \nv · u, \nwe obtain \nu·v \n= \nU\n1\nV\n1 \n+ \nU\nz\nV\nz \n+ .. ·+ \nU\nn\nV\nn \n= \nV\n1\nU\n1 \n+ \nV\nz\nU\nz \n+ ... + \nV\nn\nU\nn \n= v·u \nwhere the middle equality follows from the fact that multiplication of real numbers \nis commutative. \n(c)  Using the definitions of scalar multiplication and dot product, we have \nRemarlls \n(\ncu\n)\n·v\n= [cu\n1\n,cu\n2\n, ... ,cu\n\"\n] \n• \n[v\n1\n,v\n2\n, \n•.. \n,v\n\"\n] \nCU\n1\nV\n1 \n+ \nC\nU\nz\nV\n2 \n+ ·   ·   · + \nCU\nn\nV\nn \nc\n(\nU\n1\nV\n1 \n+ \nU\nz\nV\nz \n+ ·   ·   · + \nU\nn\nV\nn\n) \nc\n(\nu · v\n) \n• \nProperty (b) can be read from right to left, in which case it says that we  can \nfactor out a common vector u from a sum of dot products. This property also has","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":3284,"to":3411}}}}],[129,{"pageContent":"1 \n+ \nU\nz\nV\nz \n+ ·   ·   · + \nU\nn\nV\nn\n) \nc\n(\nu · v\n) \n• \nProperty (b) can be read from right to left, in which case it says that we  can \nfactor out a common vector u from a sum of dot products. This property also has \na  \"right-handed\"  analogue  that  follows from properties (b)  and  (a)  together: \n(\nv + w\n) · u   = \nv  ·  u + w  · u. \n• \nProperty (c) can be extended to give u · (cv)  =  c\n(\nu  ·  v\n) \n(Exercise 58). This \nextended version of \n( \nc) essentially says that in taking a scalar multiple of a dot product \nof vectors, the scalar can first be combined with whichever vector is more convenient. \nFor example, \n(\n�\n[-1,  -3,2]\n)\n·[6,-4, 0] = [-1,-3,2]·\n(\n�\n[6,-4, 0 ]\n)= \n[-1,  -3,2]·[3,-2,  0] =3 \nWith this approach we avoid introducing fractions into the vectors, as the original \ngrouping would have. \n• \nThe second part of ( d) uses the logical connective if and only if. Appendix A dis­\ncusses this phrase in more detail, but for the moment let us   just note that the wording \nsignals a dou ble implication-namely,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":3411,"to":3461}}}}],[130,{"pageContent":"cusses this phrase in more detail, but for the moment let us   just note that the wording \nsignals a dou ble implication-namely, \nif u  = 0, then u · u \n=  0 \nand \nif u · u  = 0, then u \n= 0 \nTheorem 1.2 shows that aspects of the algebra of vectors resemble the algebra of \nnumbers. The next example shows that we can sometimes find vector analogues of \nfamiliar identities.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":3461,"to":3470}}}}],[131,{"pageContent":"20 Chapter 1 Vectors \nExample 1.16 \ny \nb \nv  = [�] \na \nFigure 1.25 \nExample 1.11 \nTheorem 1.3 \nProve that (\nu  +  v\n) \n· \n(\nu  +  v\n)  =  u · u  +  2 (\nu · v\n) \n+  v · v for all vectors u and v in !R\nn\n. \nSolulion \n(\nu  +  v\n) \n· \n(\nu  +  v\n)  =  (\nu  +  v\n) \n· u  + \n(\nu  +  v\n) \n· v \n=u·u+v·u+u·v+v·v \n=u·u+u·v+u·v+v·v \n= \nu·u + \n2 (\nu·v\n) \n+ v·v \n(Identify the parts of Theorem 1.2 that were used at each step.) \nLength \nTo see how the dot product plays a role in the calculation oflengths, recall how lengths \nare computed in the plane. The Theorem of Pythagoras is all we need. \nIn IR\n2\n, the length of the  vector v = \n[ \n�\n] \nis the distance from the origin to the point \n(\na, b\n)\n, which, by Pythagoras' Theorem, is given by \nV\na\n2 \n+ \nb\n2\n, as in Figure 1.25. \nObserve that a\n2 \n+  b\n2 \n= \nv · v. This leads to the following definition. \nDefinition \nThe length (or norm) of a vector v = \n[ \n�:\n] \nin !R\nn \nis the nonnega-\ntive scalar \nll\nv\nll \ndefined by \n· \nv\nn \nll\nv\nll \n= \nVV:V \n= \nV \nv\ni \n+  v\n� \n+ \n·   ·   · \n+   v\n�","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":3472,"to":3574}}}}],[132,{"pageContent":"Definition \nThe length (or norm) of a vector v = \n[ \n�:\n] \nin !R\nn \nis the nonnega-\ntive scalar \nll\nv\nll \ndefined by \n· \nv\nn \nll\nv\nll \n= \nVV:V \n= \nV \nv\ni \n+  v\n� \n+ \n·   ·   · \n+   v\n� \nIn words, the  length of a vector is the square root of the sum of the squares of its \ncomponents. Note that the  square root of v · v is always defined, since v · v \n2': \n0 by \nTheorem l.2(d). Note also that the  definition can be rewritten to give \nll\nv\nll\n2 \n= \nv·v, \nwhich will be useful in proving further properties of the dot prod uct and lengths of \nvectors. \n11\n[2, 3J\n11 \n= \nv\n2\n2 \n+ \n3\n2 \n= \nvT3 \nTheorem 1.3 lists some of the main properties of vector length. \nLet v be a vector in IR\n\" \nand let c be a scalar. Then \na. \nll\nv\nll \n=  0 if and only ifv = 0 \nb. \nll\ncv\nll \n= \nl\nc\nl \nll\nv\nll \nProof \nProperty (a) follows immediately from Theorem l.2(d). To show (b), we have \nll\ncv\nll\n2 \n= (cv)·(cv)  =  c\n2\n(\nv·v\n)  =  c\n2\nll\nv\nll\n2 \nusing Theorem l.2(c). Taking square roots ofboth sides, using the factthat\nW \n= \nI \nc\nl \nfor any real number c, gives the result.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":3574,"to":3672}}}}],[133,{"pageContent":"Example 1.18 \nSection 1.2 \nLength and Angle: The Dot Product 21 \nA vector of length 1 is called a unit vector. In IR\n2\n, the set of all unit vectors can \nbe identified with the unit circle, the circle of radius 1 centered at the origin (see \nFigure 1.26\n)\n. Given any nonzero vector v, we can always find a unit vector in the \nsame direction as v by dividing v by its own length (or, equivalently, multiplying by \n1/ \nll\nv\nll \n)\n. We can show this algebraically by using property (b) of Theorem 1.3 above: \nIfu = (1/\nll\nv\nll\n)\nv, then \nll\nu\nll \n= \n11\n(\n1/\nll\nv\nll\n)\nv\nll \n= \nl\nl/\nll\nv\nll \nI \nll\nv\nll \n= (1\n/\nll\nv\nll\n)\nll\nv\nll \n= 1 \nand u is in the same direction as v, since 1 / II v II is a positive scalar. Finding a unit vec­\ntor in the same direction is often referred to as normalizing a vector (see Figure 1.27\n)\n. \ny \nFigure 1.26 \nUnit vectors in lffi2 \n>:;:. / \n� \nrr�1\nr \nFigure 1.27 \nNormalizing a vector \nv \nIn IR\n2\n, let e1 = \n[ \n�\n] \nand e\n2 \n= \n[ \n�\n]\n. Then e1 and e\n2 \nare unit vectors, since the sum of the","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":3674,"to":3758}}}}],[134,{"pageContent":")\n. \ny \nFigure 1.26 \nUnit vectors in lffi2 \n>:;:. / \n� \nrr�1\nr \nFigure 1.27 \nNormalizing a vector \nv \nIn IR\n2\n, let e1 = \n[ \n�\n] \nand e\n2 \n= \n[ \n�\n]\n. Then e1 and e\n2 \nare unit vectors, since the sum of the \nsquares of their components is 1 in each case. Similarly, in IR\n3\n, we can construct unit \nvectors \nObserve in Figure 1.28 that these vectors serve to locate the positive coordinate axes \nin IR\n2 \nand IR\n3\n. \nt \ny \nFigure 1.28 \nStandard unit vectors in lffi2 and !ffi3 \nx \nz \n.--+ \ny","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":3758,"to":3802}}}}],[135,{"pageContent":"22 \nChapter 1 Vectors \nExample 1.19 \nTheorem 1.4 \n-\nu \nFigure 1.29 \nThe Triangle Inequality \nTheorem 1.5 \nIn general, in !R\nn\n, we define unit vectors e\n1 , e\n2\n, •.. , e\nn\n, where e; has 1 in its   ith \ncomponent and zeros elsewhere. These vectors arise repeatedly in linear algebra and \nare called the standard unit vectors. \nNmmalfae the vectm v \n� \n[ -n \nSolulion \nll\nv\nll \n= \nV \n2\n2 \n+ \n( -1\n) \n2 \n+  3\n2 \n= \n\\/14\n, so a unit vector in the same direc­\ntion as v is given by \nu \n� (1\n/\nll\nv\nll\nl\nv \n� (\n1/\nv'14\n{\n-\n:\nJ \n[ \n2/\n\\/14\n] \n-1/\n\\/14 \n3/\n\\/14 \nSince property (b) of Theorem 1.3 describes how length behaves with respect to \nscalar multiplication, natural curiosity suggests that we ask whether length and vec­\ntor addition are compatible. It would be nice if we had an identity such as \nII u  +  v II \n= \nll\nu \n11 \n+ \nll\nv\nll\n, \nbut for almost any choice of vectors u and vthis turns out to be false. [See \nExercise 52(a).] However, all is not lost, for it turns out that if we replace the = sign by \n:s","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":3804,"to":3882}}}}],[136,{"pageContent":"= \nll\nu \n11 \n+ \nll\nv\nll\n, \nbut for almost any choice of vectors u and vthis turns out to be false. [See \nExercise 52(a).] However, all is not lost, for it turns out that if we replace the = sign by \n:s\n, the resulting inequality is true. The proof of this famous and important result-the \nTriangle Inequality-relies on another important inequality-the Cauchy-Schwarz \nInequality-which we will prove and discuss in more detail in Chapter 7. \nThe Cauchy-Schwarz Inequality \nFor all vectors u and v in !R\nn\n, \nl\nu\n·v\nl \n:s \nll\nn\nll ll\nv\nll \nSee Exercises 71 and 72 for algebraic and geometric approaches to the proof of this \ninequality. \nIn IR\n2 \nor IR\n3\n, where we   can use geometry,  it is clear from a diagram such as \nFigure 1.29 that \nll\nu +v\nii \n:s \nll\nn\nll \n+ \nll\nv\nll \nfor all vectors u and v. We now show that \nthis is true more generally. \nThe Triangle Inequality \nFor all vectors u and v in !R\nn\n, \nll\nu +v\nii \n:s \nll\nu\nll \n+ \nll\nv\nll","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":3882,"to":3946}}}}],[137,{"pageContent":"Section 1.\n2 \nLength and Angle: The Dot Product \n23 \nProof Since both sides of the inequality are nonnegative, showing that the square of \nthe left-hand side is less than or equal to the square of the right-hand side is equiva­\n� \nlent to proving the theorem. (Why?) We compute \nas required. \nDistance \nll\nu + v\nll\n2 \n= \n(\nu + v\n)\n- (\nu + v\n) \n= u·u + 2(u·v\n) \n+ v·v By Example 1.9 \n::; \nll\nu\nll\n2 \n+ 2\nl\nu·v\nl \n+ \nll\nv\nll\n2 \n::; \nll\nn\nll\n2 \n+ 2 \nll\nn\nll ll\nv\nll \n+ \nll\nv\nll\n2 \nBy Cauchy-Schwarz \n= \n( \nll\nn\nll \n+ \nll\nv\nll\n)\n2 \nThe distance between two vectors is the direct analogue of the distance between two \npoints on the real number line or two points in the Cartesian plane. On the number \nline (Figure 1.30), the distance between the numbers a and bis given by \nl\na -  bl. (Tak­\ning the absolute value ensures that we do not need to know which of a or b is larger.) \nThis distance is also equal to \nV \n(\na - b\n) \n2\n, and its two-dimensional generalization is \nthe familiar formula for the distanced between points (a\n1\n, a\n2\n) and  (b\n1\n, b\n2\n)-namely, \nd=\nV\n(\na\n1 \n- b\n1\n)\n2 \n+ \n(\na\n2 \n- bi\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":3948,"to":4050}}}}],[138,{"pageContent":"This distance is also equal to \nV \n(\na - b\n) \n2\n, and its two-dimensional generalization is \nthe familiar formula for the distanced between points (a\n1\n, a\n2\n) and  (b\n1\n, b\n2\n)-namely, \nd=\nV\n(\na\n1 \n- b\n1\n)\n2 \n+ \n(\na\n2 \n- bi\n)\n2\n• \na \n4 I   I   I + \n-\n2 \nFigure 1.30 \n0 \nb \nI   I +   I   I � \n3 \nd \n= \nl\na \n-\nb\nl \n= \nl\n-\n2 \n-\n31 \n= \n5 \nIn terms of vectors, if a = \n[ \n:\n: \n] \nand b = \n[ \n�\n: ], \nthen d is just the length of a -b, \nas shown in Figure 1.31. This is the  basis for the next definition. \nI \nI \nI \nd \nI \n: \na\n2 -\nb\n2 \nI \nI \nI \nI \n___________ _f] \na\n1 -\nb\n1 \nFigure 1.31 \nd \n= \nV\n(\na\n, \n-\nb,)2 \n+ \n(\na\n2 \n-\nb\n2\n)2 \n= \nI\nl\na -  b\nll \nDefinition \nThe distance \nd(u, v\n) \nbetween vectors u and v in u;g\nn \nis defined by \nd \n( \nU, \nv\n) \n= \nII \nu  -  v \nII","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":4050,"to":4174}}}}],[139,{"pageContent":"24 \nChapter 1 Vectors \nExample 1.20 \nv \nFigure 1.33 \nExample 1.21 \nFind the di,tance between u \n� \n[ 1] •nd v \n� \n[ \n_ \n� \nl \n· \nSolution We rnmpute u - v \n� \n[ �J '\" \nd(\nu,v\n) \n= \nll\nu -v\nii\n= \nV\n(\n\\/2\n)\n2 \n+ \n(\n-1\n)\n2 \n+ \n1\n2 \n= V4 = \n2 \nAngles \nThe dot product can also be used to calculate the angle between a pair of vectors. \nIn IR\n2 \nor IR\n3\n, the angle between the nonzero vectors u and v will refer to the angle (} \ndetermined by these vectors that satisfies 0 :::::: (} :::::: 180° (see Figure 1.3 2). \nv \nv\n'D \n� \nu \nu \n(} \n.\n/7 \nu \n0 \nv 4 \nu \nFigure 1.32 \nThe angle between u and v \nIn Figure 1.33, consider the triangle with sides u, v, and u - v, where(} is the angle \nbetween u and v. Applying the law of cosines to this triangle yields \nll\nu - v\nll\n2 \n= \nll\nu\nll\n2 \n+ \nll\nv\nll\n2 \n-\n2 \nll\nu\nll ll\nv\nll \ncos (} \nExpanding the left-hand side and using \nll\nv\nll\n2 \n= v · v   several times, we obtain \nll\nu\nll\n2 \n- 2\n(\nu·v\n) \n+ \nll\nv\nll\n2 \n= \nll\nu\nll\n2 \n+ \nll\nv\nll\n2 \n- 2\nll\nu\nll ll\nv\nll \ncos (} \nwhich, after simplification, leaves us with u · v = \nll\nu\nll ll\nv\nll \ncos (}. From this we obtain","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":4176,"to":4306}}}}],[140,{"pageContent":"ll\nv\nll\n2 \n= v · v   several times, we obtain \nll\nu\nll\n2 \n- 2\n(\nu·v\n) \n+ \nll\nv\nll\n2 \n= \nll\nu\nll\n2 \n+ \nll\nv\nll\n2 \n- 2\nll\nu\nll ll\nv\nll \ncos (} \nwhich, after simplification, leaves us with u · v = \nll\nu\nll ll\nv\nll \ncos (}. From this we obtain \nthe following formula for the cosine of the angle (} between nonzero vectors u and v. \nWe state it as   a definition. \nDefinition \nFor nonzero vectors u and v in !R\nn\n, \nu·v \ncos (} = \nll\nu\nll ll\nv\nll \nCompute the angle between the vectors u = \n[2, 1, -2] and v = [1, 1, 1].","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":4306,"to":4362}}}}],[141,{"pageContent":"Example 1.22 \nSection 1.2 \nLength and Angle: The Dot Product \n25 \nSolution We  calculate u·v = 2·1 + l·l + \n(\n-2\n)\n· 1=1, \nJJ\nu\nJJ \n= \nV\n2\n2 \n+ 1\n2 \n+\n(\n-2\n)\n2 \n= \nV9 =  3,  and \nJJ\nv\nJJ \n= Vl\n2 \n+  1\n2 \n+  1\n2 \n=  v3. Therefore, cos e =  1/3v3,  so \ne = cos\n-\n1\n(\n1\n/3v3\n) \n= 1.377 radians, or 78.9°. \n...._+ \nCompute the angle between the diagonals on two adjacent faces of a cube. \nSolution The dimensions of the cube do not matter, so we will work with a cube \nwith sides oflength 1. Orient the cube relative to the coordinate axes in IR\n3\n, as shown \nin Figure 1.34, and take the two side diagonals to be the vectors [1, 0, 1] and [O, 1, 1]. \nThen angle e between these vectors satisfies \nl·O +O·l +l·l \ncos \ne \n= \n-------\n\\/2 v'2 \n2 \nfrom which it follows that the  required angle is n \n/\n3 radians, or 60°. \nz \n[\nO\n, \n1, l\n] \ny \nx \nFigure 1.34 \n(Actually, we don't need to do  any calculations at all to get this answer. If we  draw \na third side diagonal joining the vertices at (1, 0, 1) and (O, 1, 1), we get an equilateral","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":4364,"to":4435}}}}],[142,{"pageContent":"] \ny \nx \nFigure 1.34 \n(Actually, we don't need to do  any calculations at all to get this answer. If we  draw \na third side diagonal joining the vertices at (1, 0, 1) and (O, 1, 1), we get an equilateral \ntriangle, since all of the side diagonals are of equal length. The angle we want is one of \nthe angles of this triangle and therefore measures 60°. Sometimes, a little insight can \nsave a lot of calculation; in this case, it gives a nice check on our work!) \nRemarks \n• \nAs this discussion shows, we usually will have to settle for an approximation \nto the angle between two vectors. However, when the angle is one of the so-called \nspecial angles (0°, 30°, 45°, 60°, 90°, or an integer multiple of the  se), we should be able \nto recognize its cosine (Table 1.1) and thus give the corresponding angle exactly. In \nall other cases, we will use a calculator or computer to approximate the desired angle \nby means of the inverse cosine function. \ne \nTable 1.1 \ncosines or Special Angles \n30° 45° \ncos e \nV4 \n-=1 \n2 \nv3 \n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":4435,"to":4461}}}}],[143,{"pageContent":"all other cases, we will use a calculator or computer to approximate the desired angle \nby means of the inverse cosine function. \ne \nTable 1.1 \ncosines or Special Angles \n30° 45° \ncos e \nV4 \n-=1 \n2 \nv3 \n2 \nv'2 \n1 \n2 \nv'2 \n60° \nVi \n2    2 \n90° \nVo \n-=O \n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":4461,"to":4483}}}}],[144,{"pageContent":"26 \nChapter 1 Vectors \nThe word orthogonal is derived \nfrom the Greek words orthos, mean­\ning \"upright;' and gonia, meaning \n\"angle:' Hence, orthogonal literally \nmeans \"right-angled'.' The Latin \nequivalent is rectangular. \nExample 1.23 \nTheorem 1.6 \nFigure 1.35 \n• \nThe derivation of the formula for the cosine of the angle between two vectors \nis valid only in IR\n2 \nor IR\n3\n, since it depends on a geometric fact: the law of cosines. In \nIR \nn\n, for n > 3, the formula can be taken as a definition instead. This makes sense, since \nI \nu·v \nI \nu·v \nthe Cauc�y-Schwarz !�equality �mplies that \nll\nu \nII \nll\nv\nll \n:::::: \n1, so \nll\nu \nII \nll\nv\nll \nranges from \n-1 to 1, \nJ\nUSt as the cosme funct10n does. \nOrthogonal Veclors \nThe concept  of perpendicularity  is fundamental  to geometry.  Anyone studying \ngeometry quickly realizes the importance and usefulness of right angles. We now gen­\neralize the idea of perpendicularity to vectors in !R\nn\n, where it is called orthogonality. \nIn IR\n2 \nor IR\n3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":4485,"to":4538}}}}],[145,{"pageContent":"geometry quickly realizes the importance and usefulness of right angles. We now gen­\neralize the idea of perpendicularity to vectors in !R\nn\n, where it is called orthogonality. \nIn IR\n2 \nor IR\n3\n, two nonzero vectors u and v are perpendicular if the angle (J between \nu·v \nthem is a right angle-that is, if (J =  1T \n/\n2 radians, or   90°. Thus, \nll\nu \nII \nll\nv\nll \n= cos 90° = 0, \nand it follows that u · v = 0. This motivates the following definition. \nDefiniliOD \nTwo vectors u and v in !R\nn \nare orthogonal to each other if u ·v = 0. \nSince 0 · v = 0 for every vector v in  !R\nn\n, the zero vector is orthogonal to every \nvector. \nIn IR\n3\n, u = [l, 1, -2] and v = [3, 1, 2] are orthogonal, since u · v \n3 + 1 - 4 = 0. \nUsing the notion of orthogonality, we get an easy proof of Pythagoras' Theorem, \nvalid in !R\nn\n. \nPythagoras' 1heoreITI \nFor all vectorsuandvinlR\nn\n, \nll\nu \n+ \nv\nll\n2 \n= \nll\nu\nll\n2 \n+ \nll\nv\nll\n2\nifand\nonly\nifua\nndva\nre \northogonal. \nProof From Example 1.16, we have \nll\nu \n+ \nv\nll\n2 \n= \nll\nu\nll\n2 \n+ \n2\n(\nu·v\n) \n+ \nll\nv\nll\n2 \nfor  all \nvectors u and v in !R\nn","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":4538,"to":4625}}}}],[146,{"pageContent":"For all vectorsuandvinlR\nn\n, \nll\nu \n+ \nv\nll\n2 \n= \nll\nu\nll\n2 \n+ \nll\nv\nll\n2\nifand\nonly\nifua\nndva\nre \northogonal. \nProof From Example 1.16, we have \nll\nu \n+ \nv\nll\n2 \n= \nll\nu\nll\n2 \n+ \n2\n(\nu·v\n) \n+ \nll\nv\nll\n2 \nfor  all \nvectors u and v in !R\nn\n. It follows immediately that \nll\nu + v\nll\n2 \n= \nll\nu\nll\n2 \n+ \nll\nv\nll\n2 \nif and \nonly if u · v = 0. See Figure 1.35. \nThe concept of orthogonality is one of the most important and useful in linear \nalgebra, and it often arises in surprising ways. Chapter 5 contains a detailed treatment \nof the topic, but we will encounter it many times before then. One problem in which \nit clearly plays a role is finding the distance from a point to a line, where \"dropping a \nperpendicular\" is a familiar step.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":4625,"to":4696}}}}],[147,{"pageContent":"_..,.,.--\nu \nProieclions \nSection 1.2 \nLength and Angle: The Dot Product 21 \nWe now consider the problem of finding the distance from a point to a line in the \ncontext of vectors. As you will see, this technique leads  to an important concept: the \nprojection of a vector onto another vector. \nAs Figure 1.36 shows, the problem of finding the distance from a point B to a \nline € (in IR\n2 \nor IR\n3\n) reduces to the problem of finding the length of the perpe  ndicular \nline segment PB or, equivalently, the length of the vector PB. If we ch  oose a point A \n\"----> \non€, then, in the right-angled triangle fl.APB, the other two vectors are the leg AP and \n----\"  -----> -----; \nthe hypotenuse AB. AP is called the projection of AB onto the line €. We will now look \nat this situation in terms of vectors. \nB \nB \ne \nA \nFigure 1.36 \nThe distance from a point to a line \nConsider two nonzero vectors u and v. Let \np \nbe the vector obtained by dropping","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":4698,"to":4726}}}}],[148,{"pageContent":"at this situation in terms of vectors. \nB \nB \ne \nA \nFigure 1.36 \nThe distance from a point to a line \nConsider two nonzero vectors u and v. Let \np \nbe the vector obtained by dropping \na perpendicular from the head of v onto u and let e be the angle between u and v, as \nfigure 1.31 \nshown in Figure 1.37. Then clearly\np \n= \nll\nP \nll\nu, where u \n= \n(\nI\n/ llu \nII \n)u is the  unit vector in \nThe projection of v onto u \nthe direction of u. Moreover, elementary trigonometry gives \nII \np \nII \n= \nII \nv \nII \ncos e, and \nu·v \nwe know that cos \ne = \nII \nu \nII \nll\nv \n11 . Thus, after substitution, we obtain \np \n= \nll\nv\nll\nC\n1\nu\nil\n·\nl\n�\nv\nll\n)\n(\nilf \n)u \n(\nu·v\n) \n= �u \n=\n(\n�)u \nu·u \nThis is the formula we want, and it is the basis of the following definition for vec­\ntors in !R\nn\n. \nDefinition \nIf u and v are vectors in !R\nn \nand u * 0, then the projection of \nv onto u is the  vector proju(v) defined by \n(\nu·v\n) \nproj0\n(\nv\n) \n=  --  u \nu·u \nAn alternative way to de  rive this formula is described in Exercise 73.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":4726,"to":4814}}}}],[149,{"pageContent":"28 Chapter 1 Vectors \nproju(v) \nFigure 1.38 \nu \nExample 1.24 \nRemarks \n• \nThe term projection comes from the idea of projecting an image onto a wall \n(with a slide projector, for example). Imagine a beam oflight with rays parallel to each \nother and perpendicular to u shining down on v. The projection of v onto u is just the \nshadow cast, or projected, by v   onto u. \n• \nIt may be helpful to think of proj0(v) as a function with variable v. Then the \nvariable v occurs only once on the right-hand side of the definition. Also, it is helpful \nto remember Figure 1.38, which reminds us that proj0(v) is a scalar multiple of the \nvector u \n(\nnot v). \n• \nAlthough in our derivation of the definition of proj0(v) we  required v as well \nas u to be nonzero (why?), it is clear from the geometry that the projection of the \nzero vector onto u is 0. The definition is in   agreement with this, since \n(\n\"\n. \n0\n)u = \nOu = 0. \nu·u \n• \nIf the angle between u and vis obtuse, as in Figure 1.38, then proj0(v) will be in","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":4816,"to":4846}}}}],[150,{"pageContent":"zero vector onto u is 0. The definition is in   agreement with this, since \n(\n\"\n. \n0\n)u = \nOu = 0. \nu·u \n• \nIf the angle between u and vis obtuse, as in Figure 1.38, then proj0(v) will be in \nthe opposite direction from u; that is, proj0(v) will be a negative scalar multiple of u. \n• \nIf u is a unit vector then proj0(v) = \n(\nu · v\n)\nu. (Why?) \nFind the projection of v onto u in each case. \n(a)  v = \n[ \n-\n�] \nand u = \n[ \n�\n] \n(c) v = [\n�\n]\nand u = \n[ \n���\n] \n3 \n1\n/\\/2 \nSolution \n(a) We compute u · v = \n[ �] \n· \n[ \n-\n�] \n=  1   and u · u = \n[ �] \n· \n[ �] \n= 5, so \n(\nu·v\n) \n1\n[\n2\n] \n[\n2/5\n] \nproj\nu\n(\nv\n)\n= \nu·u\n\"\n=\ns1\n=\n1/5 \n(b)  Since e3 is a unit vector, \n(c) We see that \nll\nu\nll \n= \nV\n� \n+ � + \nt \n=  1. Thus, \nproj0\n(\nv\n) \n= \n(\nu ·v\n)\nu = \n(\n� \n+  1  + \n�\n)\n[ \n��� \n] \n= \n3\n(\n1 \n� \n\\/2\n) \n[ \n��� \n] \n1\n/\\/2 \n1\n/\\/2 \n� \n3 \n(\nI \n: V2J \n[ \n� \nl","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":4846,"to":4965}}}}],[151,{"pageContent":"J \nExercises 1.2 \nIn Exercises 1-6,find u ·v. \n5. u= [ 1,\n\\/2\n,\\/3, o],v=  [  4, -\\/2,0,-5] \nGAS \n6. U =  [   1.12, - 3.25, 2.07, -1.83], \nv =  [ -2.29, 1.72, 4.33, -1.54] \nIn Exercises 7-12, find \nII \nu \nII \nfor the given exercise, and give \na unit vector in the direction of u. \n7. Exercise 1 \nGAs \n10. Exercise 4 \n8. Exercise 2 \n9. Exercise 3 \n11. Exercise 5 \nGAs \n12. Exercise 6 \nIn Exercises 13-16,find the distance d(u, v) between u and \nv in the given exercise. \n13. Exercise 1 \n14. Exercise 2 \n15. Exercise 3 \nGAs \n16. Exercise 4 \n17. If u, v, and w are vectors in ll�r, n 2: 2, and c is a \nscalar, explain why the following expressions make \nno sense: \n(a) \nll\nu·v\nll \n(c) u· \n(\nv·w\n) \n(b) u·v + w \n(d) c\n. ( \nu + w\n) \nIn Exercises 18-23, determine whether th e angle between \nu and v is acute, obtuse, or a right angle. \n20. u =   [4, 3, -1], v =   [l, -1, l] \nGAS \n21. U = [0.9, 2.1, 1.2], V = [-4.5, 2.6, -0.8] \n22. u= [1,2, 3, 4],v= [-3, 1,2, -2] \n23. u= [1,2, 3, 4],v= [5, 6, 7, 8]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":4967,"to":5019}}}}],[152,{"pageContent":"20. u =   [4, 3, -1], v =   [l, -1, l] \nGAS \n21. U = [0.9, 2.1, 1.2], V = [-4.5, 2.6, -0.8] \n22. u= [1,2, 3, 4],v= [-3, 1,2, -2] \n23. u= [1,2, 3, 4],v= [5, 6, 7, 8] \nIn Exercises 24-29, find th e angle between u and v in th e \ngiven exercise. \n24. Exercise 18 \n25. Exercise 19 \nSection 1.2 \nLength and Angle: The Dot Product \n29 \n26. Exercise 20 \nGAs \n28. Exercise 22 \nGAs \n27. Exercise 21 \nGAs \n29. Exercise 23 \n.. \n30. Let A = (-3, 2), B =   (1, 0), and C = (4, 6). Prove that \nMBC is a right- angled triangle. \n31. \nLet A =   (1, 1, -1), B =   (-3, 2, -2), and C =   (2, 2, -4). \nProve that �ABC is a right-angled triangle. \nGAs \n32. Find the angle between a diagonal of a cube and an ad­\njacent edge. \n33. A cube has four diagonals. Show that no two of them \nare perpendicular. \n34. A parallelogram has diagonals determined by the \nvectors \nd\n, \n� \n[H and d, \n� \n[\n-\n�-\nShow that the parallelogram is a rhombus (all sides of \nequal length) and determine the side length. \n35. The rectangle ABCD has vertices at A =   (1, 2, 3),","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":5019,"to":5061}}}}],[153,{"pageContent":"vectors \nd\n, \n� \n[H and d, \n� \n[\n-\n�-\nShow that the parallelogram is a rhombus (all sides of \nequal length) and determine the side length. \n35. The rectangle ABCD has vertices at A =   (1, 2, 3), \nB =   (3, 6, -2), and C =   (O, 5, -4). Determine the \ncoordinates of vertex D. \n36. An airplane heading due east has a velocity of \n200 miles per hour. A  wind is blowing from the north \nat 40 miles per hour. What is the resultant velocity of \nthe airplane? \n37. A boat heads north across a river at a rate of 4 miles \nper hour. If the  current is flowing east at a   rate of \n3 miles per hour, find the resultant velocity of \nthe boat. \n38. Ann is driving a motorboat across a river that is 2 km \nwide. The boat has a speed of 20 km/h in still water, and \nthe current in the river is flowing at 5 km/h. Ann heads \nout from one bank of the river for a dock directly across \nfrom her on the opposite bank. She drives the boat in a \ndirection perpendicular to the current. \n(a)  How far downstream from the dock will \nAnn land?","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":5061,"to":5090}}}}],[154,{"pageContent":"from her on the opposite bank. She drives the boat in a \ndirection perpendicular to the current. \n(a)  How far downstream from the dock will \nAnn land? \n(b) How long will it take Ann to cross the river? \n39. Bert can swim at a rate of 2 miles per hour in still \nwater. The current in a river is flowing at a   rate of \n1 mile per hour. If Bert wants to swim across the river \nto a point directly opposite, at what angle to the bank \nof the river must he swim?","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":5090,"to":5099}}}}],[155,{"pageContent":"30 \nChapter 1 Vectors \nIn Exercises 40-45, find the projection of v onto u. Draw a \nsketch in Exercises 40 and 41. \n[ \n3.01\n] \n[ \n1.34\n] \nCAS \n45. \nU \n= \n-0.33 , V = \n4.25 \n2.52 \n-1.66 \nFigure 1.39 suggests two ways in which vectors \nmay be used to compute the area of a triangle. \nThe area A of \n(a) \nu \n(b) \nFigure 1.39 \nthe trian\ngle in part (\na) is gi\nven by\nt\nll\nu\nll ll\nv-proj\nu\n(\nv\n) \nII\n, \nand part (b) suggests the trigonometric form of the \narea \nof a \ntria\nngle: \nA \n= \nt \nII \nu \nII  II \nv \nII \nsine (We can use the \nidentity sin e = v \n1  -  cos\n2 \ne to find sin e.) \nIn Exercises 46 and 47, compute the area of th e triangle \nwith th e given vertices using both methods. \n46. A  = (1, -1), B \n= (2, 2), C = (4, O) \n47. A  = (3, -1, 4), B \n= (4, -2, 6), C = (5, 0, 2) \nIn Exercises 48 and 49, find all values of the scalar k for \nwhich th e two vectors are orthogonal. \n48. u \n� \n[\n: Jv \n� \n[\n� � : l 49. u \n� \n[ \n-J \n� \n[\n_\nfl \n50. Describe all vectors v = \n[;] \nthat are orthogonal \nto u = \n[\n�\n]\n. \n51. Describe all vectors v = \n[;] \nthat are orthogonal \nto u = \n[\n�\n]\n.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":5101,"to":5196}}}}],[156,{"pageContent":"48. u \n� \n[\n: Jv \n� \n[\n� � : l 49. u \n� \n[ \n-J \n� \n[\n_\nfl \n50. Describe all vectors v = \n[;] \nthat are orthogonal \nto u = \n[\n�\n]\n. \n51. Describe all vectors v = \n[;] \nthat are orthogonal \nto u = \n[\n�\n]\n. \n52. Under what conditions are the following true for \nvectors u and v in IR\n2 \nor IR\n3\n? \n(a\n) ll\nu +v\nii \n= \nll\nu\nll \n+ \nll\nv\nll \n(b) \nll\nu +v\nii \n= \nll\nu\nll \n-\nll\nv\nll \n53. Prove Theorem 1.2(b). \n54. Prove Theorem 1.2(d). \nIn Exercises 55-57, prove th e stated property of distance \nbetween vectors. \n55. d(u, v) = d(v, u) for all vectors u and v \n56. d(u, w) \n:s \nd(u, v) + d(v, w) for all vectors u, v, and w \n57. d(u, v) = 0 if and only ifu = v \n58. Prove that u \n· \nc v = c\n( \nu \n· \nv) for all vectors u and v in !R\nn \nand all scalars c. \n59. Prove that \nll\nu \n- v\nii 2': \nll\nu\nll \n-\nll\nv\nll \nfor all vectors u and \nv in !R\nn\n. [Hint: Replace u by u - v in the Triangle \nInequality.] \n60. Suppose we know that u \n· \nv = u \n· \nw. Does it follow that \nv \n= \nw? If it does, give a proof that is valid in !R\nn\n; \notherwise, give a counterexample (i.e., a specifi c set of","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":5196,"to":5301}}}}],[157,{"pageContent":"Inequality.] \n60. Suppose we know that u \n· \nv = u \n· \nw. Does it follow that \nv \n= \nw? If it does, give a proof that is valid in !R\nn\n; \notherwise, give a counterexample (i.e., a specifi c set of \nvectors u, v, and w for which u \n· \nv = u \n· \nw but v -=F w) . \n61. \nProve that (u \n+ v) \n· \n(u  - v) \n= \nll\nu\nll\n2 \n-\nll\nv\nll\n2 \nfor all vec­\ntors u and v in !R\nn\n. \n62. (a\n) \nProve that \nll\nu + v\nll\n2 \n+ \nll\nu - v\nll\n2 \n= 2\nll\nu\nll\n2 \n+ 2\nll\nv\nll\n2 \nfor all vectors u and v in !R\nn\n. \n(h) Draw a diagram showing u, v, u + v, and u - v \nin IR\n2 \nand use (a) to deduce a result about \nparallelograms. \n1 1 \n63. \nProve \nthat u \n· \nv \n= \n-\nll\nu + v\nll\n2 \n-\n-\nll\nu - v\nll\n2 \nfor all \n4 \n4 \nvectors u and v in !R\nn\n.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":5301,"to":5390}}}}],[158,{"pageContent":"64. (a)  Prove that \nll\nu + v\nii \n= \nll\nu - v\nii \nif and \nonly if u \nand \nv are orthogonal. \n(b) Draw a diagram showing u, v, u + v, and u - v \nin IR\n2 \nand use (a) to deduce a result about \nparallelograms. \n65. (a)  Prove that u + v and u - v are orthogonal in !R\nn \nif \nan\nd o\nnly \nif\nll\nu\nll \n= \nll\nv\nll\n. \n(b) Draw a diagram showing u, v, u + v, and u - v \nin IR\n2 \nand use (a) to deduce a result about \nparallelograms. \n66. If \nll\nu\nll \n= 2, \nll\nv\nll \n= v'3, and u · \nv =   1, find \nll\nu + v\nii\n. \n67. Showthatthere are no vectors uand vsuch that \nll\nu\nll \n= 1, \nll\nv\nll \n= \n2,\na\nnd\nu·\nv\n= \n3. \n68. (a)  Prove that ifu is orthogonal to both v and w, then \nu is orthogonal to v + w. \n(b) Prove that if u is orthogonal to both v and w, then \nu is orthogonal to sv   + tw for all scalars s and t. \n69. Prove that u  is orthogonal to v -  proju(v) for all \nvectors u and v in !R\nn\n, where u * 0. \n70. (a)  Prove that proju(pr  oju(v)) = proju(v)\n· \n(b) Prove that proju(v - proju(v)) = 0. \n(c)  Explain (a) and (b) geometrically. \n71. The Cauchy-Schwarz Inequality I u ·vi","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":5392,"to":5471}}}}],[159,{"pageContent":"n\n, where u * 0. \n70. (a)  Prove that proju(pr  oju(v)) = proju(v)\n· \n(b) Prove that proju(v - proju(v)) = 0. \n(c)  Explain (a) and (b) geometrically. \n71. The Cauchy-Schwarz Inequality I u ·vi \n:::::: \nll\nu\nll ll\nv\nll \nis \nequivalent to the inequality we get by squaring both \nsides: (u \n· v)\n2\n:::::: \nll\nu\nll\n2 \nll\nv\nll\n2\n. \n(a)  In IR\n2\n, with u = \n[\n�:\n]\nand v = \n[\n:J this becomes \nProve this algebraically. [Hint: Subtract the left-hand \nside from the right-hand side and show that the \ndifference must necessarily be nonnegative.] \n(b) Prove the analogue of (a) in IR\n3\n. \nSection 1.2 \nLength and Angle: The Dot Product \n31 \n72. Figure 1.40 shows that,   in IR\n2 \nor IR\n3\n, \nll\nproju\n(\nv\n) \nII \n:::::: \nll\nv\nll\n· \n(a)  Prove that this inequality is true in general. [Hint: \nProve that proju(v) is orthogonal to v-proju(v) \nand use Pythagoras' Theorem.] \n(b) Prove that the inequality \nll\nproju \n( \nv\n) \nII \n:::::: \nll\nv\nll \nis \nequivalent to the Cauchy-Schwarz Inequality. \nproju(v) \nu \nFigure 1.40 \n73. Use the fact that proju(v) = cu for some scalar c, to­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":5471,"to":5552}}}}],[160,{"pageContent":"(b) Prove that the inequality \nll\nproju \n( \nv\n) \nII \n:::::: \nll\nv\nll \nis \nequivalent to the Cauchy-Schwarz Inequality. \nproju(v) \nu \nFigure 1.40 \n73. Use the fact that proju(v) = cu for some scalar c, to­\ngether with Figure 1.41, to find c and thereby derive \nthe formula for proju(v). \nv -cu \ncu \nu \nFigure 1.41 \n74. Using mathematical induction, prove the following \ngeneralization of the Triangle Inequality: \nll\nv\n1 \n+ \nV\nz \n+ \n· · ·\n+ v\nn\nll \n::=::: \nll\nv\n1\nll \n+ \nll\nv\nz\nll \n+ \n· · · \n+ \nll\nv\nn\nll \nfor all n 2: 1.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":5552,"to":5605}}}}],[161,{"pageContent":"Example 1.25 \nA \nB \nFigure 1.42 \nThe midpoint of AB \nc \nA \n----------\n.... \nB \nFigure 1.43 \n32 \nExploration \nVectors and Geometry \nMany results in plane Euclidean geometry can be proved using vector techniques. \nFor example, in Example 1.24, we used vectors to prove Pythagoras' Theorem. In \nthis exploration, we will use vectors to develop proofs for some other theorems from \nEuclidean geometry. \nAs an introduction to the notation and the basic approach, consider the following \neasy example. \nGive a vector description of the midpoint M of a line segment AB. \nSolution \nWe  first  convert everything to vector notation. If 0 denotes the origin and \nP is a point, let p be the vector GP. In this situation, a = GA, b = GB, m = oM, and \n-----> \n------> ----> \nAB = OB -OA = b -a (Figure 1.42). \nNow, since Mis the midpoint of AB, we have \nm - a = AM = fAB = \nt \n(\nb - a\n) \nso \nm = a+ \nt \n(\nb - a\n) \n= \nt \n(\na+ b\n) \n1.  Give a vector description of the point P that is one-third of the way from A to \nB on the line segment AB. Generalize.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":5607,"to":5652}}}}],[162,{"pageContent":"m - a = AM = fAB = \nt \n(\nb - a\n) \nso \nm = a+ \nt \n(\nb - a\n) \n= \nt \n(\na+ b\n) \n1.  Give a vector description of the point P that is one-third of the way from A to \nB on the line segment AB. Generalize. \n2.  Prove that the line segment joining the midpoints of two sides of a triangle is \nparallel to the third side and half as long. (In vector notation, prove that PQ = \nt \nAB \nin \nFigure 1.43.) \n3.  Prove that the quadrilateral PQRS (Figure 1.44), whose vertices are the mid­\npoints of the sides of an arbitrary quadrilateral ABCD, is a parallelogram. \n4.  A median of a triangle is a line segment from a vertex to the midpoint of \nthe opposite side (Figure 1.45). Prove that the three medians of any triangle are con­\ncurrent (i.e., they have a common point of intersection) at a point G that is two­\nthirds of the distance from each vertex to the midpoint of the opposite side. \n[\nHint: In \nFigure 1.46, show that the point that is two-thirds of the distance from A to Pis given \nby \nt\n( \na + b + c). Then show that \nt \n(\na + b + c\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":5652,"to":5692}}}}],[163,{"pageContent":"[\nHint: In \nFigure 1.46, show that the point that is two-thirds of the distance from A to Pis given \nby \nt\n( \na + b + c). Then show that \nt \n(\na + b + c\n) \nis two-thirds of the distance from B \nto Q and two-thirds of the distance from C to R.] The point Gin Figure 1.46 is called \nthe centroid of the triangle.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":5692,"to":5705}}}}],[164,{"pageContent":"Figure 1.44 \nFigure 1.41 \nAn altitude \nc \nFigure 1.50 \nThe circumcenter \nR \nA \nc \nFigure 1.45 \nA median \nB \nA \nc \nFigure 1.46 \nThe centroid \nB \n5.  An altitude of a triangle is a line segment from a vertex that is perpendicu­\nlar to the opposite side (Figure 1.47). Prove that the three altitudes of a triangle are \nconcurrent. [Hint: Let H be the point of intersection of the altitudes from A and Bin \nFigure 1.48. Prove that cH is orthogonal to \nAB\n.] The point Hin Figure 1.48 is called \nthe orth ocenter of the triangle. \n6.  A perpendicular bisector of a line segment is a line through the midpoint of \nthe segment, perpendicular to the segment (Figure 1.49). Prove that the perpendicular \nbisectors of the three sides of a triangle are concurrent. [Hint: Let K be the point of in­\ntersection of t\n�\nerpendicular bisectors of AC and BC in Figure 1.50. Prove that RK is \northogonal to AB.] The point Kin Figure 1.50 is called the circumcenter of the  triangle. \nc \nA\n--\n�\n-*\n��������\n--=e\nB \nFigure 1.48 \nThe orthocenter \nA \nB","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":5707,"to":5749}}}}],[165,{"pageContent":"orthogonal to AB.] The point Kin Figure 1.50 is called the circumcenter of the  triangle. \nc \nA\n--\n�\n-*\n��������\n--=e\nB \nFigure 1.48 \nThe orthocenter \nA \nB \nFigure 1.49 \nA perpendicular bisector \n7.  Let A and B be the endpoints of a diameter of a circle. If C is any point on the \ncircle, prove that LACE is a right angle. [Hint: In Figure 1.51, let 0 be the center of the \ncircle. Express everything in terms of a and c and show that AC is orthogonal to BC. J \n8.  Prove that the  line segments joining the midpoints of opposite sides of a \nquadrilateral bisect each other (Figure 1.52). \n0 \nc \nD \nFigure 1.51 \nFigure 1.52 \n33","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":5749,"to":5774}}}}],[166,{"pageContent":"34 \nChapter 1 Vectors \nExample 1.26 \nThe Latin word norma refers to a \ncarpenter's square, used for draw­\ning right angles. Thus, a normal \nvector is one that is perp endicular \nto something else, usually a plane. \nlines and Planes \nWe are all familiar with the equation of a line in the Cartesian plane. We now want \nto consider lines in IR\n2 \nfrom a vector point of view. The insights we obtain from this \napproach will allow us to generalize to lines in IR\n3 \nand then to planes in IR\n3\n. Much of \nthe linear algebra we will consider in later chapters has its origins in the simple geom­\netry oflines and planes; the ability to visualize these and to think geometrically about \na problem will serve you well. \nLines in �\n2 \nand �\n3 \nIn the xy-plane, the general form of the equation of a line  is ax+ by = c. If b * 0, then \nthe equation can be rewritten as y = -(a/b)x + c/b, which has the form y = mx + k. \n[This is the slope-intercept form; mis the slope of the line, and the point with coordi­\nnates (O, k\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":5776,"to":5805}}}}],[167,{"pageContent":"the equation can be rewritten as y = -(a/b)x + c/b, which has the form y = mx + k. \n[This is the slope-intercept form; mis the slope of the line, and the point with coordi­\nnates (O, k\n) \nis its y-intercept.\nJ \nTo get vectors into the picture, let's consider an example. \nThe line C with equation 2x + y = 0 is sh own in Figure 1.53. It is a line with slope - 2 \npassing through the origin. The left-hand side of the equation is in the form of a dot \nproduct; in fact, if we let n = \n[ \n�\n] \nand x  = \n[\n;\n]\n,then the equation becomes n · x = 0. \nThe vector n is perpendicular to the line-that is, it  is orthogonal to any vector x that \nis parallel to the line (Figure 1.54)-and it is called a normal vector to the line. The \nequation n . x = 0 is the normal form of the equation of e. \nAnother way to think about this line is to imagine a particle moving along the \nline. Suppose the particle is initially at the origin at time t = 0 and it moves along","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":5805,"to":5827}}}}],[168,{"pageContent":"Another way to think about this line is to imagine a particle moving along the \nline. Suppose the particle is initially at the origin at time t = 0 and it moves along \nthe line in such a way that its x-coordinate changes 1 unit per second. Then at t = 1 \nthe particle is at ( 1, - 2 ), at t = 1.5 it  is at (  1.5, -3  ), and, if we allow negative values oft \n(i.e., we consider where the particle was in the past), at t = -2 it is (or was) at ( -2, 4). \ny \ny \nFigure 1.53 \nFigure 1.54 \nThe line 2x + y  = 0 \nA normal vector n","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":5827,"to":5837}}}}],[169,{"pageContent":"Example 1.21 \nSection 1.3 \nLines and Planes \n35 \nThis movement is illustrated in Figure 1.55. In general, if x = t, then y = -2t, and we \nmay write this relationship in vector form as \nWhat is the significance of the vector d = \n[ \n_ \n�\n]\n? It is a particular vector parallel \nto e, called a direction vector for the line. As shown in Figure 1.56, we may write the \nequation of e as x = td. This is the vector form of the equation of the line. \nIf the line does not pass through the  origin, then we  must modify  things \nslightly. \ny \ny \ne \nFigure 1.56 \nFigure 1.55 \nA direction vector d \nConsider the line C with equation 2x + y = 5 (Figure 1.57\n)\n. This is just the line from \nExample 1.26 shifted upward 5 units. It also has slope -2, but its y-intercept is the \npoint (O, 5\n)\n. It is clear that the vectors d and n from Example 1.26 are, respectively, a \ndirection vector and a    normal vector for this line too. \nThus, n is orthogonal to every vector that is parallel to e. The point P \n= \n(\n1, 3) \n---+ \nis \non C. If X = \n(","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":5839,"to":5876}}}}],[170,{"pageContent":"direction vector and a    normal vector for this line too. \nThus, n is orthogonal to every vector that is parallel to e. The point P \n= \n(\n1, 3) \n---+ \nis \non C. If X = \n(\nx, y) represents a general point on C, then the vector PX = x -\np\nis \nparallel to e and n \n· \n(x -\np\n) \n= 0 (see Figure 1.58\n)\n. Simplified, we have n\n· \nx \n= \nn \n· p\n. \nAs a check, we compute \nn · x = \n[ �] \n· \n[;] \n= 2x + y  and n · p = \n[ �\n] \n· \n[ �] \n= 5 \nThus, the normal form n · x = n \n· \np \nis just a different representation of the general \nform of the equation of the line. (Note that in Example 1.26, \np \nwas the zero vector, so \nn \n· \np \n= 0 gave the right-hand side of the equation.)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":5876,"to":5923}}}}],[171,{"pageContent":"36 \nChapter 1 Vectors \nThe word parameter and the cor­\nresponding adjective parametric \ncome from the Greek words para, \nmeaning \"alongside;' and metron, \nmeaning \"measure:' Mathemati­\ncally speaking, a parameter is a \nvariable in terms of which other \nvariables are expressed-a new \n\"measure\" placed alongside \nold ones. \ny \ny \nn \n-t--j--+--t--t--f--\\-t-jl-t-... \nx \nfigure 1.51 \nfigure 1.58 \nThe line 2x + y  = 5 \nn \n• \n(x -p) = 0 \nThese results lead to the following definition. \nDefinition \nThe normal form of the equation of a line e in IR\n2 \nis \nn · (x -\np\n) = 0 or n · x = n · \np \nwhere \np\nis a specific point on e and n =F 0 is a normal vector fore. \n[\na\nb\n] \ni\n·\ns a \nThe general form of the equation of e is ax + by = C, where n = \nnormal vector for e. \nContinuing with Example 1.27, let us now find the vector form of the equation \nof e. Note that, for each choice of X, x -\np \nmust be parallel to-and thus a multiple \nof-the direction vector d. That is, x -\np \n= td or x = \np \n+ td for some scalar t. In \nterms of components, we have \nor","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":5925,"to":5979}}}}],[172,{"pageContent":"of e. Note that, for each choice of X, x -\np \nmust be parallel to-and thus a multiple \nof-the direction vector d. That is, x -\np \n= td or x = \np \n+ td for some scalar t. In \nterms of components, we have \nor \n[;] [ �] \n+  t\n[ \n-\n�\n] \nx = 1  +  t \ny = 3 \n-\n2t \n(\n1\n) \n(\n2\n) \nEquation \n( \n1\n) \nis the vector form of the equation of€, and the componentwise Equa­\ntions \n(\n2\n) \nare called parametric equations of the line. The variable tis called a parameter. \nHow does all of this generalize to IR\n3\n? Observe that the vector and parametric \nforms of the equations of a line carry over perfectly. The notion of the slope of a line \nin IR\n2\n-which is difficult to generalize to three dimensions-is replaced by the more \nconvenient notion of a direction vector, leading to the following definition. \nDefinition \nThe vector form of the equation of a line e in IR\n2 \nor IR\n3 \nis \nx = \np \n+ td \nwhere \np\nis a specific point on e and d =F 0 is a direction vector for e. \nThe equations corresponding to the components of the vector form of the","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":5979,"to":6035}}}}],[173,{"pageContent":"2 \nor IR\n3 \nis \nx = \np \n+ td \nwhere \np\nis a specific point on e and d =F 0 is a direction vector for e. \nThe equations corresponding to the components of the vector form of the \nequation are called parametric equations of e.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":6035,"to":6046}}}}],[174,{"pageContent":"Example 1.28 \nSection 1.3 \nLines and Planes \n31 \nWe will often abbreviate this terminology slightly, referring simply to the general, \nnormal, vector, and parametric equations of a line or plane. \nFind vector and parametric equations of the line in IR\n3 \nthrough the point P = ( 1, 2, -1 ), \npornllel to the mtm d \n� \n[ \n-:J \nSolution \nThe vector equation x = p + td is \nThe parametric form is \nRemarks \nx = \n1  +St \ny = 2  -t \nz = -1 + 3t \n• \nThe vector and parametric forms of the equation of a given line e are not \nunique-in fact, there are infinitely many,  since we may use any point on e to de­\ntermine p and any direction vector for e. However, all direction vectors are clearly \nmultiples of each other. \n[ 1 o l \nIn Example 1.28, \n(\n6, 1, 2\n) \nis another point on the line (take t =  1\n)\n, and -2 is \nanother direction vector. Therefore, \n6 \ngives a different (but equivalent) vector equation for the line. The relationship between \nthe two parameters s and t can be found by comparing the parametric equations: For","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":6048,"to":6085}}}}],[175,{"pageContent":"6 \ngives a different (but equivalent) vector equation for the line. The relationship between \nthe two parameters s and t can be found by comparing the parametric equations: For \na given point \n(\nx, y, z) on e, we have \nimplying that \nx = \n1  + St= 6 + 10s \ny = \n2 -\nt =  1  -2s \nz = -1 +  3t = 2 + \n6s \n-10s +St= S \n2s -t = -1 \n-6s + 3t = \n3 \nEach of these equations reduces to t =  1  + 2s.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":6085,"to":6103}}}}],[176,{"pageContent":"38 \nChapter 1 Vectors \nExample 1.29 \nFigure 1.59 \nn is orthogonal to infinitely many \nvectors \nn \np \nFigure 1.60 \nn ·    (x -p) = 0 \n• \nIntuitively, we  know  that  a  line is a one-dimensional object. The  idea of \n\"dimension\" will be clarified in Chapters \n3 \nand 6, but for the moment observe that \nthis idea appears to agree with the fact that the  vector form of the equation of a line \nrequires one parameter. \nOne often hears the expression \"two points determine a line:' Find a vector equation \nof the line f in IR\n3 \ndetermined by the points P = ( -1, 5, o) and Q = ( 2, 1, 1). \nSolulion \nWe may choose any point on f for p, so we will use P ( Q would also be \nfine). \n[ \n3\n] \nA convenient direction vector is d = PQ = -4  (or any scalar multiple of this). \nThus, we obtain \n1 \nPlanes in lR\n3 \nThe next question we should ask ourselves is, How does the general form of the equa­\ntion of a line generalize to IR\n3\n? We might reasonably guess that if ax + by = c is the \ngeneral form of the equation of a line in IR\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":6105,"to":6142}}}}],[177,{"pageContent":"tion of a line generalize to IR\n3\n? We might reasonably guess that if ax + by = c is the \ngeneral form of the equation of a line in IR\n2\n, then ax + by + cz = d might represent a \nline in IR\n3\n. In normal form, this equation would be n · x = n · p, where n is a normal \nvector to the line and p corresponds to a point on the line. \nTo see if this is a reasonable hypothesis, let's think about the special case of the \nequation ax + by + cz \n� \n0. In normal furn, it becomes n \n• \nx \n� \n0, wh\"e n  � \n[ \n� \nl \n· \nHowever, the set of all vectors x that satisfy this equation is the set of all vectors or­\nthogonal to n. As shown in Figure 1.59, vectors in infinitely many directions have \nthis property, determining a family of parallel planes. So our guess was incorrect: It \nappears that ax+ by+ cz = dis the equation of a plane-not a line-in IR\n3\n. \nLet's make this finding more precise. Every plane <!]' in IR\n3 \ncan be determined by \nspecifying a point p on <!I' and a nonzero vector n normal to <!I' (Figure 1.60). Thus,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":6142,"to":6173}}}}],[178,{"pageContent":"3\n. \nLet's make this finding more precise. Every plane <!]' in IR\n3 \ncan be determined by \nspecifying a point p on <!I' and a nonzero vector n normal to <!I' (Figure 1.60). Thus, \nif x represents an arbitrary point on <!I', we have n  · (x -p) = 0 or n  · x = n  · p. If \nn � \n[ \n�\n] \nand x � \n[ \n� l then, in  teems of rnmponents, the equation bernmes \nax+ by+ cz = d (where d =  n · p\n)\n. \nDefinition \nThe normal form of the equation of a plane <!I' in IR\n3 \nis \nn · (x -p) = 0  or n · x = n · p \nwhere p is a specific point on <!I' and n * 0 is a normal vector for <!I'. \nThe general form of the equation of <!I' is ax + by + cz = d, where n = \nis a normal vector for <!I'.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":6173,"to":6197}}}}],[179,{"pageContent":"Example 1.30 \nSection 1.3 \nLines and Planes \n39 \nNote that any  scalar multiple of a normal vector for a plane is another normal \nvector. \nFind the normal and general forms of the equation of the plane that contains the \npoint P \n� \n(6, O, 1) and har normal vo dor n \n� \n[\nH \n-With\np �\n[f\nl \nand x \n� \n[\n�\n]. \nwe havr n·\np � \n1 ·6 + 2'0\n�\n3·1 \n� \n9, 'o \nthe normal equation n \n· x = n \n· \np \nbecomes the general equation x + 2y + 3z = 9 . \n.+ \nGeometrically,  it is clear that parallel planes have the  same normal vector(s). \nThus, their general equations have left-hand sides that are multiples of each other. So, for \nexample, 2x + 4y + 6z = 10 is the general equation of a plane that is parallel to the \nplane in Example 1.30, since we may rewrite the equation as x + 2y + 3z = 5-from \nwhich we see that the two planes have the same normal vector n. (Note that the planes \ndo not coincide, since the right-hand sides of their equations are distinct.) \nWe may also exp ress the equation of a plane in vector or parametric form. To do","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":6199,"to":6240}}}}],[180,{"pageContent":"do not coincide, since the right-hand sides of their equations are distinct.) \nWe may also exp ress the equation of a plane in vector or parametric form. To do \nso, we observe that a  plane can also be determined by specifying one of its points \nP (by the vector \np\n) and two direction vectors u and v parallel to the plane (but not \nparallel to each other). As Figure 1.61 shows, given any point X in the plane (located \ntv \nx \n-\np  = su + tv \n, <=I -x \nSU \nfigure 1.61 \nX \n-\np =\nS\nU+ tv \nby x), we  can always find appropriate multiples su and tv of the direction vectors such \nthat x -\np \n= su + tv or x = \np \n+ s u + tv. If we write this equation componentwise, \nwe obtain parametric equations for the plane. \nDefinition \nThe vector form of the equation of a plane <lP in IR\n3 \nis \nX = \np \n+\nSU\n+ tv \nwhere \np \nis a point on <lP and u and v are direction vectors for <lP ( u and v are non -\nzero and parallel to <lP, but not parallel to each other). \nThe equations corresponding to the components of the vector form of the","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":6240,"to":6279}}}}],[181,{"pageContent":"zero and parallel to <lP, but not parallel to each other). \nThe equations corresponding to the components of the vector form of the \nequation are called parametric equations of <lP.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":6279,"to":6281}}}}],[182,{"pageContent":"40 \nChapter 1 Vectors \nExample 1.31 \nFind vector and parametric equations for the plane in Example 1.30. \n\\ \nFigure 1.62 \nTwo normals determine a line \n<;JP I \nFigure 1.63 \nThe intersection of \ntwo planes is a line \nSolulion We need to find two direction vectors. We have one point P = (6, 0, 1) in \nthe plane; if we can find two other points Q and R in <!J',   then the vectors PQ and PR \ncan serve as direction vectors (unless by bad luck they happen to be parallel!). By \ntrial and error, we observe that Q = (9, 0, O)  and R = \n(\n3, 3, O) both satisfy the general \nequation x + 2y + 3z = 9 and so lie in the plane. Then we compute \nwhich, since they are not scalar multiples of each other, will serve as direction vectors. \nTherefore, we have the vector equation of <!J', \nand the corresponding parametric equations, \nx = 6 + 3s -3t \ny \n= 3t \nz=l-s-t \n� \n[What would have happened had we chosen R = (O, 0, 3)?] \nRemarks \n• \nA plane is a two-dimensional object, and its equation, in vector or parametric","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":6283,"to":6312}}}}],[183,{"pageContent":"x = 6 + 3s -3t \ny \n= 3t \nz=l-s-t \n� \n[What would have happened had we chosen R = (O, 0, 3)?] \nRemarks \n• \nA plane is a two-dimensional object, and its equation, in vector or parametric \nform, requires two parameters. \n• \nAs Figure 1.59 shows, given a point P and a nonzero vector n in IR\n3\n, there are \ninfinitely many lines through P with n as a normal vector. However, P and two non­\nparallel normal vectors n\n1 \nand n\n2 \ndo serve to locate a line e uniquely, since e must \nthen be the line through P that is perpendicular to the plane with equation x = p + \nsn\n1 \n+ tn\n2 \n(Figure 1.62). Thus, a line  in IR\n3 \ncan also be  specified by a   pair of equations \na\n1\nx + \nb\n1\ny + \nc\n1\nz = d\n1 \na\n1\nX + \nb\n2\ny + \nC\n2\nZ \n= d\n1 \none corresponding to each normal vector. But since these equations correspond to a \npair of nonparallel planes (why nonparallel?), this is just the description of a line as \nthe intersection of two nonparallel planes (Figure 1.63). Algebraically, the line con­\nsists of all points (x, y, z\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":6312,"to":6365}}}}],[184,{"pageContent":"the intersection of two nonparallel planes (Figure 1.63). Algebraically, the line con­\nsists of all points (x, y, z\n) \nthat simultaneously satisfy both equations. We will explore \nthis concept further in Chapter 2 when we discuss the solution of systems of linear \nequations. \nTables 1.2 and 1.3 summarize the information presented so far about the equa­\ntions oflines and planes. \nObserve once again that a single (general) equation describes a line in IR\n2 \nbut \na plane in IR\n3\n. [In higher dimensions, an object (line, plane, etc.) determined by a \nsingle equation of this type is usually called a hyperplane.] The relationship among","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":6365,"to":6379}}}}],[185,{"pageContent":"Table 1.2 \nEquations or lines in !R\n2 \nSection 1.3 \nLines and Planes 41 \nNormal Form \nGeneral Form \nVector Form \nParametric Form \nn\n· x=n\n·\np \nax\n+ \nby= c \nx = p + td \n{\nx = \nP\n1 \n+ td\n1 \ny = \nP2 \n+  td\n2 \nTable 1.3 \nlines and Planes in !R\n3 \nLines \nPlanes \nNormal Form \n{n\n1 \n·x = n\n1 \n·p\n1 \nn\n2 \n• \nx =  n\n2 \n• \nP2 \nn·x =  n·p \nExample 1.32 \nGeneral Form \n{a\n1\nX + \nb\n1\ny + \nC1\nZ \n= d\n1 \na\nz\nX + \nb\n1\nY \n+ \nC\n2\nZ \n= d\nz \nax+ by+ cz = d \nVector Form \nx = p +  td \nX = p +\nS U\n+ tv \nParametric Form \n{\n;: \nz= \n{;: \nz= \nP\n1 \n+ \ntd\n1 \nP2 \n+ \ntd\n2 \np\n3 \n+ td\n3 \np\n1 \n+ su\n1 \n+ tv\n1 \np\n2 \n+ su\n2 \n+ tv\n2 \np\n3 \n+ \nS\nU\n3 \n+ tv\n3 \nthe dimension of the object, the number of equations required, and the dimension of \nthe space is given by the \"balancing formula'': \n(\ndimension of the object) + (\nnumber of general equations) = dimension of the space \nThe higher the dimension of the object, the fewer equations it needs. For \nexample, a plane in IR\n3 \nis two-dimensional, requires one general equation, and lives \nin a three-dimensional space: 2 + 1 = 3. A line in IR\n3 \nis one-dimensional and so \nneeds 3 \n-","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":6381,"to":6510}}}}],[186,{"pageContent":"example, a plane in IR\n3 \nis two-dimensional, requires one general equation, and lives \nin a three-dimensional space: 2 + 1 = 3. A line in IR\n3 \nis one-dimensional and so \nneeds 3 \n-\n1 = 2 equations. Note that the dimension of the object also agrees with \nthe number of parameters in its vector or pa  rametric form. Notions of \"dimension\" \nwill be clarified in Chapters 3 and 6, but for the time being, these intuitive observa­\ntions will serve us well. \nWe can now find the distance from a point to a line or a plane by combining the \nresults of Section 1.2 with the results from this section. \nFind the distance from the point B = (1, 0, 2) to the line € through the point \nA \n� \n(\n3, !, I\n) \nwith dimhon vedoc d \n� [\n-\nu \n--+ \nSolution As we have already determined, we need to calculate the length of PB, \n----> \nwhere Pis the  point on e at the foot of the perpendicular from B. If we label v = AB, \nthen AP = projd(v) and PB = v \n-\nprojd ( v ) (see Figure 1.64). We do the necessary \ncalculations in several steps.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":6510,"to":6541}}}}],[187,{"pageContent":"42 \nChapter 1 Vectors \nB \nv  -profa(v) \nA \nFigure 1.64 \nd\n(\nB,€\n) \n= llv \n-projd\n(\nv\n) \nII \nStep 2: \nThe projection of v onto dis \nprojd \n( \nv\n) \n= \n(:��)d \n___...---e \nd \n= \n(\n(\n-\n1\n)\n·\n(\n-\n2\n) \n+ 1·\n(\n-\n1\n) \n+ \n0 ·\n1\n)\n[\n-\no\n�\nl \n(\n-\n1\n)\n2\n+1+\n0 \nStep 3: \nThe vector we want is \nStep 4: The distance d(B, f) from B to e is \nll\nv -  prnj\n,\n(\nv\n) \nII \n� \n[ \n=\n!\n] \nUsing \nTheorem 1 .3\n(\nb\n) \nto simplify the  calculation, we have \nNole \nll\nv -  prnj\n,\n(\nv\n) \nII \n� \nl \n[ \n=\n�\n-\n= \nt\nv9 \n+ \n9 \n+ \n4 \n= \nt\nv22 \n• In terms of our earlier notation, d(B, f) = d(v, projd(v)).","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":6543,"to":6646}}}}],[188,{"pageContent":"Example 1.33 \nSection 1.3 \nLines and Planes \n43 \nIn the case where the line £ is in IR\n2 \nand its equation has the general form \na\nx + by = c, the distance d(B, €) from B = (x0, y0) is given by the formula \nl\na\nx0 + by\n0 \n-\nc\nl \nd\n(\nB, €\n) \n= \nV \na\nz \n+  b\n2 \nYou are invited to prove this formula in Exercise 39. \n(3) \nFind the distance from the point B = (1, 0, 2)  to the plane <fP whose general equation \nis x + y \n-\nz = 1. \nSolution In this case, we need to calculate the length of PB, where P is the point on \n<fP at the foot of the perpendicular from B. As Figure 1.65 shows, if A is any point on \n'ii' and we 'ituate thrnurnrnl vectu' n \n� \n[ \n_ \n: \n] \nof 'ii' '° that it; tail [, at A, then we \nneed to find the length of the projection of \nAB \nonto n. Again we do the necessary \ncalculations in steps. \nn \nFigure 1.65 \nd\n(\nB, <!P\n) \n= \nll\nproj\nn\n(AB) \nII \nI \nI \nI \nI \nI \nB \n11\np \nStep 1: By trial and error, we find any point whose coordinates satisfy the  equation \nx + y -z = 1. A = (1, 0, O) will do. \nStep 2: Set \nStep 3: The projection of v onto n is","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":6648,"to":6716}}}}],[189,{"pageContent":"n\n(AB) \nII \nI \nI \nI \nI \nI \nB \n11\np \nStep 1: By trial and error, we find any point whose coordinates satisfy the  equation \nx + y -z = 1. A = (1, 0, O) will do. \nStep 2: Set \nStep 3: The projection of v onto n is \nproj\nn \n( \nV\n) \n=(�)n \nn·n \n=\n(\n1·0+1·0 - 1\n•\n2\n)\n[ \n�1 \n1+1+\n(-1)\n2 \n-1 \n-\nt:\nl \n[=\n!\n]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":6716,"to":6755}}}}],[190,{"pageContent":"44 Chapter 1 Vectors \nStep 4: \nThe distance d(B, <JP) from B to <JP is \nllr\nrnj\n\"\n(\nv\nl\nll \n� \nH \n[ \nJ \n�\ni \n[\nJ \nIn general, the distance d(B, <JP) from the point B = \n(\nx0, y\n0\n, z\n0\n) \nto the plane whose \ngeneral equation is ax + by + cz =  d is given by the formula \nd\n(\nB, <JP\n) \nl\nax\n0 + by\n0 \n+ \nC\nZ\n0 \n-  d\ni \nV\na\n2 \n+ b\n2 \n+ c\n2 \n(4) \nYou will be asked to derive this formula in Exercise 40. \n.. \nI \nExercises 1.3 \nIn Exercises 1 and 2, write th e equation of th e line passing \nthrough P with normal vector n in (a) normal form and \n(b) general form. \nI\n.P = (0,0) ,n = \n[\n�\n] \n2.P  = (1,2) ,n = \n[ \n_\n!\n] \nIn Exercises 3-6, write th e equation of th e line passing \nthrough P with direction vector din (a) vector form and \n(b) parametric form. \n3.P  = (1,0) ,d= \n[\n-\n�] \nS.P\n� \n(0,0,0 ),d\n� \n[\n-:\nl \n4. p \n= \n( -4, 4\n)\n, d \n= \n[ \n�\n] \n•. p \n� \n( 3\n. \no. \n_ \n2 )\n. \nd \n� \nm \nIn Exercises 7 and 8, write th e equation of th e plane passing \nthrough P with normal vector n in (a) normal form and \n(b) general form. \n7.P\n� \n(0,  1,0),n\n� \n[\n;\n] \n8.P\n� \n(3, 0,-2) , n\n� \nm","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":6757,"to":6871}}}}],[191,{"pageContent":". \no. \n_ \n2 )\n. \nd \n� \nm \nIn Exercises 7 and 8, write th e equation of th e plane passing \nthrough P with normal vector n in (a) normal form and \n(b) general form. \n7.P\n� \n(0,  1,0),n\n� \n[\n;\n] \n8.P\n� \n(3, 0,-2) , n\n� \nm \nIn Exercises 9 and 10, write the equation of the plane pass­\ning through P with direction vectors u and v in (a) vector \nform and (b) parametric form. \n9. p \n� \n( o. o. 0\n)\n. \nu \n� \n[\n} � \n[\n-\n: \nl \n10. p \n� \n(\n6, -4, -3\n)\n, u \n� \n[\nl \n� \n[\n-\n:\ni \nIn Exercises 11 and 12, give th e vector equation of the line \npassing through P and Q . \n11. \nP =  (\n1, -2\n)\n, Q =  (\n3, O) \n12. P = (O, 1, -1\n)\n, Q =  (\n-2, 1, 3\n) \nIn Exercises 13 and 14, give th e vector equation of the plane \npassing through P, Q, and R. \n13. P = \n(\n1, 1, 1\n)\n, Q = (4, 0, 2\n)\n, R = (O, 1, -1\n) \n14. P = \n(\n1, 1, O), Q = \n(\n1, 0,  1\n)\n, R = (O, 1, 1\n) \n15. Find parametric equations and an equation in vector \nform for the lines in IR\n2 \nwith the following equations: \n(a) y \n= 3x -  1 \n(b) 3x + 2y \n= 5","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":6871,"to":6962}}}}],[192,{"pageContent":"16. Consider the vector equation x = p + t\n(\nq -\np\n)\n, where \np and q correspond to distinct points P and Q in IR\n2 \nor IR\n3\n. \n(a) Show that this equation describes the line segment \nPQ as t varies from 0 to 1. \n(b) For which value oft is x the midpoint of PQ, \nand what is x in this case? \n(c) Find the midpoint of PQ when P = (2, -3) and \nQ = (0, 1). \n(d) Find the midpoint of PQ when P = (1, 0, 1) \nand Q = (4, 1, -2). \n(e) Find the two points that divide PQ in part (  c) into \nthree equal parts. \n(f) Find the two points that divide PQ in part (d) into \nthree equal parts. \n17. Suggest a \"vector proof\" of the fact that, in IR\n2\n, two \nlines with slopes m\n1 \nand m\n2 \nare perpendicular if and \nonly if m\n1\nm\n2 \n= -1. \n18. The line e passes through the point P = (1, -1,  1) and \nh\n\"\n'  dfr,dion mtoc d \n� \n[ \n_\n; \nl Foe  mh of th, \nfollowing planes <if', determine whether e and <if' are \nparallel, perpendicular, or neither: \n(a) 2x + 3y -  z =  1 \n(c) x -y -z =  3 \n(h) 4x -y+ 5z=  0 \n(d) 4x + 6y -2z =  0 \n19. The plane <if' \n1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":6964,"to":7016}}}}],[193,{"pageContent":"following planes <if', determine whether e and <if' are \nparallel, perpendicular, or neither: \n(a) 2x + 3y -  z =  1 \n(c) x -y -z =  3 \n(h) 4x -y+ 5z=  0 \n(d) 4x + 6y -2z =  0 \n19. The plane <if' \n1 \nhas the equation 4x - y + 5z = 2. For \neach of the  planes <if' in Exercise 18, determine whether \n<if' \n1 \nand <if' are parallel, perpendicular, or neither. \n20. Find the vector form of the equation of the line in IR\n2 \nthat passes through P = (2, -1) and is perpendicular \nto the line with general equation 2x \n-\n3y = 1. \n21. Find the vector form of the equation of the line in IR\n2 \nthat passes through P = (2, -1) and is parallel to the \nline with general equation 2x -3y = 1. \n22. Find the vector form of the equation of the line in IR\n3 \nthat passes through P = ( -1, 0, 3) and is perpendicular \nto the plane with general equation x -  3y + 2z = 5. \n23. Find the vector form of the equation of the line in IR\n3 \nthat passes through P = (-1, 0, 3) and is parallel to \nthe line with parametric equations \nx = 1 -t \ny \n=","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":7016,"to":7049}}}}],[194,{"pageContent":"23. Find the vector form of the equation of the line in IR\n3 \nthat passes through P = (-1, 0, 3) and is parallel to \nthe line with parametric equations \nx = 1 -t \ny \n= \n2 +   3t \nz  = -2 -t \n24. Find the normal form of the equation of the plane that \npasses through P = (O, -2, 5) and is parallel to the \nplane with general equation 6x - y + 2z = 3. \nSection 1.3 \nLines and Planes \n45 \n25. A cube has vertices at the eight points (x, y, z), where \neach of x, y, and z is either 0 or 1. (See Figure 1.34.) \n(a) Find the general equations of the planes that \ndetermine the six faces (sides) of the cube. \n(b) Find the general equation of the plane that con­\ntains the diagonal from the origin to ( 1, 1, 1) and \nis perpendicular to the xy-plane. \n(c) Find the general equation of the plane that \ncontains the side diagonals referred to in \nExample 1.22. \n26. Find the equation of the set of all points that are \nequidistant from the points P = (1, 0, -2) and \nQ = (5, 2, 4).","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":7049,"to":7076}}}}],[195,{"pageContent":"contains the side diagonals referred to in \nExample 1.22. \n26. Find the equation of the set of all points that are \nequidistant from the points P = (1, 0, -2) and \nQ = (5, 2, 4). \nIn Exercises 27 and 28, find th e distance from the point Q to \nth e line e. \n27\n. \nQ \n= \n(2,\n2),f\nwith\neq\nuati\non \n[;] \n= \n[\n-\n�\n] \n+ \nt\n[ \n_\n�\n] \n28. Q \n� \n(O, !\n, O), \nM\nh 'qoation \n[\n�\n] \n� \n[\n:\nJ \n+ \n{\n-\n�\n] \nIn Exercises 29 and 30, find th e distance from th e point Q to \nthe plane <if'. \n29. Q = (2, 2,   2), <if' with equation x + y -z =  0 \n30. Q = (O, 0, O), <if' with equation x -  2y + 2z  =  1 \nFigure 1.66 suggests a way to use vectors to locate th e point \nR on e that is closest to Q. \n31. Find the point Ron f  that is closest to Qin Exercise 27. \n32. Find the point R   on e that is closest to Qin Exercise 28. \nQ \nI \nFioure 1.66 \n-----> \nr = p +PR","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":7076,"to":7135}}}}],[196,{"pageContent":"46 \nChapter 1 Vectors \nFigure 1.67 suggests a way to use vectors to locate the point \nR on <!J' that is closest to Q. \nk --\n0 \nFigure 1.61 \n------> ------> \nr = p + PQ +QR \n33. Find the point Ron <!J'  that is closest to Qin Exercise 29. \n34. Find the point Ron <!J'  that is closest to Qin Exercise 30. \nIn Exercises 35 and 36, find the distance between th e \nparallel lines. \n35. \n[;] \n[\n�\n] \n+ s\n[\n-\n�\n] \nand \n[;] [\n�\n] \n+ t\n[\n-\n�\n] \n36. [�] \n[ _�] + {] and [;] [:J + {] \nIn Exercises 37 and 38, find th e distance between the \nparallel planes. \n37. 2x + y -2z = 0 and 2x + y -2z = 5 \n38. x + y + z = \n1 \nand x + y + z = 3 \n39. Prove Equation (3) on page 43. \n40. Prove Equation (4) on page 44. \n41. Prove that, in !R\nz\n, the distance between parallel lines \nwith equations n \n· \nx  = c\n1 \nand n \n· \nx  = \nC\nz \nis given by \nl\nc\n1 \n-\nC\nz\nl \nll\nn\nll \n. \n42. Prove that  the distance between parallel planes with \nequations n · x  = d\n1 \nand n · x  = d\nz \nis  given  by \nI\nd\n, \n-  d\nz\nl \nll\nn\nll \nIf two nonparallel planes <!J' \n1 \nand <!J' \nz \nhave normal vectors n \n1 \nand n\nz","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":7137,"to":7225}}}}],[197,{"pageContent":"equations n · x  = d\n1 \nand n · x  = d\nz \nis  given  by \nI\nd\n, \n-  d\nz\nl \nll\nn\nll \nIf two nonparallel planes <!J' \n1 \nand <!J' \nz \nhave normal vectors n \n1 \nand n\nz \nand e is the angle between n\n1 \nand D\nz\n, then we defi ne \nth e angle between <!J' l and <!J' \nz \nto be either e or \n1\n80° -e, \nwhichever is an acute angle. (Figure 1.68) \n\\lJ> I \ne \\ \n� \n180 -e \nFigure 1.68 \nIn Exercises 43-44, find the acute angle between the planes \nwith the given equations. \n43. x + y + z = 0 and 2x + y -2z = 0 \n44. 3x - y + 2z = 5 and x + 4y -  z = 2 \nIn Exercises 45-46, show that th e plane and line with th e \ngiven equations intersect, and then fi nd th e acute angle of \nintersection between them. \n45. The plane given by x + y + 2z = 0 and the line \ngiven by x = 2 + t \ny \n= 1 \n-2t \nz = 3 + \nt \n46. The  plane given  by 4x -   y -   z \ngiven by x = t \ny \n= 1 \n+ 2t \nz = 2 + 3t \n6  and the line \nExercises 47-48 explore one approach to th e problem of \nfi nding th e projection of a vector onto a plane. As Fig-\nure 1.69 shows, i\nf","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":7225,"to":7287}}}}],[198,{"pageContent":"given by x = t \ny \n= 1 \n+ 2t \nz = 2 + 3t \n6  and the line \nExercises 47-48 explore one approach to th e problem of \nfi nding th e projection of a vector onto a plane. As Fig-\nure 1.69 shows, i\nf\n<!J' is a plane through th e origin in IR\n3 \nwith \nnormal vector n, and vis a vector in IR\n3\n, then p  = proj9p(v) \nis a vector in <!J' such that v -  e n = p for some scalar c. \nn \nFigure 1.69 \nProjection onto a plane","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":7287,"to":7306}}}}],[199,{"pageContent":"47. Using the fact that n is orthogonal to every vector in <!f' \n(and hence to\np\n), solve for c and thereby find an ex  pres­\nsion for \np \nin terms of v and n. \n48. Use the method of Exercise 43 to find the projection of \nSection 1.3 \nLines and Planes 41 \nonto the planes with the following equations: \n(a) \nx \n+ y + \nz \n= \n0 \n(b) \n3\nx \n-\ny + \nz \n= \n0 \n(c) x -2z = 0 \n(d) \n2x \n-\n3y + \nz \n= \n0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":7308,"to":7340}}}}],[200,{"pageContent":"48 \nExploration \nThe Cross Product \nIt would be convenient if we could easily convert the vector form x = p + su + tv of \nthe equation of a plane to the normal form n · x  =  n · p. What we need is a process \nthat, given two nonparallel vectors u and v, produces a third vector n that is orthogo­\nnal to both u and v. One approach is to use a   construction known as the cross product \nof vectors. Only valid in IR\n3\n, it is defined as follows: \nDefinition \ndefined by \n[\nU\n2\nV\n3 -\nU3\nV2\n] \nU X V = \nU\n3V\n1 \n-\nU\n1\nV\n3 \nU\n1\nV2 \n-\nU\n2V\n1 \nA shortcut that can help you remember how to calculate the cross prod uct of \ntwo vectors is illustrated below. Under each complete vector, write the first two com­\nponents of tha  t vector. Ignoring the two components on the top line, consider each \nblock of four: Subtract the prod ucts of the components connected by dashed lines \nfrom the prod ucts of the components connected by solid lines. (It helps to notice that","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":7342,"to":7382}}}}],[201,{"pageContent":"block of four: Subtract the prod ucts of the components connected by dashed lines \nfrom the prod ucts of the components connected by solid lines. (It helps to notice that \nthe first component of u X v has no ls as subscripts, the second has no 2s, and the \nthird has no 3s.) \nU\n1 \nV\n1 \nU\n2 \nx \nV\nz \nU3 \nx \nV\n3 \nU\n2V\n3 -\nU3\nV2 \nU\n1 \nx \nV\n1 \nU\n3V\n1 \n-\nU\n1\nV\n3 \nU\nz \nV\nz \nU\n1\nV\n2 \n-\nU\n2\nV\n1 \nThe following problems briefly explore the cross product. \n1. Compute u X v.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":7382,"to":7431}}}}],[202,{"pageContent":"uXv \nv \nFigure 1.10 \nFigure 1.11 \nWriting Project \n2. Show that e\n1 \nX \ne\n2 \n= e\n3\n, e\n2 \nX e\n3 \n= e\n1\n, and e\n3 \nX e\n1 \n= e\n2\n. \n3. Using the definition of a cross product, prove that u Xv (as shown in Figure 1.70) \nis orthogonal to u and v. \n4. Use the cross product to help find the normal form of the equation of the plane. \n(a)  The pfane P\"'ing thrnugh P \n� \n( 1\n. \n0, -2  ), parn!klto u \n� [:] and v � [ \n-\n�] \n(b) The plane passing through P = (0, -1, 1), Q = (2, 0, 2), and R = (1, 2, -1) \n5. Prove the following properties of the cross product: \n(a) v Xu= \n-(\nu Xv\n) \n(b) u X 0 = 0 \n(c) u Xu= 0 (d) u X  kv = k\n(\nu Xv\n) \n(e) u X  ku = 0 \n(f) u X \n(\nv + w\n) \n= u X v + u X w \n6. Prove the following properties of the cross product: \n(a) u· (v X  w)  =  (u Xv) ·w (b) u  X (v X  w)  =  (u ·w)v \n-\n(u ·v)w \n(c) \nll\nu  X  v\nll\n2  = \nll\nu\nll\n2\nll\nv\nll\n2 \n-\n(u·v)2 \n7. Redo Problems 2 and 3, this time making use of Problems 5 and 6. \n8. Let u and v be vectors in IR\n3 \nand let (} be the angle between u and v. \n(a)  Prove that \nll\nu  X \nv\nii \n= \nll\nu\nll ll\nv\nll \nsin (}\n. \n[H\nint\n:","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":7433,"to":7524}}}}],[203,{"pageContent":"7. Redo Problems 2 and 3, this time making use of Problems 5 and 6. \n8. Let u and v be vectors in IR\n3 \nand let (} be the angle between u and v. \n(a)  Prove that \nll\nu  X \nv\nii \n= \nll\nu\nll ll\nv\nll \nsin (}\n. \n[H\nint\n: \nUse \nProble\nm \n6(c).\n] \n(b)  Prove that the area A of the triangle determined by u and v (as shown in Fig­\nure 1. 71) is given by \nA= \nH\nu  Xv\nii \n( c)  Use the result in part (b) to compute the area of the triangle with vertices \nA= \n(1, 2, 1), B = (2, 1, O), and C = (5, -1, 3). \nThe Origins of the Dot Product and Cross Product \nThe notations for dot and cross prod uct that we use today were introduced in the \nlate 19th century by Josiah Willard Gibbs, a professor of mathematical physics at \nYale University. Edwin B. Wilson was a graduate student in Gibbs's class, and he \nlater wrote up his class notes, expanded upon them, and had them published in \n1901, with Gibbs's blessing, as Vector Analysis: A Text-Book for the Use of Students","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":7524,"to":7563}}}}],[204,{"pageContent":"later wrote up his class notes, expanded upon them, and had them published in \n1901, with Gibbs's blessing, as Vector Analysis: A Text-Book for the Use of Students \nof Mathematics and Physics. However, the concepts of dot and cross prod uct arose \nearlier and went by various other names and notations. \nWrite a report on the evolution of the names and notations for the dot product \nand cross product. \n1. Florian Cajori, A History of Mathematical Notations (New York: Dover, 1993). \n2. J. Willard Gibbs and Edwin Bidwell Wilson, Vector Analysis: A Text-Book for the \nUse of Students of Mathematics and Physics (New York: Charles Scribner's Sons, \n1901). Available online at http://archive.org/details/117714283. \n3. Ivor Grattan-Guinness, Companion Encyclopedia of the History and Philosophy \nof the Mathematical Sciences (London: Routledge, 2013). \n49","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":7563,"to":7575}}}}],[205,{"pageContent":"50 \nChapter 1 Vectors \nForce is defined as the product of \nmass and acceleration due to grav­\nity (which, on Earth, is 9.8 m/s\n2\n) . \nThus, a 1 kg mass exerts a down­\nward force of 1 kg X 9.8 m/s\n2 \nor \n9.8 kg • m/s\n2\n• This unit of measure­\nment is a newton (N). So the force \nexerted by a 1 kg mass is 9.8 N. \nFigure 1.12 \nThe resultant of two forces \nExample 1.34 \nApplications \nForce Veclors \nWe can use vectors to model force. For example, a wind blowing at 30 km/h in a west­\nerly direction or the Earth\n'\ns gravity acting on a 1 kg mass with a force of9.8 newtons \ndownward are each best represented by vectors since they each consist of a magnitude \nand a direction. \nIt is often the case that multiple forces act on an object. In such situations, the \nnet result of all  the forces acting together is a single force called the resultant, which \nis simply the vector sum of the individual forces (Figure 1.72). When several forces","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":7577,"to":7606}}}}],[206,{"pageContent":"net result of all  the forces acting together is a single force called the resultant, which \nis simply the vector sum of the individual forces (Figure 1.72). When several forces \nact on an object, it is possible that the resultant force is zero. In this case, the object \nis clearly not moving in any direction and we say that it  is in equilibrium. When an \nobject is in equilibrium and the force vectors acting on it are arranged head-to-tail, \nthe result is a closed polygon (Figure 1.73). \nFigure 1.13 \nEquilibrium \nAnn and Bert are trying to roll a rock out of the way. Ann pushes with a force of 20 N \nin a northerly direction while Bert pushes with a force of 40 N in an easterly direction. \n(a) \nWhat is the resultant force on the rock? \n(b) \nCarla is trying to prevent Ann and Bert from moving the rock. What force must \nCarla apply to keep the rock in equilibrium? \nSolution \n(a)  Figure 1.74 shows the position of the two  forces.  Using the paral­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":7606,"to":7622}}}}],[207,{"pageContent":"Carla apply to keep the rock in equilibrium? \nSolution \n(a)  Figure 1.74 shows the position of the two  forces.  Using the paral­\nlelogram rule, we add the two forces to get the resultant r as shown. By Pythagoras' \na a \nb b \nFigure 1.14 \nThe resultant of two forces","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":7622,"to":7629}}}}],[208,{"pageContent":"Example 1.35 \nSection 1.4 Applications \n51 \nTheorem, we  see  that \nll\nr\nll \n= \nV\n20\n2 \n+ 40\n2 \n= V20QO = 44.72 N. For  the  direc­\ntion of r, we calculate the angle e between r and Bert's easterly force. We  find that \nsine = 20\n/\nll\nr\nll \n= 0.447, so e = 26.57°. \n(b) \nIf we denote the forces exerted by Ann, Bert, and Carla by a, b, and c, respec­\ntively, then we require a + b + c = 0. Therefore c = -(a + b) = -r, so Carla \nneeds to exert a force of 44. 72 N in the direction opposite to r. \nOften, we are interested in decomposing a force vector into other vectors whose \nresultant is the given vector. This process is called resolving a vector into  com­\nponents. In two dimensions, we wish to resolve a vector into two components. \nHowever, there are infinitely many ways to do this; the most useful will be to re­\nsolve the vector into two orthogonal components. (Chapters 5 and 7 explore this \nidea more generally.) This is usually done by introducing coordinate axes and by","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":7631,"to":7661}}}}],[209,{"pageContent":"solve the vector into two orthogonal components. (Chapters 5 and 7 explore this \nidea more generally.) This is usually done by introducing coordinate axes and by \nchoosing the components so that one is parallel to the x-axis and the other to the \ny-axis. These components are usually referred to as   the horizontal and vertical \ncomponents, respectively. In Figure 1.75, f is the given vector and fx and f\ny \nare its \nhorizontal and vertical components. \ny \nx \nFigure 1.15 \nResolving a vector into components \nAnn pulls on the handle of a wagon with a force of 100 N. If the handle makes an \nangle of20° with the horiz  ontal, what is the force that tends to pull the wagon forward \nand what force tends to lift it off the ground? \nSolution \nFigure 1.76 shows the situation and the vector diagram that we need to \nconsider. \nFigure 1.16","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":7661,"to":7679}}}}],[210,{"pageContent":"52 Chapter 1 Vectors \nExample 1.36 \nWe see that \nl\nl\nf\nx \nI\nI = \n11\n£11 \ncos20° and \nl\nlf\ny \nI \n= \n1 £\n1 1 \nsin20° \nThus, \nllf\nx\nll \n= 100 (\n0.9397\n) \n= 93.97 and \nllf\ny\nll \n= 100 (\n0.3420\n) \n= 34.20. So  the \nwagon is pulled forward with a force of approximately 93.97 N and it tends to lift off \nthe ground with a force of approximately 34.20 N. \nWe solve the next example using two different methods. The first solution considers \na triangle of forces in equilibrium; the second solution uses resolution of forces into \ncomponents. \nFigure 1. 77 shows a painting that has been hung from the ceiling by two wires. If the \npainting has a mass of 5 kg and if the two wires make angles of 45 and 60 degrees with \nthe ceiling, determine the tension in each wire. \nFigure 1.11 \nSolulion 1 We assume that the painting is in equilibrium. Then the two wires must \nsupply enough upward force to balance the downward force of gravity. Gravity \nexerts a downward force of 5 X 9.8 = 49 Non the painting, so the two wires must","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":7681,"to":7727}}}}],[211,{"pageContent":"supply enough upward force to balance the downward force of gravity. Gravity \nexerts a downward force of 5 X 9.8 = 49 Non the painting, so the two wires must \ncollectively pull upward with 49 N of force. Let \nf\n1 and \nf\n2 denote the tensions in the \nwires and let r be their resultant (Figure 1. 78 ). It follows that \nII \nr II = 49 since we are \nin equilibrium.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":7727,"to":7737}}}}],[212,{"pageContent":"30\n° \nFigure 1.18 \nV\nz \n/ \n60° \n45° \n49N \nFigure 1.19 \nI \nExercises 1.4 \nForce Vectors \nSection 1.4 Applications \n53 \nUsing the law of sines, we have \nso \nll\nf\n1ll ll\nr\nll \nsin 45° \nsin 30° \nsin 105° \n11   11 \nll\nr\nll\nsin 45° \n49\n(\n0.7071\n) \nII II \nll\nr\nll\nsin 30° \n49\n(\n0.5\n) \nf\n1 \n= \n= \n= \n35.87 and  f\n2 \n= \n= = \n25.36 \nsin 105° 0.9659 \nsin 105° 0.9659 \nTherefore, the tensions in the wires are approximately 35.87 N and 25.36 N. \nSolution 2 We  resolve f\n1 \nand f\n2 \ninto horizontal and vertical components, say,  f\n1 \n= \nh1 \n+ v\n1 \nand f\n2 \n= \nh\n2 \n+ v\n2\n, and note that, as above, there is a downward force of 49 N \n(Figure 1.79). \nIt follows that \nll\nv\nzll \n= \nll\nf\n2\nll \nsin 45° = \nI� \nSince the painting is in equilibrium, the horizontal components must balance, as \nmust the vertical components. Therefore, \nII \nh\n1 \nII \n= \nll\nh\n2 \nII \nand \nll\nv\n1 \nII \n+ \nll\nv\n2 \nII \n= 49, from \nwhich it follows that \nand \nSubstituting the first of these equations into the second equation yields \nv'3\nll\nf\n2\nll \nfil \n_ \nll\nf \nII \n_ \n49\n\\/2 \n_ \n, \n;;:; \n+ \n, \n;;:; \n-\n49,  or \n2 \n-\n, \n;;:; \n- 25.36 \nVL v2 1 + v3 \nThus, \n11\n£\n1\n11 \n= v2\nll\nf\n2\nll \n=","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":7739,"to":7887}}}}],[213,{"pageContent":"and \nSubstituting the first of these equations into the second equation yields \nv'3\nll\nf\n2\nll \nfil \n_ \nll\nf \nII \n_ \n49\n\\/2 \n_ \n, \n;;:; \n+ \n, \n;;:; \n-\n49,  or \n2 \n-\n, \n;;:; \n- 25.36 \nVL v2 1 + v3 \nThus, \n11\n£\n1\n11 \n= v2\nll\nf\n2\nll \n= \n1.4142\n(\n25.36\n) = \n35.87, so the tensions in  the hires are \napproximately 35.87 N and 25.36 N, as before. \nIn Exercises 1-6, determine th e resultant of th e given \nforces. \n2. f\n1 \nacting due west with a magnitude of 15 N and \nf\n2 \nacting due south with a magnitude of 20 N \n3. f\n1 \nacting with a magnitude of 8 N and f\n2 \nacting at an \nangle of 60° to f\n1 \nwith a magnitude of 8 N \n1. f\n1 \nacting due north with a magnitude of 12 N and \nf\n2 \nacting due east with a magnitude of 5 N \n4. f\n1 \nacting with a magnitude of 4 N and f\n2 \nacting at an \nangle of 135° to f\n1 \nwith a magnitude of 6 N","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":7887,"to":7962}}}}],[214,{"pageContent":"54 Chapter 1 Vectors \n5. f1 acting due east with a magnitude of 2 N, f\n2 \nacting \ndue west with a magnitude of 6 N, and f\n3 \nacting at an \nangle of 60° to f1 with a magnitude of 4 N \n6. f1 acting due east with a magnitude of 10 N, f\n2 \nacting \ndue north with a magnitude of 13 N, f\n3 \nacting due west \nwith a magnitude of 5 N, and f4 acting due south with \na magnitude of 8 N \n7. Resolve a   force of 10 N into two forces perpendicular \nto each other so that one component makes an angle \nof 60° with the 10 N force. \n8. A 10 kg  block lies on a ramp that is inclined at an angle \nof30° (Figure 1.80). Assuming there is no friction, what \nforce, parallel to the ramp, must be applied to keep the \nblock from sliding down the ramp? \nFigure 1.80 \n9. A tow truck is towing a car. The tension in the tow \ncable is 1500 N and the cable makes a 45° with the \nhorizontal, as sh  own in Figure 1.81. What is the verti­\ncal force that tends to lift the car off the ground? \nf= 1500 N \nFigure 1.81","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":7964,"to":7993}}}}],[215,{"pageContent":"cable is 1500 N and the cable makes a 45° with the \nhorizontal, as sh  own in Figure 1.81. What is the verti­\ncal force that tends to lift the car off the ground? \nf= 1500 N \nFigure 1.81 \n10. A lawn mower has a mass of 30 kg.   It is being pushed \nwith a force of 100 N. If the handle of the lawn mower \nmakes an angle of 45° with the ground, what is the \nhorizontal component of the force that is causing the \nmower to move forward? \n11. A sign hanging outside Joe's Diner has a mass of 50 kg \n(Figure 1.82). If the supporting cable makes an angle \nof 60° with the wall of the building, determine the \ntension in the cable. \nFigure 1.82 \n12. A sign hanging in the window of Joe's Diner has a \nmass of 1 kg.  If the supporting strings each make an \nangle of 45° with the sign and the supporting hooks \nare at the same height (Figure 1.83), find the tension in \neach string. \n• \n,\n,\n• \n_\n_ \n,',f2 \n_\n,\n,\n' \n45\n° \nOPEN  FOR BUSINESS \nFigure 1.83 \n13. A painting with a mass of 15 kg is suspended by two","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":7993,"to":8028}}}}],[216,{"pageContent":"are at the same height (Figure 1.83), find the tension in \neach string. \n• \n,\n,\n• \n_\n_ \n,',f2 \n_\n,\n,\n' \n45\n° \nOPEN  FOR BUSINESS \nFigure 1.83 \n13. A painting with a mass of 15 kg is suspended by two \nwires from hooks on a ceiling. If the wires have lengths \nof 15 cm and 20 cm and the distance between the \nhooks is 25 cm, find the tension in each wire. \n14. A painting with a mass of 20 kg is suspended by two \nwires from a ceiling. If the wires make angles of 30° \nand 45° with the ceiling, find the tension in each wire.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":8028,"to":8051}}}}],[217,{"pageContent":"Chapter Review \nKev Definitions and concepts \nalgebraic properties of vectors, 10 \nangle between vectors, 24 \nbinary vector,  13 \nCauchy-Schwarz Inequality, 22 \ncross product, 48 \nhead-to-tail rule, 6 \nintegers modulo m (Z\nm\n), \nlength (norm) of a vector, \nlinear combination of \n14-16 \n20 \nprojection of a vector onto \na vector, 27 \nPythagoras' Theorem, 26 \nscalar multiplication, 7 \nstandard unit vectors, 22 \nTriangle Inequality, 22 \nunit vector, 21 \nvectors, 12 \ndirection vector, 35 \ndistance between vectors, 23 \ndot product, 18 \nequation of a line, 36 \nequation of a plane, 38-39 \nnormal vector, 34, 38 \nm-ary vector 16 \northogonal vectors, 26 \nparallel vectors,  8 \nparallelogram rule, 6 \nvector, 3 \nvector addition, 5 \nzero vector, 4 \nReview Questions \n1. Mark each of the following statements true or false: \n(a) For vectors U, v, and win rr;r, ifu + w = v + w, \nthen u =   v. \n(b) For vectors U, v, and win rr;r, ifu\n. \nw = v\n. \nw, then \nu=v. \n(c) For vectors u, v, and win IR\n3\n, if u is orthogonal","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":8053,"to":8101}}}}],[218,{"pageContent":"(a) For vectors U, v, and win rr;r, ifu + w = v + w, \nthen u =   v. \n(b) For vectors U, v, and win rr;r, ifu\n. \nw = v\n. \nw, then \nu=v. \n(c) For vectors u, v, and win IR\n3\n, if u is orthogonal \nto v, and vis orthogonal to w, then u is orthogonal \nto w. \n(d) In IR\n3\n, if a line C is parallel to a plane <:IP, then a di­\nrection vector d for e is parallel to a normal vector \nn for <:IP. \n(e) In IR\n3\n, if a line e is perpendicular to a plane <:IP, then \na direction vector d for e is a parallel to a normal \nvector n for <:IP. \n(f) In IR\n3\n, if two planes are not parallel, then they must \nintersect in a line. \n(\ng\n) In IR\n3\n, if two lines are not parallel, then they must \nintersect in a point. \n(h) If vis a  binary vector such that v · v = 0, then \nv = 0. \n(i) In l:'.\n5\n, if ab = 0 then either a = 0 or b = 0. \n(j) In l:'.6, if ab = 0 then either a = 0 or b = 0. \n4. Let A, B, C, and D be the vertices of a square centered \n----> \nat the origin 0, labeled in clo  ckwise order. If a = OA \nand b =OB, find BC in terms of a and b.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":8101,"to":8143}}}}],[219,{"pageContent":"4. Let A, B, C, and D be the vertices of a square centered \n----> \nat the origin 0, labeled in clo  ckwise order. If a = OA \nand b =OB, find BC in terms of a and b. \n5. Find the angle between the vectors [ -1,  1, 2] and \n[2, 1, -1]. \n6. Hnd ilie prnjection of v \n� \n[:] onto u \n� [ \n-\nn \n7. Find a unit vector in the xy-plane that is orthogonal \ntom \n8. Find the general equation of the plane through the \npoint ( 1,  1, 1) that is perpendicular to the line with \nparametric equations \nx = 2  -t \ny = \n3 + 2t \nz = -1 + t \n2. If u = \n[ \n-\n� \nl \nv = \n[ \n� \nl \nand the vector 4u + vis drawn \nwith its tail at the point (10, \n-\n10), find the coordinates \nof the point at the head of 4u + v. \n9. Find the general equation of the plane through the \npoint (3, 2, 5) that is parallel to the plane whose general \nequation is 2x + 3y \n-\nz = 0. \n3. Ifu = \n[\n-\n�\nl \nv = \n[\n�\nl\nand 2x + u = 3(x - v), solve \nfor x. \n10. Find the general equation of the plane through the \npoints A(l, 1, O\n)\n, B(l, 0, 1), and C\n(\nO , 1, 2).","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":8143,"to":8199}}}}],[220,{"pageContent":"equation is 2x + 3y \n-\nz = 0. \n3. Ifu = \n[\n-\n�\nl \nv = \n[\n�\nl\nand 2x + u = 3(x - v), solve \nfor x. \n10. Find the general equation of the plane through the \npoints A(l, 1, O\n)\n, B(l, 0, 1), and C\n(\nO , 1, 2). \n11. Find the area of the triangle with vertices A(l, 1, O\n)\n, \nB(l, 0, 1), and C\n(\nO , 1, 2). \n55","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":8199,"to":8225}}}}],[221,{"pageContent":"56 \nChapter 1 Vectors \n12. Find the midpoint of the line segment between \nA = \n(\n5, 1, -2) and B = (3, -7, 0). \n13. Why are there no vectors u and v in !R\nn \nsuch that \nll\nu\nll \n= \n2, \nll\nv\nll \n= \n3, \nand \nu \n· \nv \n= \n-7\n? \n14. Find the distance from the point (3, 2,  5) to the plane \nwhose general equation is 2x + 3y -z = 0. \n15. Find the distance from the point (3, 2, 5) to the line \nwith parametric equations x = t, y = 1  + t, z = 2  + t. \n16. Compute 3 -(2 + 4\n)\n3\n(\n4 + 3)\n2 \nin Z5. \n17. If possible, solve 3\n(\nx + 2) = 5 in Z7• \n18. If possible, solve 3\n(\nx + 2) = 5 in Z9• \n19. \nCompute [2, 1, 3, 3] · [3, 4, 4, 2] in z\nt. \n20. Let u = [l, 1, 1, O] in Zi. How many binary vectors v \nsatisfy u \n· \nv = O\n?","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":8227,"to":8277}}}}],[222,{"pageContent":"The world was fu ll of equations .... \nThere must be an answer fo r everything, \nif only you knew how to set fo rth \nthe questions. \n-Anne Tyler \nThe Accidental To urist \nAlfred A. Knopf, 1985, p. 235 \nSyst\nem\ns of \nLineari \nE\nquations \n2.0 Introduction: TrivialilV \nThe word trivial is derived from the Latin root tri (\"three\") and the Latin word via \n(\"road\"). Thus, speaking literally, a triviality is a place where three roads meet. This \ncommon meeting point gives rise to the other, more familiar meaning of trivial­\ncommonplace, ordinary,  or insignificant.   In medieval universities, the trivium con­\nsisted of the  three \"common\" subjects (grammar, rhetoric, and logic) that were taught \nbefore the quadrivium (arithmetic, geometry,  music, and astronomy). The \"three \nroads\" that made up the trivium were the beginning of the liberal arts. \nIn this section, we begin to examine systems oflinear equations. The same system \nof equations can be viewed in three different, yet equally important, ways-these will","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":8279,"to":8301}}}}],[223,{"pageContent":"In this section, we begin to examine systems oflinear equations. The same system \nof equations can be viewed in three different, yet equally important, ways-these will \nbe our three roads, all leading to the same solution. You will need to get used to this \nthreefold way of viewing systems of linear equations, so that it  becomes common­\nplace (trivial!) for you. \nThe system of equations we are going to consider is \n2x+ y= 8 \nx - 3y = -3 \nProblem 1 Draw the two lines represented by these equations. What is their point \nof intersection? \nProblem 2 Consider the vectors u = \n[ \n�\n] \nand v = \n[ \n_ \n�] \n. Draw the coordinate \ngrid determined by u and v. [Hint: Lightly draw the standard coordinate grid first and \nuse it as an aid in drawing the new one.] \nProblem 3 On the u-v grid, find the coordinates of w = \n[ \n_\n:\n] \n. \nProblem 4 Another way to state Problem 3 is to   ask for the coefficients x and y \nfor which xu  + yv = w. Write out the two equations to which this vector equation is","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":8301,"to":8329}}}}],[224,{"pageContent":"[ \n_\n:\n] \n. \nProblem 4 Another way to state Problem 3 is to   ask for the coefficients x and y \nfor which xu  + yv = w. Write out the two equations to which this vector equation is \nequivalent (one for each component). What do you observe? \nProblem 5 Return now to the lines you drew for Problem 1. We will refer to the \nline whose equation is 2x + y = 8 as line 1 and the line whose equation is x - 3y =   -3 \nas line 2. Plot the point (O, O) on your graph from Problem 1 and label it P\n0\n. Draw a \n51","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":8329,"to":8342}}}}],[225,{"pageContent":"58 \nChapter 2 \nSystems of Linear Equations \nTable 2.1 \nPoint \nx \ny \nP\no \n0 0 \nP\n1 \nP\n2 \nP\n3 \nP4 \nP\ns \np6 \nExample 2.1 \nhorizontal line segment from P0 to line 1 and label this new point P\n1\n. Next draw a \nvertical line segment from P\n1 \nto line 2 and label this point P\n2\n• Now draw a horizontal \nline segment from P\n2 \nto line 1, obtaining point P\n3\n• Continue in this fashion, drawing \nvertical segments to line 2 followed by horizontal segments to line 1. What appears to \nbe happening? \nProblem 6 Using a calculator with two-decimal-place accuracy, find the (approxi­\nmate) coordinates of the points P\n1\n, P\n2\n, P\n3\n,  ... , P6• (You will find it helpful to first \nsolve the first equation for x in terms of y and the second equation for y in terms \nof x.) Record your results in Table 2.1, writing the x-and y-coordinates of each point \nseparately. \nThe results of these problems show that the task of \"solving\" a system of linear \nequations may be viewed in several ways. Repeat the process described in the prob­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":8344,"to":8392}}}}],[226,{"pageContent":"separately. \nThe results of these problems show that the task of \"solving\" a system of linear \nequations may be viewed in several ways. Repeat the process described in the prob­\nlems with the following systems of equations: \n(a) 4x - 2y = 0 \nx +   2y = 5 \n(\nb\n)\n3x +2y= 9 \nx +   3y = 10 \n(\nc\n)\nx+y=S \nx-y=3 \n(\nd\n) \nx +   2y = 4 \n2x - y=3 \nAre all of your observations from Problems 1-6 still valid for these examples? Note \nany similarities or differences. In this chapter, we will explore these ideas in more detail. \nIntroduction to svstems ot linear Equations \nRecall that the general equation of a line in IR\n2 \nis of the form \nax + by = c \nand that the general equation of a plane in IR\n3 \nis of the form \nax + by + cz = d \nEquations of this form are called linear equations. \nDefinition \nA linear equation in the n variables x\n1\n, x\n2\n, ••• , xn is an  equation \nthat can be written in the form \nwhere the coefficients a\n1\n, a\n2\n, .•. , a\nn \nand the constant term b are constants. \nThe following equations are linear: \n3x - 4y = \n-1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":8392,"to":8441}}}}],[227,{"pageContent":"1\n, x\n2\n, ••• , xn is an  equation \nthat can be written in the form \nwhere the coefficients a\n1\n, a\n2\n, .•. , a\nn \nand the constant term b are constants. \nThe following equations are linear: \n3x - 4y = \n-1 \nr -ts - lft = 9 \n\\/2x  + \n:y \n-(sin \n:)z \n= 1 3.2x1 - O.Olx2 = 4\n.\n6 \nObserve that the third equation is linear because it can be rewritten in the form x\n1 \n+ \n5x\n2 \n+ x\n3 \n-  2x4 = 3. It is also important to note that, although in these examples (and \nin most applications) the coefficients and constant terms are real numbers, in some \nexamples and applications they will be complex numbers or members of \"11.\nP \nfor some \nprime number p.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":8441,"to":8476}}}}],[228,{"pageContent":"Example 2.2 \nExample 2.3 \nExample 2.4 \nSection 2.1 \nIntroduction to Systems of Linear Equations \n59 \nThe following equations are not linear: \nxy  + 2z = 1 \nxi - x� = 3 \nx \n-\n+z=2 \ny \nThus, linear equations do not contain products, reciprocals, or   other functions of the \nvariables; the variables occur only to the first power and are multiplied only by con -\nstants.  Pay particular attention to the fourth example in each list: Why is it that the \nfourth equation in the first list is linear but the fourth equation in the second list is not? \n4 \nA solution of a  linear equation a\n1\nx\n1 \n+  a\n2\nx\n2 \n+ ·   ·   · + a\nn\nx\nn \n= b is  a  vector \n[s\n1\n, s\n2\n, \n••• \n, s\nn\nl \nwhose components satisfy the equation when we substitute x\n1 \n= s\n1\n, \nX\n2 \n= \nS \n2> \n••• \n, X\nn \n= \nS\nn\n-\n(a)  [5, 4] is a solution of3x -4y = -1 because, when we substitute x = 5 and y = 4, \nthe equation is satisfied: 3(5) -4(4) = -1. [l, 1] is another solution. In general, the \nsolutions simply correspond to the points on the line determined by the given equa­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":8478,"to":8537}}}}],[229,{"pageContent":"the equation is satisfied: 3(5) -4(4) = -1. [l, 1] is another solution. In general, the \nsolutions simply correspond to the points on the line determined by the given equa­\ntion. Thus, setting x = t and solving for y, we see that the complete set of solutions \ncan be written in the parametric form [  t, � + �t]. (We could also set y equal to some \nparameter-say, s-and sol  ve for x instead; the two parametric solutions would look \ndifferent but would be equivalent. Try this.) \n(b) The linear equation x\n1 \n-x\n2 \n+  2x\n3 \n=  3   has [3, 0, O],  [O, 1, 2], and [6, 1, -1] \nas specific solutions. The  complete set of solutions corresponds to the set of points \nin the plane determined by the given equation. If we set x\n2 \n= s  and x\n3 \n= t, then a \nparametric solution is given by [3 + s -2t, s, t] . (Which values of sand t produce the \nthree specific solutions above?) \nA system of linear equations is a finite set oflinear equations, each with the same","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":8537,"to":8558}}}}],[230,{"pageContent":"three specific solutions above?) \nA system of linear equations is a finite set oflinear equations, each with the same \nvariables. A solution of a system of linear equations is a vector that is simultaneously \na solution of each equation in the system. The solution set of a system oflinear equa­\ntions is the set of all solutions of the system. We will refer to the process of finding the \nsolution set of   a system oflinear equations as \"solving the system:' \nThe system \n2x - y = 3 \nx +   3y = 5 \nhas  [   2, 1] as a solution, since it is a solution of both equations. On the other hand, \n[ 1, -1] is not a solution of the system, since it satisfies only the first equation. \nSolve the following systems of linear equations: \n(\na\n) \nx - y = 1 \nx+y=3 \n(\nb\n) \nx - y = 2 \n2x - 2y = 4 \n(\nc\n) \nx - y = 1 \nx-y=3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":8558,"to":8584}}}}],[231,{"pageContent":"60 \nChapter 2 \nSystems of Linear Equations \n.J \n(\na\n) \nFigure 2.1 \nSolution \n(a)  Adding the two equations together gives 2x = 4, so x = 2, from which we find \nthat y = 1. A quick check confirms that [ 2, 1] is indeed a solution of both equations. \nThat this is the only solution can be seen by observing that this solution corresponds \nto the (unique) point of intersection ( 2, 1) of the lines with equations x -y = 1 and \nx + y = 3, as sh  own in Figure 2.l(a). Thus, [2, 1] is a unique solution. \n(b) The second equation in this system is just twice the first, so the solutions are the \nsolutions of the first equation alone-namely, the points on the line x -y = 2. These \ncan be represented parametrically as [2 + t, t]. Thus, this system has infinitely many \nsolutions \n[Figure 2.1 (b)]. \n(c) Two numbers x and y cannot simultaneously have a difference of 1 and 3. Hence, \nthis system has no solutions. (A more algebraic approach might be to subtract the second","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":8586,"to":8606}}}}],[232,{"pageContent":"solutions \n[Figure 2.1 (b)]. \n(c) Two numbers x and y cannot simultaneously have a difference of 1 and 3. Hence, \nthis system has no solutions. (A more algebraic approach might be to subtract the second \nequation from the first, yielding the absurd conclusion 0 = -2.) As Figure 2.l(c) shows, \nthe lines for the equations are parallel in this case . \ny y \n4 \n(b) \n(c) \nA system oflinear equations is called consistent if it has at   least one solution. A sys­\ntem with no solutions is called inconsistent. Even though they are small, the three sys­\ntems in Example 2.4 illustrate the only three possibilities for the number of solutions of \na system of linear equations with real coefficients. We will prove later that these same \nthree possibilities hold for any system of linear equations over the real numbers. \nA system of linear equations with real coefficients has either \n(a)  a unique solution (a consistent system) or \n(b) infinitely many solutions (a consistent system) or","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":8606,"to":8623}}}}],[233,{"pageContent":"A system of linear equations with real coefficients has either \n(a)  a unique solution (a consistent system) or \n(b) infinitely many solutions (a consistent system) or \n( c)  no solutions (an inconsistent system). \nSolving a  svstem or Linear Equations \nTwo linear systems are called equivalent if they have the same solution sets. For \nexample, \nx-y=l \nx+y\n=3 \nand \nx-y=l \ny \n= 1 \n� \nare equivalent, since they both have the unique solution [2, l]. (Check this.)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":8623,"to":8638}}}}],[234,{"pageContent":"Example 2.5 \nExample 2.6 \nThe word matrix is derived from \nthe Latin word mater, meaning \n\"mother:' When the suffix -ix \nis added, the meaning becomes \n\"womb:' Just as a womb surrounds \na fe tus, the brackets of a matrix \nsurround its entries, and just as \nthe womb gives rise to a baby, a \nmatrix gives rise to certain types of \nfunctions called linear transforma­\ntions. A matrix with m rows and \nn columns is called an m X n \nmatrix (pronounced \"m by n\"). \nThe plural of matrix is matrices, \nnot \"matrixes:' \nSection 2.1 \nIntroduction to Systems of Linear Equations \n61 \nOur approach to solving a system of linear equations is to  transform the given \nsystem into an equivalent one that is easier to solve. The triangular pattern of the \nsecond example above (in which the second equation has one less variable than the \nfirst) is what we will aim for. \nSolve the system \nx-y-z= 2 \ny  + 3z =    5 \n5z = 10 \nSolution Starting from the last equation and working backward, we find successively \nthat","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":8640,"to":8669}}}}],[235,{"pageContent":"first) is what we will aim for. \nSolve the system \nx-y-z= 2 \ny  + 3z =    5 \n5z = 10 \nSolution Starting from the last equation and working backward, we find successively \nthat \nz =   2, y = 5 - 3(2) = -1, and x = 2 + (\n-\n1) + 2 = 3. So   the unique  solution is \n[3, -1, 2]. \nThe procedure used to solve Example 2.5 is called back substitution. \nWe  now turn to the general strategy for transforming a given system into an \nequivalent one that can be so  lved easily by back substitution. This process will be \ndescribed in greater detail in the next section; for now, we will simply observe it in \naction in a single example. \nSolve the system \nx- y-z= 2 \n3x - 3y +  2z = 16 \n2x - y +  z = 9 \nSolution To  transform this system into  one that exhibits the triangular structure \nof Example 2.5, we first need to eliminate the variable x from Equations 2 and 3. \nObserve that subtracting appropriate multiples of equation 1 from Equations 2 and","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":8669,"to":8691}}}}],[236,{"pageContent":"of Example 2.5, we first need to eliminate the variable x from Equations 2 and 3. \nObserve that subtracting appropriate multiples of equation 1 from Equations 2 and \n3 will do the trick. Next, observe that we  are operating on the coefficients, not on \nthe variables, so we can save ourselves some writing if we record the coefficients and \nconstant terms in the matrix \n[\n: \n-1 \n-3 \n-1 \n-1 2\n] \n2 16 \n1 9 \nwhere the first three columns contain the coefficients of the variables in order, the final \ncolumn contains the constant terms, and the vertical bar serves to remind us of the \nequal signs in the equations. This matrix is called the augmented matrix of the system. \nThere are various ways to convert the given system into one with the triangular \npattern we are after. The steps we will use here are closest in spirit to the more general \nmethod described in the next section. We will perform the sequence of operations on \nthe given system and simultaneously on the corresponding augmented matrix. We","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":8691,"to":8711}}}}],[237,{"pageContent":"method described in the next section. We will perform the sequence of operations on \nthe given system and simultaneously on the corresponding augmented matrix. We \nbegin by eliminating x from Equations 2 and 3. \nx-  y-z  =    2 \n3x -3y +  2z  = 16 \n2x-y+z=9 \n-1 \n-3 \n-1 \n-1 2\n] \n2 16 \n1 9","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":8711,"to":8723}}}}],[238,{"pageContent":"62 \nChapter 2 \nSystems of Linear Equations \nSubtract 3 times the first equation \nfrom the second equation: \nx-y-z= 2 \n5z = 10 \n2x -  y  +    z  = \n9 \nSubtract 2 times the first equation \nfrom the third equation: \nx-y-z= 2 \n5z = 10 \ny \n+ 3z =   5 \nInterchange Equations 2 and 3: \nx-y-z= 2 \ny +  3z =   5 \n5z = 10 \nSubtract 3 times the first row from the \nsecond row: \n[\n: \n-1 \n0 \n-1 \nSubtract 2 times the first row from the \nthird row: \n[\n: \n-1 \n-1 \n!�: \n0 5 \n1 \n3 \nInterchange rows 2 and 3: \n[\ni \n-1 \n-1 \nl\nt \n1 \n3 \n0 5 \nThis is the same system that we solved using back substitution in Example 2.5, where \nwe found that the solution was [ 3, -1, 2]. This is therefore also the solution to the sys­\ntem given in this example. Why? The calculations above show that any solution of th e \ngiven syste m is also a solution of the final one. But since the steps we just performed are \nreversible, we could recover the original system, starting with the final system. (How?)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":8725,"to":8775}}}}],[239,{"pageContent":"given syste m is also a solution of the final one. But since the steps we just performed are \nreversible, we could recover the original system, starting with the final system. (How?) \nSo any solution of the final system is also a solution of the given one. Thus, the systems \nare equivalent (as are all of the ones obtained in the intermediate steps above). More­\nover, we might just as well work with matrices instead of equations, since it is a simple \nmatter to reinsert the variables before proceeding with the back substitution. (Work­\ning with matrices is the subject of the next section.) \nRemark Calculators with matrix capabilities and computer algebra systems can \nfacilitate solving systems of linear equations, particularly when the systems are large \nor have coefficients that are not \"nice;' as is often the case in real-life applications. As \nalways, though, you should do as   many examples as you can with pencil and paper","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":8775,"to":8785}}}}],[240,{"pageContent":"or have coefficients that are not \"nice;' as is often the case in real-life applications. As \nalways, though, you should do as   many examples as you can with pencil and paper \nuntil you are comfortable with the techniques. Even if a calculator or CAS is called \nfor, think about how you would do the calculations manually before doing anything. \nAfter you have an answer, be sure to think about whether it is reasonable. \nDo not be misled into thinking that technology will always give you the answer \nfaster or more easily than calculating by hand. Sometimes it may not give you the \nanswer at all! Roundoff errors associated with the floating-point arithmetic used by \ncalculators and computers can cause serious problems and lead to wildly wrong an­\nswers to some problems. See Exploration: Lies My Computer Told Me for a glimpse \nof the problem. (You've been warned!)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":8785,"to":8795}}}}],[241,{"pageContent":"1 \nExercises 2.1 \nIn Exercises 1-6, determine which equations are linear \nequations in the variables x, y, and z. If any equation is not \nlinear, explain why not. \n1. x  -'TTY \n+ \nefs\nz = 0 2. x\n2 \n+ y\n2 \n+ z\n2 \n= 1 \n3. \nx\n-\n1 \n+ \n7y \n+  z \n= sin\n(\n;\n) \n4. 2x -  xy -5z = 0 \n5. 3 cos x -4y + z = v3 \n6. (cos3)x - 4y + z = v3 \nIn Exercises 7-10, find a linear equation that has the same \nsolution set as th e given equation (possibly with some \nrestrictions on th e variables). \n7. 2x + y = 7 -3y \n1 1 \n4 \n9. - + \n-\n= -\nx y \nxy \nx\n2 \n_ \ny\n2 \n8.\n--­\nx-y \n10. log\n1\n0 \nx -log\n1\n0 \ny = 2 \nIn Exercises 11-14, find the solution set of each equation. \n11. 3x -6y = 0 \n12. 2x\n1 \n+ 3x\n2 \n= 5 \n13. x + 2y + 3z = 4 \nIn Exercises 15-18, draw graphs corresponding to the given \nlinear systems. Determine geometrically whether each sys­\ntem has a unique solution, infinitely many solutions, or no \nsolution. Then solve each system algebraically to confirm \nyour answer. \n15. x + y = 0 \n2x + y = 3 \n17. 3x-6y=3 \n-x+2y=l \n16. x -2y = 7 \n3x +   y = 7 \n18. O.lOx -0.05y = 0.20","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":8797,"to":8871}}}}],[242,{"pageContent":"solution. Then solve each system algebraically to confirm \nyour answer. \n15. x + y = 0 \n2x + y = 3 \n17. 3x-6y=3 \n-x+2y=l \n16. x -2y = 7 \n3x +   y = 7 \n18. O.lOx -0.05y = 0.20 \n-0.06x + 0.03y = -0.12 \nIn Exercises 19-24, solve the given system by back \nsubstitution. \n19. x -2y =  1 \ny=3 \n21. x -y +    z = \n0 \n2y -z = 1 \n3z = -1 \n23. X\n1 \n+ Xz - X\n3 \n- X\n4 \n=  1 \nX\n2 \n+ X\n3 \n+ X\n4 \n= 0 \nX\n3 \n- X\n4 \n= 0 \nX\n4 \n= 1 \n20. \n2u  -  3v =  5 \n2v = 6 \n22. X\n1 \n+ 2X\n2 \n+ \n3X\n3 \n= 0 \n-5x\n2 \n+ 2x\n3 \n= 0 \n4X\n3 \n= 0 \n24. \nx -3y +   z = 5 \ny -  2z = -1 \nSection 2.1 \nIntroduction to Systems of Linear Equations \n63 \nThe systems in Exercises 25 and 26 exhibit a \"lower trian­\ngular\" pattern that makes them easy to solve by forward \nsubstitution. (We will encounter forward substitution again \nin Chapter 3.) Solve these systems. \n25. X 2 26. X\n1 \n= -1 \n2x +  y \n=  -3 \n-3x -4y + z = -10 \n-\n!\nx\n1 \n+ x\n2 \n5 \n�X\n1 \n+ 2X\n2 \n+ X3 = \n7 \nFind the augmented matrices of th e linear syste ms in \nExercises 27-30. \n27. x - y = 0 \n2x+y=3 \n29. x +Sy  = -1 \n-x +   y = -5 \n2x + 4y =  4 \n28. 2X\n1 \n+ 3X\n2 \n- X3 = 1 \nX\n1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":8871,"to":8972}}}}],[243,{"pageContent":"-\n!\nx\n1 \n+ x\n2 \n5 \n�X\n1 \n+ 2X\n2 \n+ X3 = \n7 \nFind the augmented matrices of th e linear syste ms in \nExercises 27-30. \n27. x - y = 0 \n2x+y=3 \n29. x +Sy  = -1 \n-x +   y = -5 \n2x + 4y =  4 \n28. 2X\n1 \n+ 3X\n2 \n- X3 = 1 \nX\n1 \n+ X3 = 0 \n-X\n1 \n+ 2X\n2 \n-2X\n3 \n= 0 \n30. a -2b +    d = 2 \n- a +    b -  c -3d = 1 \nIn Exercises 31and32,find a system of linear equations \nthat has the given matrix as its augmented matrix. \n31.\n[\n� =: \n� \n:J \n32. \n[\ni \n- 1 \n0 3 \n2 \n0     2 \n-1 \n3 \n�] \nFor Exercises 33-38, solve the linear systems in the given \nexercises. \n33. Exercise 27 \n35. Exercise 29 \n37. Exercise 31 \n34. Exercise 28 \n36. Exercise 30 \n38. Exercise 32 \n39. (a)  Find a system of two linear equations in the vari­\nables x and y whose so   lution set is given by the \nparametric equations x \n= \nt and y = 3 -2t. \n(b) Find another parametric solution to the system in \npart (a) in   which the parameter is sandy= s. \n40. (a) Find a system of two linear equations in the vari­\nables x\n1\n, x\n2\n, and x\n3 \nwhose so   lution set is given by \nthe parametric equations x \n1 \n= t, x \n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":8972,"to":9052}}}}],[244,{"pageContent":"part (a) in   which the parameter is sandy= s. \n40. (a) Find a system of two linear equations in the vari­\nables x\n1\n, x\n2\n, and x\n3 \nwhose so   lution set is given by \nthe parametric equations x \n1 \n= t, x \n2 \n= 1  + t, and \nX\n3 \n= 2  -t. \n(b) Find another parametric solution to the system in \npart (a) in   which the parameter is s and x\n3 \n= s.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":9052,"to":9072}}}}],[245,{"pageContent":"64 \nChapter 2 \nSystems of Linear Equations \nIn Exercises 41-44, the systems of equations are nonlinear. \nFind substitutions (changes of variables) that convert each \nsystem into a linear system and use this linear system to help \nsolve th e given system. \n42. x\n2 \n+ 2y\n2 \n= 6 \nx\n2 \n-y\n2 \n=  3 \n43. tan x \n-\n2 sin y \n2 \ntan x \n-\nsin y + cos z \n= \n2 \nsin y \n-\ncos z = -1 \n2 \n3 \n41. \n-\n+ \n-\n= 0 \nx \ny \n3 \n4 \n-\n+\n-\n=l \nx \ny \n44. -2a   + 2\n(\n3\nb\n) \n=  1 \n3\n(\n2a\n) \n-  4\n(\n3\nb\n) \n=  1 \nDirect Methods for Solving linear Svstems \nIn this section, we will look at a general, systematic procedure for solving a system \nof linear equations. This procedure is based on the idea of reducing the augmented \nmatr  ix of the given system to a form that can then be so  lved by back substitution. \nThe method is direct in the sense that it  leads directly to the solution (if one exists) in \na finite number of steps. In Section 2.5, we will consider some indirect methods that \nwork in a completely different way. \nMatrices and Echelon Form","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":9074,"to":9143}}}}],[246,{"pageContent":"a finite number of steps. In Section 2.5, we will consider some indirect methods that \nwork in a completely different way. \nMatrices and Echelon Form \nThere are two important matrices associated with a linear system. The coefficient \nmatrix contains the coefficients of the variables, and the augmented matrix (which \nwe have already encountered) is the coefficient matrix augmented by an extra column \ncontaining the constant terms. \nFor the system \nthe coefficient matrix is \nand the augmented matrix is \n2x+ y-z=3 \nx +  Sz  =  1 \n-x + 3y -   2z  = O \n[ 2 1 \n-\n1\n] \n-\n� \n� \n-\n� \n1 \n0 \n3 \n-1 3\n] \n5     1 \n-2 0 \nNote that if a variable is missing (as y is in the second equation), its coefficient 0 is \nentered in the appropriate position in the matrix. If we denote the coefficient matrix \nof a linear system by A and the column vector of constant terms by b, then the form \nof the augmented matrix is [A I b J.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":9143,"to":9175}}}}],[247,{"pageContent":"The word echelon comes from \nthe Latin word scala, meaning \n\"ladder\" or \"stairs:' The French \nword for \"ladder:' echelle, is also \nderived from this Latin base. A \nmatrix in echelon form exhibits \na staircase pattern. \nExample 2.1 \nExample 2.8 \nSection 2.2 \nDirect Methods for Solving Linear Systems \n65 \nIn solving a linear system, it will not always be possible to reduce the coefficient \nmatrix to triangular form, as we did in Example 2.6. However, we can always achieve \na staircase pattern in the nonzero entries of the final matrix. \nDefinition \nA  matrix  is in row echelon form if it  satisfies the  following \nproperties: \n1. Any rows consisting entirely of zeros are at   the bottom. \n2. In each nonzero row, the first nonzero entry (called the leading entry) is \nin a column to the left of any le   ading entries below it. \nNote that these properties guarantee that the leading entries form a staircase pat­\ntern. In particular, in any column containing a leading entry, all entries below the","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":9177,"to":9199}}}}],[248,{"pageContent":"Note that these properties guarantee that the leading entries form a staircase pat­\ntern. In particular, in any column containing a leading entry, all entries below the \nleading entry are zero, as the following examples illustrate. \nThe following matrices are in row echelon form: \n[\n: \n�\n] \n[\n: \n:\nJ \n[\ni \n�-\nl\n� \n2 \n0 \n1 \n-1 \n�-\n4 \n0 \n1 \n2 \n0 \n-1 \n1 \n2 \n-1 \n1 \n0 \n1 \n0 0 0 \n4 \n0 0 0 0 \n0 0 0 0 \n4 \nIf a matrix in row echelon form is actually the augmented matrix of a linear sys­\ntem, the system is quite easy to solve by back substitution alone. \nAssuming that each of the matrices in Example 2.7 is an augmented matrix, write out \nthe corresponding systems oflinear equations and solve them. \nSolution We first remind ourselves that the last column in an augmented matrix is \nthe vector of constant terms. The first matrix then corresponds to the system \n2x\n1 \n+ 4X\n2 \n=  1 \n-x\n2 \n=  2 \n(Notice that we have dropped the last equation 0 = 0, or Ox\n1 \n+ Ox\n2 \n= 0, which is \nclearly satisfied for any values of x\n1 \nand x\n2\n.\n) \nBack substitution gives x\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":9199,"to":9264}}}}],[249,{"pageContent":"2x\n1 \n+ 4X\n2 \n=  1 \n-x\n2 \n=  2 \n(Notice that we have dropped the last equation 0 = 0, or Ox\n1 \n+ Ox\n2 \n= 0, which is \nclearly satisfied for any values of x\n1 \nand x\n2\n.\n) \nBack substitution gives x\n2 \n= -2 and then \n2x\n1 \n= 1  -  4\n(\n-2\n) \n= 9 , so x\n1 \n=�.The solution is [�, -2]. \nThe second matrix has the corresponding system \nx\n, \n= \n1 \nX\n2 \n=  5 \n0 =  4 \nThe last equation represents Ox\n1 \n+ Ox\n2 \n= 4, which clearly has no solutions. Therefore, \nthe system has no solutions. Similarly, the system corresponding to the fourth matrix \nhas no solutions. For the system corresponding to the third matr  ix, we have","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":9264,"to":9310}}}}],[250,{"pageContent":"66 \nChapter 2 \nSystems of Linear Equations \nExample 2.9 \nX\n1 \n+ X2 + 2X3 = 1 \nX3 = 3 \nso x1 = 1 -  2(3) -x2 = -5 -x2• There are infinitely many solutions, since we may \nassign x2 any value t to get the parametric solution [ -5 -t, t, 3]. \nElemenlarv Row Operalions \nWe  now describe the procedure by which any matrix can be reduced to a matrix \nin row echelon form. The allowable operations, called elementary row operations, \ncorrespond to the operations that can be performed on a system of linear equations \nto transform it into an equivalent system. \nDefiniliOD \nThe following elementary row operations can be pe  rformed on a \nmatrix: \n1. Interchange two rows. \n2. Multiply a row by a nonzero constant. \n3. Add a multiple of a row to another row. \nObserve  that dividing a  row by a  nonzero constant  is implied in the  above \ndefinition, since, for example, dividing a row by 2 is the same as multiplying it by !. \nSimilarly, subtracting a multiple of one row from another row is the same as adding a","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":9312,"to":9335}}}}],[251,{"pageContent":"definition, since, for example, dividing a row by 2 is the same as multiplying it by !. \nSimilarly, subtracting a multiple of one row from another row is the same as adding a \nnegative multiple of one row to another row. \nWe will  use  the following shorthand notation for  the three elementary row \noperations: \n1. R; � R\nj \nmeans interchange rows i and j. \n2. kR; means multiply row i by k. \n3. \nR; + kR\nj \nmeans add k times row j to row i (and replace row i with the result). \nThe process of applying elementary row opera  tions to bring a matrix into row \nechelon form, called row reduction, is used to reduce a matrix to echelon form. \nReduce the following matr  ix to echelon form: \n[\n_\n: \n2 \n-4 \n-4 \n�\n] \n4 \n0 0 \n3 \n2 \n1 \n1 \n3 \n6 \nSolution We work column by column, from left to right and from top to bottom. \nThe strategy is to  create a leading entry in a column and then use it to create zeros \nbelow it. The entry chosen to become a leading entry is called a pivot, and this phase","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":9335,"to":9369}}}}],[252,{"pageContent":"The strategy is to  create a leading entry in a column and then use it to create zeros \nbelow it. The entry chosen to become a leading entry is called a pivot, and this phase \nof the process is called pivoting. Although not strictly necessary, it is often convenient \nto use the second elementary row operation to make each leading entry a 1.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":9369,"to":9372}}}}],[253,{"pageContent":"Section 2.2 \nDirect Methods for Solving Linear Systems \n61 \nWe begin by introducing zeros into the first column below the leading 1 in the \nfirst row: \n2 \n4 \n3 \n-4 \n0 \n2 \n3 \n-4 \n0 \n1 \n6 \n5\n] \nR\n, \n-  2\nR\n1 \n[ \nl \nR\n3  -  2\nR\n1 \n2 \nR\n,  + \nR\n1 \n0 \n5 \n� \n0 \n5 \n0 \n2 \n0 \n-1 \n3 \n-4 \n8 \n10 \n-1 \n-4 \n8 \n9 \n2 \n-\n:\n] \n-\n5 \n10 \nThe first column is now as we want it, so the next thing to do is to create a leading entry \nin the second row, aiming for the staircase pattern of echelon form. In this case, we do \nthis by interchanging rows. (We  could also add row 3 or row 4 to row 2.\n) \n2 \n-1 \n0 \n3 \n-4 \n10 \n8 \n-1 \n-4 \n9 \n8 \n2 \n-\n�\n] \n-8 \n10 \nThe pivot this time was -1. We now create a zero at the bottom of column 2, using \nthe leading entry -1 in row 2: \n2 \n-1 \n0 \n0 \n-4 \n10 \n8 \n29 \n-4 \n9 \n8 \n29 \n-\n�\n] \n-8 \n-\n5 \nColumn 2 is now done. Noting that we  already have a leading entry in column 3, \nwe just pivot on the 8 to introduce a zero below it.  This is easiest if we  first divide \nrow 3 by 8: \n[\n1 \n2 \n-4 -4 \n-\n�\n] \n�\nR, \n-1 \n10 \n9 \n� \n0 \n1 1 \n-1 \n0 \n29 \n29 \n-5","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":9374,"to":9495}}}}],[254,{"pageContent":"we just pivot on the 8 to introduce a zero below it.  This is easiest if we  first divide \nrow 3 by 8: \n[\n1 \n2 \n-4 -4 \n-\n�\n] \n�\nR, \n-1 \n10 \n9 \n� \n0 \n1 1 \n-1 \n0 \n29 \n29 \n-5 \nNow we use the leading 1 in row 3 to create a zero below it: \n·�·\n[\ni \n2 \n-4 \n-4 \n-\n�\n] \n-1 \n10 \n9 \n0 \n1 1 \n-1 \n0 0 0 \n24 \nWith this final step, we have reduced our matr  ix to echelon form. \nRemarks \n• \nThe row echelon form of a matrix is not  unique. (Find a different row echelon \nform for the matrix in Example 2.9.\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":9495,"to":9540}}}}],[255,{"pageContent":"68 \nChapter 2 \nSystems of Linear Equations \nTheorem 2.1 \n• \nThe leading entry in each row is used to create the zeros below it. \n• \nThe pivots are not necessarily the entries that are  originally in the posi­\ntions eventually occupied by the leading entries. In Example 2.9, the pivots were \n1, -1, 8, and 24. The original matrix had 1, 4, 2, and 5 in those positions on the \n\"staircase.\" \n• \nOnce we have pivoted and introduced zeros below the leading entry in a \ncolumn, that column does not change. In other words, the row echelon form emerges \nfrom left to right, top to bottom. \nElementary row operations are reversible-that is, they can be \"undone:' Thus, \nif some elementary row operation converts A into B, there is also an elementary row \noperation that converts B into A. (See Exercises 15 and 16.\n) \nDefinition \nMatrices A and B are row equivalent if there is a sequence of \nelementary row operations that converts A into B. \nThe matrices in Example 2.9, \n2 \n4 \n3 \n-4 \n0 \n2 \n3 \n-4 \n0 \n1 \n6 \n�] md [� \n2 \n-1 \n0 \n0 \n-4 \n10 \n1 \n0 \n-4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":9542,"to":9585}}}}],[256,{"pageContent":"Matrices A and B are row equivalent if there is a sequence of \nelementary row operations that converts A into B. \nThe matrices in Example 2.9, \n2 \n4 \n3 \n-4 \n0 \n2 \n3 \n-4 \n0 \n1 \n6 \n�] md [� \n2 \n-1 \n0 \n0 \n-4 \n10 \n1 \n0 \n-4 \n9 \n1 \n0 \n-\n�\n] \n-1 \n24 \nare row equivalent. In general, though, how can we tell whether two matrices are row \nequivalent? \nMatrices A and B are row equivalent if and only if they can be reduced to the same \nrow echelon form. \nProof If A and B are row equivalent, then further row operations will reduce B (and \ntherefore A\n) \nto the (same) row echelon form. \nConversely, if A and B have the same row echelon form R, then, via elementary \nrow operations, we can convert A into R and B into R. Reversing the latter sequence of \noperations, we can convert R into B, and therefore the sequence A \n� \nR \n� \nB achieves \nthe desired effect. \nRemark In practice, Theorem 2.1 is easiest to use if R is the reduced row echelon \nform of A and B, as defined on page 73. See Exercises 17 and 18. \nGaussian Elimination","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":9585,"to":9635}}}}],[257,{"pageContent":"B achieves \nthe desired effect. \nRemark In practice, Theorem 2.1 is easiest to use if R is the reduced row echelon \nform of A and B, as defined on page 73. See Exercises 17 and 18. \nGaussian Elimination \nWhen row reduction is applied to the augmented matrix of a  system of linear \nequations, we create an equivalent system that can be solved by back substitution. \nThe entire process is known as Gaussian elimination.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":9635,"to":9642}}}}],[258,{"pageContent":"Gaussian Elimination \nExample 2.10 \nSection 2.2 \nDirect Methods for Solving Linear Systems \n69 \n1. Write the augmented matrix of the system of linear equations. \n2. Use elementary row operations to reduce the augmented matrix to row \nechelon form. \n3. Using back substitution, solve the equivalent system that corresponds to the \nrow-reduced matrix. \nRemark When performed by hand, step 2 of Gaussian elimination allows quite a \nbit of choice. Here are some useful guidelines: \n(a)  Locate the leftmost column that is not  all zeros. \n(b)  Create a leading entry at the top of this column. (It will usually be easiest if you \nmake this a leading 1. See Exercise 22.\n) \n(c) Use the  leading entry to create zeros below it. \n( d)  Cover up the row containing the leading entry, and go back to step (a) to repeat the pro­\ncedure on the remaining submatrix. Stop when the entire matrix is in  row echelon form. \nSolve the system \n2x\n2 \n+ 3x3 = \n8 \n2x\n1 \n+ 3x\n2 \n+ \nx3 = \n5 \nX\n1 \n-\nX\nz \n-  2X3 = \n-\n5 \nSolution \nThe augmented matr  ix is \n[� \n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":9644,"to":9686}}}}],[259,{"pageContent":"cedure on the remaining submatrix. Stop when the entire matrix is in  row echelon form. \nSolve the system \n2x\n2 \n+ 3x3 = \n8 \n2x\n1 \n+ 3x\n2 \n+ \nx3 = \n5 \nX\n1 \n-\nX\nz \n-  2X3 = \n-\n5 \nSolution \nThe augmented matr  ix is \n[� \n2 \n3 \n-1 \n-\n�\nl \n3 \n-2 \nWe proceed to reduce this matrix to row echelon form, following the guidelines given \nfor step 2 of the process. The first nonzero column is column 1. We begin by crea ting \nCarl Friedrich Gauss (1777-1855) is generally considered to be one of the three greatest \nmathematicians of all time, along with Archimedes and Newton. He is often called the \"prince of \nmathematicians;' a nickname that he richly deserves. A child prodigy,  Gauss reportedly could do \narithmetic before he could talk. At the age of 3, he corrected an error in his father's calculations for \nthe company payroll, and as a young student, he found the formula n(n + 1 )  /2 for the sum of the \nfirst n natural numbers. When he was 19, he proved that a 17-sided polygon could be constructed","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":9686,"to":9725}}}}],[260,{"pageContent":"the company payroll, and as a young student, he found the formula n(n + 1 )  /2 for the sum of the \nfirst n natural numbers. When he was 19, he proved that a 17-sided polygon could be constructed \nusing only a straightedge and a compass, and at the age of 21, he proved, in his doctoral \ndissertation, that every polynomial of degree n with real or complex coefficients has exactly \nn zeros, counting multiple zeros-the Fundamental Theorem of Algebra. \nGauss's 1801 publication Disquisitiones Arithmeticae is generally considered to be the \nfoundation of modern number theory, but he made contributions to nearly every branch of \nmathematics as well as to statistics, physics, astronomy, and surveying. Gauss did not publish \nall of his findings, probably because he was too critical of his own work. He also did not like \nto teach and was often critical of other mathematicians, perhaps because he discovered-but \ndid not publish-their results before they did.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":9725,"to":9735}}}}],[261,{"pageContent":"to teach and was often critical of other mathematicians, perhaps because he discovered-but \ndid not publish-their results before they did. \nThe method called Gaussian elimination was known to the Chinese in the third century B.c. \nand was well known by Gauss's time. The method bears Gauss's name because of his use of it in a \npaper in which he solved a system oflinear equations to describe the orbit of an asteroid.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":9735,"to":9739}}}}],[262,{"pageContent":"10 \nChapter 2 \nSystems of Linear Equations \nExample 2 .11 \na leading entry at the top of this column; interchanging rows 1 and 3 is the best way \nto achieve this. \n[\n� \n2 \n3 \n-1 \n3 \n-2 \nS\nJ \nRi ++ R\n, \n[ \nl \ns \n� \n2 \n-s \n0 \n-1 \n3 \n2 \n-2  -S\ni \n1 \ns \n3 \n8 \nWe now create a second zero in the first column, using the leading 1: \nR, -2R1 \n[ \nl - l \n� \n0 \ns \n0 \n2 \n-2 -S\ni \ns lS \n3 \n8 \nWe now cover up the first row and repeat the procedure. The second column is \nthe first nonzero column of the submatrix. Multiplying row 2 by � will create a \nleading 1. \n[\ni \n-1 \n-2 \n-\n5\n1 \n[\ni \n-1 \ns s lS \nt\nR\n2 \n� \n2 \n3 \n8 2 \nWe now need another zero at the bottom of column 2: \n,,  - rn, \n[\nI \n� \n0 \n0 \n-1 \n1 \n0 \n-2 \n-\n5 i \n1 \n3 \n1 \n2 \n-2 \n-!\n] \n3 \nThe augmented matr  ix is now in row echelon form, and we move to step 3. The cor­\nresponding system is \nX\n1 \n-  X\nz \n-  2X3 = -S \nX\nz \n+    x3 = 3 \nX3 = 2 \nand  back  substitution \ngives x3 = 2, then x\n2 \n= 3  -x3 = 3  -  2 = 1, and \nfinally x\n1 \n= - S  +  x\n2 \n+ 2x3 = - S  +  1  +  4 = 0. We write the solution in vector \nform as","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":9741,"to":9850}}}}],[263,{"pageContent":"X\nz \n+    x3 = 3 \nX3 = 2 \nand  back  substitution \ngives x3 = 2, then x\n2 \n= 3  -x3 = 3  -  2 = 1, and \nfinally x\n1 \n= - S  +  x\n2 \n+ 2x3 = - S  +  1  +  4 = 0. We write the solution in vector \nform as \n(We are going to write the vector solutions of linear systems as column vectors from \nnow on. The reason for this will become clear in Chapter 3.\n) \nSolve the system \nw  -\nx -y +  2z = \n2w  -2x -y +  3z = 3 \n-w +    x -y \n= -3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":9850,"to":9872}}}}],[264,{"pageContent":"Section 2.2 \nDirect Methods for Solving Linear Systems \n11 \nSolution The augmented matrix is \nu \n-1 -1 \n2 \nJ \n-2 \n-1 \n3 \n-1 \n0 \nwhich can be  row reduced as follows: \nu \n-1 \n-1 \n2 \nJ \nR,\n-\n2\nR, \n[\n: \n-1 \n-1 \n2 \nJ \nR, +  R, \n-2 \n-1 \n3 \n-----+ \n0 -1 \n-1 0 0 \n-2 \n2 \n[\n: \n-1 \n-1 \n2 \ni\nl \nR\n3 + 2R 2 \n-----+ \n0 \n1 \n-1 \n0 0 0 \nThe associated system is now \nw -  x -  y \n+ \n2z = \n1 \ny-\nz =  1 \nwhich has infinitely many solutions. There is more than one way to assign param -\neters, but we will proceed to use back substitution, writing the variables correspond­\ning to the leading entries (the leading variables) in terms of the other variables (the \nfr ee variables) . \nIn this case, the leading variables are w and y, and the free variables are x and z. \nThus, y =  1  + z, and from this we obtain \nw =  1  +  x  + y -2z \n= \n1  +  x  + \n(1 \n+  z\n) \n-2z \n=2+x-z \nIf \nwe assign parameters x  = s and z  = t, the solution can be written in vector \nform as \nExample 2.11 highlights a very important property: In a consistent system, the","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":9874,"to":9950}}}}],[265,{"pageContent":"1  +  x  + \n(1 \n+  z\n) \n-2z \n=2+x-z \nIf \nwe assign parameters x  = s and z  = t, the solution can be written in vector \nform as \nExample 2.11 highlights a very important property: In a consistent system, the \nfree variables are just the variables that are not leading variables. Since the number \nof leading variables is the number of nonzero rows in the row echelon form of the \ncoefficient matrix, we can predict the number of free variables (parameters) before \nwe find the explicit solution using back substitution. In Chapter 3, we will prove that, \nalthough the row echelon form of a matrix is not  unique, the number of nonzero rows \nis the  same in all row echelon forms of a given matrix. Thus, it makes sense to give a \nname to this number.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":9950,"to":9966}}}}],[266,{"pageContent":"12 \nChapter 2 \nSystems of Linear Equations \nTheorem 2.2 \nExample 2.12 \nWilhelm Jordan (1842-1899) was \na German professor of geodesy \nwhose contribution to solving \nlinear systems was a systematic \nmethod of back substitution \nclosely related to the method \ndescribed here. \nDefinition \nThe rank of a matr  ix is the number of nonzero rows in its row \nechelon form. \nWe will denote the rank of a matrix A  by rank(A). In Example 2.10, the rank of \nthe coefficient matrix is 3, and in Example 2.11, the rank of the coefficient matr  ix is \n2. The observations we have just made justify the following theorem, which we will \nprove in more generality in Chapters 3 and 6. \nThe Rank Theorem \nLet A be the coefficient matrix of a system of linear equations with n variables. If \nthe system is consistent, then \nnumber of free variables = n -rank\n(\nA\n) \nThus, in Example 2.10, we  have 3 -  3 = 0 free variables (in other words, a unique \nsolution), and in Example 2.11, we have 4 -2 = 2 free variables, as we found.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":9968,"to":9995}}}}],[267,{"pageContent":"number of free variables = n -rank\n(\nA\n) \nThus, in Example 2.10, we  have 3 -  3 = 0 free variables (in other words, a unique \nsolution), and in Example 2.11, we have 4 -2 = 2 free variables, as we found. \nSolve the system \nX\n1 \n-\nX2 + 2X3 = \n3 \nX\n1 \n+ 2X2 -\nX3 = - 3 \n2x2 -  2X3 = \nSolulion \nWhen we row reduce the augmented matrix, we have \n[i \n-1 \n2 \n-\n:\nl \n[i \n-1 \n2 \n-\n;\n] \nR\ni -\nR\n1 \n2 \n-1 \n� \n3 \n-3 \n2 \n-2 \n2 \n-2 \n[\ni \n-1 \n2 \n-�\n] \n!.\nR2 \n� \n1 \n-1 \n2 \n-2 \n�\n-\n,\n,\n, \n[\nI \n-1 \n2 \n-\n�\n] \n� \n0 \n1 -1 \n0 0 0 \nleading to the impossible equation 0 = 5. (We could also have performed R\n3 \n-\nt \nR\n2 \nas the \nsecond elementary row operation, which would have given us the same contradiction \nbut a different row echelon form.) Thus, the system has no solutions-it is inconsistent. \nGauss-Jordan Eliminalion \n4 \nA modification of Gaussian elimination greatly simplifies the back substitution phase \nand is particularly helpful when calculations are being done by hand on a system with","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":9995,"to":10080}}}}],[268,{"pageContent":"For a short proof that the reduced \nrow echelon form of a matrix is \nunique, see the article by Thomas \nYuster, \"The Reduced Row Echelon \nForm of a Matrix Is Unique: A \nSimple Proof;' in the March 1984 \nissue of Mathematics Magazine \n(vol. 57, no. 2, pp. 93-94). \nGauss-Jordan \nElimination \nExample 2.13 \nSection 2.2 \nDirect Methods for Solving Linear Systems \n13 \ninfinitely many solutions. This variant, known as Gauss-Jordan elimination, relies \non reducing the augmented matrix even further. \nDefinition \nA matrix is in reduced row echelon form if it  satisfies the following \nproperties: \n1. It is in row echelon form. \n2. The leading entry in each nonzero row is a 1 (called a leading 1). \n3. Each column containing a leading 1 has zeros everywhere else. \nThe following matrix is in reduced row echelon form: \n1 \n2 \n0 0 \n-3 \n1 \n0 \n0 0 0 \n4 \n-1 \n0 \n0 0     0 \n3 \n-2 \n0 \n0 0     0 0 0 0 \n1 \n0 0     0 0 0 0     0 \nFor 2 X 2 matrices, the possible reduced row echelon forms are \nwhere * can be any number.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":10082,"to":10123}}}}],[269,{"pageContent":"1 \n2 \n0 0 \n-3 \n1 \n0 \n0 0 0 \n4 \n-1 \n0 \n0 0     0 \n3 \n-2 \n0 \n0 0     0 0 0 0 \n1 \n0 0     0 0 0 0     0 \nFor 2 X 2 matrices, the possible reduced row echelon forms are \nwhere * can be any number. \nIt is clear that after a matr  ix has been reduced to echelon form, further elementary \nrow opera  tions will bring it to reduced row echelon form. What is not clear (although \nintuition may suggest it) is that, unlike the row echelon form, the reduced row ech­\nelon form of a matrix is unique. \nIn Gauss-Jordan elimination, we proceed as in   Gaussian elimination but reduce \nthe augmented matrix to reduced row echelon form. \n1. Write the augmented matrix of the system of linear equations. \n2. Use elementary row operations to reduce the augmented matrix to reduced \nrow echelon form. \n3. If the resulting system is consistent, solve for the leading variables in terms \nof any remaining free variables. \nSolve the system in Example 2.11 by Gauss-Jordan elimination. \nSolUtion","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":10123,"to":10154}}}}],[270,{"pageContent":"3. If the resulting system is consistent, solve for the leading variables in terms \nof any remaining free variables. \nSolve the system in Example 2.11 by Gauss-Jordan elimination. \nSolUtion \nThe reduction proceeds as it did in Example 2.11 until we reach the echelon form: \n[\n: \n-1 \n0 \n0 \n-1 \n1 \n0 \n2 \nl \nl \n-1 1 \n0    0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":10154,"to":10171}}}}],[271,{"pageContent":"14 \nChapter 2 \nSystems of Linear Equations \nExample 2 .14 \nWe now must create a zero above the leading 1 in the second row, third column. We \ndo this by adding row 2 to row 1 to obtain \n[\n: \n-1 0 \n0 \n0     0 \nThe system has now been reduced to \nw-x \n+z=2 \ny-z=I \nIt is now much easier to solve for the leading variables: \nw=2+x-z \nand \ny=I+z \nIf we assign parameters x = s   and z = t as before, the solution can be  written in vector \nform as \nRemark From a computational point of view, it is more efficient (in the sense \nthat it requires fewer calculations) to first reduce the matrix to row echelon form \nand then, working from right to left, make each leading entry a 1  and create zeros \nabove these leading 1 s. However, for manual calculation, you will find it easier to \njust work from left to right and create the leading ls and the zeros in their columns \nas you go. \nLet's return to the geometry that brought us to this point. Just as systems oflinear \nequations in two variables correspond to lines in IR\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":10173,"to":10202}}}}],[272,{"pageContent":"as you go. \nLet's return to the geometry that brought us to this point. Just as systems oflinear \nequations in two variables correspond to lines in IR\n2\n, so linear equations in three vari­\nables correspond to planes in IR\n3\n. In fact, many questions about lines and planes can \nbe answered by solving an appropriate linear system. \nFind the line of intersection of the planes x + 2 y - z = 3 and 2x + 3y + z = 1. \nSolution First, observe that there will be a line of intersection, since the normal \nvectors \nof the two planes-[ I, 2, -I] and [2, 3,   1]-are not parallel. The points that \nlie in the intersection of the two planes correspond to the points in the solution set \nof the system \nx +   2y - z = 3 \n2x + 3y + z =  1 \nGauss-Jordan elimination applied to the augmented matrix yields","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":10202,"to":10219}}}}],[273,{"pageContent":"z \nx \ny \nFigure 2.2 \nThe intersection of two planes \nExample 2.15 \n[\n� \nSection 2.2 \nDirect Methods for Solving Linear Systems \n15 \n2 \n3 \n2 \n-\n1 \n0 \n-\n1\n1 \n3\n] \n3 \n-5 \ns \n1\n-\n7\nJ \n-\n3 \n5 \nReplacing variables, we have \nx \n+ \nSz = \n-\n7 \ny \n-\n3z = \n5 \nWe set the free variable z equal to a parameter t and thus obtain the parametric equa­\ntions of the line of intersection of the two planes: \nIn vector form, the equation is \nSee Figure 2.2. \nx = \n-\n7 \n-St \ny = \n5 +  3t \nz= \nLct p \n� \n[ _ H q  � m' u � [:J. md v \n� [ J Dcte;m;ne whcthec the lilles \nx = p +  tu and x = q + tv intersect and, if so, find their point of intersection. \nSolution We need to be careful here. Although t has been used as the parameter \nin the equations of both lines, the lines are independent and therefore so are their \nparameters. Let's use a different parameter-say, s-for the first line, so its equation \nbewmes \nx  � \np +  su. If the lines illtmed, then we wmt to find on x  � [;] thot \nsatisfies both equations simultaneously. That is, we want x = p + su = q + tv or \nSU -tv \n= q \n-","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":10221,"to":10288}}}}],[274,{"pageContent":"bewmes \nx  � \np +  su. If the lines illtmed, then we wmt to find on x  � [;] thot \nsatisfies both equations simultaneously. That is, we want x = p + su = q + tv or \nSU -tv \n= q \n-\np. \nSubstituting the given p, q, u, and v, we obtain the equations \ns -3t = \n-\n1 \ns+ \nt= \n2 \ns+ \nt= \n2 \nwhose solution is easily found to bes = �, t = �. The point of intersection is therefore","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":10288,"to":10306}}}}],[275,{"pageContent":"16 \nChapter 2 \nSystems of Linear Equations \nx \ny \nFigure 2.3 \nz \nSee Figure 2.3. (Check that substituting t = i in the other equation gives the same \npoint.) \nRemark In IR\n3\n, it is possible for two lines to intersect in a point, to be parallel, or \nto do neither. Nonparallel lines that do not intersect are ca  lled skew lines. \nTwo intersecting lines \nHomogeneous svstems \nWe have seen that every system of linear equations has either no solution, a unique \nsolution, or infinitely many solutions.  However, there is one type of system that \nalways has at least one solution. \nDefinition \nA system oflinear equations is called homogeneous ifthe constant \nterm in each equation is zero. \nIn other words, a  homogeneous system has an augmented matrix of the form \n[A I O] . The following system is homogeneous: \n2x + 3y -\nz = 0 \n-x +Sy+ 2z = 0 \nSince a homogeneous system cannot have no solution (forgive the double negative!), \nit will have either a unique solution (namely, the zero, or trivial, solution) or infinitely","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":10308,"to":10335}}}}],[276,{"pageContent":"z = 0 \n-x +Sy+ 2z = 0 \nSince a homogeneous system cannot have no solution (forgive the double negative!), \nit will have either a unique solution (namely, the zero, or trivial, solution) or infinitely \nmany solutions. The next theorem says that the latter case must occur if the number \nof variables is greater than the number of equations. \nTheorem \n2.3 \nIf [A IO] is a homogeneous system of m linear equations with n variables, where \nm < n, then the system has infinitely many solutions. \nProof Since the  system has  at least the  zero  solution, it  is  consistent. Also, \n� \nrank(A) :::::: m (why?). By the Rank Theorem, we have \nnumber of free variables = n - rank\n(\nA\n) \n2:  n  -   m  >  0 \nSo there is at least one free variable and, hence, there are infinitely many solutions. \nNote Theorem 2.3 says nothing about the  case where m  2:  n. Exercise 44 asks \nyou to give examples to show that, in this case, there can be either a unique solution \nor infinitely many solutions.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":10335,"to":10356}}}}],[277,{"pageContent":"IR and ZP are examples of fields. \nThe set of rational numbers Q and \nthe set of complex numbers C are \nother examples. Fields are covered \nin detail in courses in abstract \nalgebra. \nSection 2.2 \nDirect Methods for Solving Linear Systems \n11 \nlinear svs1ems over Zp \nThus far, all of the linear systems we have encountered have involved real numbers, \nand the solutions have accordingly been vectors in some !R\nn\n. We have seen how other \nnumber systems arise-notably, \"ll_\nP\n' When p is a prime number, \"ll_\nP \nbehaves in many \nrespects like IR; in particular, we  can add, subtract, multiply, and divide (by nonzero \nnumbers). Thus, we can also solve systems of linear equations when the variables and \ncoefficients belong to some \"ll_\nP\n' In such instances, we refer to solving a system over \"ll_\nP\n' \nFor example, the linear equation x\n1  + x\n2 \n+ x\n3 \n= 1, when viewed as an equation \nover Z\n2\n, has exactly four solutions: \n(where the last solution arises because 1 + 1 + 1 = 1 in Z\n2\n). \nwh,n wni,wth\"q\"'Hon X; + x, + X; \n�","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":10358,"to":10397}}}}],[278,{"pageContent":"1  + x\n2 \n+ x\n3 \n= 1, when viewed as an equation \nover Z\n2\n, has exactly four solutions: \n(where the last solution arises because 1 + 1 + 1 = 1 in Z\n2\n). \nwh,n wni,wth\"q\"'Hon X; + x, + X; \n� \n1 ov\" 2,, thno!utiom \n[\n:\n:\nl \nm \n� (Check these.) \nExample 2.16 \nBut we need not use trial-and-error methods; row reduction of augmented matri­\nces works just as well over \"ll_\nP \nas over IR. \nSolve the following system oflinear equations over Z\n3\n: \nX\n1 \n+ 2x\n2 \n+ \nx\n3 \n= 0 \nX\n1 \n+ X\n3 \n= 2 \nx\n2 \n+ 2x\n3 \n= \n1 \nSolution The first thing to note in examples like this one is that subtraction and \ndivision are not needed; we can accomplish the same effects using addition and mul­\ntiplication. (This, however, requires that we be working over \"ll_\nP\n, where pis a  prime; \nsee Exercise 60 at the end of this section and Exercise 57 in Section 1.1.) \nWe row reduce the augmented matrix of the system, using calculations modulo 3. \n[\ni \n2 \n;\n] \n[\ni \n2 \n1 \n;\n] \nR2\n+\n2R1 \n0 \n� \n1 \n0 \n1 \n2 \n1 \n2 \nR,\n+\nR2 \n[\ni \n0 \n:\nJ \nR\n3 + 2\nR2 \n1 \n0 \n� \n0 \n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":10397,"to":10488}}}}],[279,{"pageContent":"18 \nChapter 2 \nSystems of Linear Equations \nExample 2.11 \nR,+R, \n[ 1 Q \n� 0 1 \n0     0 \n0  1 \nl \n0  2 \n1   1 \nThus, the solution is Xi = 1, x\n2 \n= 2, x\n3 \n= 1. \nSolve the following system of linear equations over 2\n2\n: \nX\ni \n+ \nX\nz \n+ \nX\n3 \n+ \nX\n4 \n= \n1 \nX\ni \n+ \nX\n2 \n= 1 \nX\n2 \n+ \nX\n3 \n= \n0 \nX\n3 \n+ X\n4 \n= 0 \nX\ni \n+ X\n4 \n= 1 \nSolution \nThe row reduction proceeds as follows: \n1 1 1 \n1 \n0 0 \n1 \nR\n2 + R1 \n0     0 \n0 \n1 \n0 0 \nR5\n+ R, \n0 \n1 \n� \n0 0 \n1 \n0 0     0 \n0     0 0 \n1 \n0 \nR\n2 <--> \nR\n3 \nR\n1 \n+ R\n2 \n0 \nR\n5\n+R\n2 \n0 0 \n� \n0     0 \n0 \n0 \n1 0 \nR\n2 \n+ \nR\n3 \n0 \nR\n4 \n+ R3 \n0 \n0 \n� \n0     0 \n0     0 \nTherefore, we have \nX\n1 \n+ X\n4 \n= 1 \nX\nz \n+ X\n4 \n= 0 \nX\n3 \n+ X\n4 \n= \nQ \nSetting the free variable x\n4 \n= t yields \n1 \n1 1 \n0 \n1 \n0    0 \n1 \n0 \n0 0 \n0 \n1     1 \n0 0 \n0 \n1 1 \n0 \n0 \n0    0 \n0 1 \n0 \n0 \n1 1 \n0 \n0     0 \n0 \n0     0 0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":10490,"to":10645}}}}],[280,{"pageContent":"Section 2.2 \nDirect Methods for Solving Linear Systems \n19 \nSince t can take on the two values 0 and 1\n, \nthere are exactly two solutions: \nRemark For linear systems over \"11..\nP\n, there can  never be  infinitely  many \n� \nsolutions. (Why not?) Rather, when there is more than one solution, the number \nof solutions is finite and is a function of the number of free variables and p. (See \nExercise 59.\n) \n.. \n1 \nExercises 2.2 \n• \nIn Exercises 1-8, determine whether th e given matrix is in \n9. \n[\n: \n0 \n:\nJ \n10. \n[\n� \n�\n] \nrow echelon form. If it is, state whether it is also in reduced \nrow echelon form. \n!\n. \n[\ni \n0 \n�\n] \n2. \n[\n� \n0 \n1 \n�\n] \n0 \n1 \n-1 \nII\n.\n[\n: \n-\n!\n] \n0  0 \n1\n2\n. \n[\n� \n-\n4 \n-\n2 \n:\nJ \n4. \n[\n: \n0 \n:J \n1 \n6 \n3. \n[\n� \n1 \n3 \n�\n] \n0 \n0 \n0 \n0 \n1\n3\n. \n[\n: \n-\n2 \n-\n1 \ni \n[\n-\n2 \n-\n4 \n1\n:\n] \n5. \n[\ni \n0 \n3 \n-\n4 \n:\nJ \n6. \n[\n: \n0 \n:\nJ \n-\n1 \n-\n1 \n14.  -\n� \n-6 \n0 0  0 \n1 \n-3 \n-\n1 \n2 \n-3 \n5 \n0 0 \n7\n. \n[\nj \n2 \n�: \n8\n. \n[\n� \n1 \n3 \n-�\n] \n0 0 \n1 \n15. Reverse the elementary row operations used in \n1 \n0 0 \nExample 2.9 to show that we can convert \n0 0 0 \n[� \n2 \n-\n4 \n-\n4 \n-�] \n-\n1 \n10 \n9 \ninto \nIn Exercises 9-14, use elementary row operations to reduce","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":10647,"to":10815}}}}],[281,{"pageContent":"-�\n] \n0 0 \n1 \n15. Reverse the elementary row operations used in \n1 \n0 0 \nExample 2.9 to show that we can convert \n0 0 0 \n[� \n2 \n-\n4 \n-\n4 \n-�] \n-\n1 \n10 \n9 \ninto \nIn Exercises 9-14, use elementary row operations to reduce \nthe given matrix to (a) row echelon form and (b) reduced \n0 \n1   1 \n-\n1 \nrow echelon form. \n0  0  0 \n24","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":10815,"to":10844}}}}],[282,{"pageContent":"80 \nChapter 2 \nSystems of Linear Equations \n2 \n4 \n3 \n-4 \n0 \n2 \n3 \n-4 \n0 \n6 \n�\n] \n16. In general, what is the elementary row operation that \n\"undoes\" each of the three elementary row operations \nR; � �' kR;, and R; + kR/ \nIn Exercises 17 and 18, show that the given matrices are row \nequivalent and find a sequence of elementary row operations \nthat will convert A into B. \n17.A  = \n[\n3\n1 \n18. A= \n[ \n� \n-1 \n2\n] \nB = \n[\n3 -1\n] \n4 \n' \n1 0 \n-1 \ni \n[\n3 \n� \n,B = � \n0 \n1 \n5 \n2 \n-\n�\n] \n19. What is wrong with the following \"proof\" that every \nmatrix with at least two rows is row equivalent to a \nmatrix with a zero row? \nPerform R\n2 \n+ R\n1 \nand R\n1 \n+ R\n2\n• Now rows 1and 2 \nare identical. Now perform R\n2 \n- R\n1 \nto obtain a \nrow of zeros in the second row. \n20. What is the net effect of performing the following \nsequence of elementary row operations on a matrix \n(with at least two rows)? \nR\n2 \n+ \nR\n,\n, R\n, \n- R\n2\n, R\n2 \n+ \nR\n,\n, -R\n, \n21. Students frequently perform the following type of cal­\nculation to introduce a zero into a matrix: \nHowever, 3R\n2 \n-2R\n1 \nis not an elementary row opera­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":10846,"to":10940}}}}],[283,{"pageContent":"R\n2 \n+ \nR\n,\n, R\n, \n- R\n2\n, R\n2 \n+ \nR\n,\n, -R\n, \n21. Students frequently perform the following type of cal­\nculation to introduce a zero into a matrix: \nHowever, 3R\n2 \n-2R\n1 \nis not an elementary row opera­\ntion. Why not? Show how to achieve the same result \nusing elementary row operations. \n22. Consider the matrix A  = \n[ \n� \n!] \n. Show that any of \nthe three types of elementary row opera  tions can be \nused to create a leading 1 at the top of the first column. \nWhich do you prefer and why? \n23. What is the rank of each of the  matrices in Exercises 1-8? \n24. What are the possible reduced row echelon forms of \n3 X 3 matrices? \nIn Exercises 25-34, solve th e given system of equations using \neither Gaussian or Gauss-Jordan elimination. \n25. X\n1 \n+  2X\n2 \n-3X\n3 \n=  9 \n2X\n1 \n-X\n2 \n+    X\n3 \n=  0 \n26. \nx  -y \n+ \nz  =  0 \n-x + 3y +    z  =  5 \n4x\n1 \n-X\n2 \n+    x\n3 \n=  4 \n3x +    y  + 7z =  2 \n27. X\n1 \n-3X\n2 \n-  2X\n3 \n=  0 \n- X\n1 \n+ \n2X\n2 \n+ \nX\n3 \n=  0 \n2x\n1 \n+ 4x\n2 \n+ 6x\n3 \n=  0 \n28.2w + 3x -  y  + 4z = 1 \n3w  -x \n+ \nz  = 1 \n3w  -4x \n+ y  -\nz  =  2 \n29.   2r  +    s  = \n3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":10940,"to":11036}}}}],[284,{"pageContent":"4x\n1 \n-X\n2 \n+    x\n3 \n=  4 \n3x +    y  + 7z =  2 \n27. X\n1 \n-3X\n2 \n-  2X\n3 \n=  0 \n- X\n1 \n+ \n2X\n2 \n+ \nX\n3 \n=  0 \n2x\n1 \n+ 4x\n2 \n+ 6x\n3 \n=  0 \n28.2w + 3x -  y  + 4z = 1 \n3w  -x \n+ \nz  = 1 \n3w  -4x \n+ y  -\nz  =  2 \n29.   2r  +    s  = \n3 \n4r  +    s  = 7 \n2r  +  Ss  = -1 \n30. \n- X\n1 \n+ 3X\n2 \n-  2X\n3 \n+  4X\n4 \n= \n0 \n2X\n1 \n-  6X\n2 \n+ \nX\n3 \n-  2X\n4 \n=  - 3 \nX\n1 \n-3X\n2 \n+ \n4X\n3 \n-8x\n4 \n= \n2 \n31. \n� \nX\n1 \n+    X\n2 \n-X\n3 \n-  6X\n4 \n2 \nix\n, \n+ \n�\nX\nz \n-3X\n4 \n+    X\n5 \n= -1 \n32. \nVl\nx + y  + \n2z = \nVl\ny  -3z = \n-y  + \nVl\nz  = \n33. w \n+  x  + 2y +  z  = \nw-x-  y+z= \nx  +    y \n-  4X\n5 \n= \n1 \n-V2 \n1 \n0 \n-1 \nw+x \n+z= 2 \n34. a  +    b  + c  + d  = \n4 \na  +  2b  +     3c  + 4d = 10 \na  + 3b + \n6c  + lOd  = 20 \na  + 4b + lOc + 20d = 35 \n8 \nIn Exercises 35-38, determine by inspection (i.e., without \nperforming any calculations) whether a linear system with \nth e given augmented matrix has a unique solution, infinitely \nmany solutions, or no solution. Justify your answers. \n35. \n[\n: \n37. \n[\ni \n0 \n1 \n0 \n2 \n6 \n10 \n3 \n7 \n11 \n4   O\nJ \n8 0 \n12 0 \n36. \n[\n� \n38.[� \n-2 \n2 \n4 \n2 3 \n5     4 \n7     7 \n0 \n-3 \n-6 \n4 \n3 \n7 \n39. Show that if ad -  be  -=!=  0, then the system","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":11036,"to":11201}}}}],[285,{"pageContent":"35. \n[\n: \n37. \n[\ni \n0 \n1 \n0 \n2 \n6 \n10 \n3 \n7 \n11 \n4   O\nJ \n8 0 \n12 0 \n36. \n[\n� \n38.[� \n-2 \n2 \n4 \n2 3 \n5     4 \n7     7 \n0 \n-3 \n-6 \n4 \n3 \n7 \n39. Show that if ad -  be  -=!=  0, then the system \nax + by =  r \nex  +  dy \n=  s \nhas a   unique solution.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":11201,"to":11240}}}}],[286,{"pageContent":"In Exercises 40-43,for what value(s) of k, if any, will the \nsystems have (a) no solution, (b) a unique solution, and \n(c) infinitely many solutions? \n4\n0. kx + 2y = 3 \n4\n1. x  +  ky =  1 \n2x -4y = -6 \n4\n2\n. x -2y + 3z =  2 \nx  + y+ z=k \n2x -y  + 4z =  k\n2 \nkx +    y  =  1 \n43\n. x  +    y  +  kz  = \nx  +  ky + \nz  = \nkx +    y  +    z  = -2 \n44\n. Give examples of homogeneous systems of m linear \nequations in n variables with m =  n and with m > n \nthat have (a) infinitely many solutions and (b) a \nunique solution. \nIn Exercises 45 and 46, find th e line of intersection of th e \ngiven planes. \n4\n5. 3x + 2y + z = -1 and 2x - y \n+ 4z = 5 \n4\n6\n. 4x + y + z = 0 and 2x - y + 3z = 2 \n4\n7. (a)  Give an example of three planes that have a com­\nmon line of intersection (Figure 2.4\n)\n. \nFigure 2.4 \n(b) Give an example of three planes that intersect in \npairs but have no common point of intersection \n(Figure 2.5\n)\n. \nFigure 2.5 \nSection 2.2 \nDirect Methods for Solving Linear Systems \n81 \n(c) Give an example of three planes, exactly two of","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":11242,"to":11290}}}}],[287,{"pageContent":"pairs but have no common point of intersection \n(Figure 2.5\n)\n. \nFigure 2.5 \nSection 2.2 \nDirect Methods for Solving Linear Systems \n81 \n(c) Give an example of three planes, exactly two of \nwhich are parallel (Figure 2.6). \nfigure 2.6 \n(d) Give an example of three planes that intersect in a \nsingle point (Figure 2.7\n)\n. \nfigure 2.1 \nIn Exercises 48 and 49, determine whether the lines \nx = p + su and x = \nq \n+ tv intersect and, if they do, find \ntheir point of intersection. \n4\n8\n.p\n� \n[\n-\nHq\n� \n[\nH\nu\n�\n[\n_+\n� \n[\n-\ni\nl \n4\n9\n.p\n� \n[�J\nq\n� \n[:\nJ\n.\nu\n� \n[\n+\n� \nm \n50. let p \n� \nm. u \n� \n[ J. ond v \n� \nm Desaibe \nall points Q = \n(\na, b, c\n) \nsuch that the line through \nQ with direction vector v intersects the line with \nequa\ntion x = p + su. \n51. Recall that the cross prod uct of vectors u and vis a \nvector u X v that is orthogonal to both u and v. (See \nExploration: The Cross Product in Chapter 1.\n) \nIf \nU \n� \n[ :: \nl \nand F \n[ \n:: \nl","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":11290,"to":11373}}}}],[288,{"pageContent":"82 \nChapter 2 \nSystems of Linear Equations \nshow that there are infinitely many vectors \n55. x  + y \n= 1  over Z\n3 \ny+z=O \nx +  z = 1 \n56. 3x + 2y  = 1  over Z\ns \nx + 4y  = 1 \nthat simultaneously satisfy u · x = 0 and v · x = 0 \nand that all are multiples of \n57. 3x + 2y  = 1  over Z\n7 \nx + 4y  = 1 \n58. x\n1 \n+ 4x\n4 \n= 1  over Z\ns \n[\nU\n2\nV\n3 \n-\nU\n3\nV\n2\n] \nU X V = \nU\n3\nV\n1 \n-\nU\n1\nV\n3 \nX\n1 \n+  2X\n2 \n+ 4x\n3 \n= 3 \nU\n1\nV\n2 \n-\nU\n2\nV\n1 \n2X\n1 \n+  2X\n2 \n+ \nX\n4 \n= 1 \nX\n1 \n+ 3X\n3 \n= 2 \n59. Prove the following corollary to the Rank Theorem: \nShow that the lines x = p + su and x = q + tv are \nskew lines. Find vector equations of a pair of parallel \nplanes, one containing each line. \nLet A be an m X n matrix with entries in Z\np\n. Any \nconsistent system of linear equations with coefficient \nmatrix A has exactly \np\nn-rank(A) \nsolutions over Z\nP\n' \n60. When \np \nis not prime, extra care is needed in solving \na linear system (or, indeed, any equation) over Z\nP\n' \nUsing Gaussian elimination, solve the following system \nover Z6• What complications arise?","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":11375,"to":11469}}}}],[289,{"pageContent":"60. When \np \nis not prime, extra care is needed in solving \na linear system (or, indeed, any equation) over Z\nP\n' \nUsing Gaussian elimination, solve the following system \nover Z6• What complications arise? \nIn Exercises 53-58, solve th e syste ms of linear equations \nover th e indicated Z\nP\n. \n53. x  + 2y  = 1 over Z\n3 \nx  +   y  = 2 \n54. x  + y \n= 1  over Z\n2 \ny+z=O \nx +  z  = 1 \nWriting Proiect \nA History of Gaussian Elimination \n2x + 3y  = 4 \n4x + 3y  = 2 \nAs noted in the biographical sketch of Gauss in this section, Gauss did not actually \n\"invent\" the method known as Gaussian elimination. It was known in some form as \nearly as the third century B.C. and appears in the mathematical writings of cultures \nthroughout Europe and Asia. \nWrite a report on the history of elimination methods for solving systems of \nlinear equations. What role did Gauss actually play in this history, and why is his \nname attached to the method? \n1. S. Athloen and R. McLaughlin, Gauss-Jordan reduction: A  brief history,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":11469,"to":11500}}}}],[290,{"pageContent":"linear equations. What role did Gauss actually play in this history, and why is his \nname attached to the method? \n1. S. Athloen and R. McLaughlin, Gauss-Jordan reduction: A  brief history, \nAmerican Mathematical Monthly 94 (1987), pp. 130-142. \n2. Joseph F. Grear, Mathematicians of Gaussian Elimination, Notices of th e AMS, \nVol. 58, No. 6 (2011), pp. 782-792. (Available online at http://www.ams.org/ \nnotices/201106/index.html) \n3. Roger Hart, The Chinese Roots of Linear Algebra (Baltimore: Johns Hopkins \nUniversity Press, 2011). \n4. Victor J.  Katz, A History of Mathematics: An Introduction (Third Edition) \n(Reading, MA: Addison Wesley Longman, 2008).","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":11500,"to":11510}}}}],[291,{"pageContent":"cAs \nLies My Computer Told Me \nComputers and calculators store real numbers in floating-point form. For example, \n2001 is stored as 0.2001 X 10\n4\n, and -0.00063 is stored as -0.63 X 10\n-\n3\n. In general, \nthe floating-point form of a number is  :±:.M X \nlO\nk\n, where k is an integer and the \nmantissa Mis a  (decimal) real number that satisfies 0.1 ::::: M < 1. \nThe maximum number of decimal places that can be stored in the mantissa depends \non the computer, calculator, or computer algebra system. If the maximum number of \ndecimal places that can be stored is d, we say that there are d signifi cant digits. Many \ncalculators store 8 or 12 significant digits; computers can store more but still are subject \nto a limit. Any digits that are not stored are either omitted (in which case we say that the \nnumber has been truncated\n) \nor used to round the number to d significant digits. \nFor example,-rr = 3.141592654, and its floating-point form is 0.3141592654 X 10\n1\n.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":11512,"to":11536}}}}],[292,{"pageContent":"number has been truncated\n) \nor used to round the number to d significant digits. \nFor example,-rr = 3.141592654, and its floating-point form is 0.3141592654 X 10\n1\n. \nIn a computer that truncates to five significant digits, 1T would be stored as 0.31415 X \n10\n1 \n(and displayed as 3.1415\n)\n; a computer that rounds to five significant digits would \nstore 1T as 0.31416 X 10\n1 \n(and display 3.1416\n)\n. When the dropped digit is a solitary \n5, the last remaining digit is rounded so that it  becomes even. Thus, rounded to two \nsignificant digits, 0.735 becomes 0.74 while 0.725 becomes 0.72. \nWhenever truncation or rounding occurs, a roundoff error is introduced, which \ncan have a dramatic effect on the calculations. The more operations that are per­\nformed, the more the  error accumulates. Sometimes, unfortunately, there is nothing \nwe can do about this. This exploration illustrates this phenomenon with very simple \nsystems of linear equations.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":11536,"to":11559}}}}],[293,{"pageContent":"formed, the more the  error accumulates. Sometimes, unfortunately, there is nothing \nwe can do about this. This exploration illustrates this phenomenon with very simple \nsystems of linear equations. \n1. Solve  the following system of linear equations exactly (that  is,  work with \nrational numbers throughout the calculations). \nx  + y\n=O \n8\n01 \nX + \ns\noo\nY \n=  1 \n2. As a decimal, ��b = 1.00125, so, rounded to five significant digits, the system \nbecomes \nx+ \ny\n=O \nx + 1.0 012y  =  1 \nUsing your calculator or CAS, solve this system, rounding the result of every calcula­\ntion to five significant digits. \n83","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":11559,"to":11581}}}}],[294,{"pageContent":"84 \n3. Solve the system two more times, rounding first to four significant digits and \nthen to three significant digits. What happens? \n4. \nClearly, a very small roundoff error (less than or equal to 0.00125) can re­\nsult in very large errors in the solution. Explain why geometrically. (Think about the \ngraphs of the various linear systems you solved in Problems 1-3.) \nSystems such as the one you just worked with are called ill-conditioned. They \nare extremely sensitive to roundoff errors, and there is not much we can do about it. \nWe will encounter ill-conditioned systems again in Chapters 3 and 7. Here is another \nexample to experiment with: \n4.552x + 7.083y = 1.931 \n1.731x + 2.693y = 2.001 \nPlay around with various numbers of significant digits to see what happens, starting \nwith eight significant digits (if you can). \nPartial Pivoting \nIn Exploration: Lies My Computer Told Me, we saw that ill-conditioned linear sys­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":11583,"to":11599}}}}],[295,{"pageContent":"with eight significant digits (if you can). \nPartial Pivoting \nIn Exploration: Lies My Computer Told Me, we saw that ill-conditioned linear sys­\ntems can cause trouble when roundoff error occurs. In this exploration, you will dis­\ncover another way in which linear systems are sensitive to roundoff error and see that \nvery small changes in the coefficients can lead to huge inaccuracies in the solution. \nFortunately, there is something that can be done to minimize or even eliminate this \nproblem (unlike the problem with ill-conditioned systems). \n1. \n(a)  Solve the single linear equation 0.0002lx = 1 for x. \n(b)  Suppose your calculator can carry only four decimal places. The equa­\ntion will be rounded to 0.0002x = 1. Solve this equation. \nThe difference between the answers in parts (a) and (b) can be thought of as the \neffect of an error of 0.00001 on the solution of the given equation. \n2. \nNow extend this idea to a system oflinear equations. \n(a) With Gaussian elimination, solve the linear system","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":11599,"to":11615}}}}],[296,{"pageContent":"effect of an error of 0.00001 on the solution of the given equation. \n2. \nNow extend this idea to a system oflinear equations. \n(a) With Gaussian elimination, solve the linear system \n0.400x + 99.6y = 100 \n75.3x  -45.3y = \n30.0 \nusing three significant digits. Begin by pivoting on 0.400 and take each \ncalculation to three significant digits. You should obtain the \"solution\" x = \n-1.00, y = 1.01. Check that the  actual solution is x = 1.00, y = 1.00. This is a \nhuge error-200% in the x value! Can you discover what caused it? \n(b)  Solve the system in part (a) again, this time interchanging the two equa­\ntions (or, equivalently, the two rows of its augmented matrix) and pivoting \non 75.3. Again, take each calculation to three significant digits. What is the \nsolution this time? \nThe moral of the story is that, when using Gaussian or Ga  uss-Jordan elimination \nto obtain a numerical solution to a system oflinear equations (i.e., a decimal approxi­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":11615,"to":11631}}}}],[297,{"pageContent":"solution this time? \nThe moral of the story is that, when using Gaussian or Ga  uss-Jordan elimination \nto obtain a numerical solution to a system oflinear equations (i.e., a decimal approxi­\nmation), you should choose the pivots with care. Specifically, at each pivoting step, \nchoose from among all possible pivots in a column the entry with the largest absolute \nvalue. Use row interchanges to bring this element into the correct position and use it to \ncreate zeros where needed in the column. This strategy is known as partial pivoting.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":11631,"to":11637}}}}],[298,{"pageContent":"Abu Ja'far Muhammad ibn Musa \nal-Khwarizmi (c. 780-850) was \na Persian mathematician whose \nbook Hisab al-jabr w'al muqabalah \n( c. 825) described the use ofHindu­\nArabic numerals and the rules \nof basic arithmetic. The second \nword of the book's title gives rise \nto the English word algebra, and \nthe word algorithm is derived from \nal-Khwarizmi's name. \n3. Solve the following systems by Gaussian elimination, first without and then \nwith partial pivoting. Take each calculation to three significant digits. (The exact \nsolutions are given.) \n(a) O.OO\nl\nx + \n0.99\n5\ny = 1.\n00 \n-\n1\n0\n.2x + \n1.\n00y = \n-\n5\n0.0 \nExact solution: \n[;] \n[\n5\n.00\n] \n1.\n00 \n(b) \nl\nOx \n-\n7\ny \n= \n7 \n-\n3x + 2.\n09y + 6z = 3.91 \n5x \n-\ny \n+ 5z \n= \n6 \n[xl  [ \n0.  00\n] \nExact solution: y = \n-\n1.\n00 \nz \n1.\n00 \nCounting Operations:  An Introduction \nto the Analysis of Algorithms \nGaussian and Gauss-Jordan elimination are examples of algorithms: systematic pro­\ncedures designed to implement a particular task-in this case, the row reduction of","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":11639,"to":11707}}}}],[299,{"pageContent":"to the Analysis of Algorithms \nGaussian and Gauss-Jordan elimination are examples of algorithms: systematic pro­\ncedures designed to implement a particular task-in this case, the row reduction of \nthe augmented matrix of a system of linear equations. Algorithms are particularly well \nsuited to computer implementation, but not all algorithms are created equal. Apart \nfrom the speed, memory, and other attributes of the computer system on which they \nare running, some algorithms are faster than others. One measure of the so-called com­\nplexity of an algorithm (a measure of its efficiency, or ability to perform its task in a \nreasonable number of steps) is the number of basic operations it performs as a func­\ntion of the number of variables that are input. \nLet's examine this proposition in the case of the two algorithms we have for \nsolving a linear system: Gaussian and Gauss-Jordan elimination. For our pur­\nposes, the basic operations are multiplication and division; we will assume that","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":11707,"to":11719}}}}],[300,{"pageContent":"solving a linear system: Gaussian and Gauss-Jordan elimination. For our pur­\nposes, the basic operations are multiplication and division; we will assume that \nall other operations are performed much more rapidly and can be ignored. (This \nis a reasonable assumption, but we will not attempt to justify it.) We will consider \nonly systems of equations with squ\nare coefficient matrices, so, if the coefficient \nmatrix is n X n, the number of input variables is n. Thus, our task is to  find the \nnumber of operations performed by Gaussian and Gauss-Jordan elimination as \n0 \na function of n. Furthermore, we will not worry about special cases that may \n00 \narise, but rather establish the worst c\na\nse that can  arise-when the algorithm takes \nj \nas long as possible. Since this will give us an estimate of the time it will take a \n9 \ncomputer to perform the algorithm (if we know how long it takes a computer to \nperform a single operation), we will denote the number of operations performed \nby an algorithm by T\n(\nn\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":11719,"to":11741}}}}],[301,{"pageContent":"9 \ncomputer to perform the algorithm (if we know how long it takes a computer to \nperform a single operation), we will denote the number of operations performed \nby an algorithm by T\n(\nn\n)\n. We will typically  be interested in T\n(\nn\n) \nfor large values of \nn, so comparing this function for different algorithms will allow us to   determine \nwhich will take less time to execute. \n1 . \nConsider the augmented matrix \n[A \nl\nh\nJ \n4 \n9 \n85","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":11741,"to":11763}}}}],[302,{"pageContent":"86 \nCount the number of operations required to bring [A \nI \nb] to the row echelon \nform \n[\n: \n2 \n1 \n0 \n3 \n4\n] \n-1 0 \n1   1 \n(By \"operation\" we mean a multiplication or a   division.) Now count the number \nof opera  tions needed to complete the back substitution phase of Gaussian elimi­\nnation. Record the total number of operations. \n2. Count  the  number  of  opera  tions needed to  perform Gauss-Jordan \nelimination-that is, to reduce [A \nI \nb] to its reduced row echelon form \n(where the zeros are introduced into each column immediately after the leading 1 is \ncreated in that column). What do your answers suggest about the relative efficiency \nof the two algorithms? \nWe will now attempt to analyze the algorithms in a general, systematic way. Sup­\npose the augmented matrix [A \nI \nb] arises from a linear system with n equations and \nn variables; thus, [A \nI \nb] is n X \n(\nn \n+ \n1\n)\n: \n[\na\nll \n[A \nI \nb] \n= \na\n�1 \na\nn\n] \nWe will assume that row interchanges are never needed-that we can always create a","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":11765,"to":11815}}}}],[303,{"pageContent":"b] arises from a linear system with n equations and \nn variables; thus, [A \nI \nb] is n X \n(\nn \n+ \n1\n)\n: \n[\na\nll \n[A \nI \nb] \n= \na\n�1 \na\nn\n] \nWe will assume that row interchanges are never needed-that we can always create a \nleading 1 from a pivot by dividing by the pivot. \n3. (a)  Show that n operations are needed to create the first leading 1: \nl\na\n.\n, \na\n12 \na\n21 \na\n22 \na\nn\nl \na\nn\n2 \na\nl\nn \na\n2\nn \na\nnn \nb\n, \nl \nl \nl \n�\n2 \n� \na\n�I \nb\nn \na\nn\nl \n* * \n:\n, \nl \na\nz\n2 \na\n2\nn \na\nn\n2 \na\nnn \nb\nn \n(Why don't we need to count an operation for the creation of the leading 1 ?) Now \n� \nshow that n operations are needed to obtain the first zero in column 1: \nla�, \n* * \ni\nJ \n* * \na\nn\n2 \na\nnn","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":11815,"to":11908}}}}],[304,{"pageContent":"....,.. \n(Why don't we need to count an operation for the creation of the zero itself?) When \nthe first column has been \"swept out;' we have the matr  ix \n[\n� \n: \n: \n:\n] \n0 \n* \n*    * \nShow that the  total number of operations needed up to this point is n \n+  (\nn -  1\n) \nn. \n(b) Show that the total number of operations needed to reach the row \nechelon form \n[\n! \n* * \n'\n] \n*    * \n0 \n1 \n* \nis \n[ n  + \n(n -   l\n)\nn] +   [ \n(n -   1 ) \n+ \n(n -  2)(n -   1 )\n]  +   [ \n(n -  2\n) \n+ \n(n -  3)(n -  2\n)\n] \n+ \n... \n+ [2 +  1  . 2] +  1 \nwhich simplifies to \nn\n2 \n+ \n(n -   1 )\n2 \n+ \n·   ·   · \n+ 2\n2 \n+  1\n2 \n( c)  Show that the  number of opera  tions needed to complete the back substi­\ntution phase is \n1  +  2  + ·   ·   · \n+ \n(n -   1\n) \n(d)  Using summation formulas for  the  sums in  parts (b)  and  (c)  (see \nExercises 51 and 52 in Section 2.4 and Appendix B), show that the total number of \noperations, T\n(\nn\n)\n, performed by Gaussian elimination is \nT\n(n) = tn\n3 \n+  n\n2 \n-tn \nSince every polynomial function is dominated by its leading term for large values of","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":11910,"to":11987}}}}],[305,{"pageContent":"operations, T\n(\nn\n)\n, performed by Gaussian elimination is \nT\n(n) = tn\n3 \n+  n\n2 \n-tn \nSince every polynomial function is dominated by its leading term for large values of \nthe variable, we see that T\n(\nn\n) \n= \ntn\n3 \nfor large values of n. \n4. Show that Gauss-Jordan elimination has T\n(\nn\n) \n= \n!\nn\n3 \ntotal operations if we \ncreate zeros above  and below the leading ls as we go.  (This shows that, for large \nsystems of linear equations, Gaussian elimination is faster than this version of Gauss­\nJ ordan elimination.) \n81","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":11987,"to":12019}}}}],[306,{"pageContent":"88 \nChapter 2 \nSystems of Linear Equations \n�\nSpanning \nse1s and linear Independence \nExample 2.18 \nThe second of the three roads in our \"trivium\" is concerned with linear combina­\ntions of vectors. We have seen that we can view solving a system of linear equations \nas asking whether a certain vector is a linear combination of certain other vectors. \nWe explore this idea in more detail in this section. It leads to some very important \nconcepts, which we will encounter repeatedly in later  chapters. \nSpanning Sets of  Vectors \nWe can now easily answer the question raised in Section 1.1: When is a given vector \na linear combination of other given vectors? \n(o) fa th, wdoc \nm \n'linm rn mbinotion ofthe vodorn m and [ � �} \n(b) fa m ' linemomb;mtion of the vectorn m and [ � �} \nSolulion \n(a) We want to find scalars x and y such that \nExpanding, we obtain the system \nwhose augmented matrix is \nx -y \n=  1 \ny=2 \n3x -3y =  3 \n[� �� �] \n(Obser ve that the columns of the augmented matrix are just the given vectors; notice","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":12021,"to":12049}}}}],[307,{"pageContent":"Expanding, we obtain the system \nwhose augmented matrix is \nx -y \n=  1 \ny=2 \n3x -3y =  3 \n[� �� �] \n(Obser ve that the columns of the augmented matrix are just the given vectors; notice \nthe order of the vectors-in particular, which vector is the constant vector.) \nThe reduced echelon form of this matrix is \n[� : �] \n� \n(Verify  this.)  So  the solution  is x  =   3,  y \ncombination is \n2, and  the corresponding linear","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":12049,"to":12063}}}}],[308,{"pageContent":"Theorem 2.4 \nSection 2.3 \nSpanning Sets and Linear Independence \n89 \n(b)  Utilizing our observation in part (a), we obtain a linear system whose augmented \nmatrix is \nwhich reduces to \n;eve\n.\nling that th, 'Y'\"m h\n\"\n' no ,oJotion. Th°', in thi, \"\"· [ \n�] \ni' not a linm com-\nbination of m and [ J \n4 \nThe notion of a spanning set is intimately connected with the solution of linear \nsystems. Look back at Example 2.18. There we  saw that a system with augmented \nmatrix [A I b] has a solution precisely when bis a  linear combination of the columns \nof A. This is a general fact, summarized in the next theorem. \nA system of linear equations with augmented matr  ix [A I b]  is consistent if and \nonly if b is a linear combination of the columns of A. \nLet's revisit Example 2.4, interpreting it in light of Theorem 2.4. \n(a) The system \nx-y=   l \nx+y\n=  3 \nhas the unique solution x = 2, y = 1. Thus, \nSee Figure 2.S(a). \n(b) The system \nx -y  = 2 \n2x -2y  = 4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":12065,"to":12096}}}}],[309,{"pageContent":"Let's revisit Example 2.4, interpreting it in light of Theorem 2.4. \n(a) The system \nx-y=   l \nx+y\n=  3 \nhas the unique solution x = 2, y = 1. Thus, \nSee Figure 2.S(a). \n(b) The system \nx -y  = 2 \n2x -2y  = 4 \nhas infinitely many solutions of the form x = 2 + t, y = t. This implies that \nfor all values oft. Geometrically, the vectors \n[ \n�\n], [ \n= \n�\n], \nand \n[ ! ] \nare all parallel and \nso all lie along the same line through the origin [see Figure 2.S(b)].","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":12096,"to":12117}}}}],[310,{"pageContent":"90 \nChapter 2 \nSystems of Linear Equations \n5 \n4 \n-\n2 \n-\n1 \n-\n1 \n-\n2 \n-\n3 \nFigure 2.8 \ny \n(\na\n) \n2 \n3 \nExample 2.19 \ny \n5 \n-\n3 \n(b) \n(c)  The system \nx-y=I \nx-y=3 \n5 \n4 \n-\n2 \n-\n3 \nhas no solutions, so there are no values of x and y that satisfy \ny \n(c) \nIn this case, \n[ \n�\n] \nand \n[ \n= \n�\n] \nare parallel, but \n[ �] \ndoes not lie along the same line \nthrough the origin [see Figure 2.S(c)]. \nWe will often be interested in the collection of all linear combinations of a given \nset of   vectors. \nDefinition If S = {v\n1\n, v\n2\n, ... , vk} is a set of vectors in u;g\nn\n, then the set of all \nlinear combinations ofv\n1\n, v\n2\n, ... , vk is called the sp an ofv\n1\n, v\n2\n, ... , vk and is de-\nnoted by span(v\n1\n, v\n2\n, ..• , vk) or span(S). If span(S) = W, then Sis called a sp an­\nning set for u;g\nn\n. \nShow that u;g\nz \n= span( \n[ \n_ \n� \nl \n[ �]\n)\n. \nSolulion We need to show that an arbitrary vector \n[\n:\n] \ncan be written as a linear \ncombination of \n[ \n_ \n�\n] \nand \n[�\n} that is, we must show that the equation x\n[ \n_ \n�\n] \n+ \n1\n[�] \n= \n[\n:\n]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":12119,"to":12231}}}}],[311,{"pageContent":"z \n= span( \n[ \n_ \n� \nl \n[ �]\n)\n. \nSolulion We need to show that an arbitrary vector \n[\n:\n] \ncan be written as a linear \ncombination of \n[ \n_ \n�\n] \nand \n[�\n} that is, we must show that the equation x\n[ \n_ \n�\n] \n+ \n1\n[�] \n= \n[\n:\n] \ncan always be sol  ved for x and y (in terms of a and b ), regardless of the \nvalues of a and b.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":12231,"to":12265}}}}],[312,{"pageContent":"Section 2.3 \nSpanning Sets and Linear Independence \n91 \nThe augmented matr  ix is \n[ \n_ \n�  � \nI \n�\n]\n' \nand row reduction produces \n3\n1\nb\n]\n�\n1\n[\n-1 \n3\n1 \nb \n] \n1 a \n0 7   a + 2b \nat which point it is clear that the system has a (unique) solution. (Why?) If we con­\ntinue, we obtain \nb\nR\n1 \n[\n-1 \n---'-----+ \n0 \n3\n1 \nb \n] \nR\n1\n-\n3\nR, \n[\n-1 \nO \nI \n(b - 3\na\n)/\n7\n] \n1 \n(\na + 2b)/7 \n� \n0 1 \n(\na + 2b)\n/\n7 \nfrom which we see that x = \n(\n3a - b)\n/\n7 and y = (a + 2b)\n/\n7. Thus, for any choice of \na and b, we have \n� (Check this.) \nExample 2.20 \nExample 2.21 \nRemark It  is also true that  IR\n2 \n= span( \n[ \n_ \n� \nl \n[ � \nl \n[ \n�\n]\n} If,  given \n[\n: \nl \nwe can \nfind xand y such that x\n[ \n_\n�\n] \n+ y\n[�] \n= \n[\n:\nl\nthen we also have x\n[ \n_\n�\n] \n+ y\n[�]\n+ \n0 \n[ \n�\n] \n= \n[\n:\n]\n. In fact, any set of vectors that contains a spanning set for IR\n2 \nwill also be \na spanning set for IR\n2 \n(see Exercise 20\n)\n. \nThe next example is an important (easy) case of a spanning set. We will encounter \nversions of this example many times. \nLet e\n,\n, e,\n. \nand e; bethe standMd unit vedocs in HI'. Then I° ' any mtn< \n[\n; \nJ we haw \n[\n;\n] \n� \nx\n[\n�\n] \n+ y\nm \n+ z","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":12267,"to":12412}}}}],[313,{"pageContent":"versions of this example many times. \nLet e\n,\n, e,\n. \nand e; bethe standMd unit vedocs in HI'. Then I° ' any mtn< \n[\n; \nJ we haw \n[\n;\n] \n� \nx\n[\n�\n] \n+ y\nm \n+ z\nm \n� \nxe\n, \n+ye,+ ze., \nThus, IR\n3 \n= span(e\n1\n, e\n2\n, e\n3\n). \nYou should have no difficulty seeing that, in general, !R\nn\n= span(e\n1\n, e\n2\n, .•. , \ne\nn\n4 \nWhen the span of a set of vectors in !R\nn \nis not all of !R\nn\n, it is reasonable to ask for a \ndescription of the vectors' span.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":12412,"to":12461}}}}],[314,{"pageContent":"92 \nChapter 2 \nSystems of Linear Equations \nz \nFigure 2.9 \nTwo nonparallel vectors span a \nplane \nSolulion \nThinking geometrically, we can see that the set of all linear combinations of \nm and [ J i'iu'tthe plane thrnugh the migm wiili m and [ �:\n] \nOS dimti on \nvecto\" ( Figme 2. 9\n)\n. The vectm equotion of this pfane is [ � l \n� \ns m + { ] \nwhich i*t anothe; way of \"ying that [� l is m th, ,pan of m and [ J \nSuppose we want to obtain the general equation of this plane. There are several \nways to proceed. One is to  use the fact that the equation \nax + by + cz \n=  0 must be \nsatisfied by the points \n(\n1, 0, 3\n) \nand \n( \n-1, 1, - 3\n) \ndetermined by the direction vectors. \nSubstitution then leads to a system of equations in \na\n, b, and c. (See Exercise 17.\n) \nAnother method is to use the system of equations arising from the vector equation: \ns  -t =  x \nt=y \n3s  -3t = z \nIf we row reduce the augmented matr  ix, we obtain \nN�� we know that this ry'1em is consistent, since [;\n] \nis in the span of [ �\n] \nand \n[ \n_ \n�\n]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":12463,"to":12511}}}}],[315,{"pageContent":"s  -t =  x \nt=y \n3s  -3t = z \nIf we row reduce the augmented matr  ix, we obtain \nN�� we know that this ry'1em is consistent, since [;\n] \nis in the span of [ �\n] \nand \n[ \n_ \n�\n] \nby ossumption. So we must have z -3x \n� \n0 (o< 3x -z \n� \n0, in mo<e standa<d \nform), giving us the general equation we seek. \nRemark \nA normal vector to the plane in this example is also given by the cross \nproduct \nLinear Independence \nill E\"mple 218, wdound that 3 m +  2 [ �: l \n� \nm Let's abb<e�ate thi\"qua­\ntion as 3u + 2v = w. The vector w \"depends\" on u and v in the sense that it is a \nlinear combination of them. We  say that a set of vectors is linearly dependent if one","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":12511,"to":12538}}}}],[316,{"pageContent":"Theorem 2.5 \nSection 2.3 \nSpanning Sets and Linear Independence \n93 \nof them can be written as a linear combination of the others. Note that we  also have \nu = -�v + \nt\nw and v = -\n�\nu + \n!\nw. To get around the question of which vector to \nexpress in terms of the rest, the formal definition is stated as follows: \nDefinition \nA set of vectors v\n1\n, v\n2\n, .•. , vk  is linearly dependent if there are \nscalars c\n1\n, c\n2\n, .•• , c\nk\n, at least one of which is not zero, such that \nA set   of vectors that is not linearly dependent is called linearly independent. \nRemarks \n• \nIn the definition of linear dependence, the requirement that at least one of the \nscalars c\n1\n, c\n2\n, ..• , c\nk \nmust be nonzero allows for the possibility that some may be zero. \nIn the example above, u, v, and ware linearly dependent, since 3u + 2v - w = 0 and, \nin fact, all of the scalars are nonzero. On the other hand, \nso \n[ \n! \nl \n[ \n� \nl \nand \n[ \n�\n] are linearly dependent, since at least one (in fact, two) of the","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":12540,"to":12589}}}}],[317,{"pageContent":"in fact, all of the scalars are nonzero. On the other hand, \nso \n[ \n! \nl \n[ \n� \nl \nand \n[ \n�\n] are linearly dependent, since at least one (in fact, two) of the \nthree scalars 1, -2, and 0 is nonzero. (Note that the actual dependence arises simply \nfrom the fact that the first two vectors are multiples.) (See Exercise 44.\n) \n• \nSince Ov\n1 \n+ Ov\n2 \n+ \n... \n+ \nOvk \n= 0 for any vectors v\n1\n, v\n2\n, ... , vk,   linear de­\npendence essentially says that the zero vector can be expressed as a nontrivial linear \ncombination ofv\n1\n, v\n2\n, ... , v\nk\n· \nThus, linear independence means that the zero vector \ncan be expressed as a linear combination ofv\n1\n, v\n2\n, ••. , v\nk \nonly in the trivial way: c\n1\nv\n1 \n+  c\n2\nv\n2 \n+ ·   ·   · + c\nk\nv\nk \n= 0 only if c\n1 \n= 0, c\n2 \n= 0, ... , c\nk \n= 0. \nThe relationship between the intuitive notion of dependence and the formal defi­\nnition is given in the next theorem. Happily, the two notions are equivalent! \nVectors v\n1\n, v\n2\n, •.. , v\nm \nin !R\nn \nare linearly dependent if and only if at least one of the","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":12589,"to":12662}}}}],[318,{"pageContent":"nition is given in the next theorem. Happily, the two notions are equivalent! \nVectors v\n1\n, v\n2\n, •.. , v\nm \nin !R\nn \nare linearly dependent if and only if at least one of the \nvectors can be expressed as a linear combination of the others. \nProof If one of the vectors-say, v\n1 \n-is a linear combination of the others, then \nthere are scalars c\n2\n, .•• , c\nm \nsuch that v\n1 \n= c\n2\nv\n2 \n+ \n... \n+ \nc\nm\nv\nm\n. Rearranging, we obtain \nv\n1 \n-c\n2\nv\n2 \n-  ·   ·   ·   -c\nm\nv\nm \n= 0, which implies that v\n1\n, v\n2\n, ... , v\nm \nare linearly dependent, \nsince at least one of the scalars (namely, the coefficient 1 of v\n1\n) is nonzero. \nConversely, suppose that v\n1\n, v\n2\n, •.. , v\nm \nare linearly dependent. Then there are \nscalars c\n1\n, c\n2\n, ... , c\nm\n, not all zero, such that c\n1\nv\n1 \n+  c\n2\nv\n2 \n+ \n... \n+ \nc\nm\nv\nm \n= 0. Suppose \nc\n1\n-=!= 0. Then \nand we may multiply both sides by 1/ c\n1 \nto obtain v\n1 \nas a linear combination of the \nother vectors: \nv = -(\n�\n)v -  ·   ·   ·   -(\nc\nm\n)v \n1 \nC\n1 \n2 \nC\n1 \nm","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":12662,"to":12764}}}}],[319,{"pageContent":"94 \nChapter 2 \nSystems of Linear Equations \nExample 2.22 \nExample 2.23 \nNote It may appear as if we are cheating a bit in   this proof. After all, we cannot \nbe sure that v1 is a linear combination of the other vectors, nor that c1 is nonzero. \nHowever, the argument is analogous for some other vector V; or for a different scalar \nc\nj\n. Alternatively, we can just relabel things so that they work out as in the above proof. \nIn a situation like this, a mathematician might begin by saying, \"without loss of gen­\nerality, we may assume that v1 is a linear combination of the other vectors\" and then \nproceed as above. \nAny set of   vectors containing the zero vector is linearly dependent.  For if 0, v2, ..• , v m \nare in IJ�r, then we can find a nontrivial combination of the form c10 + c2v2 \n+ \n... \n+ \nC\nm Vm = 0 by setting c1 =  1 and c2 = c3 \n= ... = \nC\nm = 0\n. \nDetermine whether the following sets of vectors are linearly independent: \n(a) \n[\n�\n]\nand\n[\n-\n�\n] \n(c) \n[\n-\ni\n]\n. \n[ \nJand \nn\nl","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":12766,"to":12810}}}}],[320,{"pageContent":"+ \n... \n+ \nC\nm Vm = 0 by setting c1 =  1 and c2 = c3 \n= ... = \nC\nm = 0\n. \nDetermine whether the following sets of vectors are linearly independent: \n(a) \n[\n�\n]\nand\n[\n-\n�\n] \n(c) \n[\n-\ni\n]\n. \n[ \nJand \nn\nl \nSolution In answering any question of this type, it is a good idea to see if you can \ndetermine by inspection whether one vector is a linear combination of the others. A \nlittle thought may save a lot of computation! \n(a) The only way two vectors can be linearly dependent is if  one is a multiple of \n� \nthe other. (Why?) These two vectors are clearly not multiples, so they are linearly \nindependent. \n(b) There is no obvious dependence relation here, so we try to find scalars c1, c2, c3 \nsuch that \nThe corresponding linear system is \nC\n1 \n+ \nC\n1 \n+ \nC\nz \nand the augmented matr  ix is \n[\ni \n0 \nC\n3 =  0 \n=O \n1     O\nJ \n0    0 \n1     0 \nOnce again, we make the fundamental observation that the columns of the coefficient \nmatr  ix are just the vectors in question!","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":12810,"to":12869}}}}],[321,{"pageContent":"Section 2.3 \nSpanning Sets and Linear Independence \n95 \nThe reduced row echelon form is \n[\ni \n0 \n1 \n0 \n0 OJ \n0    0 \n1    0 \n� \n(check this), so c\n1 \n= 0, c\n2 \n= 0, c\n3 \n= 0. Thus, the given vectors are linearly independent. \n( c)  A little reflection reveals that \n� \nso the three vectors are linearly dependent. [Set up a linear system as in part (b) to \ncheck this algebraically.] \n( d)  Once again, we observe no obvious dependence, so we   proceed directly to reduce \na homogeneous linear system whose augmented matrix has as its columns the given \nvectors: \n[\n� \n� \n: \n�\n1 \n�I\n[\n� \n0 -1 2    0 \n0 \n1 \n-1 \n-1 \nIf we let the scalars be c\n1\n, c\n2\n, and c\n3\n, we have \n1    0 l �: � �: \n[ \n1 0 \n2    0 _ \nR\n, 0 1 \n2    0 0     0 \nc\n1 \n+ \n3c\n3 \n= 0 \nc\n2 \n-2c\n3 \n= 0 \n-\n� \n�\n1 \n0    0 \nfrom which we see that the  system has infinitely many solutions. In particular, there \nmust be a nonzero solution, so the given vectors are linearly dependent. \nIf we continue, we can describe these solutions exactly: c\n1 \n= -3c\n3 \nand c\n2 \n= 2c\n3\n. \nThus, for any nonzero value of c\n3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":12871,"to":12954}}}}],[322,{"pageContent":"must be a nonzero solution, so the given vectors are linearly dependent. \nIf we continue, we can describe these solutions exactly: c\n1 \n= -3c\n3 \nand c\n2 \n= 2c\n3\n. \nThus, for any nonzero value of c\n3\n, we have the linear dependence relation \n� \n(Once again, check that this is correct.) \nTheorem 2.6 \nWe summarize this procedure for testing for linear independence as a theorem. \nLet v\n1\n,  v\n2\n, ... ,  v\nm \nbe (column) vectors in IK£\nn \nand let A be the n X m matrix \n[v\n1 \nv\n2 \n·   ·   · v\nm\nl \nwith these vectors as its columns. Then v\n1\n, v\n2\n, .•• , v\nm \nare linearly \ndependent if and only if the homogeneous linear system with augmented matrix \n[A I \nO\n] has a nontrivial solution. \nProof v\n1\n, v\n2\n, ..• , v\nm \nare linearly dependent if and only if there are scalars c\n1\n, c\n2\n, ••. , c\nm\n, \nnot all zero, such that c\n1\nv\n1 \n+ c\n2\nv\n2 + \n... \n+ \nc\nm\nv\nm \n= 0. By Theorem 2.4, this is equivalent \nto sa'.i�g that the nonzero vector \n[ \n:.: ] is a solution of the system whose augmented \nmatn\nx is \n[v\n1 \nv\n2 \n.•. \nv\nm \nI \nOJ\n. \n· \nc\nm","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":12954,"to":13043}}}}],[323,{"pageContent":"96 \nChapter 2 \nSystems of Linear Equations \nExample 2.24 \nExample 2.25 \nTheorem 2.1 \nThe standard unit vectors e1, e\n2\n, and e\n3 \nare linearly independent in IR\n3\n, since the sys­\ntem with augmented matr  ix [e1 e\n2 \ne\n3 \nI OJ is already in the reduced row echelon form \n[\n: \n0 \n1 \n0 \n0  O\nJ \n0  0 \n1  0 \nand so clearly has only the trivial solution. In general, we see that e1, e\n2\n,  ... , e\nn \nwill be \nlinearly independent in !R\nn\n. \nPerforming elementary row operations on a matrix constructs linear combina­\ntions of the rows. We can use this fact to come up with another way to test vectors for \nlinear independence. \nConsider the three vectors of Example 2.23(d) as row vectors: \n[1,2, 0]  , \n(\n1, 1, -1],  and [1,4, 2] \nWe construct a matrix with these vectors as its rows and proceed to reduce it to  eche­\nlon form. Each time a row changes, we denote the new row by adding a prime symbol: \n[\n: \n2 \n1 \n4 \nFrom this we see that \nor, in terms of the original vectors, \n2 \n-\n1 \n2 \nO\nJ R;\n=\nR; + ZR; \n[ \nl \n-\n1 \n� \n0 \n2 \n0 \n2 \n-\n1 \n0 \n-3\n(\n1,2,0] + 2[1, 1, -1] + (","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":13045,"to":13118}}}}],[324,{"pageContent":"[\n: \n2 \n1 \n4 \nFrom this we see that \nor, in terms of the original vectors, \n2 \n-\n1 \n2 \nO\nJ R;\n=\nR; + ZR; \n[ \nl \n-\n1 \n� \n0 \n2 \n0 \n2 \n-\n1 \n0 \n-3\n(\n1,2,0] + 2[1, 1, -1] + (\n1,4,2] = [O, O, O] \n-\n�\nJ \n[Notice that  this  approach corresponds to  taking c\n3 \n= 1  in  the  solution  to \nExample 2.23(d).] \nThus, the rows of a matrix will be linearly dependent if elementary row opera­\ntions can be used to create a zero row. We summarize this finding as follows: \nLet v,, v , ..... v \nm \nbe ( rn   w) ve<to\" in �\" and kt A bet he m X n motrix [J with \nthese vectors as its rows. Then v1, v\n2\n, •.• , v\nm \nare linearly dependent if and only if \nrank(A) < m. \nProof Assume that v1,  v\n2\n, ••. , v\nm \nare linearly dependent. Then, by Theorem 2.2, \nat least one of the vectors can be written as a linear combination of the others.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":13118,"to":13172}}}}],[325,{"pageContent":"Theorem 2.8 \nExample 2.26 \n... \nI \nExercises 2.3 \nSection 2.3 \nSpanning Sets and Linear Independence \n91 \nWe relabel the vectors, if necessary, so that we can write v\nm \n= c\n1\nv\n1 \n+  c\n2\nv\n2 \n+ ·   ·   · + \nc\nm\n-\ni\nY\nm\n-\nl\n· \nThen the  elementary row opera  tions R\nm \n-   c\n1\nR\n1\n,  R\nm \n-   c\n2\nR\n2\n, ••• , \nR\nm \n-  c\nm\n-\ni\nR\nm\n-\nI \napplied to A will create a zero row in row m. Thus, rank(A) < m. \nConversely, assume that rank(A) < m. Then there is some sequence of row opera­\ntions that will create a zero row. A successive substitution argument analogous to that \nused in Example 2.25 can be used to show that 0 is a nontrivial linear combination of \nv\n1\n, v\n2\n, •.. , v\nm\n. Thus, v\n1\n, v\n2\n, .•. , v\nm \nare linearly dependent. \nIn some situations, we can deduce that a  set of   vectors is linearly dependent with­\nout doing any work. One such situation is when the zero vector is in the set (as in \nExample 2.22\n)\n. Another is when there are \"too many\" vectors to be independent. The","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":13174,"to":13246}}}}],[326,{"pageContent":"out doing any work. One such situation is when the zero vector is in the set (as in \nExample 2.22\n)\n. Another is when there are \"too many\" vectors to be independent. The \nfollowing theorem summarizes this case. (We will see a sharper version of this result \nin Chapter 6.\n) \nAny set of m vectors in !R\nn \nis linearly dependent if m > n. \nProof Let v\n1\n, v\n2\n, .•• , v\nm \nbe (column) vectors in !R\nn \nand let A be the n X m matrix \n[v\n1 \nv\n2 \n.•. v\nm\nl \nwith these vectors as its columns. By Theorem 2.6, v\n1\n, v\n2\n, •.. , v\nm \nare lin­\nearly dependent if and  only if the homogeneous linear system with augmented matrix \n[A I O] has a nontrivial solution. But, according to Theorem 2.6, this will always be the \ncase if A has more columns than rows; it  is the case here, since number of columns m \nis greater than number of rows n. \nThe vectors \n[ \n�\n]\n, \n[ \n! \n] \n, and \n[ \n�\n] \nare linearly dependent, since there cannot be more \nthan two linearly independent vectors in IR\n2\n• (Note that if we want to find the ac­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":13246,"to":13298}}}}],[327,{"pageContent":"The vectors \n[ \n�\n]\n, \n[ \n! \n] \n, and \n[ \n�\n] \nare linearly dependent, since there cannot be more \nthan two linearly independent vectors in IR\n2\n• (Note that if we want to find the ac­\ntual dependence relation among these three vectors, we must solve the homogeneous \nsystem whose coefficient matr  ix has the given vectors as columns. Do this!) \nIn Exercises 1-6, determine if th e vector vis a linear combi­\nnation of th e remaining vectors. \n5. \nv \n� \nm. \nu\n, \n� \nm. \nu, \n� \n[\n: \nJ \nu\n,\n� \nm \n[ \n3\n.2\n] \n[ \n1\n.\n0\n] \n[ \n3\n.\n4\n] \nGAS \n6. V = \n2.0 , \nU\n1 \n= \n0.\n4 \n, ll\n2 \n= \n1.\n4 \n, \n-2.6 \n4\n.\n8 \n-\n6.4 \nU\n3 \n= \n[\n-\n�\n:\n�\n] \n-1.0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":13298,"to":13378}}}}],[328,{"pageContent":"98 \nChapter 2 \nSystems of Linear Equations \nIn Exercises 7 and 8, determine if th e vector b is in th e span \nof th e columns of th e matrix A. \n7. A = \n[� !\nl\nb= \n[\n:\nJ \n8\n. \nA = \n[ \n� \n! \n�\n]\n, \nb = \n[ \n: \nl \n9 10  11 12 \n9. Show that IR\n2 \n= span\n(\n[\n�\n], \n[ \n_\n�\n]\n)\n. \n10. Show that IR\n2 \n= span\n( \n[ \n_ \n� \nl \n[ \n�\n]\n)\n. \n(b)  In part (a), suppose in addition that each v\nj \nis also \na linear combination of u\n1\n, ... , u\nk\n. Prove that \nspan(u\n1\n, ... , uk) = span(v\n1\n, ... , v\nm\n). \n( c) Use the result of part (b) to prove that \n[Hint: We know that IR\n3 \n= span(e\n1\n, e\n2\n, e\n3\n).] \nUse the method of Example 2.23 and Theorem 2.6 to deter­\nmine if the sets of vectors in Exercises 22-31 are linearly in­\ndependent. If, for any of these, the answer can be determined \nby inspection (i.e., without calculation), state why. For any \nsets that are linearly dependent, find a dependence relation­\nship among the vectors. \n23. \n[\n:\n]\n. \nm. \n[\n-\n�\n] \n11.Showthot\n�\n' \n� \n'\nP\n\"\n\"\n(\n[\nH \n[J \n[\n:\n]\n) \n12. Show thot \n�\n' \n� \n'\nP\n\"\n\"\n( \n[J \nm. \n[ \n_\n:\n]\n) \nIn Exercises 13-16, describe th e span of th e given vectors \n24\n• \nm.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":13380,"to":13508}}}}],[329,{"pageContent":"ship among the vectors. \n23. \n[\n:\n]\n. \nm. \n[\n-\n�\n] \n11.Showthot\n�\n' \n� \n'\nP\n\"\n\"\n(\n[\nH \n[J \n[\n:\n]\n) \n12. Show thot \n�\n' \n� \n'\nP\n\"\n\"\n( \n[J \nm. \n[ \n_\n:\n]\n) \nIn Exercises 13-16, describe th e span of th e given vectors \n24\n• \nm. \n[\n!J \nHl \n25 \n[\n:H�lE \n::r!fFr\nd(\nb)\nalg,bm\n::�\nfa \n[\n:\nJ \n26. \n[\n-\n;\ni\n. \n[\n-\n!\nJ \n[\n:\nJ \nm \n27. \n[\n;\n]\n. \n[\nH \n[\n�\n: \n15. \nm. \nu\ni \n16. \n[ \n_\n�\ni\nr\nn \n[\n-\n:\ni \n2\n8\nr�H�J \nu1 \n17. \n;\n�\ni\n:�;\n�\n;\n,\nr\n�\n� \n�\nrt�\n�\n�\n1\n�\nf\n�\n��\n,\n�\n1\n;��\n�:�\nr\n�\n;:\n:\n!\nn\n�\nf\n��\n: \n[ \nl \nl \n[\n-11 \n[ \nl \nl \n[ \no \nl \nform ax +  by + cz = 0. Solve for a, b, and c. \n29 \n-1 \n1 0 \n1 \n. \n1 \n, \n0 \n, \n1 \n, \n-1 \n18. Prove that u, v, and ware all in span(u, v, w). \n0 1 \n-1 \n1 \n19. Prove that u, v, and ware all in span(u, u + v, u + \n20 �a;;:�ve that ifu\n1\n, ... ,u\nm\nare vectorsin !R\nn\n,S= \n30 \n[\n�\nl \n[\n�\nl \n[\n�\nl \n[\n:\nl \n. \n{ \n}ad\nT \n{ \n·\n0\n1\n'\n2\n1\n'\n2\n1\n'\n2\n1 \nU\n1\n, \nU\n2\n, ... , \nU\nk \n,  n \n= \nU\n1\n, ... , U\nk\n, \nU\nk\n+\n!> \n... , \nu\nm\n}, then span(S) � span(T). [Hint: Rephrase this \nquestion in terms oflinear combinations.] \n(b) Deduce that if [R\nn \n= span(S), then [R\nn \n= span(T) \n[ \n3 \nl \n[\n-\nl \n1 \n[ \n1 \nl \n[\n-\nl \n1 \n21. (a) \nalso. \n31. \n-\n-\n�\nl \n, \n-\n�\nl \n, \n�\nl \n, \n-\n3\n� \nSuppose that vector w is a linear combination \nof vectors u\n1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":13508,"to":13777}}}}],[330,{"pageContent":"(b) Deduce that if [R\nn \n= span(S), then [R\nn \n= span(T) \n[ \n3 \nl \n[\n-\nl \n1 \n[ \n1 \nl \n[\n-\nl \n1 \n21. (a) \nalso. \n31. \n-\n-\n�\nl \n, \n-\n�\nl \n, \n�\nl \n, \n-\n3\n� \nSuppose that vector w is a linear combination \nof vectors u\n1\n, ... , uk   and that each u; is a linear \ncombination of vectors v\n1\n, ... , v\nm\n. Prove that w is \na linear combination ofv\n1\n, ... , v\nm \nand therefore \nspan(u\n1\n, ... , uk) � span(v\n1\n, ... , v\nm\n). \nIn Exercises 32-41, determine if th e sets of vectors in th e \ngiven exercise are linearly independent by converting th e","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":13777,"to":13836}}}}],[331,{"pageContent":"vectors to row vectors and using the method of Example 2.25 \nand Theorem 2.7. For any sets that are linearly dependent, \nfind a dependence relationship among th e vectors. \n32. Exercise 22 \n34. Exercise 24 \n36. Exercise 26 \n38. Exercise 28 \n40. Exercise 30 \n33. Exercise 23 \n35. Exercise 25 \n37. Exercise 27 \n39. Exercise 29 \n41. Exercise 31 \n42. (a) If the columns of an nX n matrix A   are linearly in­\ndependent as vectors in !R\nn\n, what is the rank of A? \nExplain. \n(b) If the rows of an n X n matrix A  are linearly inde­\npendent as vectors in !R\nn\n, what is the rank of A? \nExplain. \n43. (a) If vectors u, v, and ware linearly independent, will \nu + v, v + w, and u + w also be linearly indepen -\ndent? Justify your answer. \n11  f \nApplications \nSection 2.4 Applications \n99 \n(b) If vectors u, v, and w are linearly independent, will \nu - v, v - w, and u - w   also be linearly indepen­\ndent? Justify your answer. \n44. Prove that two vectors are linearly dependent if \nand only if one is a scalar multiple of the other.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":13838,"to":13872}}}}],[332,{"pageContent":"u - v, v - w, and u - w   also be linearly indepen­\ndent? Justify your answer. \n44. Prove that two vectors are linearly dependent if \nand only if one is a scalar multiple of the other. \n[Hint: Separately consider the case where one of the \nvectors is O.] \n45. Give a \"row vector proof\" of Theorem 2.8. \n46. Prove that every subset of a linearly independent set is \nlinearly independent. \n47. Suppose that S = {v\n1\n, .•. , vk> v} is a set of   vectors in \nsome !R\nn \nand that vis a  linear combination of v\n1\n, ... , \nvk.  If S' = {v\n1\n, ... , vd, prove that span(S) = span(S'). \n[Hint: Exercise 2l(b) is helpful here.] \n48. Let {v\n1\n, ... , vd be a linearly independent set of vec­\ntors in !R\nn\n, and let v be a vector in !R\nn\n. Suppose that \nv = c\n1\nv\n1 \n+ c\n2\nv\n2 \n+ · · · + c\nk \nvk with c\n1 \n* 0. Prove that \n{v, v\n2\n, .•. , vd is linearly independent. \nThere are too many applications of systems oflinear equations to do them justice in a \nsingle section. This section will introduce a few applications, to illustrate the diverse","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":13872,"to":13918}}}}],[333,{"pageContent":"There are too many applications of systems oflinear equations to do them justice in a \nsingle section. This section will introduce a few applications, to illustrate the diverse \nsettings in which they arise. \nExample 2.21 \nAllocation of Resources \nA great many applications of systems of linear equations involve allocating limited \nresources subject to a set of constraints. \nA biologist has placed three strains of bacteria (denoted I, II, and III) in a test tube, \nwhere they will feed on three different food sources (A, B, and C). Each day 2300 units \nof A, 800 units of B, and 1500 units of C are placed in the test tube, and each bacte­\nrium consumes a certain number of units of each food per day, as sh  own in Table 2.2. \nHow many bacteria of each strain can coexist in the test tube and consume all of the \nfood? \nTable 2.2 \nBacteria Bacteria Bacteria \nStrain I Strain II Strain III \nFood A 2 2 4 \nFood B \n1 \n2 \n0 \nFood C \n1 3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":13918,"to":13940}}}}],[334,{"pageContent":"100 \nChapter 2 \nSystems of Linear Equations \nExample 2.28 \nSolulion Let x\n1\n, x\n2\n,  and x\n3 \nbe the numbers of bacteria of strains I, II, and III, \nrespectively. Since each of the x\n1 \nbacteria of strain I consumes 2 units of A per day, \nstrain I consumes a total of 2x\n1 \nunits per day. Similarly, strains II and III consume a \ntotal of 2x\n2 \nand 4x\n3 \nunits of food A daily. Since we want to use up all of the 2300 units \nof A, we have the equation \n2x\n1 \n+ 2x\n2 \n+ 4x\n3 \n= 2300 \nLikewise, we obtain equations corresponding to the consumption of B and C: \nX\n1 \n+ 2X\n2 \n= 800 \nX\n1 \n+ 3X\n2 \n+ X\n3 \n= 1500 \nThus, we have a system of three linear equations in three variables. Row reduction of \nthe corresponding augmented matrix gives \n[\n: \n2 \n2 \n3 \n4  2300\n] \n[\nl \n0  800 \n� \n0 \n1  1500 \n0 \n0  0  100\n] \n0  350 \n1  350 0 \nTherefore, x\n1 \n= 100, x\n2 \n= 350, and x\n3 \n= 350. The biologist should place 100 bacteria \nof strain I and 350 of each of strains II and III in the test tube if she wants all the food \nto be consumed.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":13942,"to":14013}}}}],[335,{"pageContent":"Therefore, x\n1 \n= 100, x\n2 \n= 350, and x\n3 \n= 350. The biologist should place 100 bacteria \nof strain I and 350 of each of strains II and III in the test tube if she wants all the food \nto be consumed. \nRepeat Example 2.27, using the data on daily consumption of food (units per day) \nshown in Table 2.3. Assume this time that 1500 units of A, 3000 units ofB, and 4500 \nunits of C are placed in the test tube each day. \nTable 2.3 \nFood A \nFood B \nFood C \nBacteria \nStrain I \nBacteria \nStrain II \n2 \n3 \nBacteria \nStrain III \n1 \n3 \n5 \nSolulion Let x\n1\n, x\n2\n, and x\n3 \nagain be the numbers of bacteria of each type. The aug­\nmented matrix for the resulting linear system and the corresponding reduced echelon \nform are \n[\n: \n1 \n2 \n3 \n� �\n�\n��\ni \n� \n[\n� \n� \n5  4500 \n0  0 \n-1 \nO\nJ \n2  1500 \n0    0 \nWe see that in this case we have more than one solution, given by \nX\n1 \n- X\n3 \n= 0 \nX\n2 \n+ 2X\n3 \n= 1500 \nLetting x\n3 \n= t, we obtain x\n1 \n= t, x\n2 \n= 1500 - 2t, and x\n3 \n= t. In any applied problem,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":14013,"to":14088}}}}],[336,{"pageContent":"O\nJ \n2  1500 \n0    0 \nWe see that in this case we have more than one solution, given by \nX\n1 \n- X\n3 \n= 0 \nX\n2 \n+ 2X\n3 \n= 1500 \nLetting x\n3 \n= t, we obtain x\n1 \n= t, x\n2 \n= 1500 - 2t, and x\n3 \n= t. In any applied problem, \nwe must be careful to interpret solutions properly. Certainly the number of bacteria","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":14088,"to":14112}}}}],[337,{"pageContent":"Example 2.29 \nSection 2.4 Applications \n101 \ncannot be negative. Therefore, t 2: 0 and 1500 -2t 2: 0. The latter inequality implies \nthat t  :::::: 750, so we have 0\n:::::: \nt  :::::: 750. Presumably the number of bacteria must be a \nwhole number, so  there are exactly 751 values oft that satisfy the inequality. Thus, our \n751 solutions are of the form \none for each integer value of t such that 0 ::::::  t \n:::::: \n750. (So, although mathematically \nthis system has infinitely many solutions, physically there are only finitely many.) \n.+ \nBalancing Chemical Equations \nWhen a chemical reaction occurs, certain molecules (the reacta nts\n) \ncombine to form \nnew molecules (the products\n)\n. A balanced chemical equation is an algebraic equation \nthat gives the relative numbers of reactants and products in the reaction and has the \nsame number of atoms of each type on the left- and right-hand sides. The equation is \nusually written with the reactants on the left, the products on the right, and an arrow","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":14114,"to":14137}}}}],[338,{"pageContent":"same number of atoms of each type on the left- and right-hand sides. The equation is \nusually written with the reactants on the left, the products on the right, and an arrow \nin between to show the direction of the reaction. \nFor example, for the reaction in which hydrogen gas (H\n2\n) and oxygen (0\n2\n) com­\nbine to form water (H\n2\n0), a balanced chemical equation is \n2H\n2 \n+ \n0\n2 \n-----+ 2H\n2\n0 \nindicating that two molecules of hydrogen combine with one molecule of oxygen to \nform two molecules of water. Observe that the equation is balanced, since there are \nfour hydrogen atoms and two oxygen atoms on each side. Note that there will never \nbe a unique balanced equation for a reaction, since any positive integer multiple of \na balanced equation will also be balanced. For example, 6H\n2 \n+ 30\n2\n-----+ 6H\n2\n0 is also \nbalanced. Therefore, we usually look for the simplest balanced equation for a given \nreaction. \nWhile trial and error will often work in simple examples, the process of balancing","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":14137,"to":14169}}}}],[339,{"pageContent":"2\n-----+ 6H\n2\n0 is also \nbalanced. Therefore, we usually look for the simplest balanced equation for a given \nreaction. \nWhile trial and error will often work in simple examples, the process of balancing \nchemical equations really involves solving a homogeneous system oflinear equations, \nso we can use the techniques we have developed to remove the guesswork. \nThe combustion of ammonia (NH3) in oxygen produces nitrogen (N\n2\n) and water. \nFind a balanced chemical equation for this reaction. \nSolution If we denote the numbers of molecules of ammonia, oxygen, nitrogen, and \nwater by w, x, y, and z, respectively, then we are seeking an equation of the form \nwNH\n3 \n+ x0\n2 \n-----+ yN\n2 \n+ \nzH\n2\n0 \nComparing the numbers of nitrogen, hydrogen, and oxygen atoms in the reactants \nand products, we obtain three linear equations: \nNitrogen: w = 2y \nHydrogen: 3w = 2z \nOxygen: 2x = z \nRewriting these equations in standard form gives us a homogeneous system of three","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":14169,"to":14199}}}}],[340,{"pageContent":"and products, we obtain three linear equations: \nNitrogen: w = 2y \nHydrogen: 3w = 2z \nOxygen: 2x = z \nRewriting these equations in standard form gives us a homogeneous system of three \nlinear equations in four variables. [Notice that Theorem 2.3 guarantees that such a","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":14199,"to":14204}}}}],[341,{"pageContent":"102 \nChapter 2 \nSystems of Linear Equations \nj2\n0 \ni\n3\n0 \nFigure 2.10 \nf \n+2 \nFlow at a node: f1 + f\n2 \n= 50 \nExample 2.30 \nsystem will have (infinitely many) nontrivial solutions.] We reduce the corresponding \naugmented matrix by Gauss-Jordan elimination. \nw \n3w \n- 2y = 0 \n[ \n1  0 \n-2z = \n0\n-----+ \n3  0 \n2x \n-z=\nO \n02 \n-2 \n0 \n0 \n-\n� \n�\n]\n-----+\n[\n� \n� \n-1 0 \n0  0 \n0 \n0 \n1 \n-\nt 0\n] \n-\n! \n0 \n-\nt \n0 \nThus, \nw = \nt\nz\n, \nx = \n!\nz, and y = \nt\nz. The  smallest positive value of z that will produce \ninteger values for all four variables is the least common denominator of the fractions \nt\n, \n!\n, and \nt\n-namely, 6-which gives w = 4, x = 3, y = 2, and z = 6. Therefore, the \nbalanced chemical equation is \nNetwork Analysis \nMany practical situations give rise to networks: transportation networks, communi­\ncations networks, and economic networks, to name a few. Of particular interest are \nthe possible flows through networks. For example, vehicles flow through a network \nof roads, information flows through a data network, and goods and services flow","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":14206,"to":14283}}}}],[342,{"pageContent":"the possible flows through networks. For example, vehicles flow through a network \nof roads, information flows through a data network, and goods and services flow \nthrough an economic network. \nFor us, a network will consist of a finite number of nodes (also called junctions \nor vertices) connected by a   series of directed edges known as branches or arcs. Each \nbranch will be labeled with a flow that represents the amount of some commodity \nthat can flow along or through that branch in the indicated direction. (Think of cars \ntraveling along a network of one-way streets.) The fundamental rule governing flow \nthrough a network is conservation of flow: \nAt each node, the flow in equals the flow out. \nFigure 2.10 shows a portion of a network, with two branches entering a node and two \nleaving. The conservation of flow rule implies that the total incoming flow, \nf\n1 + \nf\n2 \nunits, must match the total outgoing flow, 20 + 30 units. Thus, we have the linear \nequation \nf\n1 \n+ \nf\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":14283,"to":14305}}}}],[343,{"pageContent":"leaving. The conservation of flow rule implies that the total incoming flow, \nf\n1 + \nf\n2 \nunits, must match the total outgoing flow, 20 + 30 units. Thus, we have the linear \nequation \nf\n1 \n+ \nf\n2 \n= 50 corresponding to this node. \nWe can analyze the flow through an entire network by constructing such equa­\ntions and solving the resulting system of linear equations. \nDescribe the possible flows through the network of water pipes shown in Figure 2.11, \nwhere flow is measured in liters per minute. \nSolution At each node, we write out the equation that represents the conservation \nof flow there. We then rewrite each equation with the variables on the left and the \nconstant on the right, to get a linear system in standard form. \nNode A: \nNodeB: \nNode C: \nNode D: \n15 = \n!1 \n+ \n14 \n!1 \n= \n!2 \n+ 10 \n!2 \n+ \nf3 \n+ 5 = 30 \nf4 \n+ 20 = \nf3 \n-----+ \n!1 \n+ \nf4 \n= 15 \n!1 \n-\n!2 \n= 10 \n!2 \n+ \nf3 \n= 25 \nf, \n-\nf4 \n= 20","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":14305,"to":14360}}}}],[344,{"pageContent":"5\n! \njQ. A \n!1 \n-\nJ\n4\n! \n2\n0 \n-\nh \nD \nFigure 2.11 \n1d \n3\n0\n! \nSection 2.4 Applications \n103 \nB \n1\n0 \n-\n5 \n+--\nc \nUsing Gauss-Jordan elimination, we reduce the augmented matrix: \n[\ni \n0  0 \n-1  0 \n1 \n0  1 \n1  15\n] \n[\n1 \n0  10 \n0 \n� \n0  25 \n0 \n-1 20 \n0 \n0 \n0 \n0 \n0 \n0 \n1 \n0 \n1  15\n] \n1 \n5 \n-1 20 \n0   0 \n.......,.. \n(Check this.) We see that there is one free variable, f\n4\n, so  we have infinitely many \nsolutions. Settingf\n4 \n=  t and expressing the leading variables in terms of f\n4\n, we  obtain \nf\n, \n= \n15 -t \n!\n2 \n= \n5 -t \nf\n3 \n= \n20 +  t \n!\n4 \n= \nThese equations describe all possible flows and allow us to analyze the network. For \nexample, we see that if we control the flow on branch AD so that t = 5 L/min, then \nthe other flows are f\n1 \n= 1 O,f\n2 \n= 0, and f\n3 \n= 25. \nWe can do even better: We can find the minimum and maximum possible flows \non each branch. Each of the flows must be nonnegative. Examining the first and sec­\nond equations in turn, we see that t :s 15 (otherwise f\n1 \nwould be negative) and t :s 5 \n(otherwisef\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":14362,"to":14459}}}}],[345,{"pageContent":"on each branch. Each of the flows must be nonnegative. Examining the first and sec­\nond equations in turn, we see that t :s 15 (otherwise f\n1 \nwould be negative) and t :s 5 \n(otherwisef\n2 \nwould be negative). The second of these inequalities is more restrictive \nthan the first, so we must use it. The third equation contributes no further restrictions \non our parameter t, so we have deduced that 0 :s t :s 5. Combining this result with \nthe four equations, we see that \n10 :sf\n, \n:s  15 \n0\n:S\nf\n2\n:S\n5 \n20 :s f\n3 \n:s 25 \n0\n:S\n.fi\n:S\n5 \nWe now have a complete description of the possible flows through this network. 4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":14459,"to":14486}}}}],[346,{"pageContent":"104 \nChapter 2 \nSystems of Linear Equations \nOhm's  Law \nKirchhoff's Laws \nExample 2.31 \nElectrical Networks \nElectrical networks are a specialized type of network providing information about \npower sources, such as batteries, and devices powered by these sources, such as light \nbulbs or motors. A  power source \"forces\" a current of electrons to flow through the \nnetwork, where it encounters various resistors, each of which requires that a  certain \namount of force be applied in order for the current to flow through it. \nThe fundamental law of electricity is Ohm's law, which states exactly how much \nforce E is needed to drive a current I through a resistor with resistance R. \nforce = resistance X current \nor \nE = RI \nForce is measured in volts, resistance in ohms, and current in amperes (or amps, for \nshort). Thus, in terms of these units, Ohm's law becomes \"volts = ohms X amps;' and \nit tells us what the \"voltage drop\" is when a current passes through a resistor-that is, \nhow much voltage is used up.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":14488,"to":14508}}}}],[347,{"pageContent":"it tells us what the \"voltage drop\" is when a current passes through a resistor-that is, \nhow much voltage is used up. \nCurrent flows out of the positive terminal of a battery and flows back into the \nnegative terminal, traveling around one or more closed circuits in the process. In \na diagram of an  electrical network, batteries are represented by �I-(where the \npositive terminal is the longer vertical bar) and resistors are represented by -'VV'v- . \nThe following two laws, whose discovery we owe to Kirchhoff, govern electrical net­\nworks. The first is a \"conservation of flow\" law at each node; the second is a \"balancing \nof voltage\" law around each circuit. \nCurrent Law (nodes) \nThe sum of the currents flowing into any node is equal to the sum of the currents \nflowing out of that node. \nVoltage Law (circuits) \nThe sum of the voltage drops around any circuit is equal to the total voltage around \nthe circuit (provided by the batteries).","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":14508,"to":14522}}}}],[348,{"pageContent":"flowing out of that node. \nVoltage Law (circuits) \nThe sum of the voltage drops around any circuit is equal to the total voltage around \nthe circuit (provided by the batteries). \nFigure 2.12 illustrates Kirchhoff's laws. In part (a), the current law gives 1\n1 \n= 1\n2 \n+ 1\n3 \n(or 1\n1 \n- 1\n2 \n-\n1\n3 \n= 0, as we will write it); part (b) gives 41 = 10, where we have used \nOhm's law to compute the voltage drop 41 at the resistor. Using Kirchhoff's laws, we \ncan set up a system of linear equations that will allow us to   determine the currents in \nan electrical network. \nDetermine the currents 1\n1\n, 1\n2\n, and 1\n3 \nin the electrical network shown in Figure 2.13. \nSolution This network has two batteries and four resistors. Current 1\n1 \nflows through \nthe top branch BCA, current 1\n2 \nflows across the middle branch AB, and current 1\n3 \nflows through the bottom branch BDA. \nAt node A, the current law gives 1\n1 \n+ 1\n3 \n= 1\n2\n, or \n1\n1 \n- 1\n2 \n+ 1\n3 \n= 0 \n(Observe that we get the same equation at node B.\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":14522,"to":14573}}}}],[349,{"pageContent":"lY',, \n1\n� \nFigure 2.12 \nCAS \nExample 2.32 \n1 \n+---\n10  volts \n4ohms \n1 \n+---\n(b) 41 = 10 \nSection 2.4 Applications \n105 \n1\n1 \nc \n1\n1 \n+---+---\n8 volts \n2ohms \n2ohms \n1\n2 \nlz \n--+ \n--+ \nA B \nI ohm \n4ohms \nh \nD \nh \n+---+---\n16 volts \nFigure 2.13 \nNext we apply the voltage law for each circuit. For the circuit CABC, the voltage \ndrops at the resistors are 2I\ni\n, I\n2\n, and 2I\ni\n. Thus, we have the equation \n4I\ni \n+ \nI\n2 \n= 8 \nSimilarly, for the circuit DABD, we obtain \nI\n2 \n+ \n4I\n3 \n= 16 \n(Notice that there is actually a third circuit, CADBC, if we \"go against the flow:' In this \ncase, we must treat the voltages and resistances on the \"reversed\" paths as negative. \nDoing so gives 2I\ni \n+ \n2I\ni \n- 4I\n3 \n= 8 \n-\n16 = -8 or 4I\ni \n- 4I\n3 \n= -8, which we observe \nis just the difference of the voltage equations for the other two circuits. Thus, we can \nomit this equation, as it contributes no new information. On the other hand, includ­\ning it does no harm.) \nWe now have a system of three linear equations in three variables: \nI\n1 \n-\nI\n2 \n+ \nI\n3 \n= 0 \n4I\n1 \n+ \nI\n2 \n8 \nI\n2 \n+ 4\nI\n3 \n= 16","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":14575,"to":14674}}}}],[350,{"pageContent":"ing it does no harm.) \nWe now have a system of three linear equations in three variables: \nI\n1 \n-\nI\n2 \n+ \nI\n3 \n= 0 \n4I\n1 \n+ \nI\n2 \n8 \nI\n2 \n+ 4\nI\n3 \n= 16 \nGauss-Jordan elimination produces \n[\n� \n-\n1 \n1 \no\nl [\n0\n0\n1 \no\no\n1 \n�\n1 \n4\n3\n1 \nl \n� 1! \n� \nHence, the currents are I\ni \n= \n1 amp, I\n2 \n= 4 amps, and I\n3 \n= 3 amps. \nRemark In some electrical networks, the currents may have fractional values or may \neven be negative. A  negative value simply means that the current in the correspond­\ning branch flows in the direction opposite that shown on the network diagram. \nThe network shown in Figure 2.14 has a   single power source A and five resistors. Find \nthe currents I, I\ni\n, ... , I\n5\n• This is an example of what is known in electrical engineering \nas a Wheatsto ne bridge circuit.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":14674,"to":14736}}}}],[351,{"pageContent":"106 \nChapter 2 \nSystems of Linear Equations \n� \n1\nii \n1\n4 \n--\nB \n2ohms \nI ohm \n--\n1 \nFigure 2.14 \nA bridge circuit \nc \nl ohm \n2ohms \n1\n3 \n! \n15 \n--\nE \n2ohms \nA \n10 volts \n--\n1 \n!\nIi \nD \nSolulion Kirchhoff's  current  law  gives  the following equations at  the  four \nnodes: \nNode B: \nI - I\n1 \n- I\n4 \n= 0 \nNode C: \nI\n1 \n- I\n2 \n- I\n3 \n= 0 \nNode D: \nI - I\n2 \n- Is  = 0 \nNode E: \nI\n3 \n+ I\n4 \n- Is = 0 \nFor the three basic circuits, the voltage law gives \nCircuit ABEDA: \nI\n4 \n+ 2Is =  10 \nCircuit BCEB : \n2I\n1 \n+ 2I\n3 \n- I\n4 \n= 0 \nCircuit CDEC: \nI\n2 \n- 2Is - 2I\n3 \n= 0 \n(Observe that branch DAB has no resistor and therefore no voltage drop; thus, there \nis no I term in the equation for circuit ABEDA. Note also that we had to change signs \nthree times because we  went \"against the current:' This poses no problem, since we \nwill let the sign of the answer determine the direction of current flow.) \nWe now have a system of seven equations in six variables. Row reduction gives \n1 \n-\n1 \n0 \n0 \n-\n1 \n0 \n0 \n1  0  0 \n0  0 \n0 \n7 \n0    1 \n-1 \n-1 \n0 \n0   0 0 \n1 \n0 \n0  0 0 \n3 \n1    0 \n-1 \n0    0 \n-1 \n0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":14738,"to":14847}}}}],[352,{"pageContent":"We now have a system of seven equations in six variables. Row reduction gives \n1 \n-\n1 \n0 \n0 \n-\n1 \n0 \n0 \n1  0  0 \n0  0 \n0 \n7 \n0    1 \n-1 \n-1 \n0 \n0   0 0 \n1 \n0 \n0  0 0 \n3 \n1    0 \n-1 \n0    0 \n-1 \n0 \n0  0 1  0  0 0 \n4 \n0 \n0 \n0 \n-1 \n0 \n� \n0  0  0 \n1  0 \n0 \n-1 \n0 \n0 \n0    0 \n2 \n10 \n0  0  0 \n0  1 \n0 \n4 \n0 \n2 \n0 \n2 \n-1 \n0 \n0 \n0  0  0 0  0 1 \n3 \n0    0 \n-2 \n0 \n-2 \n0 \n0  0  0  0  0  0 \n0 \n(Use your calculator or CAS to check this.) Thus, the solution (in amps) is I= 7, I\n1 \n= \nIs =  3, I\n2 \n= I\n4 \n= 4, and I\n3 \n=  -1. The  significance of the negative value here is that \nthe current through branch CE is flowing in the direction opposite that marked on \nthe diagram. \n-+ \nRemark \nThere is only one power source in this example, so the single 10-volt bat-\ntery sends a current of 7 amps through the network. If we substitute these values into","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":14847,"to":14927}}}}],[353,{"pageContent":"Example 2.33 \nWassily Leontief (1906-1999) was \nborn in St. Petersburg, Russia. He \nstudied at the University of Lenin­\ngrad and received his Ph.D. from the \nUniversity of Berlin. He emigrated \nto the United States in 1931, teach­\ning at Harvard University and later \nat New Yo rk University. In 1932, \nLeontiefbegan compiling data for the \nmonumental task of conducting an \ninput-output analysis of the United \nStates economy, the results of which \nwere published in 1941. He was also \nan early user of computers, which he \nneeded to solve the large-scale linear \nsystems in his models. For his pio­\nneering work, Leontief was awarded \nthe Nobel Prize in Economics in 1973. \nSection 2.4 Applications \n101 \nOhm's law, E = RI, we get 10 = 7R or R =   1?-. Thus, the entire network behaves as if \nthere were a single 1?--ohm resistor. This value is called the effective resistance (Reff) of \nthe network. \nlinear Economic Models \nAn economy is a very complex system with many interrelationships among the vari­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":14929,"to":14954}}}}],[354,{"pageContent":"the network. \nlinear Economic Models \nAn economy is a very complex system with many interrelationships among the vari­\nous sectors of the economy and the goods and services they produce and consume. \nDetermining optimal prices and levels of production subject to desired economic \ngoals requires sophisticated mathematical models. Linear algebra has proven to be a \npowerful tool in developing and analyzing such economic models. \nIn this section, we introduce two models based on the work of Harvard econo­\nmist Wassily Leontief in the 1930s. His methods, often referred to as input-output \nanalysis, are now standard tools in mathematical economics and are used by cities, \ncorporations, and entire countries for economic planning and forecasting. \nWe begin with a simple example. \nThe economy of a region consists of three industries, or sectors: service, electricity, \nand oil production. For simplicity, we assume that each industry produces a single","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":14954,"to":14967}}}}],[355,{"pageContent":"The economy of a region consists of three industries, or sectors: service, electricity, \nand oil production. For simplicity, we assume that each industry produces a single \ncommodity (goods or services) in a given year and that income (o utput) is gener­\nated from the sale of this commodity. Each industry purchases commodities from the \nother industries, including itself, in order to generate its output. No commodities are \npurchased from outside the region and no output is sold outside the region. Further­\nmore, for each industry, we assume that production exactly equals consumption (out­\nput equals input, income equals expenditure). In this sense, this is a closed economy \nthat is in equilibrium. Table 2.4 summarizes how much of each industry's output is \nconsumed by each industry. \nTable 2.4 \nProduced by (output) \nService \nElectricity \nOil \nConsumed by \nService \n1/4 1/3 1/2 \nElectricity \n1/4 \n1/3 \n1/4 \n(input) \nOil \n1/2 \n1/3 1/4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":14967,"to":14992}}}}],[356,{"pageContent":"consumed by each industry. \nTable 2.4 \nProduced by (output) \nService \nElectricity \nOil \nConsumed by \nService \n1/4 1/3 1/2 \nElectricity \n1/4 \n1/3 \n1/4 \n(input) \nOil \n1/2 \n1/3 1/4 \nFrom the first column of the table, we see that the  service industry consumes 1I4 \nof its own output, electricity consumes another 1/4, and the oil industry uses 1/2 of \nthe service industry's output. The other two columns have similar interpretations. \nNotice that the sum of each column is 1, indicating that all of the output of each \nindustry is consumed. \nLet \nX\ni\n, x\n2\n, and x\n3 \ndenote the annual output (income) of the service, electricity, \nand oil industries, respectively, in millions of dollars. Since consumption corresponds \nto expenditure, the service industry spends� \nX\ni \non its own commodity, .!. x\n2 \non elec-\n3 \ntricity, and \nt \nx\n3 \non oil. This means that the  service industry's total annual expendi-\nture is �\nX\ni \n+ .!. x\n2 \n+ \nt \nx\n3\n. Since the economy is in equilibrium, the service industry's \n3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":14992,"to":15045}}}}],[357,{"pageContent":"108 \nChapter 2 \nSystems of Linear Equations \nexpenditure must equal its annual income x\n1\n. This gives the first of the following \nequations; the other two equations are obtained by analyzing the expenditures of the \nelectricity and oil industries. \nService: \nElectricity: \nOil: \nRearranging each equation, we obtain a homogeneous system of linear equations, \n.......... \nwhich we then solve. (Check this!) \nExample 2.34 \n-�x\n1 \n+ lx\n2\n+ \nt\nx\n3 \n= 0 \n---7 n \nI \nl \n\"\n] \n[\n� \n0 \n-\n: \n\"\n] \n3 \n3 \ni\nx\n1 \n-\n�\nx\n2\n-\ni\nx\n3 \n= 0 \n2 \nl \n0 \n� \n1 \n3 \n-\n3 \n-i \n0 \n-4 0 \nt\nx\n1 \n+ lx\n2\n-�x\n3 \n= 0 \nl \n0    0  0 \n3 \n3 \nSetting x\n3 \n= t, we find that x\n1 \n= t and x\n2 \n= � t. Thus, we see that the relative outputs of \nthe service, electricity, and oil industries need to be in the ratios x\n1 \n: x\n2 \n: x\n3 \n= 4 : 3 : 4 \nfor the economy to be in equilibrium. \n4 \nRemarks \n• \nThe last example illustrates what is commonly called the Leontief closed model. \n• \nSince output corresponds to income, we can also think of x\n1\n, x\n2\n, and x\n3 \nas the \nprices of the three commodities.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":15047,"to":15147}}}}],[358,{"pageContent":"• \nThe last example illustrates what is commonly called the Leontief closed model. \n• \nSince output corresponds to income, we can also think of x\n1\n, x\n2\n, and x\n3 \nas the \nprices of the three commodities. \nWe now modify the model in Example 2.33 to accommodate an open economy, one \nin which there is an external as well as an internal demand for the commodities that \nare produced. Not surprisingly, this version is called the Leontief open model. \nConsider the three  industries of Example 2.33 but  with  consumption given by \nTable 2.5. We see that, of the commodities produced by the service industry, 20% are \nconsumed by the service industry, 40% by the electricity industry, and  10% by the oil \nindustry. Thus, only 70% of the service industry's output is consumed by this econ­\nomy. The implication of this calculation is that there is an excess of output (income) \nover input (expenditure) for the service industry. We say that the service industry is","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":15147,"to":15166}}}}],[359,{"pageContent":"omy. The implication of this calculation is that there is an excess of output (income) \nover input (expenditure) for the service industry. We say that the service industry is \nproductive. Likewise, the oil industry is pro ductive but the electricity industry is non­\nproductive. (This is reflected in the fact that the sums of the first and third columns \nare less than 1 but the sum of the second column is equal to 1). The excess output may \nbe applied to satisfy an ex  ternal demand. \nTable 2.5 \nProduced by (output) \nService \nElectricity \nOil \nConsumed by \nService \n0.20 0.50 0.10 \nElectricity \n0.40 0.20 0.20 \n(input) \nOil \n0.10 0.30 0.30","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":15166,"to":15184}}}}],[360,{"pageContent":"CAS \nExample 2.35 \nSection 2.4 Applications \n109 \nFor example, suppose there is an annual external demand (in millions of dollars) \nfor 10, 10, and 30 from the service, electricity, and oil industries, respectively. Then, \nequating expenditures (internal demand and external demand) with income (out­\nput), we obtain the following equations: \noutput \ninternal demand \nexternal demand \nService \nXi \n= 0.2X\n1 \n+ 0.5X\n2 \n+ O.lX\n3 \n+ 10 \nElectricity \nX\n2 \n= 0.4X\n1 \n+ 0.2X\n2 \n+ 0.2X\n3 \n+ 10 \nOil \nX\n3 \n= O.lx\n1 \n+ 0.3x\n2 \n+ 0.3x\n3 \n+ 30 \nRearranging, we obtain the following linear system and augmented matrix: \n0.8X\n1 \n- O.SX\n2 \n- O.lX\n3 \n= 10 \n[ \n08 \n-0.5 \n-0.1 \n10\n] \n-0.4X\n1 \n+ 0.8X\n2 \n- 0.2X\n3 \n= 10 \n---+ \n-0.4 \n0.8 -0.2   10 \n-0.l  x\n1 \n- 0.3X\n2 \n+ 0.7X\n3 \n= 30 \n-0.l \n-0.3 \n0.7 \n30 \nRow reduction yields \n[\n� \n0   0 \n61.74 \nl \n1 \n0 \n63.04 \n0 \n78.70 \nfrom which we see that the service, electricity, and oil industries must have an an­\nnual production of $61. 7 4, $63.04, and $78. 70 (million), respectively, in order to meet","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":15186,"to":15273}}}}],[361,{"pageContent":"l \n1 \n0 \n63.04 \n0 \n78.70 \nfrom which we see that the service, electricity, and oil industries must have an an­\nnual production of $61. 7 4, $63.04, and $78. 70 (million), respectively, in order to meet \nboth the internal and external demand for their commodities. \nWe will revisit these models in Section 3.7. \nFinile Linear Games \nThere are many situations in which we must consider a physical system that has only a \nfinite number of states. Sometimes these states can be altered by applying certain pro­\ncesses, each of which produces finitely many outcomes. For example, a light bulb can be \non or off and a switch can change the state of the light bulb from on to   off and vice versa. \nDigital systems that arise in computer science are often of this type. More frivolously, many \ncomputer games feature puzzles in which a certain device must be manipulated by various \nswitches to produce a desired outcome. The finiteness of such situations is perfectly suited","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":15273,"to":15290}}}}],[362,{"pageContent":"computer games feature puzzles in which a certain device must be manipulated by various \nswitches to produce a desired outcome. The finiteness of such situations is perfectly suited \nto analysis using modular arithmetic, and often linear systems over some Z\nP \nplay a role. \nProblems involving this type of situation are often called finite linear games. \nA row of five lights is controlled by five switches. Each switch changes the state (on or \noff) of the light directly above it and the states of the lights immediately adjacent to \nthe left and right. For example, if the first and third lights are on, as in Figure 2.lS(a), \nthen pushing switch A changes the state of the system to that shown in Figure 2.lS(b). \nIfwe next push switch C, then the result is the state shown in Figure 2.lS(c).","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":15290,"to":15300}}}}],[363,{"pageContent":"110 \nChapter 2 \nSystems of Linear Equations \nB \nc \nD E \n(\na\n) \nFigure 2.15 \nA B \nc \nD E \n(b) \n'-\" \nA B \nc \n(c) \nD \nIH \nE \nSuppose that initially all the lights are off. Can we push the switches in some order \nso that only the first, third, and fifth lights will be on? Can we  push the switches in \nsome order so that only the first light will be on? \nSolution The on/off nature of this problem suggests that binary notation will be helpful \nand that we should work with 2\n2\n• Accordingly, we represent the states of the five lights by \na vector in Z�, where 0 represents off and 1 represents on. Thus, for example, the vector \ncorresponds to Figure 2.IS(b). \n0 \n1 \n0 \n0 \nWe may also use vectors in Z� to represent the action of each switch. If a switch \nchanges the state of a light, the corresponding component is a 1; otherwise, it is 0. \nWith this convention, the actions of the five switches are given by \n0 0 0 \n1 1 \n0 0 \na= \n0 \n,  b = \n1 \n,  c = \n1 \n,  d = ,   e   = \n0 \n0 0 \n1 \n0 0 0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":15302,"to":15352}}}}],[364,{"pageContent":"With this convention, the actions of the five switches are given by \n0 0 0 \n1 1 \n0 0 \na= \n0 \n,  b = \n1 \n,  c = \n1 \n,  d = ,   e   = \n0 \n0 0 \n1 \n0 0 0 \nThe situation depicted in Figure 2.IS(a) corresponds to the initial state \n1 \n0 \ns   = \n1 \n0 \n0 \nfollowed by \n1 \na= \n0 \n0 \n0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":15352,"to":15379}}}}],[365,{"pageContent":"It is the vector sum (in Z�\n) \n0 \n1 \ns+a= \n1 \n0 \n0 \nObserve that this result agrees with Figure 2.15(b). \nSection 2.4 Applications \n111 \nStarting with any initial configuration s, suppose we push the switches in the order \nA, C, D, A,   C, B.   This corresponds to the vector sum s + a + c + d + a + c + b. But \nin Z�, addition is commutative, so we have \ns + a + c +  d + a + c + b = s + 2a + b + 2c +  d \n=s+b+d \nwhere we have used the fact that 2 = 0 in z:'.\n2\n. Thus, we would achieve the same result \n.-\nby pushing only B and D-and the order does not matter. (Check that this is correct.) \nHence, in this example, we do not need to push any switch more than once. \nSo, to  see if we  can achieve a  target configuration t starting from an  initial \nconfigurations, we need to determine whether there are scalars x\n1\n, .•• , x\n5 \nin z:'.\n2 \nsuch that \ns + x\n1\na + x\n2\nb + · · · + x\n5\ne = t \nIn other words, we need to solve (if possible) the linear system over z:'.\n2 \nthat corre­\nsponds to the vector equation \nx\n1\na + x\n2\nb + · · · + x\n5","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":15381,"to":15427}}}}],[366,{"pageContent":"5 \nin z:'.\n2 \nsuch that \ns + x\n1\na + x\n2\nb + · · · + x\n5\ne = t \nIn other words, we need to solve (if possible) the linear system over z:'.\n2 \nthat corre­\nsponds to the vector equation \nx\n1\na + x\n2\nb + · · · + x\n5\ne = t -s = t  + s \nIn this case, s = 0 and our first target configuration is \n1 \n0 \nt = 1 \n0 \nThe augmented matr  ix of this system has the given vectors as columns: \n1 \n0  0  0 \n1 \n1 \n0  0 \n0 \n0 1 \n0 \n1 \n0  0 \n1  1   1 \n0 \n0  0  0 \nWe reduce it over z:'.\n2 \nto obtain \n1 0  0 0 1 \n0 \n0 0  0 1 \n0  0 1 0  0 \n0  0  0 1   1 \n1 \n0  0  0 0  0 \n0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":15427,"to":15478}}}}],[367,{"pageContent":"112 \nChapter 2 \nSystems of Linear Equations \n_... \nExample 2.36 \nThus, x\n5 \nis a free variable. Hence, there are exactly two solutions (corresponding to \nx\n5 \n= 0   and x\n5 \n= 1). Solving for the other variables in terms of x\n5\n, we obtain \nX\n1 \n= \nX\n5 \nX\n2 \n= \n+  X\n5 \nX\n3 \n= \nX\n4 \n= \n+  X\n5 \nSo, when x\n5 \n= 0 and x\n5 \n= 1, we have the solutions \nX\n1 \n0 \nX\n1 \n1 \nX\n2 \nX\n2 \n0 \nX\n3 \nand \nX\n3 \n1 \nX\n4 \n1 \nX\n4 \n0 \nX\n5 \n0 \nX\n5 \nrespectively. (Check that these both work.) \nSimilarly, in the second case, we have \n1 \n0 \nt= \n0 \n0 \n0 \nThe augmented matrix reduces as follows: \n0 \n0  0 \n1 \n1 \n0  0 0 \n0 \n1  1   1 \n0  0 \n0 \n0     0 \n0 \n0 \n1   1 0 \n0 \n-----+ \n0  0 1 \n0  0 \n0  0 \n1 \n0 \n0  0  0 \n1   1 \n0 \n0  0 \n0 \n0 \n0  0 0  0 \nshowing that there is no solution in this case; that is, it is im possible to start with all \nof the lights off and turn only the first light on. \nExample 2.35 shows the power of linear algebra. Even though we might have \nfound out by trial and error that there was no solution, checking all possible ways to","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":15480,"to":15585}}}}],[368,{"pageContent":"Example 2.35 shows the power of linear algebra. Even though we might have \nfound out by trial and error that there was no solution, checking all possible ways to \npush the switches would have been extremely tedious. We might also have missed the \nfact that no switch need ever be pushed more than once. \nConsider a row with only three lights, each of which can be off, light blue, or dark blue. \nBelow the lights are three switches, A, B, and C, each of which changes the states of \nparticular lights to the next state, in the order shown in Figure 2.16. Switch A changes \nthe states of the first two lights, switch B all three lights, and switch C the last two","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":15585,"to":15592}}}}],[369,{"pageContent":".. \nDark blue \nLight blue \n� \nFigure 2.16 \nI \nExercises 2.4 \nAllocation of Resources \nFigure 2.11 \nSection 2.4 Applications \n113 \nj \nB \nc \nlights. If all three lights are initially off, is it possible to push the switches in some order \nso that the lights are off, light blue, and dark blue, in that order (as in Figure 2.17)? \nSolution Whereas Example 2.35 involved \"1!..\n2\n, this one clearly (is it clear?) involves \n\"1!..\n3\n• Accordingly, the switches correspond to the vectors \nin \nZl. and th, final wnfigmation w' \"' <riming fod, t � \n[ \nn ( Offo 0, light bl\"' i' 1, \nand dark blue is 2.)  We wish to find scalars \nX\ni\n, x\n2\n, x\n3 \nin \"1!..\n3 \nsuch that \nX\n1\na  + \nX\n2\nb \n+ \nX\n3\nC = t \n(where \nX\n; represents the number of times the ith switch is pushed). This equation \ngives rise to the augmented matr  ix [ab c I   t], which reduces over \"1!..\n3 \nas follows: \nHence, there is a unique solution: \nX\ni \n= 2, x\n2 \n= 1, x\n3 \n= 1. In other words, we must push \nswitch A twice and the other two switches once each. (Check this.) \nTable 2.6","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":15594,"to":15655}}}}],[370,{"pageContent":"3 \nas follows: \nHence, there is a unique solution: \nX\ni \n= 2, x\n2 \n= 1, x\n3 \n= 1. In other words, we must push \nswitch A twice and the other two switches once each. (Check this.) \nTable 2.6 \n1. Suppose that, in Example 2.27, 400 units of food A, \n600 units of B, and 600 units of C are placed in the test \ntube each day and the data on daily food consump­\ntion by the bacteria (in units per day) are as sh  own \nFood A \nFood B \nBacteria \nStrain I \n1 \n2 \nBacteria Bacteria \nStrain II \nStrain III \n2 \n0 \n1 1 \nin Table 2.6. How many bacteria of each strain can \ncoexist in the test tube and consume all of the food? \n2. Suppose that in Example 2.27, 400 units of food A, \n500 units of B, and 600 units of C are placed in \nthe test tube each day and the data on daily food \nFood C \n1 \n2 \nconsumption by the bacteria (in units per day) are \nas shown in Table 2.7. How many bacteria of each","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":15655,"to":15692}}}}],[371,{"pageContent":"114 \nChapter 2 \nSystems of Linear Equations \nTable 2.1 \nBacteria Bacteria Bacteria \nStrain I \nStrain II Strain III \nFood A \n2 \n0 \nFood B \n2 \n3 \nFood C \n1 1 \nstrain can coexist in the test tube and consume all \nof the food? \n3. A florist offers three sizes of flower arrangements \ncontaining roses, daisies, and chrysanthemums. Each \nsmall arrangement contains one rose, three daisies, \nand three chrysanthemums. Each medium arrange­\nment contains two roses, four daisies, and six chry­\nsanthemums. Ea ch large arrangement contains four \nroses, eight daisies, and six chrysanthemums. One \nday, the florist noted that she used a total of 24 roses, \n50 daisies, and 48 chrysanthemums in filling orders \nfor these three types of arrangements. How many \narrangements of each type did she make? \n4. (a) In your pocket you have some nickels, dimes, and \nquarters. There are 20 coins altogether and exactly \ntwice as many dimes as nickels. The total value of the \ncoins is $3.00. Find the number of coins of each type.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":15694,"to":15725}}}}],[372,{"pageContent":"quarters. There are 20 coins altogether and exactly \ntwice as many dimes as nickels. The total value of the \ncoins is $3.00. Find the number of coins of each type. \n(b) Find all possible combinations of 20 coins (nickels, \ndimes, and quarters) that will make exactly $3.00. \n5. A coffee merchant sells three blends of coffee. A bag \nof the  house blend contains 300 grams of Colombian \nbeans and 200 grams of French roast beans. A bag of the \nspecial blend contains 200 grams of Colombian beans, \n200 grams of Kenyan beans, and 100 grams of French \nroast beans. A bag of the gourmet blend contains \n100 grams of Colombian beans, 200 grams of Kenyan \nbeans, and 200 grams of French roast beans. The mer­\nchant has on hand 30 kilograms of Colombian beans, \n15 kilograms of Kenyan beans, and 25 kilograms of French \nroast beans. Ifhe wishes to use up all of the beans, how \nmany bags of each type of blend can be made? \n6. Redo Exercise 5, assuming that the house blend contains","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":15725,"to":15742}}}}],[373,{"pageContent":"roast beans. Ifhe wishes to use up all of the beans, how \nmany bags of each type of blend can be made? \n6. Redo Exercise 5, assuming that the house blend contains \n300 grams of Colombian beans, 50 grams of Kenyan \nbeans, and 150 grams of French roast beans and the \ngourmet blend contains 100 grams of Colombian beans, \n350 grams of Kenyan beans, and 50 grams of French roast \nbeans. This time the merchant has on hand 30 kilograms of \nColombian beans, 15 kilograms of Kenyan beans, and \n15 kilograms of French roast beans. Suppose one bag of \nthe house blend produces a profit of $0.50, one bag of \nthe special blend produces a profit of$1.50, and one bag \nof the gourmet blend produces a profit of $2.00. How \nmany bags of each type should the merchant prepare \nifhe wants to use up all of the beans and maximize his \nprofit? What is the maximum profit? \nBalancing Chemical Equations \nIn Exercises 7-14, balance th e chemical equation for each \nreaction. \n7. FeS\n2 \n+ \n0\n2\n-----+ Fe\n2\n0\n3 \n+ \nS0\n2 \n8. C0\n2 \n+ H\n2\n0 -----+ C6H\n1\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":15742,"to":15779}}}}],[374,{"pageContent":"profit? What is the maximum profit? \nBalancing Chemical Equations \nIn Exercises 7-14, balance th e chemical equation for each \nreaction. \n7. FeS\n2 \n+ \n0\n2\n-----+ Fe\n2\n0\n3 \n+ \nS0\n2 \n8. C0\n2 \n+ H\n2\n0 -----+ C6H\n1\n2\n06 + 0\n2 \n(This reaction takes \nplace when a green plant converts carbon dioxide and \nwater to glucose and oxygen during photosynthesis.) \n9. C\n4\nH\n1\n0 \n+ 0\n2\n-----+ C0\n2 \n+ H\n2\n0 (This reaction occurs \nwhen butane, C\n4\nH\n1\n0\n, burns in the presence of oxygen \nto form carbon dioxide and water.) \n10. C\n7\nH60\n2 \n+ \n0\n2\n-----+ H\n2\n0 + \nC0\n2 \n11. C\n5\nH\n11\n0H + 0\n2\n-----+ H\n2\n0 + C0\n2 \n(This equation rep­\nresents the combustion of amyl alcohol.) \n12. HC10\n4 \n+ P\n4\n0\n1\n0\n----+ H\n3\nP0\n4 \n+ Cl\n2\n0\n7 \n13. Na\n2\nC0\n3 \n+ C + N\n2\n-----+ NaCN + CO \ncA\n5\n14. C\n2\nH\n2\nCl\n4 \n+ Ca(OH)\nz\n-----+ C\n2\nHC1\n3 \n+ CaC1\n2 \n+ H\n2\n0 \nNetwork Analvsis \n15. Figure 2.18 shows a network of water pipes with flows \nmeasured in liters per minute. \n(a) Set up and solve a system of linear equations to find \nthe possible flows. \n(b) If the flow through AB is restricted to 5 L/min, what","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":15779,"to":15896}}}}],[375,{"pageContent":"measured in liters per minute. \n(a) Set up and solve a system of linear equations to find \nthe possible flows. \n(b) If the flow through AB is restricted to 5 L/min, what \nwill the flows through the other two branches be? \n(c) What are the minimum and maximum possible \nflows through each branch? \n( d) We have been assuming that flow is always posi­\ntive. What would negative flow mean, assum­\ning we allowed it? Give an illustration for this \nexample. \nA \nB \nFioure 2.18 \n!\n1 \n--+ \nc","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":15896,"to":15913}}}}],[376,{"pageContent":"16. The downtown core of Gotham City consists of \none-way streets, and the traffic flow has been \nmeasured at each in  tersection. For the city block \nshown in Figure 2.19, the numbers represent the \naverage numbers of vehicles per minute entering and \nleaving intersections A, B, C, and D during business \nhours. \n(a) Set up and solve a system oflinear equations to find \nthe possible flows f\n1\n, ... ,f\n4\n. \n(b) If traffic is regulated on CD so that f\n4 \n= 10 vehi­\ncles per minute, what will the average flows on the \nother streets be? \n(c) What are the minimum and maximum possible \nflows on each street? \n(\nd) How would the solution change if all of the direc­\ntions were reversed? \n10\n! \n20\nf \nJ.Q. \n!1 \n.2.... \n--\nA \nB \nh\n! \n13 \nt \n.£ \nf\n4 \n.£ \n+---\nD \nc \n10\n! \nis\nf \nFigure 2.19 \n17. A network of irrigation ditches is shown in Figure 2.20, \nwith flows measured in thousands of liters per day. \nl.QQ.. \nA \n.l2Q. \n!\n\" \n..!22. \n200 \nc \nD \nFigure 2.20 \nSection 2.4 Applications \n115 \n(a) Set up and solve a system oflinear equations to find","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":15915,"to":15978}}}}],[377,{"pageContent":"with flows measured in thousands of liters per day. \nl.QQ.. \nA \n.l2Q. \n!\n\" \n..!22. \n200 \nc \nD \nFigure 2.20 \nSection 2.4 Applications \n115 \n(a) Set up and solve a system oflinear equations to find \nthe possible flows f\n1\n, \n... ,f\n5\n• \n(b) Suppose DC is closed. What range of flow will need \nto be maintained through DB? \n(c) From Figure 2.20 it  is clear that  DB  cannot be \nclosed. (Why not?) How does your solution in part \n(a) show this? \n(\nd) From your solution in part (a), determine the mini­\nmum and maximum flows through DB. \n18. (a) Set up and solve a system oflinear equations to \nfind the possible flows in the network shown in \nFigure 2.21. \n(b) Is it possible forf\n1 \n= 100 andf6 = 150? [Answer \nthis question first with reference to your solution \nin part (a) and then directly from Figure 2.21.] \n( c) If f\n4 \n= 0, what will the range of flow be on each of \nthe other branches? \n10o\nf \n1\n50 \n! \n20o\nf \n200 \nJi \nh \nl.QQ.. \n----\nA \nB \nc \n13 \nt \nf\n4 \n! \nfs t \n16 \nh \n200 \n.l2Q. \n+---+---\nD \nE \nF \n10o\nf \n100 ! \n10ol \nFigure 2.21 \nElectrical Networks","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":15978,"to":16052}}}}],[378,{"pageContent":"the other branches? \n10o\nf \n1\n50 \n! \n20o\nf \n200 \nJi \nh \nl.QQ.. \n----\nA \nB \nc \n13 \nt \nf\n4 \n! \nfs t \n16 \nh \n200 \n.l2Q. \n+---+---\nD \nE \nF \n10o\nf \n100 ! \n10ol \nFigure 2.21 \nElectrical Networks \nFor Exercises 19 and 20, determine the currents for the \ngiven electrical networks. \n19. \n/1 \nc \n/1 \n+---\n+---\n8 volts \n1  ohm \n/2 \n/2 \n----\nA \nB \n1  ohm \n4ohms \n/\n3 \nD \n/\n3 \n+---+---\n13 volts","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":16052,"to":16111}}}}],[379,{"pageContent":"116 \nChapter 2 \nSystems of Linear Equations \n20. \nI\ni \nc \n1\n1 \n-\n-\n5 \nvolts \n1  ohm \n1\n2 \nh \n----\nA \nB \n2ohms \n4ohms \n1\n3 \nD \n1\n3 \n--\n8 volts \n21. (a) Find the currents I, I\n1\n, ... , I\n5 \nin the bridge circuit \nin Figure 2.22. \n(b) Find the effective resistance of this network. \n(c) Can you change the resistance in branch B C (but \nleave everything else unchanged) so that the  cur­\nrent through branch CE becomes \nO\n? \nI\nii \n/\n4 \n--\nB \nl ohm \n2ohms \n-\nI \nFigure 2.22 \nc \n2ohms \n1  ohm \n1\n3 \n! \n15 \n--\nE \n1 \nohm \nA \n14 volts \n-\nI \n!\nh \nD \n22. The networks in parts (a) and (b) of Figure 2.23 \nshow two resistors coupled in series and in parallel, \nrespectively. We wish to find a general formula for the \neffective resistance of each network-that is, find Reff \nsuch that E = Refl \n(a) Show that the effective resistance Reff of a network \nwith two resistors coupled in series [Figure 2.23(a) J \nis given by \n(b) Show that the effective resistance Reff of a net­\nwork   with two resistors coupled in parallel \n[\nFigure 2.23(b)] is given by \nR \n-\n---\neff \n-\n1    1 \n1\n1 \n--\n1\n2 \n--\n1\nf","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":16113,"to":16208}}}}],[380,{"pageContent":"is given by \n(b) Show that the effective resistance Reff of a net­\nwork   with two resistors coupled in parallel \n[\nFigure 2.23(b)] is given by \nR \n-\n---\neff \n-\n1    1 \n1\n1 \n--\n1\n2 \n--\n1\nf \nFigure 2.23 \n-\n+\n­\nR\n1 \nR\nz \nE \n(\na\n) \nR\ni \nR\n2 \nE \n(b) \nResistors in series and in parallel \nlinear Economic Models \n23. Consider a simple economy with just two industries: \nfarming and manufacturing. Farming consumes 1/2 of \nthe food and 1/3 of the manufactured goods. Manufac­\nturing consumes 1/2 of the food and 2/3 of the manu­\nfactured goods. Assuming the economy is closed and \nin equilibrium, find the relative outputs of the farming \nand manufacturing industries. \n24. Suppose the coal and  steel industries form a closed \neconomy. Every $1 produced by the coal industry \nrequires $0.30 of coal and $0.70 of steel. Every $1 \nproduced by steel requires $0.80 of coal and $0.20 of \nsteel. Find the annual prod uction (output) of coal and \nsteel if the total annual production is $20 million. \n25. A painter, a  plumber, and an electrician enter into a","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":16208,"to":16260}}}}],[381,{"pageContent":"steel. Find the annual prod uction (output) of coal and \nsteel if the total annual production is $20 million. \n25. A painter, a  plumber, and an electrician enter into a \ncooperative arrangement in which each of them agrees \nto work for himself/herself and the other two for a \ntotal of 10 hours per week according to the schedule \nshown in Table 2.8. For tax purposes, each person must \nestablish a value for his/her services. They agree to do \nthis so that they each come out even-that is, so that the","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":16260,"to":16268}}}}],[382,{"pageContent":"total amount paid out by each person equals the amount \nhe/she receives. What hourly rate should each person \ncharge if the rates are all whole numbers between $30 \nand $60 per hour? \nTable 2.8 \nPainter \nConsumer   Plumber \nElectrician \nSupplier \nPainter Plumber Electrician \n2 \n4 \n4 \n5 \n4 \n5 \n1 \n4 \n26. Four neighbors, each with a vegetable garden, agree to \nshare their produce. One will grow beans \n(\nB\n)\n, one will \ngrow lettuce \n(\nL\n)\n, one will grow tomatoes \n(\nT\n)\n, and one \nwill grow zucchini (Z). Table 2.9 shows what fraction \nof each crop each neighbor will receive. What prices \nshould the neighbors charge for their crops if each \nperson is to  break even and the lowest-priced crop has \na value of $50? \nTable 2.9 \nProducer \nB \nL \nT \nz \nB \n0 \n1/4 \n1/8 \n1/6 \nConsumer \nL \n1/2 \n1/4 \n1/4 \n1/6 \nT \n1/4 1/4 \n1/2 \n1/3 \nz \n1/4 1/4 \n1/8 \n1/3 \n27. Suppose the coal and steel industries form an open \neconomy. Every $1 prod uced by the coal industry \nrequires $0.15 of coal and $0.20 of steel. Every $1 \nproduced by steel requires $0.25 of coal and $0.10 of","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":16270,"to":16336}}}}],[383,{"pageContent":"economy. Every $1 prod uced by the coal industry \nrequires $0.15 of coal and $0.20 of steel. Every $1 \nproduced by steel requires $0.25 of coal and $0.10 of \nsteel. Suppose that there is an annual outside demand \nfor $45 million of coal and $124 million of steel. \n(a) How much should each industry produce to satisfy \nthe demands? \n(b) If the demand for coal decreases by $5 million \nper year while the demand for steel increases by \n$6 million per year, how should the coal and steel \nindustries adjust their production? \n28. In Gotham City, the departments of Administra­\ntion (A), Health (H), and Transportation (T) are \ninterdependent. For every dollar's worth of services \nSection 2.4 Applications \n111 \nthey produce, each department uses a certain amount \nof the services produced by the other departments \nand itself, as sh  own in Table 2.10. Suppose that, dur­\ning the year, other city departments require $1 million \nin Administrative services, $1.2 million in Health","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":16336,"to":16356}}}}],[384,{"pageContent":"and itself, as sh  own in Table 2.10. Suppose that, dur­\ning the year, other city departments require $1 million \nin Administrative services, $1.2 million in Health \nservices, and $0.8 million in Transportation services. \nWhat does the annual dollar value of the services \nprod uced by each department need to be in order to \nmeet the demands? \nTable 2.10 \nDepartment \nA \nH T \nA \n$0.20 \n0.10 0.20 \nBuy \nH \n0.10 0.10 \n0.20 \nT \n0.20 \n0.40 \n0.30 \nFinite linear Games \n29. (a) In Example 2.35, suppose all the lights are initially \noff. Can we  push the switches in some order so \nthat only the second and fourth lights will be on? \n(b) Can we push the switches in some order so that \nonly the second light will be on? \n30. (a) In Example 2.35, suppose the fourth light is \ninitially on and the other four lights are off. Can \nwe push the switches in some order so that only \nthe second and fourth lights will be on? \n(b) Can we push the switches in some order so that \nonly the second light will be on?","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":16356,"to":16389}}}}],[385,{"pageContent":"we push the switches in some order so that only \nthe second and fourth lights will be on? \n(b) Can we push the switches in some order so that \nonly the second light will be on? \n31. In Example 2.35, describe all possible configurations \nof lights that can be obtained if we start with all the \nlights off. \n32. (a) In Example 2.36, suppose that all of the lights \nare initially off. Show that it is possible to \npush the switches in some order so that the \nlights are off, dark blue, and light blue, in that order. \n(b) Show that it is possible to push the switches in \nsome order so that the lights are light blue, off, \nand light blue, in that order. \n(c) Prove that any configuration of the three lights \ncan be achieved. \n33. Suppose the lights in Example 2.35 can be off, light \nblue, or dark blue and the switches work as described","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":16389,"to":16406}}}}],[386,{"pageContent":"118 \nChapter 2 \nSystems of Linear Equations \nin Example 2.36. (That is, the switches control the same \nlights as in Example 2.35 but cycle through the colors as \nin Example 2.36.) Show that it is possible to start with \nall of the lights off and push the switches in some order \nso that the lights are dark blue, light blue, dark blue, \nlight blue, and dark blue, in that order. \n34. For Exercise 33, describe all possible configurations \nof lights that can be obtained, starting with all the \nlights off. \ncA\ns \n35. Nine squares, each one either black or white, are ar­\nranged in a 3X3 grid. Figure 2.24 shows one possible \nFigure 2.24 \nThe nine squares \npuzzle \narrangement. When touched, each square changes \nits own state and the states of some of its neighbors \n(black � white and white� black). Figure 2.25 shows \nCD \n2 \n3 \nl \n@ \n3 \n1 \n2 \n® \n* \n* * * * \n* \n* \n4 \n5 \n6 \n4 \n5 \n6 \n4 \n5 \n6 \n* * * * \n7 \n8 \n9 \n7 8 \n9 \n7 8 \n9 \nl \n2 \n3 \nl \n2 \n3 \n1 \n2 \n3 \n* \n* \n* \n@ \n5 \n6 \n* \n4 \nG) \n6 \n* * * \n4 \n5 \n® \n* \n7 \n8 \n9 \n7 \n8 \n9 \n7 \n8 \n9 \n* * * \nl 2 \n3 \nl \n2 \n3 \nl 2 \n3 \n4 \n5 \n6 \n4 \n5 \n6 \n4 \n5 \n6 \n* \n* * \n*","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":16408,"to":16512}}}}],[387,{"pageContent":"CD \n2 \n3 \nl \n@ \n3 \n1 \n2 \n® \n* \n* * * * \n* \n* \n4 \n5 \n6 \n4 \n5 \n6 \n4 \n5 \n6 \n* * * * \n7 \n8 \n9 \n7 8 \n9 \n7 8 \n9 \nl \n2 \n3 \nl \n2 \n3 \n1 \n2 \n3 \n* \n* \n* \n@ \n5 \n6 \n* \n4 \nG) \n6 \n* * * \n4 \n5 \n® \n* \n7 \n8 \n9 \n7 \n8 \n9 \n7 \n8 \n9 \n* * * \nl 2 \n3 \nl \n2 \n3 \nl 2 \n3 \n4 \n5 \n6 \n4 \n5 \n6 \n4 \n5 \n6 \n* \n* * \n* \n(j) \n8 \n9 \n* * \n7 \n® \n9 \n* \n* \n* \n7 \n8 \n® \n* * \nFigure 2.25 \nState changes for the nine squares puzzle \nC\nA\nS \nhow the state changes work. (Touching the square \nwhose number is circled causes the states of the \nsquares marked * to change.) The object of the game \nis to  turn all nine squares black. [Exercises 35 and 36 \nare adapted from puzzles that can be found in \nthe interactive CD-ROM game The Seventh Guest \n(Trilobyte Software/Virgin Games, 1992).] \n(a) If the initial configuration is the one shown in \nFigure 2.24, show that the game can be won and \ndescribe a winning sequence of moves. \n(b) Prove that the game can always be won, no matter \nwhat the initial configuration. \n36. Consider a variation on the nine squares puzzle. The \ngame is the same as that described in Exercise 35","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":16512,"to":16627}}}}],[388,{"pageContent":"(b) Prove that the game can always be won, no matter \nwhat the initial configuration. \n36. Consider a variation on the nine squares puzzle. The \ngame is the same as that described in Exercise 35 \nexcept that there are three possible states for each \nsquare: white, gray, or black. The squares change as \nshown in Figure 2.25, but now the state changes follow \nthe cycle white� gray � black� white. Show how \nthe winning all-black configuration can be achieved \nfrom the initial configuration shown in Figure 2.26. \nFigure 2.26 \nThe nine squares puzzle \nwith more states \nMiscellaneous Problems \nIn Exercises 37-53, set up and solve an appropriate system \nof linear equations to answer the questions. \n37. Grace is three times as old as Hans, but in 5 years she \nwill be twice as old as Hans is then. How old are they \nnow? \n38. The sum of Annie's, Bert's, and Chris's ages is 60. \nAnnie is older than Bert by the same number of years \nthat Bert is older than Chris. When Bert is as old as","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":16627,"to":16648}}}}],[389,{"pageContent":"now? \n38. The sum of Annie's, Bert's, and Chris's ages is 60. \nAnnie is older than Bert by the same number of years \nthat Bert is older than Chris. When Bert is as old as \nAnnie is now, Annie will be three times as old as Chris \nis now. What are their ages? \nThe preceding two problems are typical of those found in \npopular books of mathematical puzzles. However, th ey have \ntheir origins in antiquity. A Babylonian clay tablet that sur­\nvives from about 300 B.c. contains the following problem.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":16648,"to":16657}}}}],[390,{"pageContent":"39. There are two fields whose total area  is 1800 sq  uare \nyards. One field produces grain at the rate of� bushel \nper square yard; the other field produces grain at the \nrate of\nt \nbushel per square yard. If the total yield is \n1100 bushels, what is the size of each field? \nOver 2000 years ago, the Chinese developed methods for \nsolving systems of linear equations, including a version of \nGaussian elimination that did not become well known in \nEurope until the 19th century. (There is no evidence that \nGauss was aware of the Chinese methods when he devel­\noped what we now call Gaussian elimination. However, it is \nclear that the Chinese knew th e essence of the method, even \nth ough they did not justify its use.) The following problem \nis taken from the Chinese text Jiuzhang suanshu (Nine \nChapters in the Mathematical Art), written during th e early \nHan Dynasty, about 200 B.C. \n40. There are three types of corn. Three bundles of the \nfirst type, two of the second, and one of the third","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":16659,"to":16678}}}}],[391,{"pageContent":"Chapters in the Mathematical Art), written during th e early \nHan Dynasty, about 200 B.C. \n40. There are three types of corn. Three bundles of the \nfirst type, two of the second, and one of the third \nmake 39 measures. Two bundles of the first type, three \nof the second, and one of the third make 34 measures. \nAnd one bundle of the first type, two of the second, \nand three of the third make 26 measures. How many \nmeasures of corn are contained in one bundle of \neach type? \n41. Describe all possible values of a, b, c, and d that \nwill make each of the following a valid addition \ntable. [Problems 41-44 are based on the article \n''An Application of Matrix Theory\" by Paul Glaister in \nThe Mathematics Teacher, 85 (1992), pp. 220-223.] \n(\na\n) (\nb\n) \n�b \nc 3  6 \nd 4  5 \n42. What conditions on w, x, y, and z will guarantee that \nwe can find a, b, c, and d so that the  following is a valid \naddition table? \n43. Describe all possible values of a, b, c, d, e, and f \nthat will make each of the following a valid addition","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":16678,"to":16705}}}}],[392,{"pageContent":"we can find a, b, c, and d so that the  following is a valid \naddition table? \n43. Describe all possible values of a, b, c, d, e, and f \nthat will make each of the following a valid addition \ntable. \n(\na\n) \n+ a b c \nd 3  2 1 \ne 5  4  3 \nf 4  3 \n(\nb\n) \n+ a b c \nd 1 2  3 \ne 3  4  5 \nf 4  5  6 \nSection 2.4 Applications \n119 \n44. Generalizing Exercise 42, find conditions on the en­\ntries of a 3 X 3 addition table that will guarantee that \nwe can solve for a, b, c, d, e, and fas previously. \n45. From elementary geometry we know that there \nis a unique straight line through any two points \nin a plane. Less well known is the fact that there is a \nunique parabola through any three noncollinear \npoints in a plane. For each set of points below, find \na parabola with an equation of the form y = ax\n2 \n+ \nbx+ c that passes through the given points. (Sketch \nthe resulting parabola to check the validity of your \nanswer.) \n(a) \n(\nO, 1\n)\n, \n(\n-1, 4), and (2, 1\n) \n(b) (-3, 1\n)\n, (-2, 2), and \n(\n-1, 5)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":16705,"to":16752}}}}],[393,{"pageContent":"2 \n+ \nbx+ c that passes through the given points. (Sketch \nthe resulting parabola to check the validity of your \nanswer.) \n(a) \n(\nO, 1\n)\n, \n(\n-1, 4), and (2, 1\n) \n(b) (-3, 1\n)\n, (-2, 2), and \n(\n-1, 5) \n46. Through any three noncollinear points there also \npasses a unique circle. Find the circles (whose general \nequations are of the form x\n2 \n+ y\n2 \n+ ax + by + c = 0) \nthat pass through the sets of points in Exercise 45. (To \ncheck the validity of your answer, find the center and \nradius of each circle and draw a sketch.) \nThe process of adding rational functions (ratios of polyno­\nmials) by placing them over a common denominator is \nthe analogu e of adding rational numbers. The reverse \npro cess of taking a rational function apart by writing it as \na sum of simpler rational functions is useful in several \nareas of mathematics; for example, it arises in calculus \nwhen we need to integrate a rational function and in dis­\ncrete mathematics when we use generatingfunctions to","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":16752,"to":16787}}}}],[394,{"pageContent":"areas of mathematics; for example, it arises in calculus \nwhen we need to integrate a rational function and in dis­\ncrete mathematics when we use generatingfunctions to \nsolve recurrence relations. The decomposition of a rational \nfunction as a sum of partial fractions leads to a system of \nlinear equations. In Exercises 47-50,find the partial \nfraction decomposition of the given form. (The capital \nletters denote constants.) \n3x +  1 A B \n47. \n-\n2 \n----\n= \n--\n+ \n--\nx   + 2x - 3 x  -   1 x  + 3 \nx\n2 \n-3x + 3 \nA B \nC \n48. \n=\n-+\n--\n+\n---\nx\n3 \n+ 2x\n2 \n+  x \nx \nx  +  1 (x \n+ \n1)\n2 \nx  -   1 \nCAS \n49. \n----------\n(\nX  +  l)(x\n2 \n+  l)(x\n2 \n+ 4\n) \nA Bx+  C Dx +  E \n=\n--\n+ \n+\n---\nx +  1 x\n2 \n+  1 x\n2 \n+ 4 \nx\n3 \n+  x  +  1 \nA B \nCAS \n50 \n= -  + \n--\n• \nx(x  -   l)(x\n2 \n+  x  +  l)(x\n2 \n+ \n1)\n3 \nx \nx  -   1 \nCx \n+ \nD Ex +  F Gx +  H Ix +  J \n+ + + \n+\n----\nx\n2\n+  x\n+ \nl \nx\n2\n+1 (x\n2\n+1)\n2 \n(x\n2\n+1)\n3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":16787,"to":16886}}}}],[395,{"pageContent":"120 \nChapter 2 \nSystems of Linear Equations \nFollowing are two useful formulas for th e sums of powers of \nconsecutive natural numbers: \nand \nn(\nn  +  1 ) \n1\n+2\n+\n·  ··+\nn  =\n----\n2 \nn(\nn  + \n1 )(2n \n+  1\n) \n1\n2 \n+  2\n2 \n+\n·\n. \n· +  n\n2 \n= \n-------\n6 \nThe validity of these formulas for all values of n  2::  1  (or \neven n  2::  OJ can  be established using mathematical induc­\ntion (see Appendix B ). One way to make an educated guess \nas to what the formulas are, though, is to observe that we \ncan rewrite th e two formulas above as \nrespectively. This leads to th e conjecture that th e sum of pth \npowers of th e first n natural numbers is a polynomial of \ndegree p +  1 in th e variable n. \n51. Assuming that 1   +  2  + \n· · · \n+  n  = an\n2 \n+ bn + c, \nfind a, b, and c by substituting three values for n and \nthereby obtaining a system of linear equations in a, \nb, and c. \n52. Assume that 1\n2 \n+ 2\n2 \n+ \n· · · \n+ n\n2 \n= an\n3 \n+ bn\n2 \n+en  + d. \nFind a, b, c, and d. [Hint: It is legitimate to  use n = 0. \nWhat is the left-hand side in that case?]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":16888,"to":16950}}}}],[396,{"pageContent":"b, and c. \n52. Assume that 1\n2 \n+ 2\n2 \n+ \n· · · \n+ n\n2 \n= an\n3 \n+ bn\n2 \n+en  + d. \nFind a, b, c, and d. [Hint: It is legitimate to  use n = 0. \nWhat is the left-hand side in that case?] \n53. Show that 1\n3 \n+ 2\n3 \n+ \n· · · \n+ \nn\n3 \n= \n(\nn\n(\nn +  1\n)\n/2\n)\n2\n.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":16950,"to":16984}}}}],[397,{"pageContent":"This application is based on the \narticle \"A n Underdetermined \nLinear System for GPS\" by \nDan Kalman in The College \nMathematics Jo urnal, 33 (2002), \npp. 384-390. For a more in-depth \ntreatment of the ideas introduced \nhere, see G. Strang and K. Borre, \nLinear Algebra, Geodesy, and GPS \n(Wellesley-Cambridge Press, MA, \n1997). \nVignette \nThe Global Po sitioning System \nThe Global Positioning System (GPS) is used in a variety of situations for determin­\ning geographical locations. The military, surveyors, airlines, shipping companies, \nand hikers all make use of it. GPS technology is becoming so commonplace that \nsome automobiles, cellular phones, and various handheld devices are now equipped \nwith it. \nThe basic idea of GPS is a variant on three-dimensional triangulation: A point \non Earth's surface is uniquely determined by knowing its distances from three other \npoints. Here the point we wish to determine is the location of the GPS receiver, the","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":16986,"to":17006}}}}],[398,{"pageContent":"on Earth's surface is uniquely determined by knowing its distances from three other \npoints. Here the point we wish to determine is the location of the GPS receiver, the \nother points are satellites, and the distances are computed using the travel times of \nradio signals from the satellites to the receiver. \nWe will assume that Earth is a sphere on which we impose an xyz-coordinate \nsystem with Earth centered at the origin and with the positive z-axis running through \nthe north pole and fixed relative to Earth. \nFor simplicity, let's take one unit to be equal to the radius of Earth. Thus Earth's \nsurface becomes the unit sphere with equation x\n2 \n+ y\n2 \n+ z\n2 \n= 1. Time will be \nmeasured in hundredths of a second. GPS finds distances by knowing how long it \ntakes a radio signal to get from one point to another. For this we need to know the \nspeed of light, which is approximately  equal to 0.47 (Earth radii per hundredths of \na second).","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":17006,"to":17024}}}}],[399,{"pageContent":"takes a radio signal to get from one point to another. For this we need to know the \nspeed of light, which is approximately  equal to 0.47 (Earth radii per hundredths of \na second). \nLet's imagine that you are a hiker lost in the woods at point (x, y, z) at some time \nt. You don\n'\nt know where you are, and furthermore, you have no watch, so you don't \nknow what time it is. However, you have your G PS device, and it receives simultaneous \nsignals from four satellites, giving their positions and times as shown in Table 2.11. \n(Distances are measured in Earth radii and time in hundredths of a second past \nmidnight.) \nTable 2.11 \nsa1em1e Dara \nSatellite \nPosition \nTime \n(1.11, 2.55, 2.14) \n1.29 \n2 \n(2.87, 0.00, 1.43) \n1.31 \n3 \n(0.00, 1.08, 2.29) \n2.75 \n4 \n(1.54, 1.01, 1.23) 4.06 \n121","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":17024,"to":17050}}}}],[400,{"pageContent":"122 \nLet (x, y, z) be your position, and let t be the time when the signals ar  rive. The \ngoal is to solve for x, y, z, and t. Your distance from Satellite 1 can be computed as \nfollows. The signal, traveling at a speed of0.47 Earth radii/10\n-\n2 \nsec, was sent at time \n1.29 and arrived at time t, so it took t -  1.29 hundredths of a second to reach you. \nDistance equals velocity multiplied by (elapsed) time, so \nd = 0.47(t - 1.29) \nWe can also express d in terms of (x, y, z)   and the satellite's position ( 1.11, 2.55, 2.14) \nusing the distance formula: \nd = \nV\n(\nx \n- 1.11\n)\n2 \n+ \n(\ny  -   2.55\n)\n2 \n+ \n(\nz  -  2.14\n)\n2 \nCombining these results leads to the equation \n(x - 1.11)\n2 \n+ (y - 2.55)\n2 \n+ (z - 2.14)\n2 \n= 0.47\n2\n(t - 1.29)\n2 \n(1) \nExpanding, simplifying, and rearranging, we find that Equation (1) becomes \n2.22x + 5.lOy + 4.28z - 0.57t = x\n2 \n+ y\n2 \n+ z\n2 \n- 0.22t\n2 \n+ \n11.95 \nSimilarly, we can derive a corresponding equation for each of the other three satel­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":17052,"to":17104}}}}],[401,{"pageContent":"2.22x + 5.lOy + 4.28z - 0.57t = x\n2 \n+ y\n2 \n+ z\n2 \n- 0.22t\n2 \n+ \n11.95 \nSimilarly, we can derive a corresponding equation for each of the other three satel­\nlites. We end up with a system of four equations in x, y, z,   and t: \n2.22x + 5.lOy + 4.28z \n-\n0.57t = x\n2 \n+ y\n2 \n+ z\n2 \n-\n0.22t\n2 \n+ 11.95 \n5.74x \n+ \n2.86z - 0.58t = x\n2 \n+ y\n2 \n+ z\n2 \n- 0.22t\n2 \n+ 9.90 \n2.16y + 4.58z \n-\n1.21t = x\n2 \n+ y\n2 \n+ z\n2 \n-\n0.22t\n2 \n+ 4.74 \n3.08x + 2.02y + 2.46z \n-\n1.79t = x\n2 \n+ y\n2 \n+ z\n2 \n-\n0.22t\n2 \n+ \n1.26 \nThese are not linear equations, but the nonlinear terms are the same in each equation. \nIf we subtract the first equation from each of the other three equations, we obtain a \nlinear system: \n3.52\nx -  5.lOy -  1.42z - O.Olt = \n2.05 \n-2.22x - 2.94y + 0.30z - 0.64t = \n7.21 \n0.86x - 3.08y -  1.82z - 1.22t = -10.69 \nThe augmented matrix row reduces as \n[ \n3.52 \n-2.22 \n0.86 \n-5.10 \n-2.94 \n-3.08 \n-1.42 \n0.30 \n-1.82 \n-0.01 \n-0.64 \n-1.22 \n=\n�:\n�\n�1 \n� \n[� \n� \n� �\n:\n�\n: \n-10.69 \n0  0  1 0.79 \n2.97] \n0.81 \n5.91","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":17104,"to":17202}}}}],[402,{"pageContent":"from which we see that \nx = 2.97 - 0.36t \ny \n= 0.81 - 0.03t \nz = 5.91 - 0.79t \nwith t free. Substituting these equations into (1), we obtain \n(2.97 - 0.36t - 1.11)\n2 \n+ (0.81 - 0.03t - 2.55)\n2 \n+ \n(5.91 -  0.79t - 2.14)\n2 \n= 0.47\n2\n(t - 1.29)\n2 \nwhich simplifies to the quadratic equation \nThere are two solutions: \n0.54t\n2 \n- 6.65t + 20.32 = 0 \nt = 6.74 \nand  t = 5.60 \n(2) \nSubstituting into (2), we find that the first solution corresponds to \n(\nx, y, z\n) \n= (0.55, \n0.61, 0.56) and the second solution to \n(\nx,y, z\n) \n= (0.96, 0.65, 1.46). The second solution \nis clearly not on the unit sphere (Earth), so we reject it. The first solution produces \nx\n2 \n+ y\n2 \n+  z\n2 \n= 0.99, so we are satisfied that,   within acceptable roundoff error, we \nhave located your coordinates as (0.55, 0.61, 0.56). \nIn practice, GPS takes significantly more factors into account, such as the fact \nthat Earth's surface is not exactly spherical, so additional refinements are needed in­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":17204,"to":17249}}}}],[403,{"pageContent":"In practice, GPS takes significantly more factors into account, such as the fact \nthat Earth's surface is not exactly spherical, so additional refinements are needed in­\nvolving such techniques as least squares approximation (see Chapter 7). In addition, \nthe results of the GPS calculation are converted from rectangular (Cartesian) coor­\ndinates into latitude and longitude, an interesting exercise in itself and one involving \nyet other branches of mathematics. \n123","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":17249,"to":17255}}}}],[404,{"pageContent":"124 \nChapter 2 \nSystems of Linear Equations \n'� �llerative \nMelhods for Solving linear svstems \nExample 2.31 \nCarl Gustav Jacobi (1804-1851) was \na German mathematician who made \nimportant contributions to many \nfields of mathematics and physics, \nincluding geometry, number theory, \nanalysis, mechanics, and fluid \ndynamics. Although much of his \nwork was in applied mathematics, \nJacobi believed in the importance of \ndoing mathematics for its own sake. \nA fine teacher, he held positions \nat the Universities of Berlin and \nKonigsberg and was one of the most \nfamous mathematicians in Europe. \nThe direct methods for solving linear systems, using elementary row operations, lead \nto exact solutions in many cases but are subject to errors due to roundoff and other \nfactors, as we have seen. The third road in our \"trivium\" takes us down quite a different \npath indeed. In this section, we explore methods that proceed iteratively by succes­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":17257,"to":17280}}}}],[405,{"pageContent":"factors, as we have seen. The third road in our \"trivium\" takes us down quite a different \npath indeed. In this section, we explore methods that proceed iteratively by succes­\nsively generating sequences of vectors that approach a solution to a linear system. In \nmany instances (such as when the coefficient matrix is sparse-that is, contains many \nzero entries), iterative methods can be faster and more accurate than direct methods. \nAlso, iterative methods can be stopped whenever the approximate solution they gen­\nerate is sufficiently accurate. In addition, iterative methods often benefit from inac­\ncuracy: Roundoff error can actually accelerate their convergence toward a solution. \nWe will explore two iterative methods for solving linear systems: Jacobi's method \nand a refinement of it, the Gauss-Seidel meth od. In all examples, we will be consid­\nering linear systems with the same number of variables as equations, and we will","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":17280,"to":17290}}}}],[406,{"pageContent":"and a refinement of it, the Gauss-Seidel meth od. In all examples, we will be consid­\nering linear systems with the same number of variables as equations, and we will \nassume that there is a unique solution. Our interest is in   finding this solution using \niterative methods. \nConsider the system \n7x\n1 \n-    x2 = 5 \n3x\n1 \n-  5x2 = -7 \nJacobi's method begins with solving the first equation for x1 and the second equation \nfor x2, to obtain \n5  + \nXz \nX1 \n= \n7 \n(1) \n7  + 3x\n1 \nXz \n= \n5 \nWe now need an initial approximation to the solution. It turns out that it does not \nmatter what this initial approximation is, so we might as well take x1 = 0, x2 = 0. We \nuse these values in Equations (1) to get new values of x1 and x2: \n5  +  0 5 \n--= -= 0.714 \nX\nz \n= \n7 7 \n7  + 3. 0 \n5 \nNow we substitute these values into (1) to get \n5  + 1.4 \n7 \n5 \n1.400 \nX\n1 \n= \n---\n= \n0.9\n14 \n7 \n7  +  3 .\n2-\nX\nz \n= \n---\n7 \n= \n1.829 \n5 \n(written to three decimal places). We repeat this process (using the old values of x2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":17290,"to":17348}}}}],[407,{"pageContent":"5  + 1.4 \n7 \n5 \n1.400 \nX\n1 \n= \n---\n= \n0.9\n14 \n7 \n7  +  3 .\n2-\nX\nz \n= \n---\n7 \n= \n1.829 \n5 \n(written to three decimal places). We repeat this process (using the old values of x2 \nand x1 to get the new values of x1 and x2), producing the sequence of approximations \ngiven in Table 2.12.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":17348,"to":17372}}}}],[408,{"pageContent":"Table 2.12 \nn 0 \n0 \n0 \n1 \nSection 2.5 \nIterative Methods for Solving Linear Systems \n125 \n2 \n3 \n4 \n5 6 \n0\n.\n7 1\n4 \n0\n.91\n4 \n0.9\n76 \n0 .993 \n0.998 \n0.999 \n1.\n4\n00 \n1.829 \n1.9\n4\n9 \n1.98\n5 \n1.996 \n1.999 \nThe successive vectors \n[\n:\n:] \nare called iterates, so, for example, when n = \n4\n, \nthe  fourth iterate is \n[\n0\n·\n993\n]\n. We can see that the  iterates in this  example  are \n1.98\n5 \n......... \napproa ching \n[\n�\nl \nwhich is the exact solution of the given system. (Check this.) \nThe Gauss-Seidel method is named \nafter C. F. Gauss and Philipp Ludwig \nvon Seidel (1821-1896). Seidel \nworked in analysis, probability \ntheory, astronomy,  and optics. \nUnfortunately, he suffered from \neye problems and retired at a young \nage. The paper in which he described \nthe method now known as Gauss­\nSeidel was published in 1874. Gauss, \nit seems, was unaware of the \nmethod! \nWe say in this case that Jacobi's method converges. \n4 \nJacobi's method calculates the successive iterates in a two-variable system accord­\ning to the crisscross pattern shown in Table 2.13. \nTable 2.13","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":17374,"to":17447}}}}],[409,{"pageContent":"We say in this case that Jacobi's method converges. \n4 \nJacobi's method calculates the successive iterates in a two-variable system accord­\ning to the crisscross pattern shown in Table 2.13. \nTable 2.13 \nn 0 \n1 \n2 \n3 \nBefore  we  consider Jacobi's method in  the  general case, we  will  look at  a \nmodification of it that often converges faster to the solution. The Gauss-Seidel method \nis the same as the Jacobi method except that we use each new value as soon as we can. \nSo in our example, we begin by calculating x1 \n= \n(\n5 + \n0\n)\n/\n7 \n= * = 0 .\n71\n4 \nas before, but \nwe now use this value of x\n1 \nto get the next value of x\n2\n: \n7 + \n3. 2. \nX\nz \n= \n---7 \n= \n1 .829 \n5 \nWe then use this value of x\n2 \nto recalculate x\n1\n, and so on. The iterates this time are \nshown in Table 2.1\n4\n. \nWe observe that  the Gauss-Seidel method has  converged faster to the  solu­\ntion. The iterates this time are calculated according to the zigzag pattern shown in \nTable 2.\n15. \nTable 2.14 \nn 0 \n1 \n2 \n3 \n4 \n5 \n0 \n0\n.\n71\n4 \n0\n.9\n76 \n0 .998 \n1.\n000 \n1 .\n000 \n0 \n1 .829 \n1\n.98\n5 \n1.999","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":17447,"to":17522}}}}],[410,{"pageContent":"tion. The iterates this time are calculated according to the zigzag pattern shown in \nTable 2.\n15. \nTable 2.14 \nn 0 \n1 \n2 \n3 \n4 \n5 \n0 \n0\n.\n71\n4 \n0\n.9\n76 \n0 .998 \n1.\n000 \n1 .\n000 \n0 \n1 .829 \n1\n.98\n5 \n1.999 \n2.\n000 \n2.\n000","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":17522,"to":17554}}}}],[411,{"pageContent":"126 \nChapter 2 \nSystems of Linear Equations \nTable 2.15 \nn 0 \nX\nz \n1 \n2 \n3 \nThe Gauss-Seidel method also has a    nice geometric interpretation in the case of \ntwo variables. We can think of x\n1 \nand x\n2 \nas the coordinates of points in the plane. Our \nstarting point is the point corresponding to our initial approximation, (0, O). Our first \ncalculation gives x\n1 \n= �,so we  move to the point ( �, O) \n= \n(0.714, 0). Then we compute \nx\n2 \n= � \n= \n1.829, which moves us to   the point (�,�) \n= \n(0.714, 1.829). Continuing in \nthis fashion, our calculations from the Gauss-Seidel method give rise to a sequence \nof points, each one differing from the preceding point in exactly one coordinate. If \nwe plot the lines 7x\n1 \n- x\n2 \n= 5 and 3x\n1 \n- 5x\n2 \n= -7 corresponding to the two given \nequations, we find that the points calculated above fall alternately on the two lines, \nas sh  own in Figure 2.27. Moreover, they approach the point of intersection of the","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":17556,"to":17597}}}}],[412,{"pageContent":"equations, we find that the points calculated above fall alternately on the two lines, \nas sh  own in Figure 2.27. Moreover, they approach the point of intersection of the \nlines, which corresponds to the solution of the system of equations. This is what \nconvergence means! \n2 \n0.\n5 \n-\n0.\n5 \n-\n1 \n0.2 \n0.4 \nFigure 2.21 \nConverging iterates \nThe general cases of the two methods are analogous. Given a system of n linear \nequations in n variables, \na\ni \n1\nX\n1 \n+ a\ni\n2\nX\n2 \n+ · · · + a\ni\nn\nX\nn \n= b\ni \na\nz 1\nX\n1 \n+ a\nz\n2\nX\n2 \n+ · · · + a\nz\nn\nX\nn \n= b\nz \n(2) \nwe solve the first equation for x\n1\n, the second for x\n2\n, and so on. Then, beginning \nwith an initial approximation, we use these new equations to iteratively update each","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":17597,"to":17654}}}}],[413,{"pageContent":"Section 2.5 \nIterative Methods for Solving Linear Systems \n121 \nvariable. Jacobi's method uses all of the values at the kth iteration to compute the \n(k + l)st iterate, whereas the Gauss-Seidel method always uses the most recent value \nof each variable in every calculation. Example 2.39 later illustrates the Gauss-Seidel \nmethod in a three-variable problem. \nAt this point, you should have some questions and concerns about these iterative \nmethods. (Do you?) Several come to mind: Must these methods converge? If not, \nwhen do they converge? If they converge, must they converge to the solution? The \nanswer to the first question is no, as Example 2.38 illustrates. \nExample 2.38 \nApply the Gauss-Seidel method to the system \nTable 2.16 \nn 0 \n1 \n2 \nX\ni \n0 \n4 \nX\n2 \n0     3 \n-3 \nX\n1 \n-\nXz \n= 1 \n2x\n1 \n+ \nXz \n= \n5 \nwith initial approximation \n[ \n�\n]\n. \nSolution \nWe rearrange the equations to get \nX\n1 \n= 1 \n+ \nX\n2 \nX\n2 \n= 5 -  2x\n1 \n� \nThe first few iterates are given in Table 2.16. (Check these.) \n3 \n-2 \n9 \nThe actual solution to the given system is \n[","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":17656,"to":17715}}}}],[414,{"pageContent":"[ \n�\n]\n. \nSolution \nWe rearrange the equations to get \nX\n1 \n= 1 \n+ \nX\n2 \nX\n2 \n= 5 -  2x\n1 \n� \nThe first few iterates are given in Table 2.16. (Check these.) \n3 \n-2 \n9 \nThe actual solution to the given system is \n[\n::] = \n[ \n�\n]\n. Clearly,  the iterates in \nTable 2.16 are not approaching this point, as Figure 2.28 makes graphically clear in an \nexample of divergence. \n-4 \n4 \n5 \n10 \n-\n14 \n-15 \n33 \nFioure 2.28 \nDiverging iterates","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":17715,"to":17754}}}}],[415,{"pageContent":"128 \nChapter 2 \nSystems of Linear Equations \nTheorem 2.9 \nTheorem 2 .10 \nSo when do these iterative methods converge? Unfortunately, the answer to this \nquestion is ra ther tricky. We will answer it completely in Chapter 7, but for now we \nwill give a partial answer, without proof. \nLet A be the n X  n matrix \na\n2\n1 \nA= \n. \n[\n\"\" \na\nn) \na\n1\n2 \na\n22 \na\nn\n2 \nWe say that A is strictly diagonally dominant if \na\n,\n, \nl \na\n2\nn \na\nnn \nl\na\n11\nI \n> \nl\na\n1\n2\nI \n+ \nl\na\n13\nI \n+ \n· · · \n+ \nl\na\n1n\nl \nl\na\n22\nI \n> \nl\na\n2\n1\nI \n+ \nl\na\n2\n3\nI \n+ \n· · · \n+ \nl\na\n2\nn\nl \nThat is, the absolute value of each diagonal entry a11\n, \naw  ... \n, \nann is greater than the \nsum of the absolute values of the remaining entries in that row. \nIf a system of n linear equations in n variables has a strictly diagonally domi­\nnant coefficient matrix, then it has a unique solution and both the Jacobi and the \nGauss-Seidel method converge to it. \nRemark Be warned! This theorem is a one-way implication. The fact that a \nsystem is not strictly diagonally dominant does not mean that the  iterative meth­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":17756,"to":17848}}}}],[416,{"pageContent":"Gauss-Seidel method converge to it. \nRemark Be warned! This theorem is a one-way implication. The fact that a \nsystem is not strictly diagonally dominant does not mean that the  iterative meth­\nods diverge. They may or may not converge. (See Exercises 15-19.) Indeed, there are \nexamples in which one of the methods converges and the other diverges. However, if \neither of these methods converges, then it must converge to the solution-it cannot \nconverge to some other point. \nIf the Jacobi or the Gauss-Seidel method converges for a  system of n linear \nequations in n variables, then it must converge to the solution of the system. \nProof We will illustrate the idea behind the proof by sketching it out for the case of \nJacobi's method, using the system of equations in Example 2.37. The general proof \nis similar. \nConvergence means that \"as iterations increase, the values of the iterates get \ncloser and closer to a limiting value:' This means that x1 and x\n2 \nconverge to r and s,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":17848,"to":17863}}}}],[417,{"pageContent":"is similar. \nConvergence means that \"as iterations increase, the values of the iterates get \ncloser and closer to a limiting value:' This means that x1 and x\n2 \nconverge to r and s, \nrespectively, as sh  own in Table 2.17. \nWe must prove that \n[\n::] \n[\n:\n] \nis the solution of the system of equations. In \nother words, at the \n(\nk \n+ l)st iteration, the values of x1 and x\n2 \nmust stay the same as at","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":17863,"to":17881}}}}],[418,{"pageContent":"Example 2.39 \nSection 2.5 \nIterative Methods for Solving Linear Systems \n129 \nTable 2.11 \nn \nk \nk+I k+2 \nr r \nr \ns s s \nthe kth iteration. But the calculations give x\n1 \n= \n(\n5 + x\n2\n)\n/7 = \n(\n5 + s\n)\n/7 and x\n2 \n= \n(\n7 + 3x\n1)/5 = \n(\n7 + 3r)/5. Therefore, \n5 + s \n--=r \n7 \nRearranging, we see that \n7 + 3r \nand ---=  s \n5 \n7r  -s  = 5 \n3r  -   5s  = -7 \nThus, x\n1 \n= r, x\n2 \n= s satisfy the original equations, as required. \nBy now you may be wondering: If iterative methods don't always converge to the \nsolution, what good are they? Why don't we just use Gaussian elimination? First, we \nhave seen that Gaussian elimination is sensitive to roundoff errors, and this sensitiv­\nity can lead to inaccurate or even wildly wrong answers. Also, even if Gaussian elimi­\nnation does not go astray, we cannot improve on a solution once we have found it.  For \nexample, if we use Gaussian elimination to calculate a solution to two decimal places, \nthere is no way to obtain the solution to four decimal places except to start over again","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":17883,"to":17933}}}}],[419,{"pageContent":"example, if we use Gaussian elimination to calculate a solution to two decimal places, \nthere is no way to obtain the solution to four decimal places except to start over again \nand work with increased accuracy. \nIn contrast, we can achieve additional accuracy with iterative methods simply by \ndoing more iterations. For large systems, particularly those with sparse coefficient \nmatrices, iterative methods are much faster than direct methods when implemented \non a computer. In many applications, the systems that arise are strictly diagonally \ndominant, and thus iterative methods are guaranteed to converge. The next example \nillustrates one such application. \nSuppose we heat each edge of a metal plate to a constant temperature, as sh  own in \nFigure 2.29. \n50\n° \nFigure 2.29 \nA heated metal plate \no\no","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":17933,"to":17949}}}}],[420,{"pageContent":"130 \nChapter 2 \nSystems of Linear Equations \nEventually the temperature at the interior points will reach equilibrium, where the \nfollowing property can be shown to hold: \nThe temperature at each interior point Pon a  plate is the average of the tempera­\ntures on the circumference of any circle centered at P inside the plate (Figure 2.30). \nfigure 2.30 \nTo apply this property in an  actual example requires techniques from calculus. As \nan alternative, we can approximate the situation by overlaying the plate with a grid, \nor mesh, that has a finite number of interior points, as sh  own in Figure 2.31. \nFigure 2.31 \nThe discrete version of the heated \nplate problem \nThe discrete analogue of the averaging property governing equilibrium tempera­\ntures is stated as follows: \nThe temperature at each interior point P is the average of the temperatures at the \npoints adjacent to P. \nFor the example shown in Figure 2.31, there are three interior points, and each is","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":17951,"to":17969}}}}],[421,{"pageContent":"The temperature at each interior point P is the average of the temperatures at the \npoints adjacent to P. \nFor the example shown in Figure 2.31, there are three interior points, and each is \nadjacent to four other points. Let the equilibrium temperatures of the interior points","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":17969,"to":17972}}}}],[422,{"pageContent":"Section 2.5 \nIterative Methods for Solving Linear Systems \n131 \nbe t\n1\n, t\n2\n, and t\n3\n, as sh  own. Then, by the temperature-averaging property, we have \nor \n100 + 100 + G + 50 \nt   = \n--------\n! \n4 \nt\n1 \n+ \nt\n3 \n+ 0 + 50 \nt\n1 \n= \n____ \n4 \n__ \n_ \n100 + 100 + \no \n+ G \nt\n3 \n= \n4 \n250 \n-t\n1\n+4t\n2\n-t\n3\n= 50 \n-\nt\n1 \n+ 4t\n3 \n= 200 \n(3) \nNotice that this system is strictly diagonally dominant. Notice also that Equa­\ntions (3) are in the form required for Jacobi or Gauss-Seidel iteration. With an initial \napproximation of t\ni \n= 0, t\n2 \n= 0, t\n3 \n= 0, the Gauss-Seidel method gives the following \niterates. \n100 + 100 + 0 + 50 \nIteration 1: \nt\n1 \n= \n= 62.5 \n4 \n62.5 + 0 + 0 + 50 \nt\n1 \n= \n= 28.125 \n4 \n10\n0 + 100 + 0 + 28.125 \nt\n3 \n= \n= 57.031 \n4 \n100 + 100 + 28.125 + 50 \nt\n1 \n= \n= 69.531 \n4 \nIteration 2: \n69.531 + 57.031 + 0 + 50 \nt\n1 \n= \n= 44.141 \n4 \n100 + 100 + 0 + 44.141 \nt\n3 \n= \n= 61.035 \n4 \nContinuing, we find the iterates listed in Table 2.18. We work with five-significant­\ndigit accuracy and stop when two successive iterates agree within 0.001  in all \nvariables.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":17974,"to":18076}}}}],[423,{"pageContent":"t\n3 \n= \n= 61.035 \n4 \nContinuing, we find the iterates listed in Table 2.18. We work with five-significant­\ndigit accuracy and stop when two successive iterates agree within 0.001  in all \nvariables. \nThus, the equilibrium temperatures at the interior points are (to an accuracy of \n� \n0.001) t\ni \n= 74.108, t\n2 \n= 46.430, and t\n3 \n= \n61.607. (Check these calculations.) \nBy using a finer grid (with more interior points), we can get as precise informa­\ntion as we like about the equilibrium temperatures at various points on the plate. \nTable 2.18 \nn 0 \n1 \n2 \n3 \n7 \n8 \nt\ni \n0 62.500 69.531 \n73.535 \n74.107 \n74.107 \nt\n1 \n0 \n28.125 44.141 46.143 46.429 46.429 \nt\n3 \n0 57.031 \n61.035 61.536 \n61.607 61.607 \n4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":18076,"to":18118}}}}],[424,{"pageContent":"132 \nChapter 2 \nSystems of Linear Equations \n.. \nI \nExercises 2.5 \nGAS \nIn Exercises 1-6, apply Jacobi's method to the given system. \nTake th e zero vector as the initial approximation and work \nwith four-significant- digit accuracy until two successive \niterates agree within 0. 001 in each variable. In each case, \ncompare your answer with th e exact solution found using \nany direct method you like. \n1. 7X\n1 \n-\nX\n2 \n= \n6 2. 2X\n1 \n+  X\n2 \n=  5 \nX\n1 \n-  5X\n2 \n= -4 \nX\n1 \n-  X\n2 \n=  1 \n3. \n4.5X\n1 \n-0.5X\n2 \n= 1 \nX\n1 \n-3.5X\n2 \n= -1 \n4. 20X\n1 \n+ \nX\n2 \n-\nX\n3 \n= 17 \nX\n1 \n-10x\n2 \n+ \nx\n3 \n= 13 \n-x\n1 \n+ \nX\n2 \n+ 10x\n3 \n= 18 \n5. 3X\n1 \n+ \nX\n2 \n=  1 \nX\n1 \n+  4X\n2 \n+    X\n3 \n=  1 \nX\n2 \n+ 3X\n3 \n=  1 \n6. 3X\n1 \n-\nX\n2 \n=  1 \n- X\n1 \n+ 3X\n2 \n-\nX\n3 \n=  0 \n- X\n2 \n+ 3X\n3 \n-\nX\n4 \n= \n-x\n3 \n+ 3X\n4 \n=  1 \nIn Exercises 7-12, repeat the given exercise using the Gauss­\nSeidel method.  Take the zero vector as the initial approxi­\nmation and work with four-significant- digit accuracy until \ntwo successive iterates agree within 0. 001 in each variable. \nCompare th e number of iterations requ ired by th e Jacobi","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":18120,"to":18239}}}}],[425,{"pageContent":"mation and work with four-significant- digit accuracy until \ntwo successive iterates agree within 0. 001 in each variable. \nCompare th e number of iterations requ ired by th e Jacobi \nand Gauss-Seidel methods to reach such an approximate \nsolution. \n7. Exercise 1 \n9. Exercise 3 \n11. Exercise 5 \n8. Exercise 2 \n10. Exercise 4 \n12. Exercise 6 \nIn Exercises 13 and 14, draw diagrams to illustrate th e con­\nvergence of the Gauss-Seidel method with th e given system. \n13. The system in Ex  ercise 1 \n14. The system in Ex  ercise 2 \nIn Exercises 15 and 16, compute th e first four iterates, \nusing th e zero vector as th e initial approximation, to show \nthat th e Gauss-Seidel method diverges. Then show that \nthe equations can be rearranged to give a strictly diagonally \ndominant coefficient matrix, and apply the Gauss-Seidel \nmethod to obtain an approximate solution that is accurate \nto within 0. 001. \n15. X\n1 \n-  2X\n2 \n=  3 \n3x\n1 \n+ 2x\n2 \n=  1 \n16. \nX\n1 \n-  4X\n2 \n+  2X\n3 \n=  2 \n2x\n2 \n+ 4x\n3 \n=  1 \n6X\n1 \n-X\n2 \n-  2X\n3 \n=  1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":18239,"to":18290}}}}],[426,{"pageContent":"method to obtain an approximate solution that is accurate \nto within 0. 001. \n15. X\n1 \n-  2X\n2 \n=  3 \n3x\n1 \n+ 2x\n2 \n=  1 \n16. \nX\n1 \n-  4X\n2 \n+  2X\n3 \n=  2 \n2x\n2 \n+ 4x\n3 \n=  1 \n6X\n1 \n-X\n2 \n-  2X\n3 \n=  1 \n17. Draw a   diagram to illustrate the divergence of the \nGauss-Seidel method in Exercise 15. \nIn Exercises 18 and 19, th e coefficient matrix is not strictly \ndiagonally dominant, nor can the equations be rearranged \nto make it so. However, both the Jacobi and the Gauss-Seidel \nmethod converge anyway. Demonstrate that this is true of \nth e Gauss-Seidel method, starting with th e zero vector as \nth e initial approximation and obtaining a solution that is \naccurate to within 0. 01. \n18. -4x\n1 \n+ \n5X\n2 \n= 14 \nX\n1 \n-3x\n2 \n= -7 \n19. \n5x\n1 \n-2x\n2 \n+ 3x\n3 \n= -8 \nX\n1 \n+  4X\n2 \n-  4X\n3 \n= 102 \n-2x\n1 \n-2x\n2 \n+ 4x\n3 \n= -90 \n20. Continue performing iterations in Exercise 18 to \nobtain a solution that is accurate to within 0.001. \n21. Continue performing iterations in Exercise 19 to \nobtain a solution that is accurate to  within 0.001.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":18290,"to":18367}}}}],[427,{"pageContent":"obtain a solution that is accurate to within 0.001. \n21. Continue performing iterations in Exercise 19 to \nobtain a solution that is accurate to  within 0.001. \nIn Exercises 22-24, the metal plate has the constant te m­\nperatures shown on its boundaries. Find the equilibrium \ntemperature at each of the indicated interior points by \nsetting up a system of linear equations and applying either \nthe Jacobi or the Gauss-Seidel method. Obtain a solution \nthat is accurate to within 0. 001. \n22. \n50 \n50","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":18367,"to":18378}}}}],[428,{"pageContent":"23. \n24. \no\no \no\no \nt\n1 \nt\n2 \no\no \no\no \n100° \n100° \no\no \n4\n0° \nt3 \nt4 \n100° \n!00° \no\no \n20° \nf \n1 \nt\n2 \nf3 \nt4 \n40° \n100° \n20° \n100° \nIn Exercises 25 and 26, we refine the grids used in Exer­\ncises 22 and 24 to obtain more accurate information about \nth e equilibrium temperatures at interior points of the plates. \nObtain solutions that are accurate to within 0. 001, using \neither th e Jacobi or the Gauss-Seidel method. \n25. \no\no \no\no \nt\n2 \no\no \nt4 \nt\ns \n50 \n50 \n50 \n26. \no\no \no\no \n20° \n20° \no\no \nf \n1 \nt\n2 \nt3 t4 \n20° \no\no \n1\n5 \n1\n6 \nI] lg \n20° \n40° \n19 1\n1\n0 \n1\n1\n1 \n1\n1\n2 \n100° \n40° \nt\n1\n3 t\n1\n4 \nt\n1\n5 \nt\n16 \n100° \n40°  40°  100°  100° \nSection 2.5 \nIterative Methods for Solving Linear Systems \n133 \nExercises 27 and 28 demonstrate that sometimes, if we are \nlucky, th e form of an iterative problem may allow us to use a \nlittle insight to obtain an exact solution. \n27. A narrow strip of paper 1 unit long is placed along a \nnumber line so that its ends are at 0 and 1. The paper \nis folded in half, right end over left, so that its ends \nare now at 0 and i. Next, it is folded in half again, this","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":18380,"to":18493}}}}],[429,{"pageContent":"number line so that its ends are at 0 and 1. The paper \nis folded in half, right end over left, so that its ends \nare now at 0 and i. Next, it is folded in half again, this \ntime left end over right, so that its ends are at ! and \nt\n. \nFigure 2.32 shows this process. We continue folding \nthe paper in half, alternating right-over-left and left­\nover-right.  If we could continue indefinitely, it is clear \nthat the ends of the paper would converge to a point. It \nis this point that we  want to find. \n(a) Let x\n1 \ncorrespond to the left-hand end of the paper \nand x\n2 \nto the right-hand end. Make a   table with the \nfirst six values of [x\n1\n, x\n2\n] and plot the correspond­\ning points on x\n1\n, x\n2 \ncoordinate axes. \n(b) Find two linear equations of the form x\n2 \n= ax\n1 \n+  b \nand x\n1 \n= cx\n2 \n+  d that determine the new values \nof the endpoints at each iteration. Draw the corre­\nsponding lines on your coordinate axes and show \nthat this diagram would result from applying the \nGauss-Seidel method to the system of linear equa­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":18493,"to":18533}}}}],[430,{"pageContent":"of the endpoints at each iteration. Draw the corre­\nsponding lines on your coordinate axes and show \nthat this diagram would result from applying the \nGauss-Seidel method to the system of linear equa­\ntions you have found. (Your diagram should resem -\nble Figure 2.27 on page 126.) \n(c) Switching to  decimal  representation, continue \napplying the Gauss-Seidel method to approximate \nthe point to which the ends of the paper are con­\nverging to within 0.001 accuracy. \n(d) Solve the system of equations exactly and compare \nyour answers. \n28. An ant is standing on a number line at point A. It \nwalks halfway to point B and turns around. Then it \nwalks halfway back to point A, turns around again, \nand walks halfway to point B. It continues to do this \nindefinitely. Let point A be at 0 and point B be at 1. \nThe ant's walk is made up of a sequence of overlap­\nping line segments. Let x\n1 \nrecord the positions of \nthe left-hand endpoints of these segments and x\n2 \ntheir right-hand endpoints. (Thus, we begin with \nx\n1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":18533,"to":18558}}}}],[431,{"pageContent":"ping line segments. Let x\n1 \nrecord the positions of \nthe left-hand endpoints of these segments and x\n2 \ntheir right-hand endpoints. (Thus, we begin with \nx\n1 \n= 0   and x\n2 \n= \nt\n. Then we have x\n1 \n=!and x\n2 \n= \nt, \nand so on.) Figure 2.33 shows the start of the ant's \nwalk. \n(a) Make a   table with the first six values of [x\n1\n, x\n2\n] and \nplot the corresponding points on x\n1\n, x\n2 \ncoordinate \naxes. \n(b) Find two linear equations of the form x\n2 \n= ax\n1 \n+  b \nandx\n1 \n= cx\n2 \n+ dthatdeterminethenewvaluesofthe \nendpoints at each iteration. Draw the corresponding","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":18558,"to":18599}}}}],[432,{"pageContent":"134 \nChapter 2 \nSystems of Linear Equations \n� \nI !\"tf \n0 \nl \n4 2 \n0 \n) \n........----.. \n� \nI ! I \n0 \nl l \n4 2 \n_5 \n0 \nl \n2 \nD \n� \n0 \nl l \n3 \n0 \nl l \n5 \n4 2 4 4 2 \n8 \nFigure 2.32 \nFigure 2.33 \nFolding a strip of paper \nThe ant's walk \nChapter Review \nKev Definitions and concepts \nI \n3 \n4 \n3 \n4 \n3 \n4 \nlines on your coordinate axes and show that this dia­\ngram would result from applying the Gauss-Seidel \nmethod to the system of linear equations you have \nfound. (Your diagram should resemble Figure 2.27 \non page 126.\n) \n(c) Switching to  decimal  representation, continue \napplying the Gauss-Seidel method to approximate \nthe values to which x\n1 \nand x\n2 \nare converging to \nwithin 0.001 accuracy. \n(d) Solve the system of equations exactly and compare \nyour answers. Interpret your results. \naugmented matrix, \nback substitution, \ncoefficient matrix, \n61,64 \n61 \n64 \nconsistent system, 60 \nconvergence, 125-126 \ndivergence, 127 \nelementary row operations, \nfree variable, 71 \nGaussian elimination, 68-69 \nhomogeneous system, 76 \ninconsistent system, 60 \niterate, 125","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":18601,"to":18674}}}}],[433,{"pageContent":"61 \n64 \nconsistent system, 60 \nconvergence, 125-126 \ndivergence, 127 \nelementary row operations, \nfree variable, 71 \nGaussian elimination, 68-69 \nhomogeneous system, 76 \ninconsistent system, 60 \niterate, 125 \npivot, 66 \nrank of a matrix, 72 \nRank Theorem, 72 \nreduced row echelon form, \n73 \nrow echelon form, 65 \nJacobi's method, 124 \n66 \nleading variable (leading 1\n)\n, \n71-73 \nlinear equation, 58 \nrow equivalent matrices, 68 \nspan of a set of vectors, 90 \nspanning set, 90 \nGauss-Jordan elimination, \nGauss-Seidel method, 124 \n73 \nlinearly dependent vectors, 93 \nlinearly independent \nsystem of linear \nvectors, 93 \nReview Questions \n1. Mark each of the following statements true or false: \n(a) Every system oflinear equations has a solution. \n(b) Every homogeneous system oflinear equations \nhas a solution. \n(c) If a system oflinear equations has more vari­\nables than equations, then it has infinitely many \nsolutions. \n(d) If a system oflinear equations has more equations \nthan variables, then it has no solution. \nequations, 59","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":18674,"to":18718}}}}],[434,{"pageContent":"ables than equations, then it has infinitely many \nsolutions. \n(d) If a system oflinear equations has more equations \nthan variables, then it has no solution. \nequations, 59 \n(e) Determining whether bis in span(a\n1\n, ... , a\nn\n) is \nequivalent to determining whether the system \n[A \nI \nb] is consistent, where A = [a\n1 \n... \na\nn\nl\n· \n(f) In IR\n3\n, span( u, v) is always a plane through the origin. \n(\ng\n) In IR\n3\n, if nonzero vectors u and v are not parallel, \nthen they are linearly independent. \n(h) In IR\n3\n, if a set of vectors can be drawn head to tail, \none after the other so that a  closed path (polygon) \nis formed, then the vectors are linearly dependent.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":18718,"to":18751}}}}],[435,{"pageContent":"(i) If a set of vectors has the property that no \ntwo vectors in the set are scalar multiples of \none another, then the set of vectors is linearly \nindependent. \n(j)   If there are more vectors in a set of vectors than \nthe number of entries in each vector, then the set \nof vectors is line arly dependent. \n3 4 \n2. Find thernAA of the ma1'U [ i �; \n_ \n� \n3 2\n] \n-3 2 \n. \n3. Solve the linear system \nx + y-2z = 4 \nx + 3y-z = 7 \n2x + y-Sz = 7 \n4. Solve the linear system \n3w + 8x -18y + z = 35 \nw + 2x \n-\n4y \n= 11 \nw \n+ 3x -\n7y + z = 10 \n5. Solve the linear system \nover 2\n7\n. \n2x + 3y = 4 \nx + 2y = 3 \n6. Solve the linear system \nover 2\n5\n. \n3x + 2y = 1 \nx + 4y = 2 \n7. For what value(s) of k is the linear system with \naugmented matrix \n[ \n� \n2\n2\nk I \n�\n] \ninconsistent? \n6 2 \n8. Find parametric equations for the line of intersection \nof the planes x + 2y + 3z = 4 and Sx + 6y + 7z = 8. \n9. Find the point of intersection of the following lines, if \nit exists. \nChapter Review \n135 \n11. Find the general equation of the plane spanned by \n[\n}ndm","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":18753,"to":18812}}}}],[436,{"pageContent":"9. Find the point of intersection of the following lines, if \nit exists. \nChapter Review \n135 \n11. Find the general equation of the plane spanned by \n[\n}ndm \n12. ?etermine whether \n[ \n�\n]\n, \n[\n-\n�\n]\n, \n[ \n! \n] \nare linearly \nmdependent. \n- 3 \n-\n2 - 2 \n13. Determine whether IR\n3 \n= span(u, v, \nw\n) if: \n(a) u\n� \n[\n}\n� \n[\nHw\n� \n[\n:\n] \n(b) \nu\n� \n[\n-\n}\n� \n[\n-\nn\nw\n� \n[\n-\n:\n] \n14. Let a\n1 , a\n2\n, a\n3 \nbe linearly independent vectors in IR\n3\n, and \nlet A = [ a\n1 \na\n2 \na\n3\n]. Which of the following statements \nare true? \n(a) The reduced row echelon form of A is 1\n3\n. \n(b)  The rank of A is 3. \n(c) The system [A I b] has a unique solution for any \nvector bin IR\n3\n. \n(d) (a), (b), and (c) are all true. \n(e) (a) and (b) are both true, but not (c). \n15. Let a\n1 , a\n2\n, a\n3 \nbe linearly dependent vectors in IR\n3\n, not \nall zero, and let A = [a\n1 \na\n2 \na\n3\n]. What are the possible \nvalues of the rank of A? \n16. What is the maximum rank of a 5 X  3 matrix? What is \nthe minimum rank of a 5 X  3 matrix? \n17. Show that if u and v are linearly independent vectors, \nthen so are u + v and u -v.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":18812,"to":18914}}}}],[437,{"pageContent":"16. What is the maximum rank of a 5 X  3 matrix? What is \nthe minimum rank of a 5 X  3 matrix? \n17. Show that if u and v are linearly independent vectors, \nthen so are u + v and u -v. \n18. Show that span(u, v) = span(u, u + v) for any vectors \nu and v. \n19. In order for a linear system with augmented matrix \n[A I b] to be consistent, what must be true about the \nranks of A and [A I b]? \n20. Areiliematci= \nU \n! \n-\nl\nnd \n[\ni \nO \n-\n:\ni \nrow equivalent? Why or why not?","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":18914,"to":18935}}}}],[438,{"pageContent":"We [Halmos and Kaplansky J share \na philosophy about linear algebra: \nwe think basisjree, we write \nbasis-fr ee, but when the chips are \ndown we close the office door and \ncompute with matrices like fu ry. \n-Irving Kaplansky \nIn Paul Halmos: Celebrating \n50 Ye ars of  Mathematics \nJ. H. Ewing and F. W Gehring, \neds. Springer-Verlag, 1991, p. 88 \n136 \nMatrices \n3.0 \nIntroduction: Matrices in Action \nIn this chapter, we will study matrices in their own right. We  have already used \nmatrices-in the form of augmented matrices-to record information about and to \nhelp streamline calculations involving systems oflinear equations. Now you will see \nthat matrices have algebraic properties of their own, which enable us to calculate \nwith them, subject to the rules of matrix algebra. Furthermore, you will observe that \nmatrices are not static objects, recording information and data; rather, they represent \ncertain types of functions that \"act\" on vectors, transforming them into other vectors.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":18937,"to":18958}}}}],[439,{"pageContent":"matrices are not static objects, recording information and data; rather, they represent \ncertain types of functions that \"act\" on vectors, transforming them into other vectors. \nThese \"matrix transformations\" will begin to play a key role in our study of linear \nalgebra and will shed new light on what you have already learned about vectors and \nsystems of linear equations. Furthermore, matrices arise in many forms other than \naugmented matrices; we will explore some of the many applications of matrices at the \nend of this chapter. \nIn this section, we will consider a few simple examples to illustrate how matri­\nces can transform vectors. In the process, you will get your first glimpse of \"matrix \narithmetic:' \nConsider the equations \nY\n1 \n= X\n1 \n+  2X\nz \nY\nz \n= \n3x\n2 \n(\n1\n) \nWe can view these equations as describing a transformation of the vector x = \n[\n:\n:\n] \ninto the vector y = \n[\n;\n:] . If we denote the matrix of coefficients of the right-hand side \nby F, then F = \n[ \n� \n�\n]\n, and we can rewrite the transformation as","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":18958,"to":18997}}}}],[440,{"pageContent":"[\n:\n:\n] \ninto the vector y = \n[\n;\n:] . If we denote the matrix of coefficients of the right-hand side \nby F, then F = \n[ \n� \n�\n]\n, and we can rewrite the transformation as \nor, more succinctly, y = Fx. [   Think of this expression as analogous to the functional \nnotation y = f\n(\nx\n) \nyou are used to: xis the independent \"variable\" here, y is the depen­\ndent \"variable;' and Fis the name of the \"function:']","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":18997,"to":19017}}}}],[441,{"pageContent":"Section 3.0 \nIntroduction: Matrices in Action \n131 \nThus, if x = \n[ \n-\n�\n]\n, then the Equations \n( \n1\n) \ngive \ny\n1 \n= \n-2 + \n2 · 1  =  0 \n[\no\n3\n] \nor y \n= \ny\n2 \n= 3 · 1  = 3 \nWe can write this expression as \n[ \n�\n] \n= \n[ \n� �\n] \n[ \n-\n�\n]\n. \nProblem 1 Compute Fx for the following vectors x: \n(a) x= [�] (b) x = [ _ �] (c) x=[=�J \nProblem 2 The heads of the four vectors x in Problem 1 locate the four corners \nof a square in the x\n1\nx\n2 \nplane. Draw this square and label its corners A, B, C, and D, \ncorresponding to parts (a), (b), (c), and (d) of Problem 1. \nOn separate coordinate axes (labeled y\n1 \nand y\n2\n)\n, draw the four points determined \nby Fx in Problem 1. Label these points A', B ', C', and D'. Let's make the (reasonable) \nassumption that the line segment AB is transformed into the line segment A' B ', and \nlikewise for the other three sides of the square ABCD. What geometric figure is rep­\nresented by A'B 'C'D'? \nProblem 3 The center of square ABCD is the origin 0 = [ �]. What is the center of","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":19019,"to":19078}}}}],[442,{"pageContent":"likewise for the other three sides of the square ABCD. What geometric figure is rep­\nresented by A'B 'C'D'? \nProblem 3 The center of square ABCD is the origin 0 = [ �]. What is the center of \nA' B'  C' D'? What algebraic calculation confirms this? \nNow consider the equations \nZ1 \n= \nY1  - Yz \nZz \n=  -2y\nl \n(\n2\n) \nthat transform a vector \ny \n= \n[�:\n] \ninto the vector z \n= \n[\n:\n:\nJ\n. We  can abbreviate this \ntransformation as \nz \n= Gy, where \nG=\n[  1 \n-\n1\n] \n-2 \n0 \nProblem 4 We are going to find  out how G transforms the  figure A'B 'C'D'. \nCompute Gy for each of the four vectors y that you computed in Problem 1. [That \nis, compute \nz \n=  G\n(\nFx\n)\n. You may recognize this expression as being analogous to \nthe composition of functions with which you are familiar.]  Call the corresponding \npoints A\n\"\n, B\n\"\n, C\n\"\n, and D\n\"\n, and sketch the figure A\n\"\nB\n\"\nC\n\"\nD\n\" \non z\n1\nz\n2 \ncoordinate axes. \nProblem 5 By substituting Equations \n(\n1\n) \ninto Equations \n(\n2\n)\n, obtain equations for \nz\n1 \nand z\n2 \nin terms of x\n1 \nand x\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":19078,"to":19161}}}}],[443,{"pageContent":"points A\n\"\n, B\n\"\n, C\n\"\n, and D\n\"\n, and sketch the figure A\n\"\nB\n\"\nC\n\"\nD\n\" \non z\n1\nz\n2 \ncoordinate axes. \nProblem 5 By substituting Equations \n(\n1\n) \ninto Equations \n(\n2\n)\n, obtain equations for \nz\n1 \nand z\n2 \nin terms of x\n1 \nand x\n2\n• If we denote the matr  ix of these equations by H, then \nwe \nhave z = Hx. Since we also have z = GFx, it is reasonable to write \nH= GF \nCan you see how the entries of H are related to the entries of F and G? \nProblem 6 Let's do the above process the other way around: First transform the \nsquare ABCD, using G, to obtain figure A\n* \nB\n*\nC\n* \nD\n*\n. Then transform the resulting \nfigure, using F, to obtain A\n** \nB\n**\nC\n**\nD\n**\n. [Note: Don't worry about the \"variables\" x,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":19161,"to":19222}}}}],[444,{"pageContent":"138 \nChapter 3 Matrices \nAlthough numbers will usually be \nchosen from the set� of real num-\nbers, they may also be taken from \nthe set C of complex numbers or \nfrom \"ll_P' where p is prime. \nTechnically, there is a distinction \nbetween row/column matrices \nand vectors, but we will not be­\nlabor this distinction. We will, \nhowever, distinguish between \nrow matrices/vectors and column \nmatrices/vectors. This distinction \nis important-at the very least­\nfor algebraic computations, as we \nwill demonstrate. \ny, and z here. Simply substitute the coordinates of A, B, C, and D into Equations (2) \nand then substitute the results into Equations (\nl\n).] AreA **B**C**D** andA\"B\"C\"D\" \nthe same? What does this tell you about the order in which we perform the transfor­\nmations F and G? \nProblem 1 Repeat Problem 5 with general matrices \nF = \n[\ni\nll \n!\n1\n2\n]\n, \nG \n= \n[\ng\nll \ng\n1\n2\n]\n, \nand  H \n= \n[\nh\nll \nh\n1\n2\n] \n!\n2\n1 \n!\n22 \ng\n2\n1 \ng\n22 \nh\n2\n1 \nh\n22 \nThat is, if Equations \n(\n1\n) \nand Equations (2) have coefficients as specified by F and G,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":19224,"to":19295}}}}],[445,{"pageContent":"F = \n[\ni\nll \n!\n1\n2\n]\n, \nG \n= \n[\ng\nll \ng\n1\n2\n]\n, \nand  H \n= \n[\nh\nll \nh\n1\n2\n] \n!\n2\n1 \n!\n22 \ng\n2\n1 \ng\n22 \nh\n2\n1 \nh\n22 \nThat is, if Equations \n(\n1\n) \nand Equations (2) have coefficients as specified by F and G, \nfind the entries of Hin terms of the entries of F and G. The result will be a formula \nfor the \"product\" H = GF. \nProblem 8 Repeat Problems 1-6 with the following matrices. (Your formula from \nProblem 7 may help to speed up the algebraic calculations.) Note any similarities or \ndifferences that you think are significant. \n(\na\n) \nF = \n[ \n� \n-\n� \nl \nG = \n[ \n� \n�\n] \n(\nc\n)\nF=\n[\n� \n�\n]\n,c=\n[\n_� \n-\n�\n] \nMatrix Operations \n(\nb\n) \nF = \n(\nd\n) \nF = \n�\nl \nG = \n[\n�  �\n] \n-\n2\n] G = \n[\n2 \n4 \n, \n1 \n�\n] \nAlthough we  have  already encountered matrices, we  begin by stating a  formal \ndefinition and recording some facts for future reference. \nDefinition \nA matrix is a rectangular array of numbers called the entries, or \nelements, of the matr  ix. \nThe following are all examples of matrices: \n[\n� \n�\nl \n[\n� \n�l \nU\nl \n[ 51 \n1.2 \n-1 \nl \n-1 \n[ 1 \n1], \n[7] \n1 1 \n6.9 \n0 \n4.4 , \n'TT \n-7.3 \n9 \n8.5","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":19295,"to":19431}}}}],[446,{"pageContent":"elements, of the matr  ix. \nThe following are all examples of matrices: \n[\n� \n�\nl \n[\n� \n�l \nU\nl \n[ 51 \n1.2 \n-1 \nl \n-1 \n[ 1 \n1], \n[7] \n1 1 \n6.9 \n0 \n4.4 , \n'TT \n-7.3 \n9 \n8.5 \nThe size of a matrix is a description of the numbers of rows and columns it has. A \nmatrix is called m X  n (pronounced \n\"\nm by n\"\n) \nif it has m rows and n columns. Thus, \nthe examples above are matrices of sizes 2 X 2, 2 X 3, 3 X 1, 1  X 4, 3 X 3, and 1  X 1, \nrespectively. A 1  X m matrix is called a row matrix (or row vector), and an n  X  1 \nmatrix is called a column matrix (or column vector). \nWe use double-subscript notation to refer to the entries of a matrix A. The entry of \nA in row i and column j is denoted by a\ni\nj\n· Thus, if \nA=\n[\n� \n9 \n5 \nthen a\n1\n3 = -1 and a\n22 \n= 5. (The notation A\nij \nis sometimes used interchangeably with \na\nij\n.) We can therefore compactly denote a matrix A by [a;\nj\n] (or [a;\nj\nl\nm\nxn if it is impor­\ntant to specify the size of A, although the size will usually be clear from the context).","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":19431,"to":19493}}}}],[447,{"pageContent":"Example 3.1 \nExample 3.2 \nSection 3.1 Matrix Operations 139 \nWith this notation, a general m X n matrix A  has the form \n[ \na\n., \na\n1 2 \na\n2\n1 \na\nzz \nA= \na\nm\nl \na\nm\n2 \nIf the columns of A are the vectors a\n1\n, a\n2\n, ... , a\nn\n, then we may represent A as \nA = [ a\n1 \na\n2 \n· · · a\n\"\n] \nIf the rows of A are A\n1\n, A\n2\n, ... , A\nm\n, then we may represent A as \nA\n� \n[]J \nThe \ndiagonal entries of A are a 11, a\nw \na\n33\n, ... , and if m = n   (that is, if A has the same \nnumber of rows as columns), then A is called a square matrix. A square matrix whose \nnondiagonal entries are all zero is called a diagonal matrix. A diagonal matrix all \nof whose diagonal entries are the same is called a scalar matrix. If the scalar on the \ndiagonal is 1, the scalar matrix is called an identity matrix. \nFor example, let \nA= \n[ \n2 \n-1 \n5 \n4 \n0 \n6 \n0 \nThe diagonal entries of A are 2 and 4, but A is not square; B is a square matrix of size \n2  X  2 with diagonal entries 3 and 5; C is a diagonal matrix; Dis a 3 X 3 identity ma­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":19495,"to":19561}}}}],[448,{"pageContent":"A= \n[ \n2 \n-1 \n5 \n4 \n0 \n6 \n0 \nThe diagonal entries of A are 2 and 4, but A is not square; B is a square matrix of size \n2  X  2 with diagonal entries 3 and 5; C is a diagonal matrix; Dis a 3 X 3 identity ma­\ntrix. The n X n identity matrix is denoted by In (or simply I if its size is understood). \nSince we can view matrices as generalizations of vectors (and, indeed, matrices \ncan and should be thought of as being made up of both row and column vectors), \nmany of the conventions and operations for vectors carry through (in an obvious \nway) to matrices. \nTwo matrices are equal if they have the same size and if their corresponding \nentries are equal. Thus, if A  = [a\nij\nl\nm\nxn and B = [b\nij\nlrxs' then A  = B if and only if \nm = rand n = s and a\nij \n= b;\nj \nfor all i and j. \nConsider the matrices \nO\nJ \n[\n2 0     x\n] \n,  and C = \n3 \n5 3 y \nNeither A nor B can be equal to C (no matter what the values of x and \ny), since A and \nB are 2 X 2 matrices and C is 2 X 3. However, A = B if and only if a = 2, b = 0, c = 5, \nandd=3. \n4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":19561,"to":19603}}}}],[449,{"pageContent":"3 \n5 3 y \nNeither A nor B can be equal to C (no matter what the values of x and \ny), since A and \nB are 2 X 2 matrices and C is 2 X 3. However, A = B if and only if a = 2, b = 0, c = 5, \nandd=3. \n4 \nConsider the matrices \nR \n� I 1 4 3]  •nd c \n� \n[r","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":19603,"to":19614}}}}],[450,{"pageContent":"140 \nChapter 3 Matrices \nDespite the fact that R and C have the same entries in the same order, R  of-   C since \nR is 1  X  3 and C is 3  X 1. (If we read R and C aloud, they both sound the same: \n\"one, four, three:') Thus, our distinction between row matrices/vectors and column \nmatrices/vectors is an importa  nt one. \nMatrix Addilion and scalar Multiolicalion \nGeneralizing from vector addition, we define matrix addition componentwise. If A = \n[a;) and B = [b;) are m X  n matrices, their sum A +Bis  the m X  n matrix obtained \nby adding the corresponding entries. Thus, \n[We could equally well have defined A + B in terms of vector addition by specifying \nthat each column (or row) of A +   Bis the sum of the corresponding columns (or \nrows) of A and B.] If A and B are not the same size, then A +   Bis not defined. \nExample 3.3 \nLet \nExample 3.4 \nThen \nA = \n[ \n1 4 \nO\nJ B = \n[\n-3 \n-2 6     5 \n, \n3     0 \nbut neither A + C nor B + C is defined. \n-1\n] \n[\n4     3\n] \n2 \n,  and C = \n2 1 \n-\n�]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":19616,"to":19653}}}}],[451,{"pageContent":"Example 3.3 \nLet \nExample 3.4 \nThen \nA = \n[ \n1 4 \nO\nJ B = \n[\n-3 \n-2 6     5 \n, \n3     0 \nbut neither A + C nor B + C is defined. \n-1\n] \n[\n4     3\n] \n2 \n,  and C = \n2 1 \n-\n�] \nThe componentwise definition of scalar multiplication will come as no  surprise. If \nA is an m X n matrix and c is a scalar, then the scalar multiple cA is the m X n matrix \nobtained by multiplying each entry of A by c. More formally, we have \n[In terms of vectors, we could equivalently stipulate that each column (or row) of \ncA is c times the corresponding column (or row) of A.] \nFor matrix A in Example 3.3, \n[ \n2 8 \n2A = \n-4 12 \n-4 \n-6 \n-\n�\nJ \n4 \nThe matrix (-l)A is written as -A and called the negative of A. As with vectors, we \ncan use this fact to define the difference of two matrices: If A and B are the same size, \nthen \nA - B =A+ \n(\n-B\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":19653,"to":19700}}}}],[452,{"pageContent":"Example 3.5 \nMathematicians are sometimes like \nLewis Carroll's Humpty Dumpty: \n\"When I use a word;' Humpty \nDumpty said, \"it means just what \nI choose it to mean-neither more \nnor less\" (from Through the Look­\ning Glass). \nSection 3.1 Matrix Operations \n141 \nFor matrices A and B in Example 3.3, \nA-B=\n[ \nl \n-2 \n4 \n6 \n1 \n0 \n-\n�\n] \n3 \n6 \n�\n] \nA matrix all of whose entries are zero is called a zero matrix and denoted by 0 (or \nO\nm\nx\nn \nif it is important to specify its size). It should be clear that if A is any matrix and \n0 is the zero matrix of the same size, then \nA+O=A=O+A \nand \nA  -A= 0 =-A+ A \nMalrix Multiplication \nThe Introduction in Section 3.0 suggested that there is a \"product\" of matrices that is \nanalogous to the composition of functions. We now make this notion more precise. \nThe definition we are about to give generalizes what you should have discovered in \nProblems 5 and 7 in Section 3.0. Unlike the definitions of matrix addition and scalar","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":19702,"to":19742}}}}],[453,{"pageContent":"The definition we are about to give generalizes what you should have discovered in \nProblems 5 and 7 in Section 3.0. Unlike the definitions of matrix addition and scalar \nmultiplication, the definition of the product of two matrices is not a componentwise \ndefinition. Of course, there is nothing to stop us from defining a product of matrices \nin a componentwise fashion; unfortunately such a definition has few applications and \nis not as \"natural\" as the one we now give. \nDefinition \nIf A is an m x n matrix and Bis an n x r matrix, then the product \nC  =  AB is an m  X  r matrix. The (i, j) entry of the prod uct is computed as \nfollows: \nRemarks \n• \nNotice that A and B need not be the same size. However, the number of col­\numns of A must be the same as the number of rows of B. If we write the sizes of A, B, \nand AB in order, we can see at a glance whether this requirement is satisfied. More­\nover, we can predict the size of the product before doing any calculations, since the","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":19742,"to":19757}}}}],[454,{"pageContent":"and AB in order, we can see at a glance whether this requirement is satisfied. More­\nover, we can predict the size of the product before doing any calculations, since the \nnumber of rows of AB is the same as the number of rows of A, while the number of \ncolumns of AB is the same as the number of columns of B, as sh  own below: \nA B AB \nmXn nXr \nmXr \nSize of AB","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":19757,"to":19764}}}}],[455,{"pageContent":"142 \nChapter 3 Matrices \nExample 3.6 \n• \nThe formula for the entries of the product looks like a dot product, and indeed \nit is. It says that the (i, j) entry of the matrix AB is the dot product of the ith row of A \nand the jth column of B: \na\n11 \na\n1\n2 \na\nln \n[\nb\n\" \nb\nl\n) \nb\n,\n,\n: \nb\n2\n1 \nb\n2\nj \nb\n2\nr \na\ni1 \na\ni\n2 \na\nin \nb\nnl \nb\nn\nj \nb\nnr \na\nm\nl \na\nm\n2 \na\nm\nn \nNotice that, in the expression c\ni\nJ = a\ni\n1 \nb\n11 + a\ni\n2\nb\n2\n1 + \n·  ·  · \n+ a\ni\nn\nb\nn\n)\n' \nthe \"outer subscripts\" \non each ab term in the sum are always i and j whereas the \"inner subscripts\" always \nagree and increase from 1 to n. We see this pattern clearly if we write c\ni\nJ \nusing sum­\nmation notation: \nCompute AB if \nA= \n[ \n1 \n-2 \n3 \n-1 \nn \nC\ni)\n= \n�\na\nik\nb\nk\nj \nk\n�\nI \n-\n�\n] \nand B \n= \n[\n-\n: \n-\n� \n-1 \n2 \n3 \n-1 \n0 \nSolulion Since A is 2  X  3 and B is 3  X  4, the prod uct AB is defined and will be a \n2 \nX  4 matrix. The first row of the product C  =  AB is computed by taking the dot \nproduct of the first row of A with each of the columns of B in turn. Thus, \nC\n11 \n=  1\n(\n-4\n)  +  3\n(\n5\n) +  (\n-1\n)(\n-1\n) \n= 12 \nC\n1\n2 \n=  1\n(\n0\n) +  3\n(\n-2\n)  +  (\n-1\n)(\n2\n) \n= \n-8 \nC\n1\n3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":19766,"to":19920}}}}],[456,{"pageContent":"product of the first row of A with each of the columns of B in turn. Thus, \nC\n11 \n=  1\n(\n-4\n)  +  3\n(\n5\n) +  (\n-1\n)(\n-1\n) \n= 12 \nC\n1\n2 \n=  1\n(\n0\n) +  3\n(\n-2\n)  +  (\n-1\n)(\n2\n) \n= \n-8 \nC\n1\n3 \n=  1\n(\n3\n) +  3\n(\n-1\n)  +  (\n-1\n)(\n0\n) \n=  0 \nC\n1\n4=l\n(\n-1\n)+\n3\n(\n1\n) +(-1\n)(\n6\n) \n=-4 \nThe second row of C is computed by taking the dot product of the second row of A \nwith each of the columns of B in turn: \nCz\n1 \n= \n(\n-2\n)(\n-4\n)  + \n(\n-1\n)(\n5\n) +  (\n1\n)(\n-1\n) \n=  2 \nC\n22 \n= \n( \n- 2\n) ( \n0\n) \n+  ( \n-1\n) ( \n- 2\n)  +  (\n1\n) ( \n2\n) \n= \n4 \nCz\n3 \n= \n( \n- 2\n) ( \n3\n) \n+ \n( \n-1\n) ( \n- 1\n)  +  (\n1\n) ( \n0\n) \n= \n- 5 \nc\n2\n4\n=(\n-2\n)(\n-l)+(\n-l\n)(\nl\n) +  (\n1\n)(\n6\n) =\n7 \nThus, the product matrix is given by \n[\n12 \nAB= \n2 \n-8 \n0 \n4 -5 \n-\n�] \n(With a little practice, you should be able to do these calculations mentally without \nwriting out all of the details as we have done here. For more complicated examples, a \ncalculator with matrix capabilities or a computer algebra system is preferable.) \n4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":19920,"to":20068}}}}],[457,{"pageContent":"Example 3.1 \nTable 3.1 \nSection 3.1 Matrix Operations \n143 \nBefore we go further, we will consider two  examples that justify our chosen \ndefinition of matrix multiplication. \nAnn and Bert are planning to go shopping for fruit for the next week. They each want \nto buy some apples, oranges, and grapefruit, but in differing amounts. Table 3.1 lists \nwhat they intend to buy. There are two fruit markets nearby-Sam's and Theo's-and \ntheir prices are given in Table 3.2. How much will it cost Ann and Bert to do their \nshopping at each of the two markets? \nTable 3.2 \nA\npples \nGrapefruit \nOranges \nSam's \nTheo's \nAnn \nBert \nTable 3.3 \nAnn \nBert \n6 \n4 \nSam's \n$2.80 \n$4.10 \nTheo's \n$3.80 \n$4.00 \n3 \n8 \n10 \nApple \n$0.10 \n5 \nGrapefruit \n$0.40 \nOrange \n$0.10 \nSolution \nIf Ann shops at Sam's, she will spend \n6 (0.10\n) \n+  3\n( 0.40\n) \n+ 10\n( 0.10\n) \n= $2.80 \nIf she shops at Theo's, she will spend \n6(0.15)  +  3(0.30\n) \n+ 10\n( 0.20\n) \n= $3.80 \nBert will spend \n4 (0.10\n) \n+  8 (0.40\n) \n+  5\n( 0.10) = $4.10 \nat Sam's and \n4(0.15)  +  8(0.30\n) \n+  5(0.20\n) \n= $4.00","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":20070,"to":20141}}}}],[458,{"pageContent":") \n= $2.80 \nIf she shops at Theo's, she will spend \n6(0.15)  +  3(0.30\n) \n+ 10\n( 0.20\n) \n= $3.80 \nBert will spend \n4 (0.10\n) \n+  8 (0.40\n) \n+  5\n( 0.10) = $4.10 \nat Sam's and \n4(0.15)  +  8(0.30\n) \n+  5(0.20\n) \n= $4.00 \nat Theo's. (Presumably, Ann will shop at Sam's while Bert goes to Theo's.) \n$0.15 \n$0.30 \n$0.20 \nThe \"dot product form\" of these calculations suggests that matrix multiplication \nis at work here. If we organize the given information into a demand matrix D and a \nprice matrix P, we   have \nD = \n[\n6     3 \n4     8 \n[0.10 0.15\n] \nand  P = 0.40 0.30 \n0.10 0.20 \nThe calculations above are equivalent to computing the product \n[\n6     3 \nDP = \n4     8 \n] \n[\n0.\n10 \n0.\n15\n] \n[ \n] \n10 \n-\n2.80 3.80 \n0.40 0.30 \n-\n5 4.10 4.00 \n0.10 0.20 \nThus, the product matr  ix DP tells us how much each person's purchases will cost at \neach store (Table 3.3\n)\n.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":20141,"to":20202}}}}],[459,{"pageContent":"144 \nChapter 3 Matrices \nExample 3.8 \nTheorem 3.1 \nConsider the linear system \nX\n1 \n-  2X\n2 \n+ 3X\n3 \n= \n5 \n- X\n1 \n+ 3X\n2 \n+ \nX\n3 \n= 1 \n2x\n1 \n-x\n2 \n+ 4x\n3 \n= 14 \nObserve that the left-hand side arises from the matrix product \nso the system ( 1\n) \ncan be written as \n(\n1\n) \nor Ax = b, where A is the coefficient matr  ix, xis the  (column) vector of variables, and \nb is the (column) vector of constant terms. \nYou should have no difficulty seeing that every linear system can be written in the \nform Ax = b. In fact, the notation [A I b] for the augmented matr  ix of a linear system \nis just shorthand for the matrix equation Ax = b. This form will prove to be a tre­\nmendously useful way of expressing a system of linear equations, and we will exploit \nit often from here on. \nCombining this insight with Theorem 2.4, we see that Ax = b has a solution if \nand only ifb is a linear combination of the columns of A. \nThere is another fact about matrix opera  tions that will also prove to be quite use­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":20204,"to":20248}}}}],[460,{"pageContent":"and only ifb is a linear combination of the columns of A. \nThere is another fact about matrix opera  tions that will also prove to be quite use­\nful: Multiplication of a matrix by a standard unit vector can be used to \"pick out\" or \n[\n4     2 l\n] \n\"reproduce\" a column or row of a matrix. Let A = \nand consider the \n0     5 -1 \nproducts Ae\n3 \nand e\n2\nA, with the unit vectors e\n3 \nand e\n2 \nchosen so that the products \nmake sense. Thus, \n2 \n5 \n[ \n_ \n�\n] \nand e\n2\nA = [ 0 \nl] \n[ � \n= \n[\nO     5 -1] \n2 \n5 \n-\n�\nJ \nNotice that Ae\n3 \ngives us the third column of A and e\n2\nA gives us the second row of A. \nWe record the general result as a theorem. \nLet A be an m X n matrix, e; a 1  X m standard unit vector, and e\nj \nan n X  1 standard \nunit vector. Then \na. e; A is the  ith row of A and \nb. Ae\nj \nis the  jth column of A.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":20248,"to":20299}}}}],[461,{"pageContent":"Section 3.1 Matrix Operations \n145 \nProof We prove (b) and leave proving (a) as Exercise 41. If a\n1 ,  ... , a\nn \nare the columns \nof A, then the product Ae\nj \ncan be written \nAe\nj \n= Oa\n1 \n+ Oa\n2 \n+ ·   ·   · + la\nj \n+ ·   ·   · + Oa\nn \n=  a\nj \nWe could also prove (b) by direct calculation: \n[ a\n11 \na\n2\n1 \nAe. = \n} \n. \na\nm\nl \nsince the 1 in e\nj \nis the jth entry. \nPartitioned Matrices \n0 \n0 \nIt will often be convenient to regard a matrix as being composed of a number of \nsmaller submatrices. By introducing vertical and  horizontal lines into a matrix, we \ncan partition it into blocks. There is a natural way to partition many matrices, par­\nticularly those arising in certain applications. For example, consider the matr  ix \nA= \n1 0 \n0 1 \n0     0 \n0     0 \n0     2 \n0 \n1 4 \n0 1 \n-1 \n3 \n0 \n7 \n0     0     0     7 2 \nIt seems natural to partition A as \n1 0     0 \n! \n2 -1 \n0 1 0 i 1 3 \n-�-----�-----\n�\n-\nL�--------�\n-\no 0     0 \n! \n1 7 \n0     0     0!   7 \n2 \n[\n� �\n]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":20301,"to":20375}}}}],[462,{"pageContent":"0     0 \n0     0 \n0     2 \n0 \n1 4 \n0 1 \n-1 \n3 \n0 \n7 \n0     0     0     7 2 \nIt seems natural to partition A as \n1 0     0 \n! \n2 -1 \n0 1 0 i 1 3 \n-�-----�-----\n�\n-\nL�--------�\n-\no 0     0 \n! \n1 7 \n0     0     0!   7 \n2 \n[\n� �\n] \nwhere I is the 3 X 3    identity matrix, B is 3 X 2, 0 is the 2 X 3   zero matrix, and C is 2 X 2. \nIn this way, we can view A as a 2 X 2 matrix whose entries are themselves matrices. \nWhen matrices are being multiplied, there is often an advantage to be gained by \nviewing them as partitioned matrices. Not only does this frequently reveal underly­\ning structures, but it often speeds up computation, especially when the matrices are \nlarge and have many blocks of zer  os. It turns out that the multiplication of partitioned \nmatrices is just like ordinary matrix multiplication. \nWe begin by considering some special cases of partitioned matrices. Ea ch gives \nrise to a different way of viewing the prod uct of two matrices.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":20375,"to":20412}}}}],[463,{"pageContent":"matrices is just like ordinary matrix multiplication. \nWe begin by considering some special cases of partitioned matrices. Ea ch gives \nrise to a different way of viewing the prod uct of two matrices. \nSuppose A is m X n and B is n X r, so the product AB exists. If we pa  rtition B in \nterms of its column vectors, as B = [b 1 \n: b\n2 \n: \n. . . \n: b\nr\n], then","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":20412,"to":20423}}}}],[464,{"pageContent":"146 \nChapter 3 Matrices \nExample 3.9 \nExample 3.10 \nThis result is an immediate consequence of the definition of matr  ix multiplication. \nThe form on the right is called the matrix-column representation of the product. \nIf \nA=[\n� \n3 \n-\n1 \n�\n] \n[ \n4 \n-\n1 \ni \nand  B = �   � \nthen \nAb = [\n1 \nI \nQ \n3 \n:t\n] \n� \n[\n'\n�\n] \n-\n1 \nand  Ab\n2 \n= [\n� \n3 \n:\nJ\n[\n-\n�\n] \n� \n[ \n_\n�\n] \n-\n1 \n[\n1\n3 \n: 5\n] \nTherefore,AB = [Ab\n1 \n: Ah\n2\n] = . . (Checkby ordinarymatrix multiplication.) \n2:  -2 \n� \nRemark Observe that  the matrix-column representation of AB allows us to \nwrite each column of AB as a linear combination of the columns of A with entries \nfrom B as the coefficients. For example, \n(See Exercises 23 and 26.\n) \n3 \n-\n1 \nSuppose A is m X n and B is n X r, so the prod uct AB exists. If we partition A in \nterms of its row vectors, as \nthen \nOnce again, this result is a direct consequence of the  definition of matrix multiplication. \nThe form on the right is called the row-matrix representation of the product.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":20425,"to":20502}}}}],[465,{"pageContent":"then \nOnce again, this result is a direct consequence of the  definition of matrix multiplication. \nThe form on the right is called the row-matrix representation of the product. \nUsing the row-matrix representation, compute AB for the matrices in Example 3.9.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":20502,"to":20505}}}}],[466,{"pageContent":"Example 3.11 \nSolution We compute \nA\n,\nB \n� \n[1 3 2]\n[\n: \n-\n�] \n� \n[13     5 \nSection 3.1 Matrix Operations \n141 \nand A\n2\nB = \n[ 0     -1 \n[\n4 -1\n] \nl] \n�   � \n[2 \n-2] \nTherefore AB = \n[\n-\n�\n��\n-\n] \n= \n[\n-\n�\n-\n�\n--------\n�\n-\n] \nas before. \n' \nA\n2\nB 2 -2 \n' \nThe definition of the matr  ix prod uct AB uses the natural partition of A into rows \nand B into columns; this form might well be called the ro w-column representation of \nthe product. We can also partition A into columns and B into rows; this form is called \nthe column-row representation of the product. \nIn this case, we have \nso \n(\n2\n) \nNotice that the sum resembles a dot product expansion; the difference is that the in­\ndividual terms are matrices, not scalars. Let's make sure that this makes sense. Each \nterm a;B; is the product of an m X  1   and a 1 X r matrix. Thus, each a;B; is an m X r \nmatrix-the same size as AB. The products a;B; are called outer products, and \n(\n2\n) \nis \ncalled the outer product expansion of AB.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":20507,"to":20573}}}}],[467,{"pageContent":"matrix-the same size as AB. The products a;B; are called outer products, and \n(\n2\n) \nis \ncalled the outer product expansion of AB. \nCompute the outer prod uct expansion of AB for the matrices in Example 3.9. \nSolution We have \nThe outer products are \nand","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":20573,"to":20582}}}}],[468,{"pageContent":"148 \nChapter 3 Matrices \nExample 3.12 \n(Observe that computing each outer product is exactly like filling in a multiplication \ntable.) Therefore, the outer product expansion of AB is \n-1\n] \n+ \n[ \n3 \n0 \n-1 \n-\n�\nJ \n+ \n[\n: \n0\n] \n= \n[ \n13 \n5\n] \n= AB \n0 2 -2 4 \nWe will make use of the outer product expansion in Chapters 5 and 7 when we \ndiscuss the Spectral Theorem and the singular value decomposition, respectively. \nEach of the foregoing partitions is a special case of partitioning in general. A ma­\ntrix A is said to be partitioned if horizontal and vertical lines have been introduced, \nsubdividing A into submatrices called blocks. Partitioning allows A to be written as a \nmatrix whose entries are its blocks. \nFor example, \n1 0     0 \n! \n2 -1 \n0 0 i 1 3 \nA= 0     0 l i 4 0 \n________________ .. ____________ _ \n0     0     0 1 7 \n0     0     0     7 2 \n4 3 \n! \n1 2 \n-1 2 \n! \n2 1 \nand \nB \n= \n---------\n=�-_!_} ___ \n} \n__ :_}_ \n1 \n0 \n! \n0     0 \ni \n2 \n0 \n1 \ni \n0     0 \ni \n3 \nare partitioned matrices. They have the block structures \n[\nB\nll \nand B  = \nB\n2\n1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":20584,"to":20658}}}}],[469,{"pageContent":"0     0     0     7 2 \n4 3 \n! \n1 2 \n-1 2 \n! \n2 1 \nand \nB \n= \n---------\n=�-_!_} ___ \n} \n__ :_}_ \n1 \n0 \n! \n0     0 \ni \n2 \n0 \n1 \ni \n0     0 \ni \n3 \nare partitioned matrices. They have the block structures \n[\nB\nll \nand B  = \nB\n2\n1 \nIf two matrices are the same size and have been partitioned in the same way, it is clear \nthat they can be added and multiplied by scalars block by block. Less obvious is the \nfact that, with suitable partitioning, matrices can be multiplied blockwise as well. The \nnext example illustrates this process. \nConsider the matrices A and B above. If we ignore for the moment the fact that their \nentries are matrices, then A appears to be a 2 X 2 matrix and B a 2 X 3 matrix. Their \nproduct should thus be a 2 X 3 matrix given by \nAB= \n[ \nA\n11 \nA\n1\n2\n]\n[\nB\n11 \nB\n1\n2 \nB\n13\n] \nA\n2\n1 \nA\n22 \nB\n2\n1 \nB\n22 \nB\n2\n3 \n[ \nA\n11\nB\n11 \n+ A\n,\n2\nB\n2\n1 \nA\n11\nB\n1\n2 \n+ \nA\n1\n2\nB\n22 \nA\nll\nB\n13 \n+ A\n,\n2\nB\n2\n3\n] \nA\n2\n1\nB\n11 \n+ \nA\n22\nB\n2\n1 \nA\n2\n1\nB\n1\n2 \n+ \nA\n22\nB\n22 \nA\n2\n1\nB\n13 \n+ \nA\n22\nB\n2\n3 \nBut all of the products in this calculation are actually matrix products, so we   need to","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":20658,"to":20795}}}}],[470,{"pageContent":"2\n1 \nB\n22 \nB\n2\n3 \n[ \nA\n11\nB\n11 \n+ A\n,\n2\nB\n2\n1 \nA\n11\nB\n1\n2 \n+ \nA\n1\n2\nB\n22 \nA\nll\nB\n13 \n+ A\n,\n2\nB\n2\n3\n] \nA\n2\n1\nB\n11 \n+ \nA\n22\nB\n2\n1 \nA\n2\n1\nB\n1\n2 \n+ \nA\n22\nB\n22 \nA\n2\n1\nB\n13 \n+ \nA\n22\nB\n2\n3 \nBut all of the products in this calculation are actually matrix products, so we   need to \nmake sure that they are all defined. A quick check reveals that this is indeed the case, \nsince the numbers of columns in the blocks of A \n(\n3 and 2\n) \nmatch the numbers of rows \nin the blocks of B. The matrices A and B are said to be partitioned conformably for \nblock multiplication. \nCarrying out the calculations indicated gives us the product AB in partitioned form: \nA\n,,\nB\n,, \n+ A \n,\n,\nB\n,\n, \n� \nI\n,\nB\n,, \n+A \n, ,\nI\n, \n� B\n,, \n+ A\n\" � [-: \n�\ni \n+ [\n� \n-5 \n4 \n-1] \n[\n6 \n3     =     0 \n0 5","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":20795,"to":20911}}}}],[471,{"pageContent":"Section 3.1 Matrix Operations \n149 \n(When some of the blocks are zero matrices or identity matrices, as is the case here, \nthese calculations can be done quite quickly.) The calculations for the other five \nblocks of AB are similar. Check that the result is \n6 \n2\n:\n1 2 2 \n0 5 i 2 1 12 \n5  -5 i 3  3 \n' \n9 \n---------\n7-r-0----o-T-i3 \n' ' \n7 \n2 i 0     0 i 20 \n� \n(Observe that the block in the upper-left corner is the result of our calculations above.) \nCheck that you obtain the same answer by multiplying A by B in the usual way. \nExample 3.13 \nMatrix Powers \nWhen A and B are two n X n matrices, their product AB will also be an n X n matrix. \nA special case occurs when A = B. It makes sense to define A \n2 \n= AA and, in general, \nto define A \nk \nas \nA\nk \n=AA··· A \n� \nk factors \nif k is a positive integer. Thus, A \n1 \n= A, and it is convenient to define A \n0 \n= Iw \nBefore making too many assumptions, we should ask ourselves to what extent \nmatr  ix powers behave like powers of real numbers. The following properties follow","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":20913,"to":20954}}}}],[472,{"pageContent":"0 \n= Iw \nBefore making too many assumptions, we should ask ourselves to what extent \nmatr  ix powers behave like powers of real numbers. The following properties follow \nimmediately from the definitions we have just given and are the matr  ix analogues of \nthe corresponding properties for powers of real numbers. \nIf A is a square matrix and r and s are nonnegative integers, then \n1. A\nr\nAs = A\nr\n+s \n2. (A\nr\n)s  =A'\" \nIn Section 3.3, we will extend the definition and properties to include negative integer \npowers. \n(a) IfA =\n[\n�  �\n]\n,then \nA\n2 \n= \n[\n�  �\n]\n[\n� \n�\n] \n[\n� \nand, in general, \n2\n] \nA\n3 \n=A\n2\nA= \n[\n2     2\n]\n[\n1 \n2 \n, \n2     2 1 \n2\nn\n-\nl\n] \nforall n 2:  1 \n2\nn\n-\n1 \n�\n] \n[\n! \n:\nJ \nThe  above statement can be proved by mathematical induction, since it is an \ninfinite collection of statements, one for each natural number n. (Appendix B   gives a","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":20954,"to":21021}}}}],[473,{"pageContent":"150 \nChapter 3 Matrices \nbrief review of mathematical induction.) The basis step is to  prove that the formula \nholds for n = 1. In this case, \nas required. \n[\n2\n1\n-\n1 \nA\n1\n= \n2\n1\n-\n1 \n2\n1\n-\nJ\nJ \n2\n1\n-\n1 \n[\n� \n�\nJ \n=A \nThe induction hypothesis is to  assume that \nfor some integer k 2: 1. The induction step is to  prove that the formula holds for n = \nk + 1. Using the definition of matrix powers and the induction hypo  thesis, we compute \n[\n2\n(\nk\n+J\n)\n-\nJ \n2\n(\nk\n+J\n)\n-\n1 \n�\nJ \nThus, the formula holds for all n 2: 1 by the principle of mathematical induction. \n(b) If B = \n[\n0\n1 \nwe find \nand \nthen  B\n2 \n= = \n-\n1\nJ \n[\no  -\n1\nJ\n[\no  -\n1\nJ \n[\n-\n1 \no\n' \n1 \no \n1 \no o \nB\n3 \n= B\n2\nB = \n[\n-\n1 \no\nJ\n[\no  -\n1\nJ \n0 \n-\n1  1 \n0 \n[ \n_ \n� \n�\nJ \nB\n4 \n= B\n3\nB = \n[ \n0 \nl\nJ \n[\nO \n- l\nJ = \n[\nl \nO\nJ \n-\n1  0  1   0     0  1 \n0\nJ. Continuing, \n-\n1 \nThus, B\n5 \n= B, and the sequence of powers of B repeats in a cycle of four: \n[\no -1\nJ \n[\n-1 o\nJ \n[ \no \nl\nJ \n[\n1  o\nJ \n[\no -1\nJ \n1 \n0 \n' \n0 \n-\n1 \n' \n-\n1  0 \n' \n0  1 \n' \n1 \n0 \n' \nThe Transpose of  a Matrix \nThus far, all of the matrix operations we have defined are analogous to operations on","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":21023,"to":21185}}}}],[474,{"pageContent":"[\no -1\nJ \n[\n-1 o\nJ \n[ \no \nl\nJ \n[\n1  o\nJ \n[\no -1\nJ \n1 \n0 \n' \n0 \n-\n1 \n' \n-\n1  0 \n' \n0  1 \n' \n1 \n0 \n' \nThe Transpose of  a Matrix \nThus far, all of the matrix operations we have defined are analogous to operations on \nreal numbers, although they may not always behave in the sa  me way. The next opera­\ntion has no such analogue.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":21185,"to":21219}}}}],[475,{"pageContent":"Section 3.1 Matrix Operations \n151 \nDefinition The transpose of an m x n matrix A   is the n x m matrix A\nT \nobtained by interchanging the rows and columns of A. That is, the ith column of \nA\nT \nis the ith row of A for all i. \nExample 3.14 \nLet \nA = \n[ \n� \n� \n� \nl \nB = \n[\n: � \nl \nand C = [ 5 \n-1 2] \nThen their transposes are \nThe transpose is sometimes used to give an alternative definition of the dot prod­\nuct of two vectors in terms of matrix multiplication. If \nthen \nu . . .    u l [ �: j \n2 \nn \n. \nv\nn \nA useful alternative definition of the transpose is given componentwise: \n(\nA \nT)\ni\nj \n= A\nj\ni \nfor all i and \nj \nIn words, the entry in row i and column j of A\nT \nis the same as the entry in row j and \ncolumn i of A. \nThe transpose is also used to define a very important type of square matrix: a \nsymmetric matrix. \nDefinition A square matrix A is sy mmetric if A\nT\n= A-that is, if A is equal to \nits own transpose. \nExample 3.15 \nLet \nand \nB \n= \n[ \n1 \n2\n] \n-1  3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":21221,"to":21283}}}}],[476,{"pageContent":"152 \nChapter 3 Matrices \nThen A is symmetric, since A\nT \n= A; but Bis not symmetric, since B\nT \n= [ \n� \n-\n�\n] * B. \n4 \nFigure 3.1 \nA symmetric matrix \nA symmetric matrix has the property that it  is its own \"mirror image\" across its \nmain diagonal. Figure 3.1 illustrates this property for a 3 X 3 matr  ix. The correspond­\ning shapes represent equal entries; the diagonal entries (those on the dashed line) are \narbitrary. \nA componentwise definition of a symmetric matrix is also useful. It is simply the \nalgebraic description of the \"reflection\" property. \nA square matr  ix A   is symmetric if and only if A\nij \n= A\nj\ni \nfor all i and j. \n.. \nI \nExercises 3.1 \nLet \nA= [ \n_\n� \n�\nl B = [\n� \n-2 \n2 \nc � \n[\n� :J \n[ \n0 -3\n] \nD  = \n_\n2 l \n, E  = \n(\n4 \n2], F=[\n-\n�\n] \nIn Exercises 1-16, compute the indicated matrices (if \npossible). \n1. A + 2D \n2. 3D -  2A \n3.B  -C \n4. c  -B\nT \n5.AB \n6.BD \n7. D +BC \n8.BB\nT \n9. E\n(\nAF\n) \n10. F(DF) \n11. FE \n12.EF \n13. B\nT\nC\nT \n-\n(\nCB\n)T \n14. DA  - AD \n15. A\n3 \n16. U\n2 \n-D)\n2 \n17. Give an example of a nonzero 2 X 2 matrix A  such \nthat A\n2 \n= 0.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":21285,"to":21376}}}}],[477,{"pageContent":"3.B  -C \n4. c  -B\nT \n5.AB \n6.BD \n7. D +BC \n8.BB\nT \n9. E\n(\nAF\n) \n10. F(DF) \n11. FE \n12.EF \n13. B\nT\nC\nT \n-\n(\nCB\n)T \n14. DA  - AD \n15. A\n3 \n16. U\n2 \n-D)\n2 \n17. Give an example of a nonzero 2 X 2 matrix A  such \nthat A\n2 \n= 0. \n18. Let A = [ � \n�\n]\n. Find 2 X 2 matrices Band C such \nthat AB = ACbut B *-C. \n19. A factory manufactures three products (doohickies, \ngizmos, and widgets) and ships them to two ware­\nhouses for storage. The  number of units of each prod­\nuct shipped to each warehouse is given by the matrix \n[200 \nA= 150 \n100 \n75\n] \n100 \n125 \n(where a\nij \nis the number of units of product i sent to \nwarehouse j and the products are taken in alphabetical \norder). The cost of shipping one unit of each product \nby truck is $1.50 per doohickey, $1.00 per gizmo, and \n$2.00 per widget. The corresponding unit costs to ship \nby train are $1.75, $1.50, and $1.00. Organize these \ncosts into a matrix B and then use matrix multiplica­\ntion to show how the factory can compare the cost of \nshipping its prod ucts to each of the two warehouses by","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":21376,"to":21436}}}}],[478,{"pageContent":"costs into a matrix B and then use matrix multiplica­\ntion to show how the factory can compare the cost of \nshipping its prod ucts to each of the two warehouses by \ntruck and by train. \n20. Referring to Exercise 19, suppose that the unit cost \nof distributing the products to stores is the same for \neach product but varies by warehouse because of the \ndistances involved. It costs $0.75 to distribute one unit \nfrom warehouse 1 and $1.00 to distribute one unit \nfrom warehouse 2. Organize these costs into a matr  ix \nC and then use matrix multiplication to compute the \ntotal cost of distributing each product.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":21436,"to":21447}}}}],[479,{"pageContent":"In Exercises 21-22, write the given system of linear equa­\ntions as a matrix equation of th e form Ax = b. \n21. X\n1 \n- 2X\n2 \n+ 3X\n3 \n= 0 \n2x\n1 \n+    x\n2 \n-5x\n3 \n= 4 \n22. -X\n1 \n+ 2X\n3 \n= 1 \nX\n1 \n- X\nz \n=  -2 \nX\nz \n+    x\n3 \n= -1 \nIn Exercises 23-28, let \nA= \nH \nand B= \nu \n0 \n-\n�\ni \n0 \n-1 \n3 \n:\nJ \n-1 \n6 \n23. Use the matrix-column representation of the product \nto write each column of AB as a linear combination of \nthe columns of A. \n24. Use the row-matrix representation of the product to \nwrite each row of AB as a linear combination of the \nrows of B. \n25. Compute the outer product expansion of AB. \n26. Use the matrix-column representation of the product \nto write each column of BA as a linear combination of \nthe columns of B. \n27. Use the row-matrix representation of the product to \nwrite each row of BA as a linear combination of the \nrows of A. \n28. Compute the outer product expansion of BA. \nIn Exercises 29 and 30, assume that th e product AB makes \nsense. \n29. Prove that if the columns of B are linearly dependent,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":21449,"to":21512}}}}],[480,{"pageContent":"rows of A. \n28. Compute the outer product expansion of BA. \nIn Exercises 29 and 30, assume that th e product AB makes \nsense. \n29. Prove that if the columns of B are linearly dependent, \nthen so are the columns of AB. \n30. Prove that if the rows of A are linearly dependent, then \nso are the rows of AB. \nIn Exercises 31-34, compute AB by block multiplication, \nusing th e indicated partitioning. \n[ \n2  3   : \no\nl \n1 -1:   0     0 \n: \n: \n-1 1\n: \n0 \n31.\nA\n�\n[\n�\nH\ni\n�\nJ \nn� \noof1 \n0     0\n: \n1 \nSection 3.1 Matrix Operations 153 \n[\n2  3 \n32.\nA \n= \n4 \n5 \n[ \no \n: \n1 \no\nl \n0 \n�\n]\n. \nB \n� \n+!t-� \n-2 \n: \n3  2 \n33.\nA \n� \n[\n! \n�\nL\n! \nJ· \n34\n· \nA = \n[\nL:\nj\nH\n]\n' \nB = \n0     0     Oi   4 \n35. \nLet A = \n[ \nO 1\n]\n. \n-1 1 \n(a)  Compute A\n2\n, A\n3\n, .•. , A\n7\n• \n(b) What is \nA\n2\n0\n1\n5\n? Why? \n36. Let B = \n[ \n0 \n-\n0\n]\n. \nFind, with justification, \nB\n2\n0\n1\n5\n. \nv2 \nv2 \n37. Let A = [ \n� \n�\n]\n. Find a formula for A\nn \n(\nn 2 1\n) \nand \nverify your formula using mathematical induction. \n[\ncostJ \n38. LetA = \nsintJ \n-sine\n]\n. \ncostJ \n[\ncosW \n(a) Showthat A\n2\n= \n. \nsmW \n-sinW\n]\n. \ncosW \n(b) Prove, by mathematical induction, that \nn \n[\ncosntJ  -sinntJ\n] \nc \nA \n=","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":21512,"to":21664}}}}],[481,{"pageContent":"verify your formula using mathematical induction. \n[\ncostJ \n38. LetA = \nsintJ \n-sine\n]\n. \ncostJ \n[\ncosW \n(a) Showthat A\n2\n= \n. \nsmW \n-sinW\n]\n. \ncosW \n(b) Prove, by mathematical induction, that \nn \n[\ncosntJ  -sinntJ\n] \nc \nA \n= \n1or n 2 1 \nsin ntJ \ncos ntJ \n39. In each of the following, find the 4 X 4 matrix A = [ a;\nj\n] \nthat satisfies the given condition: \n(a) a\ni\nj \n= (-l)\ni\n+j \n(b)  au= \nj \n-i \n(c) a\ni\nj \n= (i -lY \n(\n(i \n+ \nj \n-\n1 )\n7T) \n(d) aiJ = sin \n4 \n40. In each of the following, find the 6 X 6 matrix A = [a\nij\n] \nthat satisfies the given condition: \n{ \ni  + \nj \nif i ::; \nj \n{ 1 \n(a) a\ni\nj \n= \n0 \nif \ni \n> \nj \n(b)   a\ni\nj \n= \n0 \n{ 1 if 6 ::; i  + \nj \n::; 8 \n(c) a = \n11 \n0 otherwise \n41. Prove Theorem 3.l(a). \nif \nI\ni \n-\nj\nl \n::::: \n1 \nif \nI\ni \n-\nj\nl \n>  1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":21664,"to":21766}}}}],[482,{"pageContent":"154 \nChapter 3 Matrices \nTheorem 3.2 \nMatrix Algebra \nIn some ways, the arithmetic of matrices generalizes that of vectors. We do not expect \nany surprises with respect to addition and scalar multiplication, and indeed there are \nnone. This will allow us to extend to matrices several concepts that we are already \nfamiliar with from our work with vectors. In particular, linear combinations, span­\nning sets, and linear independence carry over to matrices with no difficulty. \nHowever, matrices have other operations, such as matrix multiplication, that ve c­\ntors do not possess. We should not expect matr  ix multiplication to behave like multi­\nplication of real numbers unless we can prove that it does; in fact, it does not. In this \nsection, we summarize and prove some of the main properties of matrix opera  tions \nand begin to develop an algebra of matrices. \nProoerlies of  Addilion and scalar Mulliolicalion \nAll of the algebraic properties of addition and scalar multiplication for vectors","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":21768,"to":21783}}}}],[483,{"pageContent":"and begin to develop an algebra of matrices. \nProoerlies of  Addilion and scalar Mulliolicalion \nAll of the algebraic properties of addition and scalar multiplication for vectors \n(Theorem 1.1\n) \ncarry over to matrices. For completeness, we summarize these proper­\nties in the next theorem. \nAlgebraic Properties of Matrix Addition and Scalar Multiplication \nLet A, B, and C be matrices of the same size and let c and d be scalars. Then \na. A+ B \n= \nB +A \nb. (A + B) + C = A  + (B + C) \nCommutativity \nAssociativity \nc. A+ 0 = A \nd. A+ (-A)= 0 \ne.  c (A + B) = cA +  cB \nf. (c + d)A  = cA +  dA \ng. c(dA) = (cd)A \nh. IA= A \nDistributivity \nDistributivity \nThe proofs of these properties are direct analogues of the corresponding proofs \nof the vector properties and are left as exercises. Likewise, the comments following \nTheorem 1.1 are equally valid here, and you should have no difficulty using these \nproperties to perform algebraic manipulations with matrices. (Review Example 1.5","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":21783,"to":21809}}}}],[484,{"pageContent":"Theorem 1.1 are equally valid here, and you should have no difficulty using these \nproperties to perform algebraic manipulations with matrices. (Review Example 1.5 \nand see Exercises 17 and 18 at the end of this section.) \nThe associativity property allows us to unambiguously combine scalar multiplica­\ntion and addition without parentheses. If A, B, and Care matrices of the  same size, then \n(\n2A + 3B\n) \n-C = 2A + \n(\n3B -C\n) \nand so we can simply write 2A + 3B -C. Generally, then, if A\n1\n, A\n2\n, •.. , Ak are matri­\nces of   the same size and c \n1\n, c\n2\n, ... , c\nk \nare scalars, we  may form the linear combination \nc\n1\nA\n1 \n+ c\n2\nA\n2 \n+ \n· · · \n+ c\nk\nA\nk \nWe will refer to c\n1\n, c\n2\n, ... , c\nk \nas the coefficients of the linear combination. We can now \nask and answer questions about linear combinations of matrices.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":21809,"to":21854}}}}],[485,{"pageContent":"Example 3.16 \nSection 3.2 Matrix Algebra \n155 \nLetA\n1 \n= \n[ \n_\n� \n�l\nA\n2 \n= \n[\n� \n�l\nandA\n3 \n= \n[\n�  �\n]. \n(a) Is B = \n[\n� \n�\n] \nalinear combinationofA\n1\n,A\n2\n,andA\n3\n? \n(b) Is C = \n[\n� \n!\n] \na linear combination of A\n1\n, A\n2\n, and A\n3\n? \nSolution \n(a)  We want to find scalars c\n1\n, c\n2\n, and c\n3 \nsuch that c\n1\nA\n1 \n+ c\n2\nA\n2 \n+ c\n3\nA\n3 \n= B. Thus, \nThe left-hand side of this equation can be rewritten as \nComparing entries and using the definition of matrix equality, we have four linear \nequations: \nC\n2 \n+ c\n3 \n= 1 \nC\n1 \n+  c\n3 \n= 4 \n-c\nl \n+  c\n3 \n= 2 \nC\n2 \n+ C\n3 \n= 1 \nGauss-Jordan elimination easily gives \n[ \n� \n� \n� \n:j\n----+ \n[\n� \n0  � \n-\n�\nj \n-1 0  1 2 \n0  0  1 3 \n0 1 \n0  0  0   0 \n� \n(check this!), so c\n1 \n= 1, c\n2 \n=  -2, and c\n3 \n= 3. Thus, A\n1 \n- 2A\n2 \n+ 3A\n3 \n= B,   which can \nbe easily checked. \n(b) This time we want to solve \nProceeding as in part (a), we obtain the linear system \nc\n2 \n+ C\n3 \n= 1 \nC\n1 \n+  c\n3 \n= 2 \n-c\n1 \n+  c\n3 \n= 3 \nC\n2 \n+ C\n3 \n= \n4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":21856,"to":21999}}}}],[486,{"pageContent":"156 \nChapter 3 Matrices \nExample 3.11 \nRow reduction gives \nWe need go no further: The last row implies that there is no  solution. Therefore, in \nthis case, C is not a linear combination of Ai, A\n2\n, and A\n3\n• \nRemark Observe that the  columns of the augmented matrix contain the entries \nof the matrices we are given. If we read the entries of each matrix from left to right \nand top to bottom, we get the order in which the entries appear in the columns of \nthe augmented matrix. For example, we read Ai as \n\"\nO, 1, -1, O,\" which corresponds \nto the first column of the augmented matrix. It is as  if we simply \"straightened out\" \nthe given matrices into column vectors. Thus, we would have ended up with exactly \nthe same system of linear equations as in part (a) if we had asked \n{] a linemombillation of [ -l}[ � \nl \nand [ '.} \nWe  will encounter such parallels repeatedly from now on. In Chapter 6, we will \nexplore them in more detail.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":22001,"to":22024}}}}],[487,{"pageContent":"{] a linemombillation of [ -l}[ � \nl \nand [ '.} \nWe  will encounter such parallels repeatedly from now on. In Chapter 6, we will \nexplore them in more detail. \nWe can define the sp an of a set of matrices to be the set of   all linear combinations \nof the matrices. \nDescribe the span of the matrices Ai, A\n2\n, and A\n3 \nin Example 3.16. \nSolulion One way to do this is simply to write out a general linear combination of \nAi, A\n2\n, and A\n3\n• Thus, \n�\n] \n(which is analogous to the parametric representation of a plane). But suppose we \nwant to know when the matrix[; :J is in span(Ai, A\n2\n, A\n3\n). From the representa­\ntion above, we know that it is when \nfor some choice of scalars Ci, c\n2\n, c\n3\n. This gives rise to a system of linear equations \nwhose left-hand side is exactly the same as in  Example 3.16 but whose right-hand side","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":22024,"to":22056}}}}],[488,{"pageContent":"Section 3.2 Matrix Algebra \n151 \nis general. The augmented matrix of this system is \nand row reduction produces \n[\n-\nl \n� \n: f\nl \n� \n[\n� \n0 \n0 \n0 \n0 \n!\nx \n-\nt\nJ \nl \n0 -\n!\nx \n-\nt\nJ \n+ w \n1 \n!\nx + \nh \n0 \nw-z \n__. \n(Check this carefully.) The only restriction comes from the last row, where clearly we \nmust have w -  z = 0 in order to have a solution. Thus, the span of A \n1\n, A\n2\n, and A\n3 \ncon-\nExample 3.18 \nsists of all matrices \n[\n; \n: \n] \nfor which w = z. That is, span (A \n1\n, A\n2\n, A\n3\n) = \n{ \n[\n; \n: ] \n} \n. \n4 \nNole Ifwe had known this before attempting Example 3.16, we would have seen \nimmediately that B = \n[\n� \n�\n] is a linear combination of A\n[\n1\n,\n1\nA\n2\n,\n2 \na\n]\nnd A\n3\n, since it has \nthe necessary form (take w = 1, x = 4, and y = 2), but C = \n3 4 \ncannot be a linear \ncombination of A\n1\n, A\n2\n, and A\n3\n, since it does not have the proper form (1 i= 4). \nLinear independence also  makes  sense for matrices.  We  say  that  matrices \nA\n1\n, A\n2\n, ••• , Ak   of the same size are linearly independent if the only solution of the \nequation \n(1) \nis the  trivial one: c\n1 \n= c\n2 \n= \n· · · \n=","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":22058,"to":22166}}}}],[489,{"pageContent":"A\n1\n, A\n2\n, ••• , Ak   of the same size are linearly independent if the only solution of the \nequation \n(1) \nis the  trivial one: c\n1 \n= c\n2 \n= \n· · · \n= \nck \n= 0. If there are nontrivial coefficients that satisfy \n(1), then A\n1\n, A\n2\n, •.. , Ak  are called linearly dependent. \nDetermine whether the  matrices A\n1\n,  A\n2\n,  and  A\n3 \nin  Example 3.16 are  linearly \nindependent. \nSolulion We want to solve the equation c\n1\nA\n1 \n+ c\n2\nA\n2 \n+ c\n3\nA\n3 \n=  0. Writing out the \nmatrices, we have \nThis time we get a homogeneous linear system whose left-hand side is the same as \nin Examples 3.16 and 3.17. (Are you starting to spot a pattern yet?) The augmented \nmatrix row reduces to give","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":22166,"to":22211}}}}],[490,{"pageContent":"158 \nChapter 3 Matrices \nExample 3.19 \nTheorem 3.3 \nThus, c\n1 \n=  c\n2 \n=  c\n3 \n= 0, and we conclude that the matrices A\n1 , A\n2\n, and A\n3 \nare linearly \nindependent. \n4 \nProoenies of  Malrix Mulliolicalion \nWhenever we encounter a new operation, such as matrix multiplication, we must \nbe careful not to assume too much about it. It would be nice if matrix multiplication \nbehaved like multiplication of real numbers. Although in many respects it does, there \nare some significant differences. \nConsider the matrices \nA  = \n[ \n_ � \n_ \n�\n] \nand B \n= \n[ \n� \n�\n] \nMultiplying gives \nAB = \n[ \n_ � _ \n�\n] \n[ \n� \n�\n] \n[ \n_\n: \n_ \n�\n] \nand BA = \n[ \n� \n�\n] \n[ \n_ � \n_ \n�\n] \n[\n� \n�\n] \nThus, AB \n-=fa \nBA. So, in contrast to multiplication of real numbers, matrix multiplica­\ntion is not commut\na\ntive-the order of the factors in a prod uct matters! \nIt  is easy to check that A\n2 \n= \n[\n� �\n] \n(do  so!). So, for matrices, the  equation \nA\n2 \n= 0 does not imply that A  = 0 (unlike the situation for real numbers, where the \nequation x\n2 \n= 0 has only x = 0 as a solution).","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":22213,"to":22297}}}}],[491,{"pageContent":"2 \n= \n[\n� �\n] \n(do  so!). So, for matrices, the  equation \nA\n2 \n= 0 does not imply that A  = 0 (unlike the situation for real numbers, where the \nequation x\n2 \n= 0 has only x = 0 as a solution). \nHowever gloomy things might appear after the last example, the situation is not \nreally bad at all-you just need to get used to working with matrices and to constantly \nremind yourself that they are not numbers. The next theorem summarizes the main \nproperties of matrix multiplication. \nProperties of Matrix Multiplication \nLet A, B, and C be matrices (whose sizes are such that the indicated operations can \nbe performed) and let k be a   scalar. Then \na. A\n(\nB\nC\n)  =  (\nAB\n)\nC \nb. A\n(\nB +  C\n)  =\nAB +AC \nc. \n(\nA  + B )\nC =AC+ B C \nd. k(AB) = (kA)B = A(kB) \ne. I\nm\nA = A  = Al\nn \nif A is m X n \nAssociativity \nLeft distributivity \nRight distributivity \nMultiplicative identity \nProof We prove (b)  and half of (e).  We defer the proof of property  (a)  until \nSection 3.6. The remaining properties are considered in the exercises.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":22297,"to":22344}}}}],[492,{"pageContent":"Example 3.20 \nTheorem 3.4 \nSection 3.2 Matrix Algebra \n159 \n(b) To prove A(B + C) =AB +  AC, we let the rows of A be denoted by A; and the \ncolumns of B and C by b\nj \nand c\nf \nThen the jth column of B + C is b\nj \n+ c\nj \n(since addi­\ntion is defined componentwise), and thus \n[A\n(\nB +  C\n)\nJ u = A; · \n(\nb\nj \n+ c) \n= A; \n. \nb\nj \n+ A; \n. \nCJ \n= \n(\nAB\n)\n;\n1 + \n(\nAC\n)\n;1 \n= \n(\nAB + AC\n)\n;\nj \nSince this is true for all i and j, we must have A(B + C) = AB + AC. \n(e) To proveAi\nn \n=A, we note that the  identity matr  ix I\nn \ncan be  column-partitioned as \nwhere e; is  a standard unit vector. Therefore, \nAi\nn \n=  [  Ae\n1\n: \nAe\n2\n: \n•   •   ·\n: \nAe\nnl \n=  [ a\n1 \n: \na\nz \n: \n·   ·   · \n: \na\nn\n] \n=A \nby Theorem 3 .1 (b). \nWe can use these properties to further explore how closely matrix multiplication \nresembles multiplication of real numbers. \nIf A and B are square matrices of the same size, is (A + B)\n2 \n= A \n2 \n+ 2AB + B\n2\n? \nSolution \nUsing properties of matrix multiplication, we compute \n(\nA + B\n)\n2 \n= \n(\nA + B\n)(\nA + B\n) \n= \n(\nA + B\n)\nA + \n(\nA + B\n)\nB \n= A \n2 \n+ BA  + AB + B\n2 \nby left distributivity","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":22346,"to":22459}}}}],[493,{"pageContent":"2 \n= A \n2 \n+ 2AB + B\n2\n? \nSolution \nUsing properties of matrix multiplication, we compute \n(\nA + B\n)\n2 \n= \n(\nA + B\n)(\nA + B\n) \n= \n(\nA + B\n)\nA + \n(\nA + B\n)\nB \n= A \n2 \n+ BA  + AB + B\n2 \nby left distributivity \nby right distributivity \nTherefore, (A + B)\n2 \n= A\n2 \n+ 2AB + B\n2 \nif and only if A\n2 \n+ BA  +AB + B\n2 \n= A\n2 \n+ \n2AB + B\n2\n. Subtracting A \n2 \nand B\n2 \nfrom both sides gives BA + AB = 2AB. Subtracting \nAB from both sides gives BA = AB. Thus, (A + B) \n2 \n= A \n2 \n+ 2AB + B\n2 \nif and only if A \nand B commute. (Can you give an example of such a pair of matrices? Can you find \ntwo matrices that do not satisfy this property?) \nProperties of  the Transpose \nProperties of the Transpose \nLet A and B be matrices (whose sizes are such that the indicated operations can be \nperformed) and let k be a scalar. Then \na.  (A\nT\n)\nT \n=A \nb. (A + B)\nT \n=A\nT\n+ B\nT \nc.  (kAl = k(A\nT\n) \nd.  (AB)\nT \n= B\nT\nA\nT \ne.  (A\nr\nl \n= (A\nT\n) \nr \nfor all nonnegative integers r","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":22459,"to":22552}}}}],[494,{"pageContent":"160 \nChapter 3 Matrices \nExample 3.21 \nProof Properties (a) -(c) are intuitively clear and straightforward to prove (see Exercise \n30\n)\n. Proving property ( e) is a good exercise in mathematical induction (see Exercise 31\n)\n. \nWe will prove (d), since it is not what you might have expected. [Would you have sus­\npected that (AB)\nT \n= A\nT\nB\nT \nmight be true?] \nFirst, if A is m X n and Bis n X r, then B\nT \nis r X n and A\nT \nis n X m. Thus, the product \nB\nT \nA\nT \nis defined and is r X m. Since AB is m X r, (AB) \nT\nis r X m, and so (AB) \nT \nand B\nT \nA\nT \nhave the same size. We must now prove that their corresponding entries are equal. \nWe denote the ith row of a matrix X by row;(X) and its jth column by col/X). \nUsing these conventions, we see that \n[ \n(\nAB\n)T\n] ;\nj \n= \n(\nAB\n)\nj\ni \n= row/A\n) \n· col\n;\n(\nB\n) \n= col/A\nT) \n· row\n;\n(\nB\nT) \n= row\n;\n(\nB\nT)\n·col\n/A\nT) \n= \n[B\nT\nA\nT\n]\nij \n(Note that we have used the definition of matrix multiplication, the definition of the \ntranspose, and the fact that the dot product is commutative.) Since i and j are arbi­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":22554,"to":22633}}}}],[495,{"pageContent":";\n(\nB\nT)\n·col\n/A\nT) \n= \n[B\nT\nA\nT\n]\nij \n(Note that we have used the definition of matrix multiplication, the definition of the \ntranspose, and the fact that the dot product is commutative.) Since i and j are arbi­\ntrary, this result implies that (AB) \nT \n= B\nT \nA\nT\n. \nRemark Properties (b) and (d) of Theorem 3.4 can be generalized to sums and \nproducts of finitely many matrices: \n(\nA\n1 \n+ A\n2 \n+ \n· · · \n+ A\nk\nf \n= A[ + Af \n+ \n· · ·\n+A\n[ \nand \n(\nA\n1\nA\n2 \n• \n· · A\nk\nf \n=A\n[\n-\n· ·A\nfA[ \nassuming that the sizes of the matrices are such that all of the operations can be per­\nformed. You are asked to prove these facts by mathematical induction in Exercises 32 \nand 33. \nLet \nThen A\nT \n= \n[\n� \nWe have \nso \nand \nA = \n[\n� \n!\n] \nand  B = \n[� \n-1 \n�\n] \n3 \n3\n] \nso A + A\nT \n= \n[ \n2 \n4 ' \n5 \n: \n] \n, a  symmetric matrix. \nB\nT\n= \nH �l \nBB\nT \n= \n[� \n-1 \n�\ni\n[\n-\nr n \n� \n[\n': \nl\n:\nJ \n3 \nH :\nJ\n[� \n�\ni \n� \nn \n2 \n�\n] \nB\nT\nB = \n-1 \n10 \n3 \n3 \nThus, both BB\nT \nand B\nT\nB are sy  mmetric, even though Bis not  even square! (Check \nthat AA \nT \nand A\nT \nA are also symmetric.)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":22633,"to":22770}}}}],[496,{"pageContent":"Theorem 3.5 \nI \nExercises 3.2 \nSection 3.2 Matrix Algebra \n161 \nThe next theorem says that the results of Example 3.21 are true in general. \na.  If A is a square matrix, then A + A\nT \nis a symmetric matrix. \nb.  For any matrix A, AA \nT \nand A\nT \nA are symmetric matrices. \nProof \nWe prove (a) and leave proving (b) as Exercise 34. We simply check that \n(\nA + A Tf \n= A\nT \n+ \n(\nA \nTf \n= A\nT \n+ A = A + A\nT \n(using properties of the transpose and the commutativity of matrix addition). Thus, \nA + A\nT \nis equal to its own transpose and so, by definition, is symmetric. \nIn Exercises 1-4, solve th e equation for X, given that \nA = \n[ \n� \n! \n] \nand \nB = \n[ \n-\n� \n�\n]\n. \n1. X \n-\n2A +  3B = 0 \n2. 2X =A \n-\nB \n3. 2\n(\nA +   2B\n) \n=  3X \n4. 2\n(\nA - B + X\n) \n= 3\n(\nX - A\n) \nIn Exercises 5-8, write B as a linear combination of the \nother matrices, if possible. \n5. B = \n[\n� \n:\nl \nA\n1 \n= \n[ \n_\n� \n�\nl \n6. B = \n[ \n2 \n-4 \nA\n3 \n= \n[\n� \n7. B \n= \n[\n� \n[\n-1 \nA  = \n2 \n0 \n8. B \n� \n[\n: \nA, � \n[\n: \n�\nl \n�\n] \nA  = \n[\nl \nI \nQ \n�l \n�\nl \nA \n= \n[\nl \nI \nQ \n0 \n1 \n2 \n�\nl \nA\n3 \n= \n[\n� \n-2 \n-\nn \nA\n,�\n[\n� \n0 \n0 \n1 \nn \nn \n0 \nA\n3\n= \n0 \nA\nz\n= \n[\n� \n�\n] \nA\nz\n= \n[\n� \n-\n1\n] \n0 \n, \n-1\n] \n0 \n, \n1 \n�\n] \n0 \n0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":22772,"to":22947}}}}],[497,{"pageContent":"A\n1 \n= \n[ \n_\n� \n�\nl \n6. B = \n[ \n2 \n-4 \nA\n3 \n= \n[\n� \n7. B \n= \n[\n� \n[\n-1 \nA  = \n2 \n0 \n8. B \n� \n[\n: \nA, � \n[\n: \n�\nl \n�\n] \nA  = \n[\nl \nI \nQ \n�l \n�\nl \nA \n= \n[\nl \nI \nQ \n0 \n1 \n2 \n�\nl \nA\n3 \n= \n[\n� \n-2 \n-\nn \nA\n,�\n[\n� \n0 \n0 \n1 \nn \nn \n0 \nA\n3\n= \n0 \nA\nz\n= \n[\n� \n�\n] \nA\nz\n= \n[\n� \n-\n1\n] \n0 \n, \n-1\n] \n0 \n, \n1 \n�\n] \n0 \n0 \nn \n1 \n0 \n0 \n-1 \ni \n0   , \n0 \n-1 \nIn Exercises 9-12, find th e general form of the span of th e \nindicated matrices, as in Example 3.17. \n9. span(A\n1\n, A\n2\n) in Exercise 5 \n10. span(A\n1\n, A\n2\n, A\n3\n) in Exercise 6 \n11. span(A\n1\n, A\n2\n, A\n3\n) in Exercise 7 \n12. span(A\n1\n, A\n2\n, A\n3\n, A\n4\n) in Exercise 8 \nIn Exercises 13-16, determine whether the given matrices \nare linearly independent. \n13. \n[\n� \n!\nl \n[\n� \n�\n] \n14. \n[ \n� \n� \nl \n[ \n-\n� � \nl \n[ \n�  �\n] \n15. \n[ \n� \n�\n]\n, \n[\n� \n�\n]\n, \n[\n-\n� \n-\n�\ni\n, \n[\n-\n� \n-\n!\n] \n-1 0 1   1 0 2 4    5 \nI \n•. \n[\n� \n-\n� \nm\n: \n! \nm\n� \n: \nn \n[\n-\n� \n-1 \n�\ni \n0 \n0 -4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":22947,"to":23158}}}}],[498,{"pageContent":"162 \nChapter 3 Matrices \n17. Prove Theorem 3.2(a)-(d). \n18. Prove Theorem 3.2(e)-(h). \n19. Prove Theorem 3.3(c). \n20. Prove Theorem 3.3(d). \n21. Prove the half of Theorem 3.3(e) that was not proved \nin the text. \n22. Prove that, for square matrices A and B, AB = BA if \nand only if (A - B)(A + B) = A\n2 \n-  B\n2\n• \nIn Exercises 23-25, if B = \n[\n: \n�\n]\n,find conditions on a, b, \nc, and d such that AB = BA. \n23. A = \n[\n� \n1\n] \n24. A = \n[ \nl \n1 \n-1 \n-\n1\n] \n[\n1 2\n] \n25.\nA \n= \n1 \n3  4 \n26. Find conditions on a, b, c, and d such that B = \n[\n1  o\no\n] \nand \n[\no\no \ncommutes with both \n0 \n27. Find conditions on a, b, c, and d such that B = \n[\na b\n] \ncommutes with every 2 X 2 matrix. \nc d \n28. Prove that if AB and BA are both defined, then AB and \nBA are both square matrices. \nA square matrix is called upper triangular if all of th e en­\ntries below the main diagonal are zero. Thus, th e form of an \nupper triangular matrix is \n* * \n* * \n0 \n* * * \n0   0 \n* \n* \n0   0 \n0 \n* \nwhere th e entries marked \n* \nare arbitrary. A more formal \ndefinition of such a matrix A = \n[ a\ni\nj","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":23160,"to":23240}}}}],[499,{"pageContent":"upper triangular matrix is \n* * \n* * \n0 \n* * * \n0   0 \n* \n* \n0   0 \n0 \n* \nwhere th e entries marked \n* \nare arbitrary. A more formal \ndefinition of such a matrix A = \n[ a\ni\nj\n]  is that a\ni\nj \n= 0 if i > j. \n29. Prove that the product of two upper triangular n X n \nmatrices is upper triangular. \n30. Prove Theorem 3.4(a)-(c). \n31. Prove Theorem 3.4(e). \n32. Using induction, prove that for all n 2: 1, \n(A\n1 \n+ A\n2 \n+ · · · + A\nn\n)\nr \n= A\nf \n+ Af + \n· · · +A�. \n33. Using induction, prove that for all n 2: 1, \n(A\n1 \nA\n1\n· · · A\nn\n)\nT \n= A�· · ·  AfA\nf\n. \n34. Prove Theorem 3.S(b). \n35. (a) Prove that if A and Bare symmetric n X n matrices, \nthen so is A +   B. \n(b) Prove that if A is a symmetric n X n matrix, then \nso is kA  for any scalar k. \n36. (a) Give an example to show that if A and B are \nsymmetric n X n matrices, then AB need not be \nsymmetric. \n(b) Prove that if A and B are symmetric n X n matrices, \nthen AB is symmetric if and only if AB = BA. \nA square matrix is called skew-symmetric if A\nr \n= -  A.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":23240,"to":23303}}}}],[500,{"pageContent":"symmetric. \n(b) Prove that if A and B are symmetric n X n matrices, \nthen AB is symmetric if and only if AB = BA. \nA square matrix is called skew-symmetric if A\nr \n= -  A. \n37. Which of the following matrices are skew-symmetric? \n(a) \n[ \n_\n� \n�\n] \n(b) \n[\n� \n-\n�\n] \n(<) \n[\n-\n: -� \n-\n�] (d) \n[\n-\n: � �] \n38. Give a componentwise definition of a skew-symmetric \nmatr  ix. \n39. Prove that the main diagonal of a skew-symmetric ma­\ntrix must consist entirely of zeros. \n40. Prove that if A and Bare sk  ew-symmetric n X n \nmatrices, then so is A +   B. \n41. If A and B are sk  ew-symmetric 2 X 2 matrices, under \nwhat conditions is AB skew-symmetric? \n42. Prove that if A is an n X n matrix, then A - A\nr \nis \nskew-symmetric. \n43. (a) Prove that any square matrix A can be written \nas the sum of a symmetric matrix and a skew­\nsy\nmmetric matrix. [Hint: Consider Theorem 3.5 \nand Exercise 42.] \n(b) Illu,tmte pact (o) fo, the nrnt'ix A \n� \n[ \n� \n2 \n5 \n8 \nThe trace of an n X n matrix A = [ a\nij\n]  is the sum of th e en­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":23303,"to":23357}}}}],[501,{"pageContent":"sy\nmmetric matrix. [Hint: Consider Theorem 3.5 \nand Exercise 42.] \n(b) Illu,tmte pact (o) fo, the nrnt'ix A \n� \n[ \n� \n2 \n5 \n8 \nThe trace of an n X n matrix A = [ a\nij\n]  is the sum of th e en­\ntries on its main diagonal and is denoted by tr(A). That is, \ntr\n(\nA\n) \n= a\n11 \n+ a\n22 \n+  · · · \n+ a\n\"\" \n44. If A and B are n X n matrices, prove the following \nproperties of the trace: \n(a) tr(A + B) = tr(A) + tr(B) \n(b) tr(kA)  = ktr(A  ), where k is a scalar \n45. Prove that if A and B are n X n matrices, then \ntr(AB) = tr(BA). \n46. If A is any matrix, to what is tr  (AA \nT\n) equal? \n47. Show that there are no 2 X 2 matrices A and B such \nthat AB - BA = 1\n2\n•","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":23357,"to":23394}}}}],[502,{"pageContent":"The Inverse of a  Matrix \nSection 3.3 \nThe Inverse of a Matrix \n163 \nIn this section, we return to the matrix description Ax= b of a system of linear equa­\ntions and look for ways to use matrix algebra to solve the system. By way of analogy, \nconsider the equation ax =  b, where a, b, and x represent real numbers and we want \nto solve for x. We can quickly figure out that we want x = b/a as the solution, but we \nmust remind ourselves that this is true only if a *  0. Proc eeding more slowly, assum­\ning that a * 0, we will reach the solution by the following sequence of steps: \n1 1 (1 ) b b b \nax\n=  b =:>\n-\n(\nax\n)  = \n-\n(\nb\n) =:> \n-\n(\na\n) \nx  =  -\n=:>  l \n· \nx  =  -\n=:> x  = \n-\na a a a a a \n(This example shows how much we do in   our head and how many properties of arith­\nmetic and algebra we take for granted!) \nTo imitate this procedure for the matrix equation Ax=  b, what do we need? We \nneed \nto  find a matrix A' (analogous to  1 /a) \nsuch that A' A  = I, an  identity matrix","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":23396,"to":23433}}}}],[503,{"pageContent":"To imitate this procedure for the matrix equation Ax=  b, what do we need? We \nneed \nto  find a matrix A' (analogous to  1 /a) \nsuch that A' A  = I, an  identity matrix \n(analogous to 1). If such a matrix exists (analogous to the requirement that a  *  O\n)\n, \nthen we can do the following sequence of calculations: \nAx=  b =:> A'(Ax\n)  = A'b =:> (A'A\n)\nx = A'b =:>Ix  = A'b =:> x = A'b \n� \n(Why would each of these steps be justified?) \nOur goal in this section is to  determine precisely when we can find such a matrix \nA'. In fact, we are going to insist on a bit  more: We want not only A' A  = I but also \n� \nAA'  = I. This requirement forces A and A' to be square matrices. (Why?) \nExample 3.22 \nExample 3.23 \nDefinition If A is an n x n matrix, an inverse of A is an n x n matr  ix A' with \nthe property that \nAA'  =  I and A' A  =  I \nwhere I =  I\nn \nis the n X n identity matr  ix. If such an A' exists, then A is called \ninvertible. \nIf A  = \n[\n� \n5\n] , \n[ \n3 \n, then A = \n3 \n-1 \nAA ,  = \n[ \n2 \n5\n] \n[ \n3 \n-\n5\n] \n1 3 -1 2 \n[\n� \n-\n5\n] \n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":23433,"to":23486}}}}],[504,{"pageContent":"AA'  =  I and A' A  =  I \nwhere I =  I\nn \nis the n X n identity matr  ix. If such an A' exists, then A is called \ninvertible. \nIf A  = \n[\n� \n5\n] , \n[ \n3 \n, then A = \n3 \n-1 \nAA ,  = \n[ \n2 \n5\n] \n[ \n3 \n-\n5\n] \n1 3 -1 2 \n[\n� \n-\n5\n] \n2 \nis an inverse of A, since \no\n]\na\nn\nd\nA'A  =\n[ \n3 \n-\n5\n]\n[\n2 \n5\n] \n1 -1 2 1 3 \nShow that the  following matrices are not invertible: \n(a) 0 = \n[\n� �\n] \n(b) B = \n[\n� \n�\n] \nSolution \n(a) It is easy to see that the zero matr  ix 0 does not have an inverse. If it did, then there \nwould be a matrix O' such that 00' =I= O'O. But the product of the zero matrix \nwith any other matrix is the zero matrix, and so 00' could never equal the identity","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":23486,"to":23548}}}}],[505,{"pageContent":"164 \nChapter 3 Matrices \nTheorem 3.6 \nmatr  ix I. (Notice that this proof makes no reference to the size of the matrices and so \nis true for n X n matrices in general.) \n(b) Suppose B has an inverse B' = \n[\n; \n: \n] \n. The  equation BB' = I gives \n[\n� \n!\n][; \n:\nJ \n= \n[\n� \n�\n] \nfrom which we get the equations \nw \n+ 2y \n= 1 \nx + 2z = 0 \n2w \n+ 4y \n=O \n2x + 4z = 1 \nSubtracting twice the first equation from the third yields 0 = -2, which is clearly \nabsurd. Thus, there is no solution. (Row reduction gives the same result but is not \nreally needed here.) We deduce that no such matr  ix B' exists; that is, Bis not invert­\nible. (In fact, it does not even have an inverse that works on one side, let alone two!) \n4 \nRemarks \n• \nEven though we have seen that matr  ix multiplication is not, in general, com-\nmutative, A' (if it exists) must satisfy A' A = AA'. \n• \nThe examples above raise two questions, which we will answer in this section: \n(\n1\n) \nHow can we know when a matrix has an inverse? \n(\n2\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":23550,"to":23598}}}}],[506,{"pageContent":"mutative, A' (if it exists) must satisfy A' A = AA'. \n• \nThe examples above raise two questions, which we will answer in this section: \n(\n1\n) \nHow can we know when a matrix has an inverse? \n(\n2\n) \nIf a matr  ix does have an inverse, how can we find it? \n• \nWe have not ruled out the possibility that a  matr  ix A might have more than \none inverse. The next theorem assures us that this cannot happen. \nIf A is an invertible matrix, then its inverse is unique. \nProof In mathematics, a standard way to show that there is just one of something is \nto show that there cannot be more than one. So, suppose that A has two inverses-say, \nA' and A\n\"\n. Then \nThus, \nAA' = I = A' A \nand AA\n\" \n= I = A\n\" \nA \nA' = \nA'I = A'(AA\n\"\n) = \n(\nA\n'\nA\n)\nA\n\" \n=IA\n\" \n=\nA\n\" \nHence, A' = A\", and the inverse is unique. \nThanks to this theorem, we can now refer to the inverse of an invertible matrix. \nFrom now on, when A is invertible, we will denote its (unique) inverse by A\n-\n1 \n(pro­\nnounced \n\"\nA inverse\"). \nWarning \n1 \nDo not be tempted to write A \n-\nl \n= \nA","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":23598,"to":23656}}}}],[507,{"pageContent":"From now on, when A is invertible, we will denote its (unique) inverse by A\n-\n1 \n(pro­\nnounced \n\"\nA inverse\"). \nWarning \n1 \nDo not be tempted to write A \n-\nl \n= \nA\n! There is no such operation as \n\"division by a matrix.\" Even if there were, how on earth could we divide the scalar 1 by","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":23656,"to":23671}}}}],[508,{"pageContent":"Theorem 3.1 \nTheorem 3.8 \nSection 3.3 \nThe Inverse of a Matrix \n165 \nthe matrix A? If you ever feel tempted to \"divide\" by a matrix, what you really want to do \nis multiply by its inverse. \nWe can now complete the analogy that we  set up at the beginning of this section. \nIf A is an invertible n X n matrix, then the system of linear equations given by \nAx = b   has the unique solution x =A \n-\nl\nb for any bin !R\nn\n. \nProof Theorem 3.7 essentially formalizes the observation we made at the beginning \nof this section. We will go through it again, a little more carefully this time. We are \nasked to prove two things: that Ax = b has a solution and that it  has only one solution. \n(In mathematics, such a proof is called an \"existence and uniqueness\" proof.) \nTo show that a  solution exists, we need only verify that x =A \n-\nl\nb works. We check \nthat \nSo A \n-\nl\nb satisfies the equation Ax = b, and hence there is at least this solution. \nTo show that this solution is   unique, suppose \ny \nis another solution. Then A \ny \n= b,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":23673,"to":23705}}}}],[509,{"pageContent":"-\nl\nb works. We check \nthat \nSo A \n-\nl\nb satisfies the equation Ax = b, and hence there is at least this solution. \nTo show that this solution is   unique, suppose \ny \nis another solution. Then A \ny \n= b, \nand multiplying both sides of the equation by A \n-\nl \non the left, we obtain the chain of \nimplications \nA\n-\n1\n(\nA\ny\n) = A\n-\n1\nb => (\nA\n-\n1\nA)\ny \n= A\n-\n1\nb => I\ny \n= A\n-\n1\nb => \ny \n= A\n-\n1\nb \nThus, \ny \nis the same solution as before, and therefore the solution is unique. \nSo, returning to the questions we raised in the Remarks before Theorem 3.6, how \ncan we tell if a matrix is invertible and how can we find its inverse when it is invert­\nible? We will give a general procedure shortly, but the situation for 2 X 2 matrices is \nsufficiently simple to warrant being singled out. \nIf A = \n[\n: �\n]\n, then A is invertible if ad -  b e * 0, in which case \nA\n-\n1 \n-\n1 \n[ \nd \n-\na\nb\n] \nad  -  b e -e \nIf ad -be = 0, then A is not invertible. \nThe expression ad -be is called the determinant of A, denoted det A. The formula \n[a b\n] \n1 \nfor the inverse of","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":23705,"to":23781}}}}],[510,{"pageContent":"A\n-\n1 \n-\n1 \n[ \nd \n-\na\nb\n] \nad  -  b e -e \nIf ad -be = 0, then A is not invertible. \nThe expression ad -be is called the determinant of A, denoted det A. The formula \n[a b\n] \n1 \nfor the inverse of \n(when it exists) is thus --times the matrix obtained by \ne d dctA \ninterchanging the entries on the main diagonal and changing the signs on the other \ntwo entries. In addition to giving this formula, Theorem 3.8 says that a 2 X 2 matrix \nA is invertible if and only if det A * 0. We  will see in Chapter 4 that the determinant \ncan be defined for all square matrices and that this result remains true, although there \nis no simple formula for the inverse oflarger square matrices. \nProof Suppose that det A = ad -  b e * 0. Then \n[a     b\n] \n[ \nd     - b\n] = \n[ad -  b e \ne     d - e a ed -de \n-ab+ ba] \n=\n[\nad -  b e \n0 \n] = det A\n[\nl \nO\nJ \n- eb + da 0 ad  -  be 0  1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":23781,"to":23824}}}}],[511,{"pageContent":"166 \nChapter 3 Matrices \nSimilarly, \n[ \nd -b\n] [\na \nb\n] \n= det A\n[\nl \nO\nJ \n- e a e d 0 1 \nSince det A *  0, we can multiply both sides of each equation by 1 / det A to obtain \nand \n[\n: \n�\n]\n(\nde�A\n[\n-� \n-\n�\n]\n) \n= \n[\n� \n�\n] \n(\nde�A\n[\n-� \n-\n�\n]\n)\n[\n: \n�\n] \n= \n[\n� \n�\n] \n[Note that we have used property (d) of Theorem 3.3.] Thus, the matr  ix \n1 \n[ \nd -\na\nb\n] \ndet A - e \nsatisfies the definition of an inverse, so A   is invertible. Since the inverse of A is unique, \nby Theorem 3.6, we must have \nA\n-\n1 \n-\n--\n1 \n[ \nd -b\n] \ndet\nA - e \na \nConversely, assume that ad -  be = 0. We will consider separately the cases where \na * 0 and where a = 0. If a * 0, then d = be/a, so the matrix can be written as \nwhere k = e/a. In other words, the second row of A is a multiple of the first. Referring \nto Example 3.23(b), we see that if A has an inverse\n[; \n;\n]\n, then \nand the corresponding system of linear equations \naw \n+ by \n= 1 \nax \n+ bz = 0 \nkaw + kby \n=\nO \nkax + kbz = 1 \n� \nhas no solution. (Why?) \nIf a = 0, then ad -  b e = 0 implies that b e = 0, and therefore either b or e is 0.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":23826,"to":23919}}}}],[512,{"pageContent":"aw \n+ by \n= 1 \nax \n+ bz = 0 \nkaw + kby \n=\nO \nkax + kbz = 1 \n� \nhas no solution. (Why?) \nIf a = 0, then ad -  b e = 0 implies that b e = 0, and therefore either b or e is 0. \nThus, A is of the form \n[\n� �\n] \nor \n[\n� \n�\n] \nIn the  first case, \n[ \n� �\n] [\n; \n; \n]  [ \n� �\n] \n* \n[ \n� \n�\n]\n. Similarly, \n[ \n� \n�\n] \ncannot \n� \nhave an inverse. (Verify this.) \nCo\nnsequently, if ad -  be \n= 0, then A is not invertible.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":23919,"to":23964}}}}],[513,{"pageContent":"Example 3.24 \nSection 3.3 \nThe Inverse of a Matrix \n161 \n[\nl \n2\n] \n[\n12 \nFind the inverses of A  = \n3 4 \nand B = \n4 \n-\n1 5\n] \n-\n5 \n, if they exist. \nSolution \nWe have <let A= 1(4) \n-\n2 \n(\n3\n) \n= \n-\n2 * 0, so A is invertible, with \nA\n-\n1 \n_\n_ \n1\n_\n[ \n4 \n-\n2 -3 \n-\n2\n] = \n[\n-\n:   �\n] \n1 \n-\n-\n-\n2 2 \n� (Check this.) \nOn the other hand, <let B = 12 (\n-\n5) \n-\n(\n-\n1 5) (4) = 0, so Bis not invertible. \n-+-\nExample 3 .25 \nUse the inverse of the coefficient matr  ix to solve the linear system \nTheorem 3.9 \nx + 2y = 3 \n3x + 4y = \n-\n2 \nSolution The coefficient matrix is the matr  ix A  = \n[\n� \n!\nl \nwhose inverse we com­\nputed in Example 3.24. By Theorem 3.7, Ax= b has the unique solution x = A\n-\n1\nb. \nHere we have b  = \n[ \n_ \n�\n]\n; thus, the solution to the given system is \nx = \n[\n-\n� \n-\nt\nJ \n[ \n_\n�\n] \n[\n-\n�\n] \nRemark Solving a linear system Ax= b via x = A\n-\n1\nb would appear to be a   good \nmethod. Unfortunately, except for 2 X 2 coefficient matrices and matrices with cer­\ntain special forms, it is almost always faster to use Gaussian or Gauss-Jordan elimi­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":23966,"to":24071}}}}],[514,{"pageContent":"-\n1\nb would appear to be a   good \nmethod. Unfortunately, except for 2 X 2 coefficient matrices and matrices with cer­\ntain special forms, it is almost always faster to use Gaussian or Gauss-Jordan elimi­\nnation to find the solution directly. (See Exercise 13.) Furthermore, the technique of \nExample 3.2\n5 \nworks only when the coefficient matrix is square and invertible, while \nelimination methods can always be applied. \nProperties of Invertible Malrices \nThe following theorem records some of the most important properties of invertible \nmatrices. \na.  If A is an invertible matrix, then A \n-\nI is invertible and \nb.  If A is an invertible matrix and c is a nonzero scalar, then cA is an invertible \nmatrix and \n(cA )\n-\n1 \n= \n_l_\nA\n-\n1 \nc \nc.  If A and B are invertible matrices of the same size, then AB is   invertible and","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":24071,"to":24098}}}}],[515,{"pageContent":"168 \nChapter 3 Matrices \nd. If A is an invertible matrix, then A\nT \nis invertible and \ne.  If A is an invertible matrix, then A\nn \nis invertible for all nonnegative inte­\ngers n and \n(A\n\"\n)\n-\n1 \n=  (A\n-\n1\n)\n\" \nProof We will prove properties (a), (c), and (e), leaving properties (b) and (d) to be \nproven in Exercises 14 and 15. \n(a) To show that A \n-\ni \nis invertible, we must argue that there is a matrix X such that \nA\n-\n1\nx =I=  XA\n-\n1 \nBut A certainly satisfies these equations in place of X, so A \n-\nl \nis invertible and A is an \ninverse of A\n-\n1\n. Since inverses are unique, this means that \n(\nA\n-\n1\n)\n-\n1 \n=A. \n( c)  Here we must show that there is a matrix X such that \n(\nAB\n)\nX =I= X\n(\nAB\n) \nThe claim is that substituting B\n-\n1\nA \n-\nl \nfor X works. We  check that \nwhere we  have used associativity to shift the  parentheses. Similarly, \n(B\n-\n1\nA \n-\nl\n) \n(\nAB) = I \n� \n(check!), so AB is invertible and its inverse is B\n-\n1\nA\n-\n1\n. \n(e)  The basic idea here is easy enough. For example, when n =  2, we have \nA\n2\n(A\n-\n1\n)\n2 \n= AAA\n-\nI\nA\n-\nI \n= A\nL4. \n-\nI \n= AA\n-\nI \n= I \nSimilarly, (A \n-\nI)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":24100,"to":24204}}}}],[516,{"pageContent":"� \n(check!), so AB is invertible and its inverse is B\n-\n1\nA\n-\n1\n. \n(e)  The basic idea here is easy enough. For example, when n =  2, we have \nA\n2\n(A\n-\n1\n)\n2 \n= AAA\n-\nI\nA\n-\nI \n= A\nL4. \n-\nI \n= AA\n-\nI \n= I \nSimilarly, (A \n-\nI) \n2 \nA \n2 \n= I. Thus, (A \n-\nI) \n2 \nis the inverse of A \n2\n. It is not difficult to see that \na similar argument works for any higher integer value of n. However, mathematical \ninduction is the  way to carry out the proof. \nThe basis step is when n = 0, in which case we are being asked to prove that A \n0 \nis \ninvertible and that \nThis is the same as showing that I is invertible and that r \n1 \n= I, which is clearly true. \n� \n(Why? See Exercise 16.) \nNow we assume that the result is true when n = k, where k is a specific nonnega­\ntive integer. That is, the induction hypothesis is to assume that A \nk \nis invertible and that \nThe induction step  requires that  we  prove that A\nk\n+I is  invertible and that \n(A\nk\n+\n1\n)\n-\n1 \n= (A\n-\n1\n)\nk\n+\n1\n. Now we know from (c) that A\nk\n+\n1 \n=  A\nk\nA is invertible, since A \nand (by hypothesis) A\nk","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":24204,"to":24287}}}}],[517,{"pageContent":"The induction step  requires that  we  prove that A\nk\n+I is  invertible and that \n(A\nk\n+\n1\n)\n-\n1 \n= (A\n-\n1\n)\nk\n+\n1\n. Now we know from (c) that A\nk\n+\n1 \n=  A\nk\nA is invertible, since A \nand (by hypothesis) A\nk \nare both invertible. Moreover, \n(A\n-\n1\n)\nk +\n1 \n=  (A\n-\n1\n)\nk\nA\n-\n1 \n= (A\nk\n)\n-\n1\nA\n-\n1 \n=  (AA\nk\n)\n-\n1 \n=  (A\nk + 1\n)\n-\n1 \nby the induction hypothesis \nby \nproperty (c)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":24287,"to":24348}}}}],[518,{"pageContent":"Example 3.26 \nSection 3.3 \nThe Inverse of a Matrix \n169 \nTherefore, A\nn \nis invertible for all nonnegative integers n, and \n(\nA \nn\n)\n-\n1 \n= \n(\nA \n-\nl\n) \nn \nby the \nprinciple of mathematical induction. \nRemarks \n• \nWhile all of the properties of Theorem 3.9 are useful, \n( \nc) is the one you should \nhighlight. It is perhaps the most important algebraic property of matrix inverses. It \nis also the one that is easiest to get wrong. In Exercise 17, you are asked to give a \ncounterexample to show that, contrary to what we might like, \n(AB)\n-\n1 \n* A \n-\nl \nB\n-\n1 \nin \ngeneral. The correct property, \n(AB)\n-\n1 \n= \nB\n-\n1\nA\n-\n1\n, is sometimes called the socks-and­\nshoes rule, because, although we put our socks on before our shoes, we take them off \nin the reverse order. \n• \nProperty (c) generalizes to products of finitely many invertible matrices: If A\n1\n, \nA\n2\n, ... , A\nn \nare invertible matrices of the same size, then A \n1\nA\n2\n• • • \nA\nn \nis invertible and \n(\nA\n1\nA\n2 \n· · · \nA\nn\n)\n-\n1 \n= \nA\n;;-\n1 \n· · · \nA2\n1\nA �\n1 \n(See Exercise 18.\n) \nThus, we can state:","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":24350,"to":24442}}}}],[519,{"pageContent":"1\n, \nA\n2\n, ... , A\nn \nare invertible matrices of the same size, then A \n1\nA\n2\n• • • \nA\nn \nis invertible and \n(\nA\n1\nA\n2 \n· · · \nA\nn\n)\n-\n1 \n= \nA\n;;-\n1 \n· · · \nA2\n1\nA �\n1 \n(See Exercise 18.\n) \nThus, we can state: \nThe inverse of a product of invertible matrices is the prod uct of their inverses in \nthe reverse order. \n1 1 1 \n• \nSince, for  real numbers, --* -+ -,  we should not  expect that, for \na+ b \na \nb \nsquare matrices, \n(\nA+ B)\n-\n1 \n= A\n-\n1 \n+ B\n-\n1 \n(and, indeed, this is not true in general; see \nExercise 19\n)\n. In fact, except for special matrices, there is no formula for \n(\nA  + B)\n-\n1\n. \n• \nProperty (e) allows us to define negative integer powers of an invertible \nmatrix: \nDefinition \nIf A is an  invertible matrix and n is a positive integer, then A \n-\nn \nis \ndefined by \nWith this definition, it can be shown that the rules for exponentiation, A\nr \nA\ns \n= A'·\n+\ns \nand \n(\nAT = A'5, hold for all integers rand s, provided A is invertible. \nOne use of the algebraic properties of matrices is to help solve equations involving","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":24442,"to":24526}}}}],[520,{"pageContent":"r \nA\ns \n= A'·\n+\ns \nand \n(\nAT = A'5, hold for all integers rand s, provided A is invertible. \nOne use of the algebraic properties of matrices is to help solve equations involving \nmatrices. The next example illustrates the process. Note that we  must pay particular \nattention to the order of the matrices in the prod uct. \nSolve the following matrix equation for X (assuming that the matrices involved are \nsuch that all of the indicated operations are defined): \nA\n-\n1\n(\nB\nX)\n-\n1 \n= (A\n-\n1\nB\n3\n)\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":24526,"to":24554}}}}],[521,{"pageContent":"110 \nChapter 3 Matrices \nSolulion \nThere are many ways to proceed here. One solution is \nA\n-\n1\n(\nBX)\n-\n1 \n= \n(A\n-\n1\nB\n3\n)\n2 \n==? \n((\nBX)\nA )\n-\n1 \n= \n(A\n-\n1\nB\n3\n)\n2 \n=} \n[\n((\nBX)\nA)\n-\n1\n]\n-\n1 \n= \n[\n(\nA\n-\n1\nB\n3\n)\n2\n]\n-\n1 \n==? (\nBX)\nA \n= \n[ \n(A\n-\n1\nB\n3\n)(\nA\n-\n1\nB\n3\n) \nJ\n-\n1 \n==? (\nBX)\nA \n= \nB\n-\n3\n(\nA\n-\n1\n)-\n1\nB\n-\n3\n(\nA\n-\n1\n)-\n1 \n==? BXA = \nB\n-\n3\nAB\n-\n3\nA \n==? B\n-\n1\nBXAA\n-\n1 \n= \nB\n-\n1\nB\n-\n3\nAB\n-\n3\nAA\n-\n1 \n==? IXI = \nB\n-\n4\nAB\n-\n3\nI \n==? X = \nB\n-\n4\nAB\n-\n3 \n� \n(Can you justify each step?) Note the careful use of Theorem 3.9(c) and the expansion \nof \n(\nA \n-\n1\nB\n3\n) \n2\n• We have also made liberal use of the associativity of matrix multiplica­\ntion to simplify the placement (or elimination) of parentheses. \nElemen1arv Malrices \nWe are going to use matrix multiplication to take a different perspective on the row \nreduction of matrices. In the process, you will discover many new and important \ninsights into the nature of invertible matrices. \nIf \nwe find that \nIn other words, multiplying A by E (on the left) has the same effect as interchanging \nrows 2 and 3 of A. What is significant about E? It is simply the matrix we obtain by","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":24556,"to":24716}}}}],[522,{"pageContent":"If \nwe find that \nIn other words, multiplying A by E (on the left) has the same effect as interchanging \nrows 2 and 3 of A. What is significant about E? It is simply the matrix we obtain by \napplying the same elementary row operation, R\n2 \n� R\n3\n, to the identity matrix J\n3\n. It \nturns out that this always works. \nDefiniliOD \nAn elementary matrix is any matr  ix that can be obtained by per­\nforming an elementary row operation on an identity matrix. \nSince there are three types of elementary row operations, there are three cor­\nresponding types of elementary matrices. Here are some more elementary matrices. \nExample 3.21 \nLet","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":24716,"to":24734}}}}],[523,{"pageContent":"Theorem \n3.10 \nSection 3.3 \nThe Inverse of a Matrix \n111 \nEach of these matrices has been obtained from the identity matrix I\n4 \nby applying a \nsingle elementary row operation. The matrix E\n1 \ncorresponds to 3R\n2\n, E\n2 \nto R\n1 \n� R\n3\n, \nand E\n3 \nto R\n4 \n- 2R\n2\n. Observe that when we left-multiply a 4 X n matrix by one of these \nelementary matr  ices, the corresponding elementary row operation is performed on \nthe matrix. For example, if \n[\n\"\n\" \na\n1\n2 \n\"\"\n] \na\n2\n1 \na\n22 \na\n2\n3 \nA= \na\n31 \na\n3\n2 \na\n33 \na\n4\n1 \na\n4\n2 \na\n4\n3 \nthen \n[ \na\nu \na\n1\n2 \n\"\"\n] \n[\n\"\n\" \na\n3\n2 \na\n,,\n: \n3a\n2\n1 \n3a\n22 \n3a\n2\n3 \n' \nE\n2\nA = \na\nz 1 \na\n22 \na\nz 3 \nE\n1\nA = \n' \na\n31 \na\n3\n2 \na\n33 \na\n11 \na\n1\n2 \na\n13 \na\n4\n1 \na\n4\n2 \na\n4\n3 \na\n4\n1 \na\n4\n2 \na\n4\n3 \nand \na\nu \na\n1\n2 \na\n13 \nE\n3\nA = \n[ \na\n, , \na\n22 \na\n,\n, \nl \na\n31 \na\n3\n2 \na\n33 \na\n4 1 \n-2a\n2\n1 \na\n4\n2 \n-2a\n22 \na\n43 \n-2a\n2\n3 \nExample 3.27 and Exercises 24-30 should convince you  that any elementary \nrow operation on any matrix can be accomplished by left-multiplying by a suitable \nelementary matrix. We record this fact as a theorem, the proof of which is omitted. \nLet Ebe the elementary \nmatrix \nobtain\ned by performing \nan elementary \nrow opera­\ntion on \nI\nn\n-","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":24736,"to":24923}}}}],[524,{"pageContent":"elementary matrix. We record this fact as a theorem, the proof of which is omitted. \nLet Ebe the elementary \nmatrix \nobtain\ned by performing \nan elementary \nrow opera­\ntion on \nI\nn\n-\nIf the same elementary \nrow operation \nis performed \non an \nn \nX \nr \nmatrix \nA, \nthe result is the same as the matrix \nEA. \nRemark From a computational point of view, it is not a good idea to use el­\nementary matrices to perform elementary row operations-just do them directly. \nHowever, elementary matrices can provide some valuable insights into invertible \nmatrices and the solution of systems oflinear equations. \nWe have already observed that every elementary row operation can be \"undone:' \nor \"reversed:' This same observation applied to elementary matrices shows us that \nthey are invertible. \nExample \n3.28 \nLet \n0 \n0 \n0 \n4 \n0 \n�\n] \n,  and  E\n3 \n= \n[ \n� \n� \n�\n] \n1 \n-2  0  1 \nThen E\n1 \ncorresponds to R\n2 \n� R\n3\n, which is undone by doing R\n2 \n� R\n3 \nagain. Thus, \n� \nE\n1 \n-\nl \n= E\n1\n. (Check by showing that El = E\n1\nE\n1 \n= I.\n) \nThe matrix E\n2 \ncomes from 4R\n2\n,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":24923,"to":25000}}}}],[525,{"pageContent":"112 \nChapter 3 Matrices \nTheorem 3.11 \nTheorem 3.12 \nwhich is undone by performing iR\n2\n• Thus, \nwhich can be easily checked. Finally, E\n3 \ncorresponds to the elementary row opera­\ntion R\n3 \n- 2R\n1\n, which can be undone by the elementary row operation R\n3 \n+ \n2R\n1\n. So, \nin this case, \n(Again, it is easy to check this by confirming that the product of this matr  ix and E\n3\n, \nin \nboth orders, is I. ) \nNotice that not only is each elementary matr  ix invertible, but its inverse is another \nelementary matr  ix of the same type. We record this finding as the next theorem. \nEach elementary matrix is invertible, and its inverse is an elementary matrix of the \nsame type. \nThe Fundamen1a1 Theorem ol lnvenible Malrices \nWe  are now in a position to prove one of the main results in this book-a set of \nequivalent characterizations of what it means for a matrix to be invertible. In a sense, \nmuch of linear algebra is connected to this theorem, either in the development of","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":25002,"to":25035}}}}],[526,{"pageContent":"equivalent characterizations of what it means for a matrix to be invertible. In a sense, \nmuch of linear algebra is connected to this theorem, either in the development of \nthese characterizations or in their application. As you might expect, given this intro­\nduction, we will use this theorem a great deal. Make it your friend! \nWe refer to Theorem 3.12 as the first version of the Fundamental Theorem, since \nwe will add to it in subsequent chapters. You are reminded that, when we say that a  set \nof statements about a  matr  ix A are equivalent, we mean that, for a given A, the state­\nments are either all true or all false. \nThe Fundamental Theorem of Invertible Matrices: Version 1 \nLet A be an n X n matrix. The following statements are equivalent: \na. A is invertible. \nb. Ax = b has a unique solution for every bin !R\nn\n. \nc. Ax = 0 has only the  trivial solution. \nd. The reduced row echelon form of A is Iw \ne. A is a product of elementary matrices.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":25035,"to":25051}}}}],[527,{"pageContent":"Section 3.3 \nThe Inverse of a Matrix \n113 \nProof \nWe will establish the theorem by proving the circular chain of implications \n(\na\n) \n=} \n(\nb\n) \n=} \n(\nc\n) \n=} \n(\nd\n) \n=} \n(\ne\n) \n=} \n(\na\n) \n(a) =} (b) We have already shown that if A is invertible, then Ax = b has the \nunique solution x = A\n-\n1\nb for any bin !R\nn \n(Theorem 3.7). \n(b) =} (c) Assume that Ax = b  has a unique solution for any bin !R\nn\n. This implies, \nin particular, that Ax = 0 has a unique solution. But a homogeneous system Ax = 0 \nalways has x = 0 as one solution. So in this case, x = 0 must be th e solution. \n(c) =} (d)  Suppose that Ax = 0 has only the trivial solution. The corresponding \nsystem of equations is \na\n11\nx\n1 \n+  a\n1\n2\nx\n2 \n+ \n· · · \n+ a\n1n\nx\nn \n= 0 \na\n11\nX\n1 \n+  a\n2\n1\nX\n2 \n+ \n... \n+ a\n1n\nX\nn \n= 0 \nand we are assuming that its solution is \n=O \n=O \nIn other words, Gauss-Jordan elimination applied to the augmented matr  ix of the \nsystem gives \n[\n\"\" \na\n1\n2 \na\nln \nf\nl \n[! \n0 \n0 \n!\n] \n[A\nl\noJ \n= \na\n�\n, \na\n2\n1 \na\n2\nn \n0 \n[I\nn\nl\noJ \n� \na\nn\n] \na\nn\n2 \na\nnn \n0 \n1 \nThus, the reduced row echelon form of A is I\nn\n-","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":25053,"to":25177}}}}],[528,{"pageContent":"system gives \n[\n\"\" \na\n1\n2 \na\nln \nf\nl \n[! \n0 \n0 \n!\n] \n[A\nl\noJ \n= \na\n�\n, \na\n2\n1 \na\n2\nn \n0 \n[I\nn\nl\noJ \n� \na\nn\n] \na\nn\n2 \na\nnn \n0 \n1 \nThus, the reduced row echelon form of A is I\nn\n-\n( d) =} (e) If we assume that the reduced row echelon form of A is I\nn\n, then A can be \nreduced to I\nn \nusing a finite sequence of elementary row operations. By Theorem 3.10, \neach one of these elementary row operations can be achieved by left-multiplying by an \nappropriate elementary matrix. If the appropriate sequence of elementary matrices is \nE\n1\n, E\n2\n, ••. , E\nk \n(in that order), then we have \nE\nk\n· · · \nE\n2\nE\n1\nA = I\nn \nAccording to Theorem 3.11, these elementary matrices are all invertible. Therefore, \nso is their product, and we have \nAgain, each E;\n-\n1 \nis another elementary matr  ix, by Theorem 3.11, so we have written \nA as a product of elementary matrices, as required. \n(e) =} (a)  If A is a product of elementary matrices, then A is invertible, since \nelementary matrices are invertible and products of invertible matrices are invertible.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":25177,"to":25256}}}}],[529,{"pageContent":"114 \nChapter 3 Matrices \nExample 3.29 \n\"Never bring a cannon on stage in \nAct I unless you intend to fire it by \nthe last act:' \n-\nAnton Chekhov \nTheorem 3.13 \nIf possible, express A = \n[ \n� \n� \nJ \nas a product of elementary matrices. \nSolution \nWe row reduce A as follows: \nA = \n[\n2 3\nJ \n� \n[\nl \n3\nJ \n�· \n[\nl \n3\nJ \n1  3 2 3 0 -3 \n� \n[\nl \n0 \nJ \n� \n[\nl \nO\nJ \n= I \n0  -3 0  1 \n2 \nThus, the reduced row echelon form of A is the identity matrix, so the Fundamental \nTheorem assures us that A is invertible and can be written as a product of elementary \nmatrices. We have E\n4\nE\n3\nE\n2\nE\n1\nA =   I, where \nare the elementary matrices corresponding to the four elementary row opera  tions \nused to reduce A to I. As in   the proof of the theorem, we have \n[\nO \nl\nJ \n[\nl \nO\nJ \n[\nl \n-\nl\nJ \n[\nl \nO\nJ \n1  0 2 1  0    1  0 -3 \nas required. \nRemark Because the sequence of elementary row operations that transforms A \ninto I is not unique, neither is the representation of A   as a product of elementary \nmatrices. (Find a different way to express A as a product of elementary matrices.)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":25258,"to":25337}}}}],[530,{"pageContent":"into I is not unique, neither is the representation of A   as a product of elementary \nmatrices. (Find a different way to express A as a product of elementary matrices.) \nThe Fundamental Theorem is surprisingly powerful. To illustrate its power, we \nconsider two of its consequences. The first is that,   although the definition of an in -\nvertible matr  ix states that a  matr  ix A   is invertible if there is a matrix B such that both \nAB = I and BA = I  are satisfied, we need only check one of these equations. Thus, we \ncan cut our work in half! \nLet A be a square matrix. If B is a square matrix such that either AB = I or BA = I, \nthen A is invertible and B = A \n-\n1\n• \nProof Suppose BA = I.  Consider the equation Ax = 0. Left-multiplying by B, we have \nBAx = BO. This implies that x =  Ix = 0. Thus, the system represented by Ax= 0 has the \nunique solution x = 0. From the equivalence of (c) and (a) in the Fundamental Theo­\nrem, we know that A  is invertible. (That is, A \n-\n1 \nexists and satisfies AA -\ni","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":25337,"to":25356}}}}],[531,{"pageContent":"unique solution x = 0. From the equivalence of (c) and (a) in the Fundamental Theo­\nrem, we know that A  is invertible. (That is, A \n-\n1 \nexists and satisfies AA -\ni \n= I = A -\ni \nA.) \nIf we  now right-multiply both sides of BA = I  by A \n-\ni\n, we obtain \nBAA \n-\ni \n= IA \n-\ni \n=} BI = A \n-\ni \n=} B = A \n-\ni \n(The proof in the case of AB = I  is left as Exercise 41.) \nThe next consequence of the Fundamental Theorem is the basis for an efficient \nmethod of computing the inverse of a matrix.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":25356,"to":25383}}}}],[532,{"pageContent":"Theorem 3.14 \nExample 3.30 \nSection 3.3 \nThe Inverse of a Matrix \n115 \nLet A be a square matrix. If a sequence of elementary row opera  tions reduces A \nto I, then the same sequence of elementary row operations transforms I into A \n-\ni\n. \nProof If A  is row equivalent to I,  then we  can  achieve  the reduction by left­\nmultiplying by a   sequence E\n1\n, E\n2\n, ••• , Ek \nof elementary matrices. Therefore, we have \nEk· · · E\n2\nE\n1\nA = I. Setting B =   Ek· · · E\n2\nE\n1 \ngives BA = I.   By Theorem 3.13, A  is invert­\nible and A \n-\nl \n= B. Now applying the same sequence of elementary row operations to \nI is equivalent to left-multiplying I by E   k· · · E\n2\nE\n1 \n= B. The result is \nE\nk\n·· ·E\n2\nE\n1\nI =BI= B = A\n-\n1 \nThus, I is transformed into A \n-\ni \nby the same sequence of elementary row operations. \nThe Gauss-Jordan Melhod for Compuling lhe Inverse \nWe can perform row operations on A and I simultaneously by constructing a \"super­\naugmented matrix\" [A \nI\nI]. Theorem 3.14 shows that if A is row equivalent to I [which,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":25385,"to":25437}}}}],[533,{"pageContent":"We can perform row operations on A and I simultaneously by constructing a \"super­\naugmented matrix\" [A \nI\nI]. Theorem 3.14 shows that if A is row equivalent to I [which, \nby the Fundamental Theorem (d) � (a), means that A  is invertible], then elementary \nrow operations will yield \nIf A cannot be reduced to I, then the Fundamental Theorem guarantees us   that A is \nnot invertible. \nThe procedure just described is simply Gauss-Jordan elimination performed on an \nn X 2n, instead of an n X \n(\nn + 1), augmented matrix. Another way to view this pro­\ncedure is to  look at the problem of finding A -I as solving the matrix equation AX = I\nn \nfor an n X n matrix X. (This is sufficient, by the Fundamental Theorem, since a right \ninverse of A must be a two-sided inverse.) If we denote the columns of X by x\n1\n, \n... \n, Xn\n' \nthen this matrix equation is equivalent to solving for the columns of X, one at a time. \nSince the columns of I\nn \nare the standard unit vectors e\n1\n, ... , e\nn\n, we  thus have n systems","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":25437,"to":25465}}}}],[534,{"pageContent":"1\n, \n... \n, Xn\n' \nthen this matrix equation is equivalent to solving for the columns of X, one at a time. \nSince the columns of I\nn \nare the standard unit vectors e\n1\n, ... , e\nn\n, we  thus have n systems \nof linear equations, all with coefficient matrix A: \nSince the same sequence of row opera  tions is needed to bring A to reduced row \nechelon form in each case, the augmented matrices for these systems, [A I e\n1\n] ,  ... , \n[A I e\nn\n], can be combined as \nWe now apply row opera  tions to try to reduce A to I\nn\n, which, if successful, will simul­\ntaneously solve for the columns of A \n-\ni\n, transforming I\nn \ninto A \n-\ni\n. \nWe illustrate this use of Gauss-Jordan elimination with three examples. \nFind the inverse of \nA=\n[\n�\n� \n-�i \n1  3 -3 \nif it exists.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":25465,"to":25506}}}}],[535,{"pageContent":"116 \nChapter 3 Matrices \nExample 3.31 \nSolulion Gauss-Jordan elimination prod uces \n[\n: \n2 \n-1 \n0 \n�\n: \n[A \nI\nIJ \n2    4 \n0 \n3 \n-3 \n0  0 \nR2 - 2R , \n[\n: \n2 \n-1 \n0 \n�\n] \nR3 - R\n1 \n-2 \n6 \n-2 \n1 \n� \n1 \n-2 \n-1  0 \n[\n: \n2 \n-1 0 \n�\n] \nH:J\nR\n, \n-3 \nI \n-\n2 \n1 \n-2 \n-1 0 \n[\n: \n2 \n-1 \n0 \n�\n: \nR3-R2 \n1 \n-3 \n1 \nI \n� \n-\n2 \n0 \n-2 \nI \n2 \nR, +R3 \n[\n: \n2 \n0  -1 \nI \n1\n] \nR\n2 +\n3\nR3 \n2 \n� \n1 \n0 \n-5 \n1 \n0 \n-2 \n! \n2 \n3 \n[\n: \n0  0 \n9 \n-\n2 \n-\n:i \nTherefore, \nR, - 2R 2 \n1 \n� \n0 \n-5 \n0 \n1 \n-2 \nA\n-\n1\n=   -5 \n3 \n[ \n9 \n-t \n-5\n] \n-2 \nt \n1 \n1 \n! \n2 \n(You should always check that AA -I = I by direct multiplication. By Theorem 3.13, \nwe do not need to check that A \n-\ni \nA = I too.) \nRemark Notice that we have used the variant of Gauss-Jordan elimination that \nfirst introduces all of the zeros below the leading ls, from left to right and top to \nbottom, and then creates zeros above the leading ls, from right to left and bottom to \ntop. This approach saves on calculations, as we noted in Chapter 2, but you may find \nit easier, when working by hand, to create all of the zeros in each column as you go. \nThe answer, of course, will be the same.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":25508,"to":25646}}}}],[536,{"pageContent":"it easier, when working by hand, to create all of the zeros in each column as you go. \nThe answer, of course, will be the same. \nFind the inverse of \nif it exists. \nA=\n[\n-\n! \n-1 \n-\n:i \n-2 \n2  -2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":25646,"to":25658}}}}],[537,{"pageContent":"Example 3.32 \nSection 3.3 \nThe Inverse of a Matrix \n111 \nSolution We proceed as in Example 3.30, adjoining the identity matrix to A and \nthen \ntrying to manipulate [A \nI \nI] into [I \nI \nA\n-\n1 \n]. \n[\n-\n: \n1 \n-4 \n1 \n0 \n�\n] \n[A \nI\nIJ \n-1 \n6 \n0 \n1 \n-2 \n2 \n-2 \n0  0 \nR\n2+2\nR1 \n[\n: \n1 \n-4 \n0 \n�\n] \nR3 + R1 \n1 \n-2 \n2 \n1 \n� \n3 \n- 6 \n1 \n0 \n,\n,\n-\n,\n.\n, \n[\nI \n2 \n-1 \n0 \n�\n] \n� \n0 \n-3 \n2 \n1 \n0  0 0 \n-5  -3 \nAt this point, we see that it  is not  possible to reduce A to I, since there is a row of zeros \non the left-hand side of the augmented matr  ix. Consequently, A is not invertible. \n4 \nAs the next example illustrates, everything works the same way over \"11..\nP\n, where \npis prime. \nFind the inverse of \nA = \n[\n� \n�\n] \nif it exists, over \"11..\n3\n. \nSolution 1 \nWe use the Gauss-Jordan method, remembering that all calculations are \nin \"11..\n3\n. \nThus  A\n-\n1 \n= \n[\n0 \n, \n2 \n[A \nI \nI] \n= \n[\n� \n211 \n0 0 \n�\n] \n2\nR1 \n[\n� \n1 12 \n�\n] \n� \n0  0 \nR\n1\n+\nR1 \n[\n� \n1 12 \n�\n] \n� \n1  2 \nR1\n+ 2R2 \n[\n� \n0 10 \n�\n] \n� \n1  2 \n�\n]\n, and it is easy to check that, over \"11..\n3\n, AA \n-\nl \n= I. \nSolution 2 Since A is a 2 X  2 matrix, we can also compute A \n-\nl","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":25660,"to":25812}}}}],[538,{"pageContent":"�\n] \n2\nR1 \n[\n� \n1 12 \n�\n] \n� \n0  0 \nR\n1\n+\nR1 \n[\n� \n1 12 \n�\n] \n� \n1  2 \nR1\n+ 2R2 \n[\n� \n0 10 \n�\n] \n� \n1  2 \n�\n]\n, and it is easy to check that, over \"11..\n3\n, AA \n-\nl \n= I. \nSolution 2 Since A is a 2 X  2 matrix, we can also compute A \n-\nl \nusing the formula \ngiven in Theorem 3.8. The determinant of A is \ndet A = 2\n(\n0\n) \n- 2\n(\n2\n) \n= -1 = 2 \nin \"11..\n3 \n(since 2 + 1 = O\n)\n. Thus, A \n-\nl \nexists and is given by the formula in Theorem 3.8. \nWe must be careful here, though, since the formula introduces the \"fraction\" l/det A","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":25812,"to":25873}}}}],[539,{"pageContent":"118 \nChapter 3 Matrices \nand there are no fractions in Z\n3\n. We must use multiplicative inverses rather than \ndivision. \nInstead \nof l/\ndet A = 1\n/\n2, we use T\n1\n; that is, we find the number x that satisfies \nthe equation 2x = 1 in Z\n3\n• It  is easy to see that x = 2 is the solution we want: In Z\n3\n, \nT\n1 \n= 2, since 2(2) = 1. The formula for A\n-\n1 \nnow becomes \nA\n-\n1 \n= T\n1\n[ \n_\n� \n-�\n] \n= 2\n[\n� \n�\n] \n= \n[\n� \n�\n] \nwhich agrees with our previous solution. \n.. \nI \nExercises \n3.3 \nIn Exercises 1-10, find the inverse of th e given matrix (if it \nexists) using Theorem 3.8. \n1. \n[\n� �\n] \n2. \n[\n� \n-\n�\n] \n3. \n[\n! \n!\n] \n4. \n[ \n_� �\n] \n5. \n[\ni n \n[ \nl\n/V2 \n6. \n-1;\nV2 \nl\n/V2\n] \n1\n/\n\\/2 \n[\n-1.5 \n7. \n0.5 \n-4.2\n] \n2.4 \n8. \n[\n3.55 0.25\n] \n8.52 0.60 \n9. \n[\n� -�\n] \n[\nl/a \nIO. \n1\n/ c \nl/b\n] \nl/ d \n, where \nneither a, b, c, nor dis O \nIn Exercises 11 and 12, solve th e given system using th e \nmethod of Example 3.25. \n11. 2X +    y  = -1 \n12. X\n1 \n- X\n2 \n= 1 \n5x + 3y = \n2 \n2x\n1 \n+ x\n2 \n= 2 \n13.\nLet\nA \n= \n[\n� \n!\nl\nh\n1 \n= \n[\n;\n]\n,h\n2 \n= \n[\n-\n�\nJ\n,and\nb\n3 \n= \n[\n�\n]\n. \n(a) Find A\n-\n1 \nand use it to solve the three systems \nAx = h\n1\n, Ax = h\n2\n, and Ax = h\n3\n.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":25875,"to":26040}}}}],[540,{"pageContent":"11. 2X +    y  = -1 \n12. X\n1 \n- X\n2 \n= 1 \n5x + 3y = \n2 \n2x\n1 \n+ x\n2 \n= 2 \n13.\nLet\nA \n= \n[\n� \n!\nl\nh\n1 \n= \n[\n;\n]\n,h\n2 \n= \n[\n-\n�\nJ\n,and\nb\n3 \n= \n[\n�\n]\n. \n(a) Find A\n-\n1 \nand use it to solve the three systems \nAx = h\n1\n, Ax = h\n2\n, and Ax = h\n3\n. \n(h)  Solve all   three systems at the same time by row re­\nducing the augmented matrix [A \nI \nh\n1 \nh\n2 \nh\n3\n] using \nGauss-Jordan elimination. \n(c) Carefully count the total number of individual \nmultiplications that you performed in (a) and in \n(b). You should discover that, even for this 2 X 2 \nexample, one method uses fewer operations. \nFor larger systems, the difference is even more \npronounced, and this explains why computer \nsystems do not use one of these methods to solve \nlinear systems. \n14. Prove Theorem 3.9(b). \n15. Prove Theorem 3.9(d). \n16. Prove that the n X n identity matrix I\nn \nis invertible and \nthat l\nn\n-\nl \n= Iw \n17. (a) Give a counterexample to show that (AB)\n-\n1 \n* \nA\n-\n1\nB\n-\n1 \nin general. \n(h) Under what conditions on A and Bis (AB)\n-\n1 \n= \nA\n-\n1\nB\n-\n1\n? Prove your assertion. \n18. By induction, prove that if A\n1\n, A\n2\n, ••• , A","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":26040,"to":26148}}}}],[541,{"pageContent":"17. (a) Give a counterexample to show that (AB)\n-\n1 \n* \nA\n-\n1\nB\n-\n1 \nin general. \n(h) Under what conditions on A and Bis (AB)\n-\n1 \n= \nA\n-\n1\nB\n-\n1\n? Prove your assertion. \n18. By induction, prove that if A\n1\n, A\n2\n, ••• , A\nn \nare invertible \nmatrices of the same size, then the product A \n1\nA\n2 \n· · · A\nn \nis invertible and (A\n1\nA\n2 \n•   •   • A\nn\n)\n-\n1 \n= A;;-\n1 \n· · · \nA2\n1\nA�\n1\n• \n19. Give a counterexample to show that (A + B)\n-\n1 \n* \nA\n-\n1 \n+ B\n-\n1 \nin general. \nIn Exercises 20-23, solve the given matrix equation for X. \nSimplify your answers as much as possible. (In the words of \nAlbert Einstein, \"Everything should be made as simple as pos­\nsible, but not simpler.\") Assume that all matrices are invertible. \n20. XA\n2 \n= A\n-\n1 \n21. AXB = \n(\nBA\n)\n2 \n22. \n(\nA\n-\n1\nX\n)-\n1 \n= \nA\n(\nB\n-\n2\nA\n)-\n1 \n23. ABXA\n-\n1\nB\n-\n1 \n= I\n+  A \nIn Exercises 24-30, let \n[\n: \n2 \n-1\n] \n[\n1 \n-1 \n�\nl \nA  = \n1 \n� \n,  B = \n� \n1 \n-1 \n2 \n-1 \nc \n� \n[\n: \n2 \n-1\n] \nH \n2 \n-\n: \n] \n1 \n' \nD= \n-1 \n-1 \n-1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":26148,"to":26287}}}}],[542,{"pageContent":"In each case, find an elementary matrix E that satisfies th e \ngiven equation. \n24.EA = B \n27.E C =A \n25.EB =A \n28.EC =  D \n26.EA =  C \n29.ED  =  C \n30. Is there an elementary matrix E such that EA = D? \nWhy or why not? \nIn Exercises 31-38, find th e inverse of th e given elementary \nmatrix. \n31. \n[\n� \n�\n] \n32. \n[\n� \n�\n] \n33. \n[\n� \n�\n] \n34. \n[ \n_\n� \n�\n] \n35. \n[\ni \n0 \n-\n�] \n[\n: \n0 \n:\nJ \n1 \n36. \n1 \n0 0 \n37. \n[\ni \n0 \nn'H \n38. \n[\ni \n0 \nlH \nc \n0 0 \nIn Exercises 39 and 40, find a sequence of elementary \nmatrices E\n1 , E\n2\n, .•. , E\nk \nsuch that E\nk· · · \nE\n2\nE 1\nA =I.  Use this \nsequence to write both A and A -\ni \nas products of elementary \nmatrices. \n39.A  = \n[ \n_\n� \n-\n�\nJ \n40. A= \n[\n� \n�] \n41. Prove Theorem 3.13 for the case of AB= I. \n42. (a) Prove that if A is invertible and AB = 0, then \nB= 0. \n(b) Give a counterexample to show that the result in \npart (a) may fail if A is not invertible. \n43. (a) Prove that if A is invertible and BA  = CA, then \nB = C. \n(b) Give a counterexample to show that the result in \npart (a) may fail if A is not invertible.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":26289,"to":26384}}}}],[543,{"pageContent":"43. (a) Prove that if A is invertible and BA  = CA, then \nB = C. \n(b) Give a counterexample to show that the result in \npart (a) may fail if A is not invertible. \n44. A square matrix A is called idempotent if A \n2 \n= A. \n(The word idempotent comes from the Latin idem, \nmeaning \"same;' and potere, meaning \"to have power:' \nThus, something that is idempotent has the \"same \npower\" when squared.) \n(a) Find three idempotent 2 X 2 matrices. \n(b) Prove that the only invertible idempotent n X n \nmatrix is the identity matrix. \n45. Show that if A is a square matrix that satisfies the \nequation A\n2 \n-2A +\nI= 0, then A\n-\n1 \n= 2I -A. \n46. Prove that if a symmetric matrix is invertible, then its \ninverse is symmetric also. \nSection 3.3 \nThe Inverse of a Matrix \n119 \n47. Prove that if A and B are square matrices and AB is \ninvertible, then both A and B are invertible. \nIn Exercises 48-63, use the Gauss-Jordan method to find th e \ninverse of the given matrix (if it exists). \n48. \n[\n� \n50. \n[\n� \n52. \n[\n: \n54. \n[\n: \n56. \n[\n: \n:\nJ \n-\n�\n] \n-\n� \n-\n�\n] \n0  -1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":26384,"to":26440}}}}],[544,{"pageContent":"In Exercises 48-63, use the Gauss-Jordan method to find th e \ninverse of the given matrix (if it exists). \n48. \n[\n� \n50. \n[\n� \n52. \n[\n: \n54. \n[\n: \n56. \n[\n: \n:\nJ \n-\n�\n] \n-\n� \n-\n�\n] \n0  -1 \n� \n:\nJ \n� �\n] \n[ \nV2 \n-\n4V2 \n58. \n0 \n0 \n0  2V2 \nV2 \n0 \n0 \n1 \n0 \n3 \n61. \n[! \n!] over Z\n5 \n63. \n[\n: \n5 \nO\nJ \n2 4  over Z\n7 \n6  1 \n49. \n[\n-\n2   4\n] \n3  -1 \n60. \n[\n0\n1 \n62. \n[\n: \nPartitioning large square matrices can sometimes make their \ninverses easier to compute, particula rly if the blocks have \na nice form. In Exercises 64-68, verify by block multiplica­\ntion that the inverse of a matrix, if partitioned as shown, is \nas claimed. (Ass ume that all inverses exist as needed.)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":26440,"to":26517}}}}],[545,{"pageContent":"180 \nChapter 3 Matrices \n65.\n[\n� �r\nl \n[\n-\n(\nB C\n)\n-\n1 \n(\nB C\n)\n-\n1\nB ] \nC(\nB\nC)\n-\n1 \nI -C(\nB\nC)\n-\n1\nB \nIn Exercises 69-72, partition th e given matrix so that you \ncan apply one of the formulas from Exercises 64-68, and \nthen calculate th e inverse using that formula. \n66. [� �r\nl \n[ \n( I -BC\n)\n-\n1 \n-(I -  B\nC)\n-\n1\nB ] \n-C(I -  B C\n)\n-\n1 \nI  + C(I -  B C\n)\n-\n1\nB \n67. [� �r\nl \n0  0 \n1  0 \n3  1 \n2  0 \n�\n] \n[ \n-\n(\nBD\n-\n1\nC )\n-\n1 \n-\nD\n-\n1\nc(\nBD\n-\n1\nc)\n-\nl \n(\nBD\n-\n1\nC )\n-\n1\nBD\n-\n1 \n] \nD\n-\n1 \n-  D\n-\n1\nc(\nBD\n-\n1\nc)\n-\n1\nBD\n-\nl \n70. The matrix in Exercise 58 \n0 \n68.\n[\nA\nC     D\nB\n]\n-\n1\n-\n-[\nR\np \nQ\n] \nS \n, \nwhe\nre \nP \n=\n(\nA \n-\nBD\n-\n1\nC\n)\n-\n1\n, \n71. \n[\n� \n�\n: \n0 \n1 \n72. \nQ \n= -PBD\n-\n1\n, R = -D\n-\n1\nCP, and S = D\n-\n1 \n-1 \n1 \n+ D\n-\n1\nCPBD\n-\n1 \n• \n1 \n0 \nThe LU Factorization \nJust as it is natural (and illuminating) to factor a natural number into a product of \nother natural numbers-for example, 30 = 2 \n· 3 · 5-it is also frequently helpful to fac­\ntor matrices as products of other matrices. Any representation of a matrix as a product \nof two or more other matrices is called a matrix factorization. For example, \n[\n3  -1\n] = \n[\nl  0\n]\n[\n3  -1\n] \n9  -5 3  1  0 -2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":26519,"to":26705}}}}],[546,{"pageContent":"of two or more other matrices is called a matrix factorization. For example, \n[\n3  -1\n] = \n[\nl  0\n]\n[\n3  -1\n] \n9  -5 3  1  0 -2 \nis a matrix factorization. \nNeedless to say, some factorizations are more useful than others. In this section, \nwe introduce a matrix factorization that arises in the solution of systems of linear \nequations by Gaussian elimination and is particularly well suited to computer imple­\nmentation. In subsequent chapters, we will encounter other equally useful matrix \nfactorizations. Indeed, the topic is a rich one, and entire books and courses have been \ndevoted to it. \nConsider a system of linear equations of the form Ax = b, where A is an n X n \nmatrix. Our goal is to show that Gaussian elimination implicitly factors A into a    prod­\nuct of matrices that then enable us to   solve the given system (and any other system \nwith the same coefficient matrix) easily. \nThe following example illustrates the basic idea. \nExample 3.33 \nLet \n1 \n-1 \n5","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":26705,"to":26732}}}}],[547,{"pageContent":"The LU factorization was introduced \nin 1948 by the great English \nmathematician Alan M. Turing \n(1912-1954) in a paper entitled \n\"Rounding-off Errors in Matrix \nProcesses\" (Quarterly Jo urnal of \nMechanics and Applied Mathematics, \n1 (1948), pp. 287-308). During \nWorld War II, Turing was \ninstrumental in cracking the \nGerman \"Enigma'' code. However, \nhe is best known for his work in \nmathematical logic that laid the \ntheoretical groundwork for the \ndevelopment of the digital computer \nand the modern field of artificial \nintelligence. The \"Turing test\" \nthat he proposed in 1950 is still \nused as one of the benchmarks in \naddressing the question of whether \na computer can be considered \n\"intelligent:' \nSection 3.4 \nThe LU Factorization \n181 \nRow reduction of A proceeds as follows: \n-1 \n5 \nu (1) \nThe three elementary matrices £\n1\n, E\n2\n, E\n3 \nthat accomplish this reduction of A to \nechelon form U are (in order): \nHence, \nSolving for A, we get \n0 \n1 \n0 \nA = E\n1\n-\n1\nE\n2\n-\n1\nE\n3\n-\n1\nU = \nThus, A can be factored as \n[\n� \nu \n0 \n1 \n0 \n�\nJ\nU \n0 \n1 \n0 \n0 \n�\nm \n1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":26734,"to":26805}}}}],[548,{"pageContent":"1\n, E\n2\n, E\n3 \nthat accomplish this reduction of A to \nechelon form U are (in order): \nHence, \nSolving for A, we get \n0 \n1 \n0 \nA = E\n1\n-\n1\nE\n2\n-\n1\nE\n3\n-\n1\nU = \nThus, A can be factored as \n[\n� \nu \n0 \n1 \n0 \n�\nJ\nU \n0 \n1 \n0 \n0 \n�\nm \n1 \n0 \n0 \n�\nJ\nu\n� \nLU \n-2 \nA= LU \n0 \n�\nJ\nu \n1 \n-2 \nwhere U is an upper triangular matrix (see the exercises for Section 3.2), and Lis unit \nlower triangular. That is, L has the form \nr\n�\n[\n:: !\n] \nwith zeros above and ls on the main diagonal. \nThe preceding example motivates the following definition. \nDefinition \nLet A be a square matrix. A factorization of A as A =   LU, where \nL is unit lower triangular and U is upper tr  iangular, is called an LU factorization \nof A. \nRemarks \n• \nObserve that the matrix A in Example 3.33 had an LU factorization because no \nrow interchanges were needed in the row reduction of A. Hence, all of the elementary \nmatrices that arose were unit lower triangular. Thus, L was guaranteed to be unit","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":26805,"to":26879}}}}],[549,{"pageContent":"182 \nChapter 3 Matrices \nlower triangular because inverses and products of unit lower triangular matrices are \nalso unit lower triangular. (See Exercises 29 and 30.) \nIf a zero had appeared in a pivot position at any step, we would have had to swap \nrows to get a nonzero pivot. This would have resulted in L no longer being unit lower \n� \ntriangular. We will comment further on this observation below. (Can you find a ma­\ntrix for which row interchanges will be necessary?) \nTheorem 3.15 \nExample 3.34 \n• \nThe notion of an LU factorization can be   generalized to nonsquare matrices \nby simply requiring U to be a matrix in row echelon form. (See Exercises 13 and 14.) \n• \nSome books define an LU factorization of a square matrix A  to be any factor­\nization A =   LU, where L is lower triangular and U is upper triangular. \nThe first remark above is essentially a proof of the following theorem. \nIf A is a square matrix that can be reduced to row echelon form without using any","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":26881,"to":26899}}}}],[550,{"pageContent":"The first remark above is essentially a proof of the following theorem. \nIf A is a square matrix that can be reduced to row echelon form without using any \nrow interchanges, then A has an LU factorization. \nTo see why the LU factorization is useful, consider a linear system Ax = b,   where \nthe coefficient matrix has an LU factorization A =   LU. We can rewrite the system \nAx = bas LUx = b or L(Ux) = b. Ifwe now define y =   Ux, then we can solve for x in \ntwo stages: \n1. Solve Ly = b  for y by forward substitution (see Exercises 25 and 26 in Section 2.1). \n2. Solve Ux = y  for x by back substitution. \nEach of these linear systems is straightforward to solve because the coefficient matri­\nces L and U are both triangular. The next example illustrates the method. \nUse an LU factorization of A = [ \n! \n-2 \n1 \n-1 \n5 \nSolution \nIn Example 3.33, we found that \n!\n] \nto ,olveAx \n� \nb, whece b \n� \n[ -: l \nA = [ 2 \n� \n�\ni [� \n-\n� \n-�\n] \n=LU \n-1  -2  1   0    0    2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":26899,"to":26934}}}}],[551,{"pageContent":"Use an LU factorization of A = [ \n! \n-2 \n1 \n-1 \n5 \nSolution \nIn Example 3.33, we found that \n!\n] \nto ,olveAx \n� \nb, whece b \n� \n[ -: l \nA = [ 2 \n� \n�\ni [� \n-\n� \n-�\n] \n=LU \n-1  -2  1   0    0    2 \nAs outlined above, to solve Ax = b   (which is the same as L( Ux) = b ), we first solve \nLy\n� \nb fm y \n� \n[\nf \n:J Thi'\n'\n' ju,t the Hnem-'l\"tem \nY\n1 \n2y\n1 \n+ \nY\n2 \n-4 \n-y\n1 \n-2y\n2 \n+ y\n3 \n= \n9 \nForward substitution (that is,  working from top to bottom) yields \nY\ni\n= \nl,y\n2 \n=  -4 \n-\n2y\ni \n= \n-\n6\n,\ny3 = 9 +\nY\ni\n+ 2y\n2 \n=  -2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":26934,"to":27004}}}}],[552,{"pageContent":"Section 3.4 \nThe LU Factorization \n183 \n2x\n1 \n+ \nx\n2 \n+ 3x\n3 \n= \n-3x\n2 \n-3x\n3 \n=  -6 \n2x\n3 \n= -2 \nand back substitution quickly produces \nX\n3 \n=  -1, \n-3x\n2 \n=  -6 + 3x\n3 \n=  -9 so that x\n2 \n= 3, and \n2x\n1 \n=  1 -  x\n2 \n-3x\n3 \n=  1   so that x\n1 \n= \nt \nThmfoce, the rn   lution to the given 'Y'tem Ax \n� \nb ;, x \n� \n[ \n_ \n! l · \nAn Easv wav to  Find LU Factorizations \nIn Example 3.33, we computed the matr  ix L as a product of elementary matrices. \nFortunately, L can be computed directly from the row reduction process without our \nneeding to compute elementary matrices at all. Remember that we  are assuming that \nA can be reduced to row echelon form without using any row interchanges. If this is \nthe case, then the entire row reduction process can be   done using only elementary \nii>-\"'--\nrow operations of the form R; -   kR\n1\n. (Why do we not need to use the remaining \nelementary row operation, multiplying a row by a nonzero scalar?) In the operation \nR; -  kR\nj\n, we will refer to the scalar k as the multiplier. \nExample 3.35","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":27006,"to":27067}}}}],[553,{"pageContent":"1\n. (Why do we not need to use the remaining \nelementary row operation, multiplying a row by a nonzero scalar?) In the operation \nR; -  kR\nj\n, we will refer to the scalar k as the multiplier. \nExample 3.35 \nIn Example 3.33, the elementary row operations that were used were, in order, \nR\n2 \n-2R\n1 \nR\n3 \n+ R\n1 \n= R\n3 \n- (\n-l\n)\nR\n1 \nR\n3 \n+ 2R\n2 \n= R\n3 \n-\n(\n-2\n)\nR\n2 \n(multiplier = 2) \n(multiplier = -1) \n(multiplier = -2) \nThe multipliers are precisely the entries of L that are below its diagonal! Indeed, \nand L\n2\n1 \n= 2, L\n3\n1 \n=  -1, and L\n3\n2 \n= -2. Notice that the elementary row operation \nR; -  kR\n1 \nhas its multiplier k placed in the \n(\ni, j\n) \nentry of L. \nFind an LU factorization of \nA= r \n! \n4 \n! -� �\n1 \n3     2 \n5 \n-1 \n-9  5 -2 \n-\n4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":27067,"to":27134}}}}],[554,{"pageContent":"184 \nChapter 3 Matrices \nSolulion \nReducing A to row echelon form, we have \n[j \n3 \n4 \n8 \nA= \n2    5 \n5 \n-2 \n-4\n] \n[\n' \n-10 \nR\n2\n-2R1 \nQ \nR3-R   1 \n-1 R4- ( - 3)R1 0 \n-4 \n-----+ \n0 \n[\n; \nR\n,\n-\n!\nR\n2 \nR4-4R1 \n-----+ \n.. -HJ•,\n[\n: \n-----+ \n0 \n0 \n1 \n3 \n-4\n] \n2  2 \n-2 \n2 \n-1\n! \n8  7 \n3 \n-4\n] \n2    2 \n-2 \n0 \n4 \n0 \n- 1 \n-8 \n3 \n-4\n] \n2  2 \n-2 \n=U \n0 \n1 \n4 \n0  0 \n-4 \nThe first three multipliers are 2, 1, and -3, and these go into the subdiagonal entries \nof the first column of L. So, thus far, \nThe next two multipliers are \nt \nand 4, so   we continue to fill out L: \nThe final multiplier, \n-\n1, replaces the last * in  L to give \nThus, an LU factorization of A is \nA= \n[ \n! \n4 \n! \n-� �i \n[ \n� \n0 \n� �\ni \n[\n� \n2 \n� \n=�i \n= \nLU \n3  2    5 -1 \n1 \nt \n0   0  0  1    4 \n-9  5  -2 -4 \n-3  4  -1  1   0  0  0 -4 \nas is   easily checked.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":27136,"to":27242}}}}],[555,{"pageContent":"Section 3.4 \nThe LU Factorization \n185 \nRemarks \n• \nIn applying this method, it is importa nt to note that the  elementary row opera­\ntions R; - kR\nj \nmust be pe  rformed from top to bottom within each column (using the \ndiagonal entry as the pivot), and column by column from left to right. To illustrate \nwhat can go wrong if we do not obey these rules, consider the following row reduction: \nA=\n[\n� \n� �\ni \n�' \n[\n� \n�  �\ni � \n[\n� \n-\n� \n-\n�\n] \nU \n2  2  1 \n0  0 -1 \n0    0 -1 \nThis time the multipliers would be placed in L as follows: L32 = 2, L21 = 1. We would \nget \nL\n�\n[\ni: �\n] \n.,._.. \nbut A * LU. (Check this! Find a correct LU factorization of A.) \n• \nAn alternative way to construct L is to   observe that the  multipliers can be \nobtained directly from the matrices obtained at the intermediate steps of the row \nreduction process. In Example 3.33, examine the pivots and the corresponding col­\numns of the matrices that arise in the row reduction \n1 \n-1 \n5 \nu","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":27244,"to":27293}}}}],[556,{"pageContent":"reduction process. In Example 3.33, examine the pivots and the corresponding col­\numns of the matrices that arise in the row reduction \n1 \n-1 \n5 \nu \nThe first pivot is 2, which occurs in the first column of A. Dividing the entries of \nthis column vector that are on or below the diagonal by the pivot produces \n±\n[ \nJ \n[ \n_\n�\n-\nThe next pivot is -3, which occurs in the second column of A1. Dividing the entries \nof this column vector that are on or below the diagonal by the pivot, we obtain \nThe final pivot (which we did not need to use) is 2, in the third column of U. Divid­\ning the entries of this column vector that are on or below the diagonal by the pivot, \nwe obtain \n±\n[\nJ [J \nIf we place the resulting three column vectors side by side in a matrix, we have \nu \n-\n� \nJ \nwhich is exactly L once the above-diagonal entries are filled with zeros.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":27293,"to":27321}}}}],[557,{"pageContent":"186 \nChapter 3 Matrices \nIn Chapter 2, we remarked that the row echelon form of a matr  ix is not unique. \nHowever, if an invertible matrix A  has an LU factorization A= LU, then this factoriza­\ntion is unique. \nTheorem 3 .16 \nIf A is an invertible matr  ix that has an LU factorization, then L and U are unique. \nProof Suppose A =   LU and A = L\n1 \nU\n1 \nare two LU factorizations of A. Then LU = \nL\n1 \nU\n1\n, where L and L\n1 \nare unit lower triangular and U and U\n1 \nare upper tria  ngular. In \nfact, U and U\n1 \nare two (possibly different) row echelon forms of A. \nBy Exercise 30, L\n1 \nis invertible. Because A  is invertible, its reduced row echelon \nform is an identity matrix I  by the Fundamental Theorem of Invertible Matrices. \n� \nHence U also row reduces to I (why?) and so U is invertible also. Therefore, \nLj\n1\n(\nLU\n)\nu\n-\n1 \n= Lj\n1\n(\nL\n1 \nU\n1\n)\nu\n-\n1 \nso \n(\nL\nl\n1\nL\n)(\nuu\n-\n1\n) \n= \n(\nL\nl\n1\nL\n1\n)(\nU\nI \nu\n-\n1\n) \nHence, \n(\nLl\n1\nL\n)\nI = I\n(\nU\nI \nu\n-\n1\n) \nso  Li\n1\nL =  U\n1 \nu\n-\nI \nBut Lj\n1\nL is unit lower triangular by Exercise 29, and  U\n1 \nu\n-\n1 \nis upper triangular. \n�","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":27323,"to":27426}}}}],[558,{"pageContent":"Lj\n1\n(\nLU\n)\nu\n-\n1 \n= Lj\n1\n(\nL\n1 \nU\n1\n)\nu\n-\n1 \nso \n(\nL\nl\n1\nL\n)(\nuu\n-\n1\n) \n= \n(\nL\nl\n1\nL\n1\n)(\nU\nI \nu\n-\n1\n) \nHence, \n(\nLl\n1\nL\n)\nI = I\n(\nU\nI \nu\n-\n1\n) \nso  Li\n1\nL =  U\n1 \nu\n-\nI \nBut Lj\n1\nL is unit lower triangular by Exercise 29, and  U\n1 \nu\n-\n1 \nis upper triangular. \n� \n(Why?) It follows that Lj\n1\nL =  U\n1 \nu\n-\n1 \nis both unit lower triangular and upper tri­\nangular. The only such matr  ix is the identity matrix, so Lj\n1\nL = I and U\n1 \nu\n-\n1 \n= I. It \nfollows that L = L\n1 \nand U = U\n1\n, so the LU factorization of A is unique. \nThe pr LU Factorization \nWe now explore the problem of adapting the LU factorization to handle cases where \nrow interchanges are necessary during Gaussian elimination. Consider the matr  ix \nA straightforward row reduction produces \nwhich is not an upper triangular matrix. However, we can easily convert this into \nupper triangular form by swapping rows 2 and 3 of B to get \nAlternatively, we can swap rows 2 and 3 of A first. To this end, let P be the elementary \nmatrix","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":27426,"to":27528}}}}],[559,{"pageContent":"Theorem 3.11 \nExample 3.36 \nSection 3.4 \nThe LU Factorization \n181 \ncorresponding to interchanging rows 2  and 3, and let E be the product of the \nelementary matrices that then reduce PA  to U (so that E\n-\n1 \n= Lis unit lower triangu­\nlar). Thus EPA = U, so A= (EP)\n-\n1\nU = P\n-\n1\nE\n-\n1\nU = P\n-\n1\nLU. \nNow this handles only the case of a single row interchange. In general, P will be \nthe product P = P\nk\n· ·  ·P\n2\nP\n1 \nof all the row interchange matrices P\n1\n,P\n2\n, ••• , P\nk \n(where \nP\n1 \nis performed first, and so on). Such a matrix Pis called a permutation matrix. Ob­\nserve that a  permutation matr  ix arises from permuting the rows of an identity matrix \nin some order. For example, the following are all permutation matrices: \n[\n� \n�\n]\n. \n[\n: \n0 \n0 \nFortunately, the inverse of a permutation matrix is easy to compute; in fact, no calcu­\nlations are needed at all! \nIf Pis a  permutation matrix, then p\n-\nl \n= P\nT\n. \nProof We must show that p\nT\np =  I. But the ith row of p\nT \nis the same as the  ith","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":27530,"to":27593}}}}],[560,{"pageContent":"lations are needed at all! \nIf Pis a  permutation matrix, then p\n-\nl \n= P\nT\n. \nProof We must show that p\nT\np =  I. But the ith row of p\nT \nis the same as the  ith \ncolumn of P, and these are both equal to the same standard unit vector e, because P \nis a permutation matrix. So \n(\np\nr\np\n)\n;; \n= \n(\nith row of p\nr\n)(\nith column of P\n) \n= e\nr\ne = e \n· \ne =  1 \nThis shows that diagonal entries of p\nT\np are all ls. On the other hand, if j i=   i, then \nthe jth column of Pis a different standard unit vector from e-say e'. Thus, a typical \noff-diagonal entry of p\nr\np is given by \n(\np\nr\np\n)\niJ \n= \n(\nith row of p\nr\n)\n(jth column of P\n) \n= e\nr\ne' = e \n· \ne' = 0 \nHence p\nT \nP is an identity matrix, as we wished to show. \nThus, in general, we can factor a square matrix A  as A = P\n-\n1\nLU =P\nr \nLU. \nDefinition Let A be a square matrix. A factorization of A as A = P\nT\nLU, where \nP is a permutation matrix, L is unit lower triangular, and U is upper triangular, is \ncalled a Pr LU factorization of A. \nFind o p\nT \nLU fadmizotion of A \n� \n[\n: � n","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":27593,"to":27669}}}}],[561,{"pageContent":"T\nLU, where \nP is a permutation matrix, L is unit lower triangular, and U is upper triangular, is \ncalled a Pr LU factorization of A. \nFind o p\nT \nLU fadmizotion of A \n� \n[\n: � n \nSolution First we reduce A to row echelon form. Clearly, we need at least one row \ninterchange. \n[\n: \n0 \n!\n] \n[\ni \n2 \n!\n] \n[\ni \n2 \n-\n�\nl \nA= \n2 \nR1 \n+-*\nRz \n0 \nR,\n- 2\nR1 \n0 \n� � \n1 \n-3 \n[\n: \n2 \n-\n!\n] \nR\nz +-*\nR3 \n-3 \n� \n0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":27669,"to":27721}}}}],[562,{"pageContent":"188 \nChapter 3 Matrices \nTheorem 3.18 \nWe have used two row interchanges (R\n1 \n� R\n2 \nand then R\n2 \n� R\n3\n), so the required \npermutation matrix is \n[\ni \n0 \n�\nm \n1 \n:\nJ \n[\n: \n1 \n�\n] \np = P\n2\nP\n1 \n= \n0 0 0 \n0 0 \nWe now find an LU factorization of PA . \n[\n: \n1 \n�\nm \n0 \n!\n] \n[\n� \n2 \n!\n] \n[\n: \n2 \nPA = \n0 \n2 \n1 \nR2 -\n2\nR1 \n-3 \n0 \n1 \n0 \n0 \nHence L\n2\n1 \n= 2, and so \nA\n� \nP'LU \n� \n[\n� \n: \nrn\n� � \n:\nJ\n[\ni -� \n-\n!\n: \nThe discussion above justifies the following theorem. \nEvery square matrix has a P\nT\nLU factorization. \n-\n!\n] \nu \nRemark Even for an invertible matrix, the P\nT\nLU factorization is not unique. In \nExample 3.36, a single row interchange R\n1 \n� R\n3 \nalso would have worked, leading to \na different P. However, once P has been determined, L and U are unique. \nComputational Considerations \nIf A is n X n, then the total number of operations (multiplications and divisions) required \nto solve a linear system Ax = b  using an LU factorization of A) is T (n) \n= \nn\n3 \n/\n3, the same \nas is   required for Gaussian elimination. (See the Exploration \"Counting Operations;'","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":27723,"to":27830}}}}],[563,{"pageContent":"to solve a linear system Ax = b  using an LU factorization of A) is T (n) \n= \nn\n3 \n/\n3, the same \nas is   required for Gaussian elimination. (See the Exploration \"Counting Operations;' \nin Chapter 2.) This is hardly surprising since the forward elimination phase produces \nthe LU factorization in \n= \nn\n3 \n/\n3 steps, whereas both forward and backward substitution \nrequire \n= \nn\n2 \n/2 steps. Therefore, for large values of n, the n\n3 \n/\n3 term is dominant. From \nthis point of view, then, Gaussian elimination and the LU factorization are equivalent. \nHowever, the LU factorization has other advantages: \n• \nFrom a storage point of view, the LU factorization is very compact because \nwe can overwrite the entries of A with the entries of L and U as they are computed. In \nExample 3.33, we found that \nA= \n[ \n� \n-\n� \n�\ni \n[ \n� \n� �\ni \n[\n� \n-\n� \n-\n�\n] \n=LU \n-2 5  5 -1  -2  1   0    0    2 \nThis can be stored as \n[ \n� \n=\n� \n-\n�\n] \n-1  -2 \n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":27830,"to":27887}}}}],[564,{"pageContent":"Section 3.4 \nThe LU Factorization \n189 \nwith the entries placed in the order (1,1), (1,2), (1,3), (2,1), (3,1), (2,2), (2,3), (3,2), \n(3,3). In other words, the subdiagonal entries of A are replaced by the corresponding \nmultipliers. (Check that this works!) \n• \nOnce an LU factorization of A has been computed, it can be used to solve as \nmany linear systems of the form Ax = b as we like. We just need to apply the method \nof Example 3.34, varying the vector b each time. \n• \nFor matrices with certain special forms, especially those with a large number \nof zeros (so-called \"sparse\" matrices) concentrated off the diagonal, there are methods \nthat will simplify the computation of an LU factorization. In these cases, this method \nis faster than Gaussian elimination in solving Ax = b. \n• \nFor an invertible matrix A, an LU factorization of A can be used to find A\n-\n1\n, \nif necessary. Moreover, this can be done in such a way that it  simultaneously yields a \nfactorization of A\n-\n1\n. (See Exercises 15-18.)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":27889,"to":27913}}}}],[565,{"pageContent":"-\n1\n, \nif necessary. Moreover, this can be done in such a way that it  simultaneously yields a \nfactorization of A\n-\n1\n. (See Exercises 15-18.) \nRemark If you have a CAS (such as MATLAB) that has the LU factorization \nbuilt in, you may notice some differences between your hand calculations and the \ncomputer output. This is because most CAS's will automatically try to   perform partial \npivoting to reduce roundoff errors. (See the Exploration \"Partial Pivoting;' in Chapter \n2.) Turing's paper is an extended discussion of such errors in the context of matrix \nfactorizations. \nThis section has served to introduce one of the most useful matrix factorizations. \nIn subsequent chapters, we will encounter other equally useful factorizations. \nI Exercises 3.4 \nIn Exercises 1-6, solve th e system Ax = b using the given \nLU factorization of A. \n1. A= \n2. A = \n3. A = \n4. A = \n[\n-2 \nl\n]  [ \n1  O\nJ\n[\n-2 \nl\n] \nb =\n[\nS\nJ \n2  5 -1  1    0  6 ' \n1 \n[� -�\n] \n= \n[t \n�\n][\n� \n-\n!\nl\nb=\n[\n�\n] \n[\n-\n! \n_\n:  =\n�J \nH \n_\n; \n�\nl \nx \n[\n� \n4 \n=\n!], \nb \n= \n[\n-\n�\n1 \n0  0 -� 0 \n[ \n� \n=\n� \n�\n1 \n[ \n; \n0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":27913,"to":27998}}}}],[566,{"pageContent":"LU factorization of A. \n1. A= \n2. A = \n3. A = \n4. A = \n[\n-2 \nl\n]  [ \n1  O\nJ\n[\n-2 \nl\n] \nb =\n[\nS\nJ \n2  5 -1  1    0  6 ' \n1 \n[� -�\n] \n= \n[t \n�\n][\n� \n-\n!\nl\nb=\n[\n�\n] \n[\n-\n! \n_\n:  =\n�J \nH \n_\n; \n�\nl \nx \n[\n� \n4 \n=\n!], \nb \n= \n[\n-\n�\n1 \n0  0 -� 0 \n[ \n� \n=\n� \n�\n1 \n[ \n; \n0 \n�\n1 \n-1 2  2 \n-� 0  1 \nx [� \n-� H \nb \n� \nU_ \n5. A = \n6. A = \nIn Exercises 7-12,find an LU factorization of the given matrix. \n[ \n1    2\n] \n[\n2  -4\n] \n7\n· \n-3  -1 \ns. 3    1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":27998,"to":28087}}}}],[567,{"pageContent":"190 \nChapter 3 Matrices \n9. \n[\n: \n; \n: \nl \n11.\n[ \n� \n: \n_\n; \n-:i \n-\n1   -2 \n-9 \n0 \n12. \n[\n-\n� \n! \n-\n� \n�\ni \n4  4 \n7  3 \n6  9 \n5  8 \nGeneralize the definition of LU factorization to nonsquare \nmatrices by simply requiring U to be a matrix in row ech­\nelon form. With this modification, fi nd an LU factorization \nof th e matrices in Exercises 13 and 14. \n[1 c \n-\n�\n] \n13. 0  3 \n3 \n0  0  0 \n[ \n1 \n2 \n0 \n-1 \n-\n�\n] \n-2 \n-7 3 \n8 \n14. \n� \n1 \n3 \n5 \n3 \n-3 \n-6 \nFor an invertible matrix with an LU factorization A =   LU, \nboth L and U will be invertible and A \n-\nl \n= u\n-\n1 \nL \n-\n1\n. In \nExercises 15 and 16,find L\n-\n1\n,  u\n-\n1\n, and A\n-\n1 \nfor the given \nmatrix. \n15. A in Exercise 1 \n16. A in Exercise 4 \nThe inverse of a matrix can also be computed by solving sev­\neral systems of equations using the method of Example 3.34. \nFor an n X n matrix A, to fi nd its inverse we need to solve \nAX = I\nn \nfor the n X n matrix X. Writing this equation as \nA [ x\n1 \nx\n2 \n• \n· · x\nn\n]  =  [ e\n1 \ne\n2 \n· · · e\nn\n], using the matrix-column \nform of AX, we see that we need to solve n systems of linear \nequations: Ax\n1 \n= e\n1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":28089,"to":28199}}}}],[568,{"pageContent":"n \nfor the n X n matrix X. Writing this equation as \nA [ x\n1 \nx\n2 \n• \n· · x\nn\n]  =  [ e\n1 \ne\n2 \n· · · e\nn\n], using the matrix-column \nform of AX, we see that we need to solve n systems of linear \nequations: Ax\n1 \n= e\n1\n, Ax\n2 \n= e\n2\n, ... , AX\nn \n= e\nn\n· \nMoreover, we \ncan use the factorization A =   LU to solve each one of these \nsystems. \nIn Exercises 17 and 18, use th e approach just outlined to \nfi nd A \n-\nl \nfor th e given matrix. Compare with th e method of \nExercises 15 and 16. \n17. A in Exercise 1 \n18. A in Exercise 4 \nIn Exercises 19-22, write the given permutation matrix as a \nproduct of elementary (row interchange) matrices. \n1\n9\n. \n[\n: \n�\n] \n20.\n[� \n0  0 \n�\n] \n0 \n0 \n1 \n0 \n1 \n0 \n0  0 \n0  0 \n1 \n0  0 \n21.[ \n1 \n0 \n;\n] \n1 \n0  0  0  0 \n0  0 \n22. \n0  0  0 \n1 \n0 \n0  0 \n0  0  0  0 \n1 \n0 \n1 \n0     0  0  0 \nIn Exercises 23-25, fi nd a P\nT\nLU factorization of th e given \nmatrix A. \nH \n!\n] \n[\n-\n; \n0 \n1 \n3 \n23.A = \n2 24. A= \n2 \n3 \n-1 \n[\n-\n; \n-1 \ni\nl \n1 \n25.A = \n1 \n-1 \n0    1 \n26. Prove that there are exactly n! n X n permutation \nmatrices. \n�\n]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":28199,"to":28314}}}}],[569,{"pageContent":"T\nLU factorization of th e given \nmatrix A. \nH \n!\n] \n[\n-\n; \n0 \n1 \n3 \n23.A = \n2 24. A= \n2 \n3 \n-1 \n[\n-\n; \n-1 \ni\nl \n1 \n25.A = \n1 \n-1 \n0    1 \n26. Prove that there are exactly n! n X n permutation \nmatrices. \n�\n] \nIn Exercises 27-28, solve th e system Ax = busing the given \nfactorization A = P\nT\nLU. B ecause pp\nT \n= I, P\nT\nLUx = b can \nbe rewritten as LUx = Pb. This system can then be solved \nusing th e method of Example 3.34. \n27. A = \n[ \n� \n� \n-\n� \nl \n[ \n� \n� \n� \nl \n[ \n�  C : \nl \n1 -1 0  0  1 2 -\nt \n1 \nx \n[\n: \n3 \n�l \nl \n� \nP'LU, b \n� \nm \n1 \n0 \n[\n! \n3 \n�-\n[\n: \n:\nJ \n[\n: \n0 \n:\nJ \n28.A = 0 \n1 \n0 0 \n-1 \nx \n[\n� \n1 \nr \n[ \n�:i \n-1 \n= P\nT\nLU, b = \n0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":28314,"to":28414}}}}],[570,{"pageContent":"Section 3.5 \nSubspaces, Basis, Dimension, and Rank \n191 \n29. Prove that a product of unit lower triangular matrices \nis unit lower triangular. \n31. A in Exercise 1 \n32. A in Exercise 4 \n33. If A is symmetric and invertible and has an LDU \nfactorization, show that U = L\nr\n. \n30. Prove that every unit lower triangular matr  ix is \ninvertible and that its inverse is also unit lower \ntriangular. \n34. If A is symmetric and invertible and A = LDL \nT \n(with L \nunit lower triangular and D diagonal), prove that this \nfactorization is unique. That is, prove that if we also \nhave A = L\n1\nD\n1\nL[ (with L\n1 \nunit lower triangular and D\n1 \ndiagonal), then L = L\n1 \nand D = D\n1 • \nAn LDU factorization of a square matrix A is a factoriza­\ntion A = LDU, where L is a unit lower triangular matrix, \nD is a diagonal matrix, and U is a unit upper triangu -\nlar matrix (upper triangula r with ls on its diagonal). In \nExercises 31and32,find an LDU factorization of A. \nz \n2u \n+ \nv \nx \ny \nFigure 3.2 \nSubspaces,  Basis,  Dimension, and Rank","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":28416,"to":28459}}}}],[571,{"pageContent":"lar matrix (upper triangula r with ls on its diagonal). In \nExercises 31and32,find an LDU factorization of A. \nz \n2u \n+ \nv \nx \ny \nFigure 3.2 \nSubspaces,  Basis,  Dimension, and Rank \nThis section introduces perhaps the most important ideas in the entire book. We have \nalready seen that there is an interplay between geometry and algebra: We can often \nuse geometric intuition and reasoning to obtain algebraic results, and the power of \nalgebra will often allow us to   extend our findings well beyond the geometric settings \nin which they first arose. \nIn our study of vectors, we have already encountered all of the concepts in this \nsection informally. Here, we will start to become more formal by giving definitions \nfor the key ideas.  As you'll see, the notion of a subspace is simply an algebraic \ngeneralization of the geometric examples of lines and planes through the origin. The \nfundamental concept of a basis for a subspace is then derived from the idea of direc­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":28459,"to":28478}}}}],[572,{"pageContent":"generalization of the geometric examples of lines and planes through the origin. The \nfundamental concept of a basis for a subspace is then derived from the idea of direc­\ntion vectors for such lines and planes. The concept of a basis will allow us to   give a \nprecise definition of dimension that agrees with an intuitive, geometric idea of the \nterm, yet is flexible enough to allow generalization to other settings. \nYou will also begin to see that these ideas shed more light on what you already \nknow about matrices and the solution of systems of linear equations. In Chapter 6, \nwe will encounter all of these fundamental ideas again, in more detail. Consider this \nsection a \"getting to know you\" session. \nA plane through the origin in IR\n3 \n\"looks like\" a copy of IR\n2\n• Intuitively, we would \nagree that they are both \"two-dimensional:' Pressed further, we might also say that \nany calculation that can be done with vectors in IR\n2 \ncan also be done in a plane through","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":28478,"to":28495}}}}],[573,{"pageContent":"2\n• Intuitively, we would \nagree that they are both \"two-dimensional:' Pressed further, we might also say that \nany calculation that can be done with vectors in IR\n2 \ncan also be done in a plane through \nthe origin. In particular, we can add and take scalar multiples (and, more generally, \nform linear combinations) of vectors in such a plane, and the results are other vec­\ntors in the same plane. We say that, like IR\n2\n, a plane through the origin is closed with \nrespect to the operations of addition and scalar multiplication. (See Figure 3.2.) \nBut are the vectors in this plane two- or three-dimensional objects? We might \nargue that they are three-dimensional because they live in IR\n3 \nand therefore have three \ncomponents. On the other hand, they can be described as a linear combination of just \ntwo vectors-direction vectors for the plane-and so are two-dimensional objects liv­\ning in a two-dimensional plane. The notion of a subspace is the key to resolving this \nconundrum.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":28495,"to":28514}}}}],[574,{"pageContent":"192 \nChapter 3 Matrices \nExample 3.31 \nTheorem 3.19 \nDefinition A subspace of !R\nn \nis any collection S of vectors in !R\nn \nsuch that: \n1. The zero vector 0 is in S. \n2. If u and v are in S, then u + vis in S. (S is closed under addition.) \n3. If u is in S and c is a scalar, then cu is in S. (S is closed under scalar \nmultiplication.) \nWe could have combined properties (2) and (3) and required, equivalently, that S  be \nclosed under linear combinations: \nIf u\n1\n, u\n2\n, •.• , u\nk \nare in S and c\n1\n, c\n2\n, .•. , c\nk \nare scalars, \nthen c\n1\nu\n1 \n+ c\n2\nu\n2 \n+ · · · + c\nk\nu\nk \nis in S. \nEvery line and plane through the origin in IR\n3 \nis a subspace of IR\n3\n. It should be clear \ngeometrically that properties (1) through (3) are satisfied. Here is an algebraic proof \nin the case of a plane through the origin. You are asked to give the corresponding \nproof for a line in Exercise 9. \nLet <JP be a plane through the origin with direction vectors v\n1 \nand v\n2\n. Hence, <JP = \nspan (v\n1\n, v\n2\n). The zero vector 0 is in <JP, since 0 = Ov\n1 \n+ Ov\n2\n• Now let","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":28516,"to":28578}}}}],[575,{"pageContent":"proof for a line in Exercise 9. \nLet <JP be a plane through the origin with direction vectors v\n1 \nand v\n2\n. Hence, <JP = \nspan (v\n1\n, v\n2\n). The zero vector 0 is in <JP, since 0 = Ov\n1 \n+ Ov\n2\n• Now let \nbe two vectors in <JP. Then \nThus, u +vis a  linear combination ofv\n1 \nand v\n2 \nand so is in <JP. \nNow let c be a scalar. Then \nwhich shows that cu is also a linear combination ofv\n1 \nand v\n2 \nand is therefore in <JP. We \nhave shown that <JP satisfies properties ( 1) through ( 3) and hence is a subspace of IR\n3 \n4 \nIf you look carefully at the details of Example 3.3 7, you will notice that the  fact \nthat v\n1 \nand v\n2 \nwere vectors in IR\n3 \nplayed no role at all in the verification of the prop­\nerties. Thus, the algebraic method we used should generalize beyond IR\n3 \nand apply \nin situations where we can no longer visualize the geometry. It does. Moreover, the \nmethod of Example 3.37 can serve as a \"template\" in more general settings. When we \ngeneralize Example 3.3 7 to the span of an arbitrary set of vectors in any !R\nn","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":28578,"to":28622}}}}],[576,{"pageContent":"method of Example 3.37 can serve as a \"template\" in more general settings. When we \ngeneralize Example 3.3 7 to the span of an arbitrary set of vectors in any !R\nn\n, the result \nis important enough to be called a theorem. \nLet v\n1\n, v\n2\n, ... , vk be vectors in !R\nn\n. Then span(v\n1\n, v\n2\n, ... , vk) is a subspace of !R\nn\n. \nProof Let S = span (v\n1\n, v\n2\n, ... , vk)\n· \nTo check property (1) of the definition, we simply \nobserve that the zero vector 0 is in  S, since 0 = Ov\n1 \n+ Ov\n2 \n+ \n· · · \n+ Ovk\n.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":28622,"to":28654}}}}],[577,{"pageContent":"Example 3.38 \nSection 3.5 \nSubspaces, Basis, Dimension, and Rank \n193 \nNow let \nbe two vectors in S. Then \nu + v \n= \n(\nc\n1\nv\n1 \n+ c\n2\nv\n2 \n+ · · · + c\nk\nv\nk\n) \n+ \n(\nd\n1\nv\n1 \n+ d\n2\nv\n2 \n+ · · · + d\nk\nv\nk\n) \n= \n(\nc\n1 \n+ d\n1\n)\nv\n1 \n+ \n(\nc\nz \n+ d\nz\n)\nV\n2 \n+ · · · + \n(\nc\nk \n+ d\nk\n)\nv\nk \nThus, u +   vis a  linear combination of v\n1\n, v\n2\n, •.. , vk and so is in S. This verifies prop­\nerty \n(\n2\n)\n. \nTo show property (3), let c be a scalar. Then \ncu = c\n(\nc\n1\nv\n1 \n+ c\n2\nv\n2 \n+ \n· · · \n+ c\nk\nv\nk\n) \n= \n(\ncc\n1\n)\nv\n1 \n+ \n(\ncc\n2\n)\nv\n2 \n+ \n· · · \n+ \n(\ncc\nk\n)\nv\nk \nwhich shows that cu is also a linear combination of v\n1\n, v\n2\n, ... , vk and is therefore \nin S. We have shown that S satisfies properties (1) through (3) and hence is a subspace \nof\nll�r\n. \nWe will refer to span (v\n1\n, v\n2\n, .•. , vk)   as the subspace sp anned by v\n1\n, v\n2\n, ..• , vk\n. \nWe will often be able to save a lot  of work by recognizing when Theorem 3.19 can be \napplied. \nShow that the set of all vectors \n[\n;\n] \nthat satisfy the conditions x = 3y and z  = -2y \nforms a subspace of IR\\\n3\n. \nz \nSolullon Su bstitufog the two conditions into \n[ \n�\n] \nyields","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":28656,"to":28805}}}}],[578,{"pageContent":"applied. \nShow that the set of all vectors \n[\n;\n] \nthat satisfy the conditions x = 3y and z  = -2y \nforms a subspace of IR\\\n3\n. \nz \nSolullon Su bstitufog the two conditions into \n[ \n�\n] \nyields \nSince y is arbitrary, the given set of   vectors is span ( \n[ \n�\n] \n) and is thus a subspace \noflR\\\n3\n, by Theorem 3.19. \n-2 \n4 \nGeometrically, the set of vectors in Example 3.38 represents the line through the \nmigin in GI' with dimtion vectm [ \n_ \nn","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":28805,"to":28833}}}}],[579,{"pageContent":"194 \nChapter 3 Matrices \nExample 3.39 \nDetermine whether the set of all vectors \n[\n;\n] \nthat satisfy the conditions x = \n3y \n+ 1 \nand z  = \n-\n2\ny \nis a subspace of IR\n3\n. \nz \nSolulion \nThis time, we have all vectors of the form \n[\n3y\n: \nl l \n-\n2\ny \n....,.. \nThe zero vector is not of this form. (Why not? Try solving \n[ \n3y\n: \n1 \nl \n-\n2y \nproperty (1) does not hold, so this set cannot be a   subspace of IR\n3\n. \n[\nH\n)\nH'\"\"' \nExample 3.40 \nDetermine whether the set of all vectors \n[;\n], where \ny \n= :x2-, is a subspace of IR\n2\n. \nSolulion These are the vectors of the form \n[\n:\n2 \n]\n-call this set S.   This time 0 = \n[ \n�\n] \nbelongs to S (take x = O), so property (1) holds. Let u = \n[\n:\n�\n] \nand v  = [:�\n]\nbe in S. \nThen \nu + v  = \n[\nx� + x\n�\nJ \nX\n1 \n+ X\nz \nwhich, in general, is not  in S, since it  does not  have  the  correct form; that  is, \nx\n� \n+ x?  * \n(\nx1 + x\n2\n) \n2\n. To be specific, we look for a counterexample. If \nu = \n[ \n�\n] and v  = \n[ \n! \n] \nthen both u and v are in S, but their sum u + v  = \nproperty (2) fails and Sis not a subspace of IR\n2\n. \n[ \n! \n] \nis not in S since 5 * \n3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":28835,"to":28938}}}}],[580,{"pageContent":"u = \n[ \n�\n] and v  = \n[ \n! \n] \nthen both u and v are in S, but their sum u + v  = \nproperty (2) fails and Sis not a subspace of IR\n2\n. \n[ \n! \n] \nis not in S since 5 * \n3\n2\n. Thus, \n.+ \nRemark In order for a set S to be a   subspace of some !R\nn\n, we must prove that \nproperties (1) through \n(\n3\n) hold in general. However, for S to fail to be a   subspace of!R\nn\n, \nit is enough to show that one of the three properties fails to hold. The easiest course is \nusually to find a single, specific counterexample to illustrate the failure of the property. \nOnce you have done so, there is no need to consider the other properties. \nSubspaces Associaled wilh Malrices \nA great many examples of subspaces arise in the context of matrices. We have already \nencountered the most important of these in Chapter 2; we now revisit them with the \nnotion of a subspace in mind.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":28938,"to":28972}}}}],[581,{"pageContent":"Example 3.41 \nSection 3.5 \nSubspaces, Basis, Dimension, and Rank \n195 \nDefinition \nLet A be an m x n matrix. \n1. The row sp ace of A is the subspace row(A) of !R\nn \nspanned by the rows of A. \n2. The column sp ace of A is the subspace col(A) of !R\nm \nspanned by the columns \nof A. \nRemark Observe that,   by Example 3.9 and the Remark that follows it, col(A) \nconsists precisely of all vectors of the form Ax where xis in !R\nn\n. \nConsider the matr  ix \n[ \n1   -1 i \nA= 0 \n1 \n3  -3 \n(')  Dctmnine wheth\" b \n� \n[ \n�\n] \n;, in the column 'P\"' of A. \n(b) Determine whether w =  [ 4 \n5] is in the row space of A. \n(c) Describe row(A) and col(A). \nSolution \n(a) By Theorem 2.4 and the discussion preceding it, bis a  linear combination of the \ncolumns of A if and only if the linear system Ax = b   is consistent. We row reduce \nthe augmented matrix as follows: \n[\n� \n�: \n�\n] \n� \n[\n� \n� \n�\n] \nThus, the system is consistent (and, in fact, has a    unique solution). Therefore, b is","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":28974,"to":29021}}}}],[582,{"pageContent":"the augmented matrix as follows: \n[\n� \n�: \n�\n] \n� \n[\n� \n� \n�\n] \nThus, the system is consistent (and, in fact, has a    unique solution). Therefore, b is \nin col(A). (This example is just Example 2.18, phrased in the terminology of this \nsection.) \n(b) As we also saw in Section 2.3, elementary row operations simply create linear \ncombinations of the rows of a matrix. That is, they produce vectors only in the row \nspace of the matrix. If the vector w is in ro  w(A), then w is a linear combination of the \nrows of A, so if we augment A by  was \n[ \n�\n]\n, it will be possible to apply elementary row \nopera  tions to this augmented matr  ix to reduce it to form \n[ \n�\n'\n] \nusing only elementary \nrow operations of the form R; + kR\nj\n, where i > j-in other words, workingfro m top \n___..... \nto bottom in each column. (Why?) \nIn this example, we have \n[ \n�\n] \n[\n� \n-\n�\ni �: = !�: \n[\n� \n-\n�\ni \nR\n4\n- 9R2 \n[\n� \n-\n�\ni \n3  -3 \n� \n0   0 \n� \n0   0 \n�� �� �� \n4   5 \n0   9 0   0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":29021,"to":29084}}}}],[583,{"pageContent":"196 \nChapter 3 Matrices \n� \nTherefore, w is a linear combination of the rows of A (in fact, these calculations show \nthat w = 4 [l \n-1] + 9   [O  l ]-how?), and thus w is in row(A). \nTheorem 3.20 \nTheorem 3.21 \n( c)  It is easy to check that, for any vector w =  [ x y] , the augmented matrix \n[ \n�\n] \nreduces to \nin a similar fashion. Therefore, every vector in IR\n2 \nis in row(A), and so row(A) = IR\n2\n. \nFinding col(A) is identical to solving Example 2.21, wherein we determined that \nit coincides with the plane (through the origin) in IR\n3 \nwith equation 3x -z = 0. (We \nwill discover other ways to answer this type of question shortly.) \nRemark We could also have answered part (b) and the first part of part (c) by \nobserving that any question about the rows of A is the corresponding question about \nthe columns of A\nT\n. So, for example, w is in row(A) if and only if w\nT \nis in col(A \nT\n). This \nis true if and only if the system A \nT\nx = w\nT \nis consistent. We can now proceed as in \npart (a). (See Exercises 21-24.)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":29086,"to":29123}}}}],[584,{"pageContent":"T\n. So, for example, w is in row(A) if and only if w\nT \nis in col(A \nT\n). This \nis true if and only if the system A \nT\nx = w\nT \nis consistent. We can now proceed as in \npart (a). (See Exercises 21-24.) \nThe observations we have made about the relationship between elementary row \noperations and the row space are summarized in the following theorem. \nLet B be any matrix that is row equivalent to a matrix A.  Then row(B) = row(A). \nProof The matr  ix A   can be transformed into B by a sequence of row operations. \nConsequently, the rows of B are linear combinations of the rows of A; hence, linear \ncombinations of the rows of B are linear combinations of the rows of A. (See Exer­\ncise 21 in Section 2.3.) It follows that row(B) � row(A). \nOn the other hand, reversing these row operations transforms B into A. There­\nfore, the above argument shows that row(A) � row(B). Combining these results, we \nhave row(A) =  row(B). \nThere is another important subspace that we  have already encountered: the set","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":29123,"to":29145}}}}],[585,{"pageContent":"fore, the above argument shows that row(A) � row(B). Combining these results, we \nhave row(A) =  row(B). \nThere is another important subspace that we  have already encountered: the set \nof solutions of a homogeneous system of linear equations. It is easy to prove that this \nsubspace satisfies the three subspace properties. \nLet A be an m X n matrix and let N be the set of solutions of the homogeneous \nlinear system Ax = 0. Then N is a subspace of !R\nn\n. \nProof [Note that x must be a (column) vector in !R\nn \nin order for Ax to be defined and \nthat 0 = O\nm \nis the zero vector in !R\nm\n.] Since AO\nn \n= O\nm\n, O\nn \nis in N. Now let u and v be \nin N. Therefore, Au = 0 and Av = 0. It follows that \nA\n(\nu + v\n) \n= Au + Av = 0 + 0 = 0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":29145,"to":29173}}}}],[586,{"pageContent":"Theorem 3.22 \nSection 3.5 \nSubspaces, Basis, Dimension, and Rank \n191 \nHence, u +vis  in N. Finally, for any scalar c, \nA\n(\ncu\n) \n= c\n(\nAu\n) \n= cO = 0 \nand therefore cu is also in N. It follows that N is a subspace of !R\nn\n. \nDefinition \nLet A be an m X n matrix. The null sp ace of A is the subspace of \n!R\nn \nconsisting of solutions of the homogeneous linear system Ax = 0. It is denoted \nby null(A). \nThe fact that the null space of a matr  ix is a subspace allows us to   prove what in -\ntuition and examples have led us to   understand about the solutions of linear systems: \nThey have either no solution, a unique solution, or infinitely many solutions. \nLet A  be  a matrix whose entries are  real  numbers.  For  any system of linear \nequations Ax = b, exactly one of the following is true: \na.  There is no solution. \nb. There is a unique solution. \nc.  There are infinitely many solutions. \nAt first glance, it is not entirely clear how we should proceed to prove this theo­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":29175,"to":29206}}}}],[587,{"pageContent":"a.  There is no solution. \nb. There is a unique solution. \nc.  There are infinitely many solutions. \nAt first glance, it is not entirely clear how we should proceed to prove this theo­\nrem. A  little reflection should persuade you that what we are really being asked to \nprove is that if (a) and (b) are not true, then ( c) is the  only other possibility. That is, if \nthere is more than one solution, then there cannot be just two or even finitely many, \nbut there must be infinitely many. \nProof If the system Ax = b has either no solutions or exactly one solution, we are \ndone. Assume, then, that there are at least two distinct solutions of Ax = b-say, x\n1 \nand x\n2\n. Thus, \nAx\n, \n= b  and  Ax\n2 \n= b \nwith x\n1 \n* x\n2\n. It follows that \nA\n(\nx\n,  -\nx\n2\n) \n= Ax\n1 \n-\nAx\n2 \n= b \n-\nb = 0 \nSet X\no \n= x\n1 \n- x\n2\n. Then X\no \n* 0 and AX\no \n= 0. Hence, the null space of A is nontrivial, \nand since null(A) is closed under scalar multiplication, CX\no \nis in null(A) for every","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":29206,"to":29258}}}}],[588,{"pageContent":",  -\nx\n2\n) \n= Ax\n1 \n-\nAx\n2 \n= b \n-\nb = 0 \nSet X\no \n= x\n1 \n- x\n2\n. Then X\no \n* 0 and AX\no \n= 0. Hence, the null space of A is nontrivial, \nand since null(A) is closed under scalar multiplication, CX\no \nis in null(A) for every \nscalar c. Consequently, the null space of A contains infinitely many vectors (since it \ncontains at least every vector of the form CX\no \nand there are infinitely many of the  se). \nNow, consider the (infinitely many) vectors of the form x\n1 \n+ CX\no\n, as cvaries through \nthe set of real numbers. We have \nA\n(\nx\n, \n+ cx\n0\n) \n= Ax\n, \n+ cAX\no \n= b +   cO \n= b \nTherefore, there are infinitely many solutions of the equation Ax = b. \nBasis \nWe can extract a bit more from the intuitive idea that subspaces are generalizations \nof planes through the origin in IR\n3\n. A plane is spanned by any two vectors that are","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":29258,"to":29312}}}}],[589,{"pageContent":"198 \nChapter 3 Matrices \nExample 3.42 \nExample 3.43 \nExample 3.44 \nparallel to the plane but are not parallel to each other. In algebraic parlance, two \nsuch vectors span the plane and are linearly independent. Fewer than two vectors will \nnot work; more than two vectors is not necessary. This is the essence of a basis for a \nsubspace. \nDefinition \nA basis for a subspace S of ll�r is a set of vectors in S that \n1. spans S and \n2. is line arly independent. \nIn Section 2.3, we saw that the standard unit vectors e1, e\n2\n,  ... e\nn \nin !R\nn \nare linearly \nindependent and span !R\nn\n. Therefore, they form a basis for !R\nn\n, called the standard \nbasis. \n4 \nIn Example 2.19, we showed that IR\n2 \n= span(\n[\n_\n�\n]\n,\n[\n�\n]\n). Since \n[\n_\n�\n]\nand\n[\n�\n] \nare \nalso linearly independent (as they are not multiples), they form a basis for IR\n2\n. \n4 \nA subspace can (and will) have more than one basis. For example, we have just \nseen that IR\n2 \nhas the standard basis \n{ \n[ \n� \nl \n[ \n�\n]\n} \nand the basis \n{ \n[ \n_ \n� \nl \n[ \n�\n] \n} \n. How-","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":29314,"to":29388}}}}],[590,{"pageContent":"2\n. \n4 \nA subspace can (and will) have more than one basis. For example, we have just \nseen that IR\n2 \nhas the standard basis \n{ \n[ \n� \nl \n[ \n�\n]\n} \nand the basis \n{ \n[ \n_ \n� \nl \n[ \n�\n] \n} \n. How-\never, we will prove shortly that the number of vectors in a basis for a given subspace will \nalways be the same. \nFind a basis for S =  span \n(\nu, v, w) , where \nSolution The vectors u, v, and w already span S, so they will be a basis for S if they \nare also linearly independent. It is easy to determine that they are not; indeed, w = \n2u -3v. Therefore, we can ignore w, since any linear combinations involving u, v, \nand w can be rewritten to involve u and v alone. (Also see Exercise 47 in Section 2.3.) \nThis  implies that S = span \n(\nu, v, w) = span (u, v), and since u and v are certainly \n� \nlinearly independent (why?   ), they form a basis for S.    (Geometrically, this means that \nu, v, and w all lie in the same plane and u and v can serve as a set of direction vectors \nfor this plane.) \n4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":29388,"to":29430}}}}],[591,{"pageContent":"Example 3.45 \nExample 3.46 \nSection 3.5 \nSubspaces, Basis, Dimension, and Rank \n199 \nFind a basis for the row space of \n[\n-! \n3 \n-1  0 \nA= \n2 \n1 \n-2 \n1 \n6 \nSolution \nThe reduced row echelon form of A is \n0 \n1 \n0 \n0 \n1 \n2 \n0 \n0 \n0 \n0 \n1 \n0 \nBy Theorem 3.20, row(A) = row(R), so it    is enough to find a basis for the row space \nof R. But row(R) is clearly spanned by its nonzero rows, and it is easy to check that \nthe staircase pattern forces the first three rows of R to be linearly independent. (This \nis a general fact, one that you will need to establish to prove Exercise 33.) Therefore, \na basis for the row space of A is \n{[ 1  0 \n0  -1], [0  1  2  0 3], [0  0  0 \n4]} \nWe can use the method of Example 3.45 to find a basis for the su  bspace spanned \nby a   given set of vectors. \nRework Example 3.44 using the method from Example 3.45. \nSolution We transpose u, v, and w to get row vectors and then form a matr  ix with \nthese vectors as its rows: \nProceeding as in Example 3.45, we reduce B to its reduced row echelon form","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":29432,"to":29475}}}}],[592,{"pageContent":"Solution We transpose u, v, and w to get row vectors and then form a matr  ix with \nthese vectors as its rows: \nProceeding as in Example 3.45, we reduce B to its reduced row echelon form \n[ 1  0 !>\n] \n� \n� \n-! \nand use the nonzero row vectors as a basis for the row space. Since we started with \ncolumn vectors, we must transpose again. Thus, a  basis for span(u, v, w) is \n{\n[\nHU\nl\nl \nRemarks \n• \nIn fact, we do not need to go  all the way to reduced row echelon form-row ech­\nelon form is far enough. If U is a row echelon form of A, then the nonzero row vectors","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":29475,"to":29493}}}}],[593,{"pageContent":"200 \nChapter 3 Matrices \nExample 3.41 \nof U will form a basis for row(A) (see Exercise 33). This approach has the advantage of \n(often) allowing us to avoid fractions. In Example 3.46, B can be reduced to \nwhich gives us the basis \n\\HH\n-\nrn \nfor span \n(\nu, v, w). \n• \nObserve that the methods used in Example 3.44, Example 3.46, and the Remark \nabove will generally produce different bases. \nWe now turn to the problem of finding a basis for the column space of a matrix A. \nOne method is simply to transpose the matrix. The column vectors of A become the \nrow vectors of A\nT\n' \nand we can apply the method of Example 3.45 to find a basis for \nrow(A \nT\n). Transposing these vectors then gives us a basis for col(A). (You are asked to \ndo this in Ex  ercises 21-24.) This approach, however, requires performing a new set \nof row operations on A\nT\n. \nInstead, we prefer to take an approach that allows us to   use the row reduced form \nof A that we  have already computed. Recall that a  product Ax of a matrix and a ve c­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":29495,"to":29524}}}}],[594,{"pageContent":"of row operations on A\nT\n. \nInstead, we prefer to take an approach that allows us to   use the row reduced form \nof A that we  have already computed. Recall that a  product Ax of a matrix and a ve c­\ntor corresponds to a linear combination of the columns of A with the entries of x as \ncoefficients. Thus, a  nontrivial solution to Ax = 0 represents a dependence relation \namong the columns of A. Since elementary row operations do not affect the solution \nset, if A is row equivalent to R, th e columns of A have th e same dependence relation­\nships as the columns of R. This important observation is the basis (no pun intended!) \nfor the technique we now use to find a basis for col(A). \nFind a basis for the column space of the matrix from Example 3.45, \n[-� \n1 \n3 \n1 \n-:i \n-\n1 \n0    1 \nA= \n2 \n-2 \n6 \n1 \nSolution \nLet a; denote a column vector of A and let r; denote a column vector of the \nreduced echelon form \nR \n� \n[\n� \n0 \n1 \n0 \n-\ni\nl \n1 \n2 \n0 \n0  0 1 \n0  0  0 \nWe can quickly see by inspection that r\n3 \n= r\n1 \n+ 2r\n2 \nand rs  = -r\n1 \n+ 3r\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":29524,"to":29576}}}}],[595,{"pageContent":"reduced echelon form \nR \n� \n[\n� \n0 \n1 \n0 \n-\ni\nl \n1 \n2 \n0 \n0  0 1 \n0  0  0 \nWe can quickly see by inspection that r\n3 \n= r\n1 \n+ 2r\n2 \nand rs  = -r\n1 \n+ 3r\n2 \n+ 4r\n4\n• (Check \nthat, as predicted, the corresponding column vectors of A satisfy the same depen­\ndence relations.) Thus, r\n3 \nand rs  contribute nothing to col  (R). The remaining column","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":29576,"to":29608}}}}],[596,{"pageContent":"Example 3.48 \nSection 3.5 \nSubspaces, Basis, Dimension, and Rank \n201 \nvectors, r\n1\n, r\n2\n, and r\n4\n, are linearly independent, since they are just standard unit vec­\ntors. The corresponding statements are therefore true of the column vectors of A. \nThus, among the column vectors of A, we eliminate the dependent ones ( a\n3 \nand a\n5\n), \nand the remaining ones will be linearly independent and hence form a basis for col(A). \nWhat is the fastest way to find this basis? Use the columns of A that correspond to the \ncolumns of R containing the leading ls. A basis for col(A) is \nr .... ,..,\n} \n� \n{ \n[\n-\n;H\n-\n�H\n-\nm \nWarning Elementary row operations change the column space! In our example, \ncol(A) * col(R), since every vector in col(R) has its fourth component equal to 0 but \nthis is ce rtainly not true of col (A). So we must go back to the original matrix A to get \nthe column vectors for a basis of   col(A). To be specific, in Example 3.47, r\n1\n, r\n2\n, and r\n4 \ndo not form a basis for the column space of A.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":29610,"to":29650}}}}],[597,{"pageContent":"the column vectors for a basis of   col(A). To be specific, in Example 3.47, r\n1\n, r\n2\n, and r\n4 \ndo not form a basis for the column space of A. \nFind a basis for the null space of matrix A  from Example 3.47. \nSolution There is really nothing new here except the terminology. We simply have \nto find and describe the solutions of the homogeneous system Ax = 0. We have al­\nready computed the reduced row echelon form R of A, so all that remains to be done \nin Gauss-Jordan elimination is to solve for the leading variables in terms of the free \nvariables. The final augmented matrix is \nIf \n0 \n0 \n0 \n1 \n2 \n0 \n0 \n0 \n0 \n1 \n0 \nthen the leading ls are in columns 1, 2, and 4, so we solve for x\n1\n, x\n2\n, and x\n4 \nin terms of \nthe free variables x\n3 \nand x\n5\n• We get x\n1 \n= \n-x\n3 \n+ x\n5\n, x\n2 \n= -2x\n3 \n- 3x5\n, and x\n4 \n= -4x\n5\n. \nSetting x\n3 \n= s and x\n5 \n= t, we obtain \nX\n1 \n-s +  t \n-1 \n1 \nX\n2 \n-2s  -3t \n-2 \n-3 \nx= \nX\n3 \ns \n=  s \n1 \n+  t \n0 \n=su+tv \nX\n4 \n-4t \n0 \n-4 \nX\n5 \n0 \nThus, u and v span null(A ), and  since they are linearly independent, they form a basis \nfor null(A). \n4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":29650,"to":29737}}}}],[598,{"pageContent":"202 \nChapter 3 Matrices \nTheorem 3.23 \nSherlock Holmes noted, \"When \nyou have eliminated the impos­\nsible, whatever remains, however \nimprobable, must be the truth\" \n(from The Sign of Four by Sir \nArthur Conan Doyle) . \nFollowing is a summary of the most effective procedure to use to find bases for \nthe row space, the column space, and the null space of a matrix A. \n1. Find the reduced row echelon form R of A. \n2. Use the nonzero row vectors of R (containing the leading ls) to form a basis for \nrow(A). \n3. Use the column vectors of A that correspond to the columns of R containing the \nleading ls (the pivot columns) to form a basis for col(A). \n4. Solve for the le   ading variables of Rx = 0 in terms of the free variables, set the \nfree variables equal to parameters, substitute back into x, and write the result as \na linear combination off vectors (where f is the number of free variables). These \nf vectors form a basis for null(A).","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":29739,"to":29758}}}}],[599,{"pageContent":"a linear combination off vectors (where f is the number of free variables). These \nf vectors form a basis for null(A). \nIf we do not need to find the null space, then it is faster to simply reduce A to row \nechelon form to find bases for the row and column spaces. Steps 2 and 3 above remain \nvalid (with the substitution of the word \"pivots\" for \"leading ls\"). \nDimension and Rank \nWe have observed that although a subspace will have different bases, each basis has \nthe same number of vectors. This fundamental fact will be of vital importance from \nhere on in this book. \nThe Basis Theorem \nLet S be a subspace of !R\nn\n. Then any two bases for S have the same number of \nvectors. \nProof Let B = {u\n1\n, u\n2\n, ... , u,.} and C = {v\n1\n, v\n2\n, .•• , v\n,\n} be  bases for S. We need to \nprove that r = s. We do so by showing that neither of the other two possibilities, r < s \nor r > s, can occur. \nSuppose that r < s. We will show that this forces C to be a linearly dependent set \nof vectors. To this end, let \n(1)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":29758,"to":29787}}}}],[600,{"pageContent":"or r > s, can occur. \nSuppose that r < s. We will show that this forces C to be a linearly dependent set \nof vectors. To this end, let \n(1) \nSince B is a basis for S, we can write each V; as a linear combination of the \nelements \"f \nV\n1 \n= \na\n11 \nu\n, \n+ a\n12\nu\n2 \n+ \n· · · + a\n,\nr\nu\nr \nV\nz \n= \na\n11 \nu\n, \n+ a\nz\n2\nU\n2 \n+ · · · + a\nzr\nU\nr \nSubstituting the Equations (2) into Equation (1), we obtain \n(2)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":29787,"to":29827}}}}],[601,{"pageContent":"Example 3.49 \nExample 3.50 \nRegrouping, we have \nSection 3.5 \nSubspaces, Basis, Dimension, and Rank \n203 \n(\nc\n1\na\nll \n+ \nC\n2\na\n2\n1 \n+ ... + c\n,\na\ns\n1\n)U\n1 \n+ \n(\nc\n1\na\n1\n2 + \nC\n2\na\n22 \n+ ... + c\ns\na\nsz)Uz \n+ ·   ·   · + \n(\nc\n1\na\n1r \n+ c\n2\na\n2\nr \n+ ·   ·   · + c\n,\na\n)\nu\n, \n= 0 \nNow, since Bis a  basis, the u/s are linearly independent. So each of the expressions in \nparentheses must be zero: \nc\n1\na\n11 \n+ c\n2\na\n2\n1 \n+ ·   ·   · + c\n,\na\n,\n1 \n= 0 \nC\n1\na\n1\n2 \n+ \nC\n2\na\n22 \n+ ... + c\ns\na\ns\n2 \n= 0 \nThis is a homogeneous system of r linear equations in the s   variables c1\n, \nc2\n,  •.. , c5• (The \nfact that the variables appear to the left of the coefficients makes no difference.) Since \nr < s, we know from Theorem 2.3 that there are infinitely many solutions. In particu­\nlar, there is a nontrivial solution, giving a nontrivial dependence relation in Equa­\ntion (1). Thus, C is a linearly dependent set of   vectors. But this finding contradicts the \nfact that C was given to be a basis and hence linearly independent. We conclude that \nr","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":29829,"to":29928}}}}],[602,{"pageContent":"tion (1). Thus, C is a linearly dependent set of   vectors. But this finding contradicts the \nfact that C was given to be a basis and hence linearly independent. We conclude that \nr \n< sis not possible. Similarly (interchanging the roles of B and C), we find that r > s \nleads to a contradiction. Hence, we must have r = s, as desired. \nSince all bases for a given subspace must have the same number of vectors, we can \nattach a name to this number. \nDefinition If Sis a  subspace of !R\nn\n, then the number of vectors in a basis for S \nis called the dimension of S, denoted dim S. \nRemark The zero vector 0 by itself is always a subspace of !R\nn\n. (Why?) Yet any set \ncontaining the zero vector (and, in particular, {  0}) is linearly dependent, so { 0} cannot \nhave a basis. We define dim {O} to be 0. \nSince the standard basis for !R\nn \nhas n vectors, dim !R\nn \n= n. (Note that this result agrees \nwith our intuitive understanding of dimension for n :s 3.)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":29928,"to":29949}}}}],[603,{"pageContent":"have a basis. We define dim {O} to be 0. \nSince the standard basis for !R\nn \nhas n vectors, dim !R\nn \n= n. (Note that this result agrees \nwith our intuitive understanding of dimension for n :s 3.) \nIn Examples 3.45 through 3.48, we found that row(A) has a basis with three vectors, \ncol(A) has a basis with three vectors, and null(A) has a basis with two vectors. Hence, \ndim(row(A  )) = 3, dim(col(A )) = 3, and dim(null(A)) = 2. \nA single example is not enough on which to speculate, but the fact that the row \nand column spaces in Example 3.50 have the same dimension is no accident. Nor is \nthe fact that the sum of dim(col(A)) and dim(null(A)) is 5, the  number of columns of \nA. We now prove that these relationships are true in general.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":29949,"to":29962}}}}],[604,{"pageContent":"204 \nChapter 3 Matrices \nTheorem 3.24 \nThe rank of a matrix was first de­\nfined in 1878 by Georg Frobenius \n(1849-1917), although he defined \nit using determinants and not as we \nhave done here. (See Chapter 4.) \nFrobenius was a German \nmathematician who received his \ndoctorate from and later taught \nat the University of Berlin. Best \nknown for his contributions to \ngroup theory, Frobenius used \nmatrices in his work on group \nrepresentations. \nTheorem 3.25 \nThe row and column spaces of a matrix A have the same dimension. \nProof Let R be the reduced row echelon form of A. By Theorem 3.20, row(A) = \nrow(R), so \ndim(row(A)) = dim(row(R)) \nLet this number be called r. \n= number of nonzero rows of R \n= number ofleading ls of R \nNow col(A) * col(R), but the columns of A and R have the same dependence \nrelationships. Therefore, dim(col(A )) = dim(col(R)). Since there are r leading ls, R \nhas r columns that are standard unit vectors, e1, e\n2\n,  ... ,  er. (These will be vectors in \n!R\nm","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":29964,"to":29994}}}}],[605,{"pageContent":"relationships. Therefore, dim(col(A )) = dim(col(R)). Since there are r leading ls, R \nhas r columns that are standard unit vectors, e1, e\n2\n,  ... ,  er. (These will be vectors in \n!R\nm \nif A and R are m X n matrices.) These r vectors are linearly independent, and the \nremaining columns of R are linear combinations of them. Thus, dim(col(R)) = r. It \nfollows that dim(row(A)) = r = dim(col(A )), as we wished to prove. \nDefinition \nThe rank of a matr  ix A   is the dimension of its row and column \nspaces and is denoted by rank(A). \nFor Example 3.50, we can thus write rank(A) = 3. \nRemarks \n• \nThe preceding definition agrees with the more informal definition of rank that \nwas introduced in Chapter 2. The advantage of our new definition is that it is much \nmore flexible. \n• \nThe  rank  of a  matr  ix  simultaneously gives us  information  about linear \ndependence among the row vectors of the matrix and among its column vectors. In","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":29994,"to":30014}}}}],[606,{"pageContent":"more flexible. \n• \nThe  rank  of a  matr  ix  simultaneously gives us  information  about linear \ndependence among the row vectors of the matrix and among its column vectors. In \nparticular, it tells us the number of rows and columns that are linearly independent \n(and this number is the same in each case!). \nSince the row vectors of A are the column vectors of A\nr\n,  Theorem 3.24 has the \nfollowing immediate corollary. \nFor any matrix A, \nProof We have \nrank\n(\nA \nT) \n= rank\n(\nA\n) \nrank\n(\nA\nT) \n= dim \n(\ncol\n(\nA\nT)) \n= dim \n(\nrow\n(\nA\n)) \n= rank\n(\nA\n) \nDefinition \nThe nullity of a matrix A is the dimension of its null space and is \ndenoted by nullity(A).","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":30014,"to":30056}}}}],[607,{"pageContent":"Theorem 3.26 \nExample 3.51 \nSection 3.5 \nSubspaces, Basis, Dimension, and Rank \n205 \nIn other words, nullity(A) is the dimension of the solution space of Ax = 0, which \nis the same as the number of free variables in the solution. We can now revisit the \nRank Theorem (Theorem 2.2), rephrasing it in terms of our new definitions. \nThe Rank Theorem \nIf A is an m X n matrix, then \nrank\n(\nA\n) \n+ nullity\n(\nA\n) \n= n \nProof Let R be the reduced row echelon form of A, and suppose that rank(A) = r. \nThen R has r leading ls, so there are r leading variables and n \n-\nr free variables in the \nsolution to Ax = 0. Since dim(null(A)) = n \n-\nr, we have \nrank\n(\nA\n) \n+ nullity\n(\nA\n) \n= r + (n \n-\nr\n) \n= n \nOften, when we need to know the nullity of a matrix, we do not need to know the \nactual solution of Ax = 0. The Rank Theorem is   extremely useful in such situations, \nas the following example illustrates. \nFind the nullity of each of the following matrices: \nM= \n[\nI �: \nand \nN � \n[\n� \n1 \n-2 \n-\n:\n1 \n4 \n-3 \n7","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":30058,"to":30115}}}}],[608,{"pageContent":"as the following example illustrates. \nFind the nullity of each of the following matrices: \nM= \n[\nI �: \nand \nN � \n[\n� \n1 \n-2 \n-\n:\n1 \n4 \n-3 \n7 \nSolution Since the two columns of Mare clearly linearly independent, rank(M) =  2. \nThus, by the Rank Theorem, nullity(M) = 2 -rank(M) = 2 - 2 = 0. \nThere is no obvious dependence among the rows or columns of N, so we apply \nrow opera  tions to reduce it to \n[\n: \n2 \n0 \n-2 \n1 \n0 \n-�\n] \nWe have reduced the matrix far enough (we do not need reduced row echelon form \nhere, since we are not lo   oking for a basis for the null space). We see that there are only \ntwo nonzero rows, so rank(N) = 2. Hence, nullity(N) = 4 \n-\nrank(N) = 4 \n-\n2 =  2. \n4 \nThe  results of this section allow us to extend the  Fundamental Theorem of \nInvertible Matrices (Theorem 3.12).","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":30115,"to":30154}}}}],[609,{"pageContent":"206 \nChapter 3  Matrices \nTheorem 3.21 \nThe nullity of a matrix was defined \nin 1884 by James Joseph Sylvester \n(1814-1887), who was interested in \ninvariants -properties of matrices \nthat do not change under certain \ntypes of transformations. Born \nin England, Sylvester became the \nsecond president of the London \nMathematical Society. In 1878, \nwhile teaching at Johns Hopkins \nUniversity in Baltimore, he \nfounded the American Jo urnal of \nMathematics, the first mathematical \njournal in the United States. \nThe Fundamental Theorem of Invertible Matrices: Version 2 \nLet A be an n X n matrix. The following statements are equivalent: \na. A is invertible. \nb. Ax = b   has a unique solution for every bin ll�r. \nc. Ax = 0 has only the trivial solution. \nd. The reduced row echelon form of A is I\nn\n-\ne.  A   is a product of elementary matrices. \nf.  rank(A) = n \ng.  nullity(A) = 0 \nh.  The column vectors of A are linearly independent. \ni.  The column vectors of A span ll�r.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":30156,"to":30185}}}}],[610,{"pageContent":"n\n-\ne.  A   is a product of elementary matrices. \nf.  rank(A) = n \ng.  nullity(A) = 0 \nh.  The column vectors of A are linearly independent. \ni.  The column vectors of A span ll�r. \nj.  The column vectors of A form a basis for ll�r. \nk. The row vectors of A are linearly independent. \n1. The row vectors of A span !R\nn\n. \nm. The row vectors of A form a basis for !R\nn\n. \nProof We have already established the equivalence of (a) through (e). It remains to \nbe shown that statements (f) to (m) are equivalent to the first five statements. \n(f) � (g) Since rank(A) + nullity(A) = n when A is an n X n matrix, it follows from \nthe Rank Theorem that rank(A) = n if and only if nullity(A) =  0. \n(f) ==> (d) ==> ( c) ==> (h) If rank(A) = n, then the reduced row echelon form of A has \nn leading ls and so is I\nn\n- From (d) ==> (c) we know that Ax = 0 has only the trivial \nsolution, which implies that the column vectors of A are linearly independent, since \nAx is just a linear combination of the column vectors of A.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":30185,"to":30209}}}}],[611,{"pageContent":"solution, which implies that the column vectors of A are linearly independent, since \nAx is just a linear combination of the column vectors of A. \n(h) ==> (i) If   the column vectors of A are linearly independent, then Ax = 0 has only \nthe trivial solution. Thus, by (c) ==> (b), Ax = b   has a unique solution for every bin \n!R\nn\n. This means that every vector bin !R\nn \ncan be written as a linear combination of the \ncolumn vectors of A, establishing (i). \n(i) ==> (j)  If the column vectors of A  span W, then col(A)  =  !R\nn \nby definition, \nso rank(A) =  dim(col(A )) = n. This is (f), and we have already established that \n(f) ==> (h). We  conclude that the  column vectors of A are linearly independent and so \nform a basis for !R\nn\n, since, by assumption, they also span !R\nn\n. \n(j) ==> (f)  If the column vectors of A form a basis for !R\nn\n, then, in particular, they are \nlinearly independent. It follows that the  reduced row echelon form of A contains n \nleading ls, and thus rank(A) = n.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":30209,"to":30233}}}}],[612,{"pageContent":"n\n, then, in particular, they are \nlinearly independent. It follows that the  reduced row echelon form of A contains n \nleading ls, and thus rank(A) = n. \nThe above discussion shows that (f) ==> (d) ==> (c) ==> (h) ==> (i) ==> (j) ==> \n(f) � (g). Now recall that, by Theorem 3.25, rank(A\nT\n) \n= rank(A), so what we have \njust proved gives us the corresponding results about the column vectors of A\nr\n_ These \nare then results about the row vectors of A, bringing (k), (1), and (m) into the network of \nequivalences and completing the proof. \nTheorems such as the Fundamental Theorem are not merely of theoretical inter­\nest. They are tremendous labor-saving devices as well. The Fundamental Theorem \nhas already allowed us to cut in half the work needed to check that two square matri­\nces are inverses. It also simplifies the task of showing that certain sets of vectors are \nbases for !R\nn\n. Indeed, when we have a set of n vectors in !R\nn\n, that set will be a basis for \n!R\nn","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":30233,"to":30257}}}}],[613,{"pageContent":"ces are inverses. It also simplifies the task of showing that certain sets of vectors are \nbases for !R\nn\n. Indeed, when we have a set of n vectors in !R\nn\n, that set will be a basis for \n!R\nn \nif either of the necessary properties of linear independence or spanning set is true. \nThe next example shows how easy the calculations can be.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":30257,"to":30266}}}}],[614,{"pageContent":"Example 3.52 \nTheorem 3.28 \nShow that the vectors \nform a basis for IR\n3\n. \nSection 3.5 \nSubspaces, Basis, Dimension, and Rank \n201 \nmr\nn \nand m \nSolution \nAccording to the Fundamental Theorem, the vectors will form a basis for \nIR\n3 \nif and only if a matrix with these vectors as its columns (or rows) has rank 3. We \nperform just enough row operations to determine this: \nA \n� \n[\n� \n-\n� \nf\nl\n--7 \n[\n� \n-\n�  J \nWe see that A has rank 3, so the given vectors are a basis for IR\n3 \nby the equivalence of \n(f) and (j). \n4 \nThe next theorem is an application of both the Rank Theorem and the Funda­\nmental Theorem. We will require this result in Chapters 5 and 7. \nLet A be an m X n matrix. Then: \na.  rank(A \nT\nA) = rank(A) \nb. The n X n matrix A\nT \nA is invertible if and only if rank(A) = n. \nProof \n(a) Since A\nT \nA is n X n, it has the same number of columns as A. The Rank Theorem \nthen tells us that \nrank\n(\nA\n) \n+ nullity\n(\nA\n) \n= n = rank\n(\nA \nT\nA\n) \n+ nullity\n(\nA \nT\nA\n) \nHence, to show that rank(A) = rank(A \nT\nA), it is enough to show that nullity(A)  =","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":30268,"to":30340}}}}],[615,{"pageContent":"then tells us that \nrank\n(\nA\n) \n+ nullity\n(\nA\n) \n= n = rank\n(\nA \nT\nA\n) \n+ nullity\n(\nA \nT\nA\n) \nHence, to show that rank(A) = rank(A \nT\nA), it is enough to show that nullity(A)  = \nnullity(A \nT\nA). We will do so by   establishing that the  null spaces of A and A\nT \nA are the \nsame. \nTo this end, let x be in null(A) so that Ax = 0. Then A\nT \nAx = A \nT\no = 0, and thus \nxis in null(A\nT \nA). Conversely, let x be in null(A\nT \nA). Then A \nT\nAx = 0, so x\nT\nA \nT \nAx = \nx\nT\nO = 0. But then \nand hence Ax = 0, by Theorem 1.2(d). Therefore, x is in null(A), so null(A) \nnull(A\nT\nA), as required. \n(b) By the Fundamental Theorem, the n X n matrix A\nT \nA is invertible if and only if \nrank(A \nT\nA) = n. But, by (a) this is so if and only if rank(A) = n. \nCoordinates \nWe now return to one of the questions posed at the very beginning of this section: \nHow should we view vectors in IR\n3 \nthat live in a plane through the origin? Are they \ntwo-dimensional or three-dimensional? The notions of basis and dimension will help \nclarify things.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":30340,"to":30405}}}}],[616,{"pageContent":"208 \nChapter 3  Matrices \nTheorem 3.29 \nExample 3.53 \nA plane through the origin is a two-dimensional subspace of IR\n3\n, with any set of \ntwo direction vectors serving as a basis. Basis vectors locate coordinate axes in the \nplane/subspace, in turn allowing us to   view the plane as a \"copy\" of IR\n2\n• Before we \nillustrate this approach, we prove a theorem guaranteeing that \"coordinates\" that arise \nin this way are unique. \nLet S be a subspace of !R\nn \nand let B = {v\n1\n, v\n2\n, ... , v\nk\n} be a basis for S. For every \nvector v in S, there is exactly one way to write v as a linear combination of the basis \nvectors in B: \nProof Since B is a basis, it spans S, so v can be written in at least one way as a linear \ncombination of v\n1\n, v\n2\n, ... , v\nk\n. Let one of these linear combinations be \nOur task is to show that this is the only way to write v as a linear combination of \nv\n1 , v\n2\n, ... , v\nk\n. To this end, suppose that we also have \nv \n= d\n1\nv\n1 \n+ d\n2\nv\n2 \n+ \n· · · \n+ d\nk\nv\nk \nThen","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":30407,"to":30461}}}}],[617,{"pageContent":"Our task is to show that this is the only way to write v as a linear combination of \nv\n1 , v\n2\n, ... , v\nk\n. To this end, suppose that we also have \nv \n= d\n1\nv\n1 \n+ d\n2\nv\n2 \n+ \n· · · \n+ d\nk\nv\nk \nThen \nRearranging (using properties of vector algebra), we  obtain \n(\nc\n1 \n-  d\n1\n)\nv\n1 \n+ \n(\nc\n2 \n-  d\n2\n)\nv\n2 \n+ \n· · · \n+ \n(\nc\nk \n-  d\nk\n)\nv\nk \n= 0 \nSince \nBis a  basis, v1, v\n2\n, ... , v\nk \nare linearly independent. Therefore, \n(\nc\n1 \n-  d\n1\n) = (\nc\n2 \n-  d\n2\n) = \n· · · \n= (\nc\nk \n-  d\nk\n) = \n0 \nIn other words, c\n1 \n= d\n1 , c\n2 \n= d\n2\n, ••• , c\nk \n= d\nk\n, and the two linear combinations are \nactually the same. Thus, there is exactly one way to write v as a linear combination of \nthe basis vectors in B. \nDefinition \nLet S be a subspace of!R\nn \nand let B = {v\n1\n, v\n2\n, ... , v\nk\n} be a basis for \nS. Let v be a vector in S,  and write v \n= \nc\n1v1 + c\n2\nv\n2 \n+ · · · + c\nk\nv\nk\n. Then c 1 , c\n2\n, ... , c\nk \nare called the coordinates of v with respect to B, and the column vector \nis called the coordinate vector of v with respect to B. \nLet E = { e\n1\n, e\n2\n, e\n3\n} be the standard basis for IR\n3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":30461,"to":30587}}}}],[618,{"pageContent":"2\n, ... , c\nk \nare called the coordinates of v with respect to B, and the column vector \nis called the coordinate vector of v with respect to B. \nLet E = { e\n1\n, e\n2\n, e\n3\n} be the standard basis for IR\n3\n. Find the coordinate vector of \nwith respect to E.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":30587,"to":30601}}}}],[619,{"pageContent":"Section 3.5 \nSubspaces, Basis, Dimension, and Rank \n209 \nSolution \nSince v = 2e\n1 \n+ \n7\ne\n2 \n+ \n4\ne\n3\n, \n[v]\nE \n= \n[\n4\n2\n7\n] \nIt should be clear that the  coordinate vector of every (column) vector in !R\nn \nwith \nrespect to the standard basis is just the vector itself. \nExample 3.54 \nID Exompk 3.44, we\n<\naw iliat u \n� \n[\n-\n} \n� \nm and w \n� \n[\n-;lace th\"e v\" \ntors in the same subspace (plane through the origin) S of IR\n3 \nand that B =  { u, v} is a \nbasis for S. Since w = 2u -3v, we have \nSee Figure 3.3. \nx ---. \nI \nExercises 3.5 \nIn Exercises 1-4, let S be th e collection of vectors \n[;\n] \nin IR\n2 \nthat satisfy the given property. In each case, either prove that \nS forms a subspace of IR\n2 \nor give a counterexample to show \nthat it does not. \n1.x  = 0 \n2.x 2 O ,y 2 0 \n3.y  =  2x \n4.xy 2 0 \nIn Exm;,,, 5-8, let S be the collea;on of vato\" \n[\n;\n] \n;n D;l' \nthat satisfy the given property. In each case, either prove that \nS forms a subspace of IR\n3 \nor give a counterexample to show \nthat it does not. \n5.x  = \ny \n=  z \n6. z  = 2x,y  = 0 \n[w]\nB \n= \n[ \n_\n�\n] \nz \n- 3v","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":30603,"to":30687}}}}],[620,{"pageContent":"that satisfy the given property. In each case, either prove that \nS forms a subspace of IR\n3 \nor give a counterexample to show \nthat it does not. \n5.x  = \ny \n=  z \n6. z  = 2x,y  = 0 \n[w]\nB \n= \n[ \n_\n�\n] \nz \n- 3v \n,w = 2u -3v \ny \n7. x -  y  +  z  =  1 \nFigure 3.3 \nThe coordinates of a vector with \nrespect to a basis \n8. \nI\nx  -  y \nI \n= \nI\nY \n-  z \nI \n9. Prove that every line through the origin in IR\n3 \nis a sub­\nspace of IR\n3\n. \n10. Suppose S consists of all points in IR\n2 \nthat are on the \nx-axis or the y-axis (or both). (S is called the union of \nthe two axes.) Is Sa subspace of IR\n2\n? Why or why not? \nIn Exercises 11 and 12, determine whether bis in col( A) \nand whetherw is in row(A), as in Example 3.41. \n11.\nA\n=\n[\n� \n� \n-\n�\nl\nb\n=\n[\n�\n]\n,w\n=\n[-1 \n1] \n12.  A\n� \n[\n� -� \n�\n}\n� \n[i}w\n� \n[2 \n4 \n-5","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":30687,"to":30764}}}}],[621,{"pageContent":"210 \nChapter 3  Matrices \n13. In Exercise 11, determine whether w is in row(A), \nusing the method described in the Remark following \nExample 3.41. \n14. In Exercise 12, determine whether w is in row(A), \nusing the method described in the Remark following \nExample 3.41. \n15. \nIf A ;, tho  mateix in F.xmi\" 11, b � \n[ \n�: }n null (A)? \n16. If  A i'1he mate ix in Exmi\" 12, i\n<v \n� \n[ \n-\n: }n nu!l(A)? \nIn Exercises 17-20, give bases for row( A), col( A), and null(A). \n17. A = \n[\n� \n[\n0\n1 \n2\n1   -3\n1 l \n18. A= \n1   -1  -4 \n0 \n-\n�\n] \n19. A = \n[\n� \n1 \n0 \nJ \n1 \n-1 \n1 \n-1 \nH \n-4 \n0 \n2 \n!\n] \n2     2 \n-2 \n4 \n20. A = \nIn Exercises 21-24,find bases for row( A) and col( A) in th e \ngiven exercises using A\nT\n. \n21. Exercise 17 22. Exercise 18 \n23. Exercise 19 \n24. Exercise 20 \n25. Explain carefully why your answers to Exercises 17 \nand 21 are both correct even though there appear to be \ndifferences. \n26. Explain carefully why your answers to Exercises 18 \nand 22 are both correct even though there appear to \nbe differences.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":30766,"to":30832}}}}],[622,{"pageContent":"and 21 are both correct even though there appear to be \ndifferences. \n26. Explain carefully why your answers to Exercises 18 \nand 22 are both correct even though there appear to \nbe differences. \nIn Exercises 27-30, find a basis for the span of the given \nvectors. \n27. \n[\n-\nil \n[\n-\n�\nJ \n[ \nJ 28. \n[\n-\n:J m. \n[\n:\nJ m \n29.  [2 -3  1], [ 1   -1 \nO], [4   -4  1] \n30. [ 0  1 -2  1 ] , [ 3 \n1   -1  0], [ 2  1  5 \nFor Exercises 31 and 32, find bases for the spans of th e \nvectors in the given exercises from among th e vectors \nthemselves. \n31. Exercise 29 \n32. Exercise 30 \n1] \n33. Prove that if R is a matr  ix in echelon form, then a basis \nfor row(R) consists of the nonzero rows of R. \n34. Prove that ifthe columns of A are linearly indepen­\ndent, then they must form a basis for col(A). \nFor Exercises 35-38, give the rank and th e nullity of the \nmatrices in the given exercises. \n35. Exercise 17 \n36. Exercise 18 \n37. Exercise 19 \n38. Exercise 20 \n39. If A is a 3 X 5 matr  ix, explain why the columns of A \nmust be linearly dependent.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":30832,"to":30876}}}}],[623,{"pageContent":"matrices in the given exercises. \n35. Exercise 17 \n36. Exercise 18 \n37. Exercise 19 \n38. Exercise 20 \n39. If A is a 3 X 5 matr  ix, explain why the columns of A \nmust be linearly dependent. \n40. If A is a 4 X 2 matr  ix, explain why the rows of A must \nbe linearly dependent. \n41. If A is a 3 X 5 matrix, what are the possible values of \nnullity(A)? \n42. If A is a 4 X 2 matrix, what are the possible values of \nnullity(A)? \nIn Exercises 43 and 44, find all possible values of rank( A) as \na varies. \n2 \nal \n4a  2 \n-2 \n1 \n44. A = \n[ \n� \n� \n= \n� \nl \n-\n2 \n-\n1 \na \nAnswer Exercises 45-48 by considering the matrix with th e \ngiven vectors as its columns. \n45. Do \n[J \n[ \nH \n[\n:\nJ \nform a b\"i' foe\n�\n'\n? \n46. Do \n[\n-J \n[\n-\nn \n[\n-1\n] \nfocm a bM1' fm \n�\n'\n? \n47. Do \nr \ni \nl m m \nrn focm a bMi' foe\n�\n'\n'","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":30876,"to":30941}}}}],[624,{"pageContent":"Section 3.6 \nIntroduction to Linear Transformations \n211 \n57. If   A is m X n, prove that every vector in null(A) is \northogonal to every vector in row(A). \n58. If A and B are n X n matrices of rank n, prove that AB \nhas rank n. \n49. Do m \n[\n: J m form a b\n.\n,i, fo, Zl? \n59. (a) Prove that rank (AB) \n:::::: \nrank(B). [Hint: Review \nExercise 29 in Section 3.1.] \n(b) Give an example in which rank(AB) < rank(B). \n60. (a) Prove that rank (AB) \n:::::: \nrank(A). [Hint: Review \nExercise 30 in Section 3.1 or use transposes and \nExercise 59(a).] \n50. Do m \n[\n: J \n[ \n� l fmm a bn\n<\ni• fo, Zj? \n(b) Give an example in which rank(AB) < rank(A). \n61. (a) Prove that if U is invertible, then rank( UA ) = \nrank(A). [Hint: A = U\n-\n1\n(UA).] \nIn Exercises 51 and 52, show that w is in span(B) and find \nthe coordinate vector [w]8. \n(b) Prove that if Vis invertible, then rank(A V) = \nrank(A). \n62. Prove that an m X n matrix A has rank 1 if and only if \nA can be written as the outer product uv\nT \nof a vector u \nin !Rm and v in !R\nn\n.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":30943,"to":30988}}}}],[625,{"pageContent":"(b) Prove that if Vis invertible, then rank(A V) = \nrank(A). \n62. Prove that an m X n matrix A has rank 1 if and only if \nA can be written as the outer product uv\nT \nof a vector u \nin !Rm and v in !R\nn\n. \n63. If an m X n matrix A has rank r, prove that A  can be \nwritten as the sum of r matrices, each of which has \nrank 1. [Hint: Find a way to use Exercise 62.] \nIn Exercises 53-56, compute the rank and nullity of the \n64. Prove that, for m X n matrices A and B, rank (A + B) \n:::::: \nrank(A) +  rank(B). \ngiven matrices over the indicated \"ll..\nP\n\" \n65. Let A be an n X n matrix such that A \n2 \n= 0. Prove that \nrank(A) \n:::::: \nn/2. [Hint: Show that col(A) � null(A) and \nuse the Rank Theorem.] \n53. \n[\n� \n1 \n1 \n0 \n55. \n[\n� \n3 \n3 \n0 \n56. \n[\n! \n4 \n3 \n0 \n:J ov°' z, \n54. \n[\n� \n1 \n1 \n0 \n�] om Z, \n66. Let A be a skew-symmetric n X n matrix. \n(See page 162). \n1 \n0 \n4 \n0 \n5 \n2 \n�]om Z; \n(a) Prove that x\nT \nAx= 0 for all x in !R\nn\n. \n(b) Prove that I+ A is invertible. [Hint: Show that \nnull(I +A) = {O}.] \n0 \n�\n] \nom Z, \n1 \n2 \n1 \n0 \nIntroduction to linear Transformations","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":30988,"to":31064}}}}],[626,{"pageContent":"1 \n0 \n4 \n0 \n5 \n2 \n�]om Z; \n(a) Prove that x\nT \nAx= 0 for all x in !R\nn\n. \n(b) Prove that I+ A is invertible. [Hint: Show that \nnull(I +A) = {O}.] \n0 \n�\n] \nom Z, \n1 \n2 \n1 \n0 \nIntroduction to linear Transformations \nIn this section, we begin to explore one of the themes from the introduction to this \nchapter. There we saw that matrices can be used to transform vectors, acting as a type \nof \"function\n'\n' of the form w =  T\n(\nv\n)\n, where the independent variable v and the de­\npendent variable ware vectors. We will make this notion more precise now and look \nat several examples of such matrix transformations, leading to the concept of a linear \ntransformation-\na \npowerful idea that we  will encounter repeatedly from here on.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":31064,"to":31100}}}}],[627,{"pageContent":"212 \nChapter 3  Matrices \nWe begin by recalling some of the  basic concepts associated with functions. You will \nbe familiar with most of these ideas from other courses in which you encountered func­\ntions of the form f: IR ---+ IR [such as f(x\n) \n= x\n2\n] that transform real numbers into real num­\nbers. What is new here is that vectors are involved and we are interested only in functions \nthat are \"compatible\" with the vector operations of addition and scalar multiplication. \nConsider an example. Let \nThen \nThi, ,hows that A  tmmfmmn into w  � \n[ \n_ \n:J \nWe can describe this transformation more generally. The matrix equation \n[ \n� \n-\n� \nl \n[\n;\n] \n= \n[ \n2x \n� \ny \nl \n3 4 3x + 4y \ngives a formula that shows how A transforms an arbitrary vector \n[\n;\n] \nin IR\n2 \ninto the \nvectm \n[ \n2x \"_ y] in D;l'. We denote this tmmfmmotion by T, and w'ite \n3x + 4y \nr\n,\n(\n[ \nx\n]\n) � \n[ \n2x \n� \nY l \ny \n3x + 4y \n(Although technically sloppy, omitting the parentheses in definitions such as this one","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":31102,"to":31159}}}}],[628,{"pageContent":"vectm \n[ \n2x \"_ y] in D;l'. We denote this tmmfmmotion by T, and w'ite \n3x + 4y \nr\n,\n(\n[ \nx\n]\n) � \n[ \n2x \n� \nY l \ny \n3x + 4y \n(Although technically sloppy, omitting the parentheses in definitions such as this one \nis a common convention that saves some writing. The description of TA becomes \nwith this convention.) \nWith this example in mind, we now consider some terminology. A transformation \n(or mapping or function) T from !R\nn \nto !R\nm \nis a rule that assigns to each vector v in !R\nn \na unique vector T(v) in !R\nm\n. The domain of Tis !R\nn\n, and the codomain of Tis !R\nm\n. We \nindicate this by writing T : !R\nn\n---+ !R\nm\n. For a vector v in the domain of T, the vector T(v) \nin the co domain is called the image of v under (the action of) T. The set of all possible \nimages T(v) (as v varies throughout the domain of T) is called the range of T. \nIn our example, the domain of TA is IR\n2 \nand its codomain is IR\n3\n, so we write \nT,, D;l' --.  D;l'. The image of v � \n[ \n_ \n:\nJ \nis w  � T,\n(\nv\n) \n� \n[ \n_ \ni J What is thecange of","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":31159,"to":31217}}}}],[629,{"pageContent":"Example 3.55 \nSection 3.6 \nIntroduction to Linear Transformations 213 \nTA? It consists of all vectors in the codomain IR\n3 \nthat are of the form \nTA\n[\nx\n] \n= \n[ \n2x \n� \ny\n] \n= x\n[\n�\n] \n+ y\n[\n-\n�\n] \ny \n3x + 4y 3 4 \nwhi<h d�mib\"  the \"t  of •ll linm wmbimtiom of the  wlumn ve<to\" \n[ \n�\n] \nand \n[\n-\n�\ni \nof A.  In othe; wo'd', the  'ange of T IB the  wlumn 'P\n\"\n'e of A! (We \nwill have more to say about this later-for now we'll simply note it as an interesting \nobservation.) Geometrically, this shows that the range of TA is the plane through the \norigin in IR\n3 \nwith direction vectors given by the column vectors of A. Notice that the \nrange of TA is strictly smaller than the co domain of TA. \nlinear Transformations \nThe example TA above is a special case of a more general type of transformation called \na linear transformation. We will consider the general definition in Chapter 6, but the \nessence of it is that these are the transformations that \"preserve\" the vector operations \nof addition and scalar multiplication.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":31219,"to":31268}}}}],[630,{"pageContent":"essence of it is that these are the transformations that \"preserve\" the vector operations \nof addition and scalar multiplication. \nDefinition A transformation T: !R\nn\n---+ !R\nm \nis called a linear transformation if \n1. T(u + v) = T(u) + T(v)  for all u and v in !R\nn \nand \n2. T(cv) = cT(v)  for all v in !R\nn \nand all scalars c. \nConsider once again the transformation T: IR\n2 \n---+ IR\n3 \ndefined by \nr\n[\n;\n] \n� \n[ \n:: ; � l \nLet's check that Tis a  linear transformation. To verify (1), we let \nu = \n[\n;\n:\nJ \nand  v = \n[\n;\n:\nJ \nThen","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":31268,"to":31304}}}}],[631,{"pageContent":"214 \nChapter 3  Matrices \nTheorem 3.30 \nTo show (2\n), \nwe let v = \n[\n;\n] \nand let c be a scalar. Then \nT\n(\ncv) = \nr\n(\nc\n[\n;\n]\n) \n= \nr\n(\n[\n:\n;\n]\n) \n[ \n2\n(\ncx) \nc\n� \n(\ncy) ] = \n[ \nc\n(\n2\n:\nx__ \ny) ] \n3\n(\ncx\n) \n+ 4\n(\ncy\n) \nc\n(\n3x + 4y\n) \n= cT\n(\nv) \nThus, Tis a  linear transformation. \nRemark The definition of a linear transformation can be streamlined by com­\nbining (1) and (2\n) \nas sh  own below. \nT: !R\nn \n� !R\nm \nis a linear transformation if \nIn Exercise 53\n, \nyou will be asked to show that the statement above is equivalent \nto the original definition.  In practice, this equivalent formulation can save some \nwriting-try it! \nAlthough the linear transformation Tin Example 3.55 originally arose as a matrix \ntransformation TA, it  is a simple matter to recover the matrix A from the definition of \nT given in the example. We observe that \nrn \nT � TA, whm A � [: -� J (N otke that when the vmiahles x ond y \"' J;ned \nup, the matrix A is just their coefficient matrix.) \nRecognizing that a transformation is a matrix transformation is important, since,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":31306,"to":31385}}}}],[632,{"pageContent":"up, the matrix A is just their coefficient matrix.) \nRecognizing that a transformation is a matrix transformation is important, since, \nas the next theorem shows, all matrix transformations are linear transformations. \nLet A be an m X n matrix. Then the matrix transformation TA : !R\nn \n� !R\nm \ndefined by \nis a linear transformation.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":31385,"to":31393}}}}],[633,{"pageContent":"-\ny \nExample 3.56 \ny \n(\n1, \n2) \nT \n(x, \ny\n) \nT \nI \nI \nI \n�+----+1 �--+-��:1--•x \nI \nI \nI \nI \nI \nI \nI \n• \n• \n(x, \n-\ny\n) \n(\n1, -\n2) \nFigure 3.4 \nReflection in the x-axis \nExample 3.51 \nx \nFigure 3.5 \nA 90° rotation \nSection 3.6 \nIntroduction to Linear Transformations 215 \nProof \nLet u and v be vectors in !R\nn \nand let c be a scalar. Then \nand \nT\nA\n(\ncv\n) = A (\ncv\n) = c (\nAv\n) = cT\nA\n(\nv\n) \nHence, T\nA \nis a linear transformation. \nLet F : IR\n2 \n---+ IR\n2 \nbe the transformation that sends each point to its ref  lection in the \nx-axis. Show that Fis a  linear transformation. \nSolution From Figure 3.4, it is clear that F sends the point (x, y) to the point (x, -y). \nThus, we may write \nWe could proceed to check that Fis linear, as in   Example 3.55 (this one is even easier \nto check!), but it is faster to observe that \nTherefore, F\n[\n;\n] \n= A\n[\n;\n]\n, where A = \n[ \n� \n_ \n�\n]\n, so F   is a matrix transformation. It \nnow follows, by Theorem 3.30, that Fis a  linear transformation. \nLet R : IR\n2 \n---+ IR\n2 \nbe the transformation that rotates each point 90° counterclockwise","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":31395,"to":31486}}}}],[634,{"pageContent":"� \n_ \n�\n]\n, so F   is a matrix transformation. It \nnow follows, by Theorem 3.30, that Fis a  linear transformation. \nLet R : IR\n2 \n---+ IR\n2 \nbe the transformation that rotates each point 90° counterclockwise \nabout the origin. Show that R  is a linear transformation. \nSolution \nAs Figure 3.5 shows, R sends the point (x, y) to the point \n(\n-\ny, x). Thus, \nwe have \nHence, R is a matrix transformation and is therefore linear. \nObserve that if we multiply a matrix by  standard basis vectors, we obtain the col­\numns of the matrix. For example, \nWe can use this observation to show that every linear transformation from !R\nn \nto \n!R\nm \narises as a matrix transformation.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":31486,"to":31512}}}}],[635,{"pageContent":"216 \nChapter 3  Matrices \nTheorem 3.31 \nLet T : !R\nn \n� !R\nm \nbe a linear transformation. Then T is a matrix transformation. \nMore specifically, T =  T\nA\n> \nwhere A is the m X n matrix \nProof Let e\n1 , e\n2\n, ••. ,  e\nn \nbe the standard basis vectors in !R\nn \nand let x be a vector \nin !R\nn\n. We can write x = x\n1\ne\n1 + x\n2\ne\n2 \n+· · · + x\nn\ne\nn \n(where the x/s are the components \nofx). We also know that T\n(\ne\n1)\n, T\n(\ne\n2\n)\n, •.• , T\n(\ne\nn\n) \nare (column) vectors in !R\nm\n. Let A = \n[T\n(\ne\n1)\n: T\n(\ne\n2\n)\n: •   • \n·\n: \nT\n(\ne\nn\n)\nl \nbe the m  X n matrix with these vectors as its columns. \nThen \nT\n(\nx\n) \n=  T (x\n1\ne\n1 \n+ X\n2\ne\n2 \n+ ... \n+ x\nn\ne\nn\n) \n=  x\n1 \nT\n( e\n1\n) \n+ x\n2 \nT ( e\n2\n) \n+ · · · + X\nn \nT ( e\nn\n) \n� \n[ T\n( e\n,\n) \nT ( e\n,\n) : \n· · · \nT (e,)\n] [ }] � Ax \nas required. \nThe matrix A in Theorem 3.31 is called the standard matrix of the linear trans­\nformation T. \nExample 3.58 \nShow that a rotation about the origin through an angle e defines a linear transforma­\ntion from IR\n2 \nto IR\n2 \nand find its standard matrix. \nSolulion Let Re be the rotation. We  will give a geometric argument to establish","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":31514,"to":31640}}}}],[636,{"pageContent":"tion from IR\n2 \nto IR\n2 \nand find its standard matrix. \nSolulion Let Re be the rotation. We  will give a geometric argument to establish \nthe fact that R0 is linear. Let u and v be vectors in IR\n2\n• If they are not parallel, then \nFigure 3.6(a) shows the parallelogram rule that determines u + v. If we now apply R0, \nthe entire parallelogram is rotated through the angle e, as shown in Figure 3.6(b ).  But the \ndiagonal of this parallelogram must be R0\n(\nu\n) \n+ R0\n(\nv\n)\n, again by the parallelogram rule. \n,.......... \nHence, R0(u + v\n) \n= R0\n(\nu\n) \n+ R0\n(\nv\n)\n. (What happens ifu and v are parallel?) \ny y \nu\n+\nv \nr \n-\nI \nI \nI \nI \nu \nRe\n(\nv\n)\n' \nx \n(a) \n(b) \nFigure 3.6 \nx \nSimilarly, if we apply Re to v and cv, we obtain R0\n(\nv\n) \nand Re\n(\ncv\n)\n, as shown \nin Figure 3.7. But since the  rotation does not  affect lengths, we must then have \n,.......... \nR8\n(\ncv\n) \n= cR8\n(\nv\n)\n, as required. (Draw diagrams for the cases 0 < c < 1, -1 < c < 0, \nand c < -1.)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":31640,"to":31713}}}}],[637,{"pageContent":"y \nRe \n(\ncv\n) \nFigure 3.1 \nSection 3.6 \nIntroduction to Linear Transformations \n211 \nC\nV \nFigure 3.8 \nR\n0(e1) \ny \n'--_,-' \ncos 8 \n(\n1\n, \n0\n) \nTherefore, Re is a linear transformation. According to Theorem 3.31, we can find \nits matrix by  determining its effect on the standard basis vectors e1 and e\n2 \nof IFR\n2\n• Now, \nas Figure 3.8 shows, R\n0[\n1\n] \n= \n[\nc\n�\ns \n0\n]\n. \n0 \nsm () \nWe can find R\ne [ \n�\n] \nsimilarly, but it is faster to observe that R\ne \n[ \n�\n] \nmust be per-\npendicular (counterclockwise) to R\n0[\n1\n] \nand so, by Example 3.57, R8\n[\n0\n] \n= \n[\n-sin()\n] \n(F\n. \n3 9) \n0 \n1 cos() \n1gure .. \n[\ncos () \nTherefore, the standard matrix of Re is \n. \nsm () \nFigure 3.9 \nR\ne(ez) \ny \n-sin ()\n]\n. \ncos () \nThe result of Example 3.58 can now be used to compute the effect of any rota­\ntion. For example, suppose we wish to rotate the point (2, -1) through 60° about the \norigin. (The convention is that a positive angle corresponds to a counterclockwise","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":31715,"to":31800}}}}],[638,{"pageContent":"218 \nChapter 3  Matrices \ny \nrotation, while a negative angle is clockwise.) Since cos 60° = 1\n/\n2 and sin 60° = \n'\\/3\n/\n2, we compute \nFigure 3.10 \nA 60° rotation \ny \n(\n2\n, -I) \nExample 3.59 \n(\nx\n, \ny\n) \nT \n-\n+---\n-\n--+-\n-\n-+ \nx \n(\nx\n, \n0\n) \nFigure 3.11 \nA projection \nR \n[ \n2\n] \n= \n[\ncos 60° \n6\n0 \n-1 sin 60° \n-sin 60°\n] [ \n2\n] \ncos 60° -1 \n[ \n1\n/\n2   -'\\/3/\n2\n][ \n2\n] \n'\\/3\n/\n2 \n1\n/\n2   -1 \n[ \n(\n2 + '\\/3\n)\n/\n2\n] \n(\n2'\\/3 - 1\n)\n/2 \nThus, the image of the point (2, -1) under this rotation is the point ((2 + '\\/3)\n/\n2, \n(2'\\/3 - 1)\n/\n2) \n= \n(1.87, 1.23), as shown in Figure 3.10. \n(a) Show that the transformation P : IR\n2\n---+ IR\n2 \nthat projects a point onto the x-   axis is \na linear transformation and find its standard matrix. \n(b) More generally, if e is a line through the origin in IR\n2\n, show that the transforma­\ntion Pe : IR\n2\n---+ IR\n2 \nthat projects a point onto e is a linear transformation and find its \nstandard matrix. \nSolution \n(a) As Figure 3.11 shows, P sends the point (x, y) to the point (x, O). Thus,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":31802,"to":31902}}}}],[639,{"pageContent":"tion Pe : IR\n2\n---+ IR\n2 \nthat projects a point onto e is a linear transformation and find its \nstandard matrix. \nSolution \n(a) As Figure 3.11 shows, P sends the point (x, y) to the point (x, O). Thus, \nIt follows that Pis a  matrix transformation (and hence a linear transformation) with \nstandard matrix \n[ \n� \n�\n] \n. \n(b) Let the line e have direction vector d and let v be  an arbitrary vector. Then Pe is \ngiven by projd(v), the projection ofv onto d, which you'll recall from Section 1.2 has \nthe formula \nThus, to show that Pe is linear, we  proceed as follows: \n(\nd· (u  + v\n)\n) \nPe(u + v) \n= \nd \nd·d \n= \n( \nd·u\nd\n�\nd\nd·v\n)\nd \n= \n-\n+\n-\nd \n(\nd·u d·v\n) \nd·d   d·d \n(\nd·u\n) \n(\nd•v\n) \n= \nd·d \nd  + \nd·d \nd = Pe(u) + Pe(v) \nSimilarly,  Pe(cv) = cPe(v) for any  scalar  c  (Exercise 52). Hence, Pe  is  a  linear \ntransformation.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":31902,"to":31959}}}}],[640,{"pageContent":"and \nSection 3.6 \nIntroduction to Linear Transformations 219 \nTo find the standard matr  ix of Pe, we apply Theorem 3 .31. If we  let d = \n[ \n�\n:\n]\n, then \nThus, the standard matrix of the projection is \nd\n1\nd\n2] \n= \n[ \nd\nU\n( d\nf \n+  d\nD \nd\n� \nd\n1\nd 2/\n(\nd? +  d\ni\n) \nd\n1\nd 2/\n( d\ni \n+ \nd\ni\n)\n] \nd\nU\n( d\nl \n+  d\ni\n) \nAs a check, note that in part (a) we could take d = e1 as a direction vector for the \nx-axis. Therefore, d\n1 \n= 1 and d\n2 \n= 0, and we obtain A = \n[ \n� \n�\n]\n, as before. \n4 \nNew Linear Transformations from Old \nIf T : !R\nm \n� !R\nn \nand S : !R\nn \n� [R\nP \nare linear transformations, then we may follow T by \nS to form the composition of the two transformations, denoted S 0 T. Notice that, in \norder for S 0 T to make sense, the codomain of T and the domain of S must match \n(in this case, they are both !R\nn\n) \nand the resulting composite transformation S 0 T goes \nfrom the domain of T to the codomain of S (in this case, S 0 T: !R\nm\n� [R\nP\n)\n. Figure 3.12 \nshows schematically how this composition works. The formal definition of composi­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":31961,"to":32043}}}}],[641,{"pageContent":"from the domain of T to the codomain of S (in this case, S 0 T: !R\nm\n� [R\nP\n)\n. Figure 3.12 \nshows schematically how this composition works. The formal definition of composi­\ntion of transformations is taken directly from this figure and is the same as the cor­\nresponding definition of composition of ordinary functions: \n(\nSo T\n)(\nv\n) \n= S\n(\nT\n(\nv\n)) \nOf course, we would like S 0 T to be a linear transformation too, and happily we \nfind that it is. We can demonstrate this by showing that S 0 T satisfies the definition of \na linear transformation (which we will do in Chapter 6), but, since for the time being \nwe are assuming that linear transformations and matrix transformations are the same \nthing, it is enough to show that S 0 Tis a  matrix transformation. We will use the nota­\ntion [    T] for the standard matrix of a linear transformation T. \n�m \n�n \n�\nP \nT \nS \nv \n•�•�•S(T(v)) \n= (S 0 T) (v) \nT(v) \nFigure 3.12 \nThe composition of transformations","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":32043,"to":32080}}}}],[642,{"pageContent":"220 \nChapter 3  Matrices \nTheorem 3.32 \nLet T: !R\nm\n---+ !R\nn \nand S : !R\nn \n---+ !R\nP \nbe linear transformations. Then S 0 T: !R\nm \n---+ !R\nP \nis a linear transformation. Moreover, their standard matrices are related by \n[S 0 TJ  = [SJ [ T\nJ \nProof Let [SJ =A and [TJ = B. (Notice that A is p X n and Bis n X m.\n) \nIf vis a  vector \nin !R\nm\n, then we simply compute \n(\nS 0 T\n)(\nv\n) \n= S\n(\nT\n(\nv\n)) \n= S\n(\nBv\n) \n= A\n(\nBv\n) \n= \n(\nAB\n)\nv \n......... \n(Notice here that the dimensions of A and B guarantee that the  product AB makes \nsense.) Thus, we see that the effect of S 0 Tis to multiply vectors by AB, from which \nit follows immediately that S 0 T is a matrix (hence, linear) transformation with \n[S 0 TJ =    [SJ [T\nJ\n. \nExample 3.60 \nIsn't this a great result? Say it in words: \"The matr  ix of the composite is the prod­\nuct of the matrices:' What a lovely formula! \nConsider the linear transformation T: IR\n2 \n---+ IR\n3 \nfrom Example 3.55, defined by \nr\n[\n:\n:\nJ \n= \n[ \n2x\n1 \nx\n� x\n2 \n] \n3x1 \n+ \n4X\n2 \nand the linear transformation S : IR\n3 \n---+ IR\n4 \ndefined by \nSolulion","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":32082,"to":32167}}}}],[643,{"pageContent":"Consider the linear transformation T: IR\n2 \n---+ IR\n3 \nfrom Example 3.55, defined by \nr\n[\n:\n:\nJ \n= \n[ \n2x\n1 \nx\n� x\n2 \n] \n3x1 \n+ \n4X\n2 \nand the linear transformation S : IR\n3 \n---+ IR\n4 \ndefined by \nSolulion \nWe see that the standard matrices are \n[SJ \nso Theorem 3.32 gives \n[ ! \n_ \n� \n-\n� \nl \nand \nI \nT I \n[\n�  -:\nl \n[\n� \n0 \n[\nSo T\nJ \n[SJ   [TJ \n3 \n-1 \n-\nm\n� \n-\n�\ni \n[ \n: \n-\n�\n] \n-1 \n1 \n6 \n3 \nIt follows that \n[\n-\n1-l \nl \n[\n:\n;\n] \n[\n'\" \n+ \n�, l \n(\nS aT\n)\n[\n:J \n3x\n1 \n- 7x\n2 \n-x\ni \n+ \nX\n2 \n6x\n1 + \n3x\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":32167,"to":32264}}}}],[644,{"pageContent":"Example 3.61 \nSection 3.6 \nIntroduction to Linear Transformations \n221 \n(In Exercise 29, you will be asked to check this result by setting \n[;:] = r[:\n:\nJ = [ 2x\n1 \nx\n� x\n2 \n] \ny\n3 \n3x\n, \n+ 4x\n2 \nand substituting these values into the definition of S, thereby calculating \n(\nS 0 T\n) [ \nx\n,\n] \ndice<tly.) \n.:+ \nFind the standard matr  ix of the transformation that first rotates a point 90° counter­\nclockwise about the origin and then reflects the result in the x-axis. \nSolution \nThe rotation R and the reflection F were discussed in Examples 3.57 and \n3.56, respectively, where we found their standard matrices to be [ R ] = [ \n� \n-\n�\n] and \n[ F] = \n[ \n1 \nO\n]\n. \nIt follows that the  composition F 0 R has for its matrix \n0 -1 \n[\n1 o\n][\no -1\n]    [ \no -1\n] \n[Fa R ] = [F] [ R ] = \n= \n0 -1 1 0 -1 0 \nii--\"\"--\n(Check that this result is correct by considering the effect of F 0 R on the standard \nbasis vectors e1 and e\n2\n. Note the importance of the order of the transformations:","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":32266,"to":32325}}}}],[645,{"pageContent":"= \n0 -1 1 0 -1 0 \nii--\"\"--\n(Check that this result is correct by considering the effect of F 0 R on the standard \nbasis vectors e1 and e\n2\n. Note the importance of the order of the transformations: \nR is performed before F, but we write F 0 R. In this case, R ° F also makes sense. Is \nRoF = FoR?\n) \n4 \nInverses of  linear Transformations \nConsider the effect of a 90° counterclockwise rotation about the origin followed by \na 90° clockwise rotation about the origin. Clearly this leaves every point in IR\n2 \nun­\nchanged. Ifwe denote these transformations by R9\n0 \nand R_9\n0 \n(remember that a    nega­\ntive angle measure corresponds to clockwise direction), then we may express this \nas \n(\nR9\n0 \n° R_9\n0) \n(v) = v   for every v in IR\n2\n. Note that, in this case, if we perform the \ntransformations in the other order, we get the same end result: \n(\nR_9\n0 \n° R9\n0) \n(v) = v \nfor every v in IR\n2\n• \nThus, R9\n0 \n° R\n_\n9\n0 \n(and R\n_\n9\n0 \n° R9\n0 \ntoo) is a linear transformation that leaves every \nvector in IR\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":32325,"to":32380}}}}],[646,{"pageContent":"(\nR_9\n0 \n° R9\n0) \n(v) = v \nfor every v in IR\n2\n• \nThus, R9\n0 \n° R\n_\n9\n0 \n(and R\n_\n9\n0 \n° R9\n0 \ntoo) is a linear transformation that leaves every \nvector in IR\n2 \nunchanged. Such a transformation is called an identity transformation. \nGenerally, we have one such transformation for every !R\nn\n-namely, I: !R\nn\n� !R\nn \nsuch \nthat I(v) =   vfor everyv in !R\nn\n. (If it is important to keep track of the dimension of the \nspace, we might write I\nn \nfor clarity.) \nSo, with this notation, we have R9\n0 \n° R_9\n0 \n=I= R_9\n0 \n° R9\n0 . A pair of transforma­\ntions that are related to each other in this way are called inverse transformations. \nDefinition \nLet S and T be linear transformations from !R\nn \nto !R\nn\n. Then S and T \nare inverse transformations if S 0 T = I\nn \nand T 0 S = I\nn\n-","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":32380,"to":32437}}}}],[647,{"pageContent":"222 \nChapter 3  Matrices \nRemark Since this definition is symmetric with respect to S and T, we will say \nthat, when this situation occurs, Sis the  inverse of   T and Tis the  inverse of S. Further­\nmore, we will say that S and Tare invertible. \nIn terms of matrices, we see immediately that if S and Tare inverse transforma-\n� \ntions, \nthen [SJ [T\nJ \n= [S 0 T\nJ \n= \n[\nJ\nJ \n= I, where the last I is the identity matrix. (Why \nis  the  standard matrix of the  identity transformation the  identity matrix?) We \nmust \nalso have [T][SJ \n= \n[T 0 SJ \n= \n[\nJ\nJ \n= I.  This shows that [SJ and [T\nJ \nare inverse \nmatrices. It shows something more: If a linear transformation Tis invertible, then its \nstandard matr  ix [ T \nJ \nmust be invertible, and since matrix inverses are unique, this \nmeans that the inverse of Tis also unique. Therefore, we can unambiguously use the \nnotation r\n-\n1 \nto refer to th e inverse of T. Thus, we can rewrite the above equations as \n[TJ [T\n-\n1\nJ \n=I= \n[T\n-\n1\nJ [T\nJ\n, showing that the matrix of r\n-\n1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":32439,"to":32489}}}}],[648,{"pageContent":"notation r\n-\n1 \nto refer to th e inverse of T. Thus, we can rewrite the above equations as \n[TJ [T\n-\n1\nJ \n=I= \n[T\n-\n1\nJ [T\nJ\n, showing that the matrix of r\n-\n1 \nis the  inverse matrix of [T\nJ\n. \nWe have just proved the following theorem. \nTheorem 3.33 \nExample 3.62 \nLet T: !R\nn\n---+ !R\nn \nbe an invertible linear transformation. Then its standard matrix \n[ T J \nis an invertible matrix, and \nRemark Say this one in words too: \"The matr  ix of the inverse is the inverse of \nthe matrix:' Fabulous! \nFind the standard matrix of a 60° clockwise rotation about the  origin in IR\n2\n• \nSolulion Earlier we computed the matrix of a 60° counterclockwise rotation about \nthe origin to be \n-V3/2\n] \n1\n/2 \nSince a 60° clockwise rotation is the inverse of a 60° counterclockwise rotation, we can \napply Theorem 3.33 to obtain \n-  ( \n)-1 -\n[ \n1\n/\n2 \n[R\n-\n6\no\nJ \n-\n[  R\n6\no \nJ \n-\nV3/\n2 \n-V3/2\n]\n-\n1 \n1/2 \n[ \n1\n/2 \n-V3/\n2 \nV3/\n2\n] \n1\n/2 \nlt--l'--\n(Check the calculation of the matrix inverse. The fastest way is to  use the 2 X 2 short­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":32489,"to":32567}}}}],[649,{"pageContent":"apply Theorem 3.33 to obtain \n-  ( \n)-1 -\n[ \n1\n/\n2 \n[R\n-\n6\no\nJ \n-\n[  R\n6\no \nJ \n-\nV3/\n2 \n-V3/2\n]\n-\n1 \n1/2 \n[ \n1\n/2 \n-V3/\n2 \nV3/\n2\n] \n1\n/2 \nlt--l'--\n(Check the calculation of the matrix inverse. The fastest way is to  use the 2 X 2 short­\ncut from Theorem 3.8. Also, check that the resulting matrix has the right effect on the \nstandard basis in IR\n2 \nby drawing a diagram.) \n4 \nExample 3.63 \nDetermine whether projection onto the x-axis is an invertible transformation, and if \nit is, find its inverse. \nSolulion The standard matrix of this projection Pis [ \n� \nsince its determinant is 0. Hence, Pis not  invertible either. \n�\n]\n, which is not invertible \n4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":32567,"to":32618}}}}],[650,{"pageContent":"T \n(a\n, \nb\n) \nI \n�\n(a\n, \nb\n'\n) \nI \n(a\n, 0\n) \nI \n•\n(a\n, \nb\n''\n) \nFigure 3.13 \nSection 3.6 \nIntroduction to Linear Transformations 223 \nRemark \nFigure 3.13 gives some idea why Pin Example 3.63 is not invertible. The \nprojection \"collapses\" IR\n2 \nonto the x-axis. For P to be invertible, we would have to have \na way of \"undoing\" it, to recover the point (a, b\n) \nwe started with. However, there are \ninfinitely many candidates for the image of (a, O) under such a hypothetical \"inverse:' \nWhich one should we use? We cannot simply say that P\n-\n1 \nmust send \n(\na, 0) to \n(\na, b ), \nsince this cannot be a definition when we have no way of knowing what b should be. \n(See Exercise 42.) \nProjections are not invertible \nAssocialivilv \nTheorem 3.3(a) in Section 3.2 stated the associativity property for matr  ix multipli-\n� \ncation: A(BC) = (AB)C. (If you didn't try to prove it then, do so now. Even with all \nmatrices restricted 2  X   2, you will get some feeling for the notational complexity","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":32620,"to":32670}}}}],[651,{"pageContent":"� \ncation: A(BC) = (AB)C. (If you didn't try to prove it then, do so now. Even with all \nmatrices restricted 2  X   2, you will get some feeling for the notational complexity \ninvolved in an   \"elementwise\" proof, which should make you appreciate the proof we \nare about to give.) \nOur approach to the proof is via linear transformations. We have seen that every \nm X n matrix A gives rise to a linear transformation TA : !R\nn \n---+ !R\nm\n; conversely, every \nlinear transformation T : !R\nn \n---+ !R\nm \nhas a corresponding m X n matrix [ T]. The two \ncorrespondences \nare inversely related; that is, given A, [  TA] =A, and given T, T\nr\nT\nJ \n= T. \nLet R =  TA, S =   Tn, and T =  Tc. Then, by Theorem 3.32, \nA\n(\nBC\n) \n= \n(\nAB\n)\nC  if and only if  R 0 \n(\nS 0 T\n) \n= \n(\nR 0 S\n) \na \nT \n� \nWe now prove the latter identity. Let x be in the domain of T   [and hence in the do­\nmain of both R 0 (S 0 T) and (R 0 S) 0 T-why?]. To prove that R 0 (S 0 T) = (R 0 S) 0 T,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":32670,"to":32713}}}}],[652,{"pageContent":"(\nS 0 T\n) \n= \n(\nR 0 S\n) \na \nT \n� \nWe now prove the latter identity. Let x be in the domain of T   [and hence in the do­\nmain of both R 0 (S 0 T) and (R 0 S) 0 T-why?]. To prove that R 0 (S 0 T) = (R 0 S) 0 T, \nit is enough to prove that they have the same effect on x. By repeated application of the \ndefinition of composition, we have \n(\nR\na \n(\nS a \nT\n))(\nx\n) \n= R\n((\nS 0 T\n)(\nx\n)) \n= R\n(\nS\n(\nT\n(\nx\n))) \n= \n(\nR 0 S\n)(\nT\n(\nx\n)) \n= \n((\nR 0 S\n) \n0 T\n)(\nx\n) \n� \nas required. (Carefully check how the definition of composition has been used four \ntimes.) \nI Exercises 3.6 \nThis section has served as an introduction to linear transformations. In Chap­\nter 6, we will take a more detailed and more general look at these transformations. \nThe exercises that follow also contain some additional explorations of this important \nconcept. \n1. Let TA  : IR\n2 \n---+ IR\n2 \nbe the matrix transformation corre­\nsponding to A= \n[\n� \n-\n:\n]\n. Find TA (u) and TA (v), \n2. Let TA : IR\n2 \n---+ IR\n3 \nbe the matrix transformation corre-\n[ 3   -1\n] \nsponding to A = \n� \n! \n. Find TA (   u) and \nwhere u = \n[","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":32713,"to":32798}}}}],[653,{"pageContent":"sponding to A= \n[\n� \n-\n:\n]\n. Find TA (u) and TA (v), \n2. Let TA : IR\n2 \n---+ IR\n3 \nbe the matrix transformation corre-\n[ 3   -1\n] \nsponding to A = \n� \n! \n. Find TA (   u) and \nwhere u = \n[\n�\n] \nand v = \n[ \n-\n�l \nTA (v), wh ere u = \n[\n�\n]\nand v = \n[ \n_\n�\nJ\n.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":32798,"to":32833}}}}],[654,{"pageContent":"224 \nChapter 3  Matrices \nIn Exercises 3-6, prove t\nh\nat t\nh\ne \ng\niven transformation is a \nlinear transformation, usin\ng \nt\nh\ne definition (or t\nh\ne Remark \nfo llowin\ng \nExample 3.55). \nIn Exercises 7-10, \ng\nive a counterexample to s\nh\now t\nh\nat t\nh\ne \ng\niven transformation is not a linear tra  nsformation. \n7. r\n[\n; \nJ \n[\n:2 \nJ \n8. r\n[\n; \nJ \n10. r\n[\n; \nJ \n[ \nl\nx\nl\n] \nI\nY\nI \n9. r\n[\n;\nJ \n[\nx 7 J \n[\nx + \nl\n] \ny - 1 \nIn Exercises 11-14, find t\nh\ne standard matrix of t\nh\ne linear \ntransformation in t\nh\ne \ng\niven exercise. \n11. Exercise 3 \n12. Exercise 4 \n13. Exercise 5 \n14. Exercise 6 \nIn Exercises 15-18, s\nh\now t\nh\nat t\nh\ne \ng\niven tra  nsforma-\ntion from IR\n2 \nto IR\n2 \nis linear by s\nh\nowin\ng \nt\nh\nat  it is a matrix \ntransformation. \n15. F reflects a vector in the y-axis. \n16. R rotates a vector 45° counterclockwise about the \norigin. \n17. D stretches a vector by a factor of2 in the x-component \nand a factor of 3 in they-component. \n18. P projects a vector onto the line y = x. \n19. The three types of elementary matrices give rise to five","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":32835,"to":32940}}}}],[655,{"pageContent":"and a factor of 3 in they-component. \n18. P projects a vector onto the line y = x. \n19. The three types of elementary matrices give rise to five \ntypes of 2 X 2 matrices with one of the following forms: \n[ \n� \n�\n] \nor \n[ \n� �\n] \n[\n� \n�\n] \n[ \n� \n�\n] \nor \n[\n� \n�\n] \nEach of these elementary matrices corresponds to a linear \ntransformation from IR\n2 \nto IR\n2\n• Draw pictures to illustrate \nthe effect of each one on the unit square with vertices at \n(0, \nO), (1, O), (0, 1), and \n(1, 1). \nIn Exercises 20-25, find t\nh\ne standard matrix of t\nh\ne \ng\niven \nlinear tra  nsformation from IR\n2 \nto IR\n2\n. \n20. Counterclockwise rotation through 120° about the \norigin \n21. Clockwise rotation through 30° about the origin \n22. Projection onto the line y = 2x \n23. Projection onto the line y = -x \n24. Reflection in the line y = x \n25. Reflection in the line y = -x \n26. Let e be a   line through the origin in IR\n2\n, Pe the linear \ntransformation that projects a vector onto e, and Fe the \ntransformation that reflects a vector in e.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":32940,"to":32998}}}}],[656,{"pageContent":"25. Reflection in the line y = -x \n26. Let e be a   line through the origin in IR\n2\n, Pe the linear \ntransformation that projects a vector onto e, and Fe the \ntransformation that reflects a vector in e. \n(a) Draw diagrams to show that Fe is linear. \n(b) Figure 3.14 suggests a way to find the matrix of Fe, \nusing the fact that the diagonals of a parallelogram \nbisect each other. Prove that Fe\n(\nx\n) \n= 2Pe\n(\nx\n) -  x, \nand use this result to show that the standard matrix \nof Fe is \n(where the direction vector of e is d = \n[ \n�\n:\n]\n). \n(c) If the angle between e and the positive x-axis is e, \nshow that the matrix of Fe is \ny \nFigure 3.14 \n[\ncosW \nsinW \nsinW\n] \n-cosW \nIn Exercises 27 and 28, apply part (b) or (c) of Exercise 26 \nto find t\nh\ne standard matrix of t\nh\ne transformation. \n27. Reflection in the line y = 2x","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":32998,"to":33039}}}}],[657,{"pageContent":"28. Reflection in the line y = \n\\/3\nx \n29. Check the  formula for S 0 Tin Example 3.60, by \nperforming the suggested direct substitution. \nIn Exercises 30-35, verify T\nh\neorem 3.32 by findin\ng \nt\nh\ne \nmatrix of S 0 T (a) by direct substitution and (b) by matrix \nmultiplication of [S] [T]. \nIn Exercises 36-39, find t\nh\ne standard ma  trix of t\nh\ne compos­\nite transformation from IR\n2 \nto IR\n2\n• \n36. Counterclockwise rotation through 60°, followed by \nreflection in the line y = x \n37. Reflection in the y-axis, followed by clockwise rotation \nthrough 30° \n38. Clockwise rotation through 45°, followed by projec­\ntion onto the y-axis, followed by clockwise rotation \nthrough 45° \n39. Reflection in the line y \n= \nx\n, \nfollowed by counterclock­\nwise rotation through 30°, followed by reflection in the \nliney \n= \n-\nx \nIn Exercises 40-43, use matrices to prove t\nh\ne \ng\niven state­\nments a  bout transformations from IR\n2 \nto IR\n2\n• \n40. If Re denotes a rotation (about the origin) through the \nangle (), then R\" 0 R\n13 \n= Ra\n+\nf3\n· \nSection 3.6","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":33041,"to":33099}}}}],[658,{"pageContent":"h\ne \ng\niven state­\nments a  bout transformations from IR\n2 \nto IR\n2\n• \n40. If Re denotes a rotation (about the origin) through the \nangle (), then R\" 0 R\n13 \n= Ra\n+\nf3\n· \nSection 3.6 \nIntroduction to Linear Transformations \n225 \n41. If() is the  angle between lines e and m (through the \norigin), then F\nm \n° Fe= R\n+\nw\n· \n(See Exercise 26.) \n42. (a) If Pis a  projection, then P 0 P = P. \n(b) The matrix of a projection can never be invertible. \n43. If e, m\n, \nand n are three lines through the origin, then \nF\nn \n° F\nm \n° Fe is also a  reflection in a line through the \norigin. \n44. Let T be a linear transformation from IR\n2 \nto IR\n2 \n(or \nfrom IR\n3 \nto IR\n3\n)\n. Prove that T maps a straight line to a \nstraight line or a point. [Hint: Use the vector form of \nthe equation of a line.] \n45. Let T be a linear transformation from IR\n2 \nto IR\n2 \n(or \nfrom IR\n3 \nto IR\n3\n)\n. Prove that T maps parallel lines to \nparallel lines, a single line, a pair of points, or a single \npoint. \nIn Exercises 46-51, let ABCD be t\nh\ne square  wit\nh \nvertices","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":33099,"to":33167}}}}],[659,{"pageContent":"2 \nto IR\n2 \n(or \nfrom IR\n3 \nto IR\n3\n)\n. Prove that T maps parallel lines to \nparallel lines, a single line, a pair of points, or a single \npoint. \nIn Exercises 46-51, let ABCD be t\nh\ne square  wit\nh \nvertices \n(-1, 1), (1, 1), (1, -1), and (-l, -1). Us e t\nh\ne results in \nExercises 44 and 45 to find and draw t\nh\ne ima\ng\ne of ABCD \nunder t\nh\ne \ng\niven transformation. \n46. T in Exercise 3 \n47. Din Exercise 17 \n48. P in Exercise 18 \n49. The projection in Exercise 22 \n50. T in Exercise 31 \n51. The transformation in Exercise 37 \n52. Prove that Pe(cv) = cPe(v) for any scalar c \n[Example 3.59(b)]. \n53. Prove that T: !R\nn \n---+ !R\nm \nis a linear transformation if and \nonly if \nfor all v1\n, \nv\n2 \nin !R\nn \nand scalars c1\n, \nc\n2\n. \n54. Prove that (as noted at the beginning of this section) \nthe range of a linear transformation T : !R\nn \n---+ !R\nm \nis the \ncolumn space of its matr  ix [ T]. \n55. If A is an invertible 2 X 2 matr  ix, what does the \nFundamental Theorem of Invertible Matrices assert \nabout the corresponding linear transformation TA in","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":33167,"to":33231}}}}],[660,{"pageContent":"is the \ncolumn space of its matr  ix [ T]. \n55. If A is an invertible 2 X 2 matr  ix, what does the \nFundamental Theorem of Invertible Matrices assert \nabout the corresponding linear transformation TA in \nlight ofExercise 19?","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":33231,"to":33236}}}}],[661,{"pageContent":"Figure 3.15 \nCanadarm \n226 \nVignette \nRobotics \nIn 1981, the U.S. Space Shuttle Columbia blasted off equipped with a device called the \nShuttle Remote Manipulator System (SRMS). This robotic arm, known as Canadarm, \nhas proved to be a vital tool in all subsequent space shuttle missions, providing strong, \nyet precise and delicate handling of its payloads (see Figure 3.15). \nCanadarm has been used to place satellites into their proper orbit and to retrieve \nmalfunctioning ones for repair, and it has also performed critical repairs to the shut­\ntle itself. Notably, the robotic arm was instrumental in the successful repair of the \nHubble Space Telescope. Since 1998, Canadarm has played an important role in the \nassembly and operation of the International Space Station. \n;;; \n<( \nz \n35 \n<( \nz \nA robotic arm consists of a series of links of fixed length connected at \nj\noints where \nthey can rotate. Each link can therefore rotate in space, or (through the effect of the","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":33238,"to":33261}}}}],[662,{"pageContent":";;; \n<( \nz \n35 \n<( \nz \nA robotic arm consists of a series of links of fixed length connected at \nj\noints where \nthey can rotate. Each link can therefore rotate in space, or (through the effect of the \nother links) be translated parallel to itself, or move by a combination (composition) of \nrotations and translations. Before we can design a mathematical model for a robotic \narm, we need to understand how rotations and translations work in composition. To \nsimplify matters, we will assume that our arm is in IR\n2\n.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":33261,"to":33276}}}}],[663,{"pageContent":"In Section 3.6, we saw that the matrix of a   rotation R about the origin through an \n[\ncose  -sine] \nangle e is a linear transformation with matrix \n. \n(Figure 3.16(a)). If \n[ \n] \nsme    cose \nv \n= : , then a translation along vis the  transformation \nT\n(\nx\n) \n= \nx + v or, equivalently, T \n[\n;\n] \n= \n[\n; \n: : \n] \n(Figure 3.16(b)). \ny y \nT\n(\nx\n) \n= x \n+ \nv \nR\n(\nx\n) \nx \n(a) Rotation \n(b) Translation \nFigure 3.16 \nUnfortunately, translation is not a linear transformation, because T(O) * 0. How­\never, there is a trick that will get us around this problem. We can represent the vector \nx � \n[\n;\n] \n\"' th, mtoc \n[ \n�\n] \nin D;l'. This i& <oll,d \"P\"\"'nting x in homogrn,om coor­\ndinates. Then the matrix multiplication \nrepresents the translated vector T(x) in homogeneous coordinates. \nWe can treat rotations in homogeneous coordinates too. The matrix multiplication \n[cose \nsine \n0 \n-sine \ncose \n0 \no\n]\n[\nx\n] \n[xcose  -  ysinel \n� \n; \n= \nxsine \n: \nycose \nrepresents the rotated vector R(x) in homogeneous coordinates. The composition T 0 R","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":33278,"to":33352}}}}],[664,{"pageContent":"[cose \nsine \n0 \n-sine \ncose \n0 \no\n]\n[\nx\n] \n[xcose  -  ysinel \n� \n; \n= \nxsine \n: \nycose \nrepresents the rotated vector R(x) in homogeneous coordinates. The composition T 0 R \nthat gives the rotation R followed by the translation Tis now represented by the product \n[\ni \n0 \n1 \n0 \na\nl [cose \nb \nsine \n1    0 \n[Note that R o  T* T oR.] \n-sine \ncose \n0 \no\nl \n[cose \n� \n= si\n�\ne \n-sine \ncose \n0 \n�\n] \nTo model a robotic arm, we give each link its own coordinate system (called a \nframe\n) \nand examine how one link moves in relation to those to which it is directly \nconnected. To be specific, we let the coordinate axes for the link A; be X;   and y;, with \nthe X;-axis aligned with the link. The length of A; is denoted by \na\n;, and the angle \n221","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":33352,"to":33406}}}}],[665,{"pageContent":"228 \nbetween X; and X;\n-\n1 \nis denoted by e\n;\n. The joint between A; and A;\n-\n1 \nis at  the point \n(0, O\n) \nrelative to A; and \n(\na;_\n1\n, O\n) \nrelative to A;_\n1\n. Hence, relative to A;_\n1\n, the coordinate \nsystem for  A;  has  been  rotated  through e i and  then  translated along \n[\na\n� I\n] \n(Figure 3.17). This transformation is represented in homogeneous coordinates by \nthe matrix \nFigure 3.11 \nTo give a specific example, consider Figure 3.18(a). It shows an arm with three \nlinks in which A \n1 \nis in   its initial position and each of the other two links has been \nrotated 45° from the previous link. We will take the length of each link to be 2 units. \nFigure 3.18(b) shows A\n3 \nin its initial frame. The transformation \n[cos45 \nT\n3 \n= sin\n0 \n45 \n-sin45 \ncos 45 \n0 \n-1\n/\\/2 \nl/v2 \n0 \ncauses a rotation of 45° and then a translation by 2   units. As shown in 3.18(c), this \nplaces A\n3 \nin its   appropriate position relative to A\n2\n's frame. Next, the transformation \n[cos45 \nT\n2 \n= \nsin\n0 \n45 \n-sin45 \ncos 45 \n0 \n2\n] \n[ l/v2 \n� \n= 1\n/\n:2 \n-1\n/\nv2 \nl/v2 \n0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":33408,"to":33487}}}}],[666,{"pageContent":"places A\n3 \nin its   appropriate position relative to A\n2\n's frame. Next, the transformation \n[cos45 \nT\n2 \n= \nsin\n0 \n45 \n-sin45 \ncos 45 \n0 \n2\n] \n[ l/v2 \n� \n= 1\n/\n:2 \n-1\n/\nv2 \nl/v2 \n0 \nis applied to the previous result. This places both A\n3 \nand A\n2 \nin their correct posi­\ntion relative to A\n1\n, as shown in Figure 3.18(d). Normally, a third transformation Ti \n(a rotation) would be applied to the previous result, but in our case, Ti is the identity \ntransformation because A\n1 \nstays in its initial position. \nTypically, we want to know the coordinates of the end (the \"hand\") of the robotic \narm, given the length and angle parameters-this is known as fo rward kinematics. \nFollowing the above sequence of calculations and referring to Figure 3.18, we see that","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":33487,"to":33528}}}}],[667,{"pageContent":"Y\n1 \nY\n3 \n(a) A three-link chain \n(b) A 3 in its initial frame \nY 2 Y\nI \n(c)  T3 puts A3 in Az's initial frame \n(d) \nT\n2\nT3  puts A3 in A\n1\n's initial fra  me \nFigure 3.18 \nwe need to determine where the point (2, O) ends up after T\n3 \nand T\n2 \nare applied. Thus, \nthe arm's hand is at \n-1/Vl \n1/V2 \n0 \nwhich represents the point \n(\n2 +  V2, 2 + Vl\n) \nin homogeneous coordinates. It is eas ily \nchecked from Figure 3.lS(a) that this is correct. \nThe methods used in this example generalize to robotic arms in three dimen­\nsions, although in IR\n3 \nthere are more degrees of freedom and hence more variables. \nThe method of homogeneous coordinates is also useful in other applications, notably \ncomputer graphics. \n229","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":33530,"to":33567}}}}],[668,{"pageContent":"230 \nChapter 3  Matrices \nAndrei A. Markov (1856-1922) \nwas a Russian mathematician who \nstudied and later taught at the \nUniversity of St. Petersburg. He \nwas interested in number theory, \nanalysis, and the theory of con­\ntinued fractions, a recently devel­\noped field that Markov applied \nto probability theory. Markov \nwas also interested in poetry, and \none of the uses to which he put \nMarkov chains was the analysis \nof patterns in poems and other \nliterary texts. \nExample 3.64 \nApplications \nMarkov Chains \nA market research team is conducting a controlled survey to determine people's pref­\nerences in toothpaste. The sample consists of 200 people, each of whom is asked to \ntry two brands of toothpaste over a period of several months. Based on the responses \nto the survey, the research team compiles the following statistics about toothpaste \npreferences. \nOf those using Brand A in any month, 70% continue to use it the following month,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":33569,"to":33593}}}}],[669,{"pageContent":"to the survey, the research team compiles the following statistics about toothpaste \npreferences. \nOf those using Brand A in any month, 70% continue to use it the following month, \nwhile 30% switch to Brand B; of those using Brand B in any month, 80% continue to \nuse it the following month, while 20% switch to Brand A. These findings are summa­\nrized in Figure 3.19, in which the percentages have been converted into decimals; we \nwill think of them as probabilities. \n0.30 \n0.70�0.80 \n0.20 \nFigure 3.19 \nFigure 3.19 is a simple example of a (finite) Markov chain. It represents an evolv­\ning process consisting of a finite number of states. At each step or point in time, the \nprocess may be in any one of the states; at the next step, the process can remain in its \npresent state or switch to one of the other states. The state to which the process moves \nat the next step and the probability of its doing so depend only on the present state","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":33593,"to":33608}}}}],[670,{"pageContent":"present state or switch to one of the other states. The state to which the process moves \nat the next step and the probability of its doing so depend only on the present state \nand not on the past history of the process. These probabilities are called transition \nprobabilities and are assumed to be constants (that is, the probability of moving from \nstate i to state j is always the same). \nIn the toothpaste survey described above, there are just two states-using Brand \nA  and  using Brand  B-and the  transition probabilities are  those indicated in \nFigure 3.19. Suppose that, when the survey begins, 120 people are using Brand A and \n80 people are using Brand B. How many people will be using each brand 1 month \nlater? 2 months later? \nSolution The number of Brand A users after 1 month will be 70% of those initially \nusing Brand A (those who remain loyal to Brand A) plus 20% of the Brand B users \n(those who switch from B to A): \n0.70\n(\n120\n) \n+ 0.20\n(\n80\n) = \n100","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":33608,"to":33629}}}}],[671,{"pageContent":"using Brand A (those who remain loyal to Brand A) plus 20% of the Brand B users \n(those who switch from B to A): \n0.70\n(\n120\n) \n+ 0.20\n(\n80\n) = \n100 \nSimilarly, the number of Brand B users after 1 month will be a   combination of those \nwho switch to Brand B and those who continue to use it: \n0.30\n(\n120\n) \n+ 0.80\n(\n80\n) = \n100","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":33629,"to":33650}}}}],[672,{"pageContent":"Section 3.7 Applications \n231 \nWe can summarize these two equations in a single matrix equation: \n[\n�\n:�\n� �\n:!\n�\n] \n[l\n!\n�\n] \n= \n[\n�\n��\n] \n, \n[\n120\n] \n[\n100\n] \nLets call the matrix P and label the vectors Xo = \nand x\n1 \n= \n. (Note \n80 \n100 \nthat the  components of each vector are the numbers of Brand A and Brand B users, \nin that order, after the number of months indicated by the subscript.) Thus, we have \nx\n1 \n= Px\n0\n. \nExtending the notation, let xk be the vector whose components record the distri­\nbution of toothpaste users after k months. To determine the number of users of each \nbrand after 2 months have elapsed, we simply apply the same reasoning, starting with \nx\n1 \ninstead of x\n0\n. We obtain \n[\n0.70 \nX\n2 \n= Px\n1 \n= \n0.30 \n0.20\n] \n[\n100\n] \n[ \n90\n] \n0.80 100 \n-\n110 \nfrom which we see that there are now 90 Brand A users and 110 Brand B users. \nThe vectors xk  in Example 3.64 are called the state vectors of the Markov chain, \nand the matrix Pis called its transition matrix. We have just seen that a  Markov chain \nsatisfies the relation \nx\nk\n+\n1 \n= Px\nk","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":33652,"to":33728}}}}],[673,{"pageContent":"and the matrix Pis called its transition matrix. We have just seen that a  Markov chain \nsatisfies the relation \nx\nk\n+\n1 \n= Px\nk \nfork = 0, 1, 2, ... \nFrom this result it follows that we  can compute an arbitrary state vector iteratively \nonce we know x\n0 \nand P. In   other words, a Markov chain is completely determined by \nits transition probabilities and its initial state. \nRemarks \n• \nSuppose, in Example 3.64, we wanted to keep track of not the actual numbers \nof toothpaste users but, rather, the relative numbers using each brand. We could con­\nvert the data into percentages or fractions by dividing by 200, the total number of \nusers. Thus, we would start with \nX\no \n= \n[\n�\ns\n�\no\n�\n] \n= \n[\n0.60\n] \n2\n00 \n0.40 \nto reflect the fact that, initially, the Brand A-Brand B split is 60%-40%. Check by \n[\n0.50\n] \ndirect calculation that PXo = \n, which can then be taken as x\n1 \n(in agreement \n0.50 \nwith the 50-50 split we computed above). Vectors such as these, with nonnegative \ncomponents that add up to 1, are called probability vectors.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":33728,"to":33775}}}}],[674,{"pageContent":", which can then be taken as x\n1 \n(in agreement \n0.50 \nwith the 50-50 split we computed above). Vectors such as these, with nonnegative \ncomponents that add up to 1, are called probability vectors. \n• \nObserve how the transition probabilities are arranged within the transition \nmatrix P. We can think of the columns as being labeled with the present states and the \nrows as being labeled with the next states: \nPresent \nA \nA \n[\n0.70 \nNext \nB  0.30 \nB \n0.20\n] \n0.80","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":33775,"to":33795}}}}],[675,{"pageContent":"232 \nChapter 3  Matrices \nThe word stochastic is derived \nfrom the Greek adjective \nstokhastikos, meaning \"capable of \naiming\" (or guessing). It has come \nto be applied to anything that is \ngoverned by the laws of probability \nin the sense that probability makes \npredictions about the likelihood of \nthings happening. In probability \ntheory, \"stochastic processes\" form \na generalization of Markov chains. \nY\nA0.49 \n7\nA\n�B0.21' \nA \n� \nY\nA0.06 \nB \n�B0.24* \nfigure 3.20 \nNote also that the columns of P are probability vectors; any square matr  ix with this \nproperty is called a stochastic matrix. \nWe can realize the deterministic nature of Markov chains in another way. Note \nthat we  can write \nand, in general, \nx\nk \n= P\nk\nx\n0 \nfor k = 0, 1, 2, ... \nThis leads us to examine the powers of a transition matrix. In Example 3.64, we \nhave \np\n2 \n= \n[\n0.\n7\n0 \n0.30 \n0.20\n] \n[\n0.\n7\n0 \n0.80 0.30 \n0.20\n] = \n[\n0.\n55 \n0.80 \n0.4\n5 \n0.30\n] \n0.\n7\n0 \nWhat are we to make of the entries of this matrix? The first thing to observe is that P\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":33797,"to":33865}}}}],[676,{"pageContent":"have \np\n2 \n= \n[\n0.\n7\n0 \n0.30 \n0.20\n] \n[\n0.\n7\n0 \n0.80 0.30 \n0.20\n] = \n[\n0.\n55 \n0.80 \n0.4\n5 \n0.30\n] \n0.\n7\n0 \nWhat are we to make of the entries of this matrix? The first thing to observe is that P\n2 \nis another stochastic matrix, since its columns sum to 1. (You are asked to prove this \nin Exercise 14.) Could it be that P\n2 \nis also a transi  tion matrix of some kind? Consider \none of its entries-say, (P\n2\n)\n2\n1 \n= 0.4\n5\n. The tree diagram in Figure 3.20 clarifies where \nthis entry came from. \nThere are four possible state changes that can occur over 2 months, and these \ncorrespond to the four branches (or paths) of length 2 in the tree. Someone who \ninitially is using Brand A can end up using Brand B 2 months later in two different \nways (marked * in the figure): The person can continue to use A after 1 month and \nthen switch to B (with probability 0.7(0.3) = 0.21), or the person can switch to B after \n1 month and then stay with B (with probability 0.3(0.8) = 0.24). The sum of these \nprobabilities gives an overall probability of 0.4\n5","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":33865,"to":33916}}}}],[677,{"pageContent":"1 month and then stay with B (with probability 0.3(0.8) = 0.24). The sum of these \nprobabilities gives an overall probability of 0.4\n5\n. Observe that these calculations are \nexactly what we do when we compute (P\n2\n)n. \nIt follows that (P\n2\n)n = 0.45 represents the probability of moving from state 1 \n(Brand A) to state 2 (Brand B) in two transitions. (Note that the order of the sub­\nscripts is the reverse of what you might have guessed.) The  argument can be general­\nized to show that \n(P\nk\n)\nij \nis the probability of moving from state j to state i in k transitions. \nIn Example 3.64, what will happen to the distribution of toothpaste users in the \nlong run? Let's work with probability vectors as state vectors. Continuing our calcula­\ntions (rounding to three decimal places), we find \nx \n-\nx \n-\nx \n-\nPx \n-\n[\n0.60\n] \n[\n0.\n5\n0\n] \n[\n0.\n7\n0 \n0 \n-\n0.40 \n' \n1 \n-\n0.50 \n' \n2 \n-\n1 \n-\n0.30 \n0.20\n]\n[\n0.\n5\n0\n] = \n[\n0.45\n]\n, \n0.80 0.50 \n0.\n55 \n[\n0.7\n0 \nX\n3  = Px\n2 \n= \n0.30 \n0.20\n] \n[\n0.45\n] = \n[\n0.42\n5\n] \nx = \n[\n0.412\n] \nx = \n[\n0.406\n] \n0.80 0.\n55 \n0.\n575 \n' \n4 \n0.588 \n'  5 \n0.594 ' \n[\n0.403\n] \n[\n0.402\n]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":33916,"to":34023}}}}],[678,{"pageContent":"[\n0.\n7\n0 \n0 \n-\n0.40 \n' \n1 \n-\n0.50 \n' \n2 \n-\n1 \n-\n0.30 \n0.20\n]\n[\n0.\n5\n0\n] = \n[\n0.45\n]\n, \n0.80 0.50 \n0.\n55 \n[\n0.7\n0 \nX\n3  = Px\n2 \n= \n0.30 \n0.20\n] \n[\n0.45\n] = \n[\n0.42\n5\n] \nx = \n[\n0.412\n] \nx = \n[\n0.406\n] \n0.80 0.\n55 \n0.\n575 \n' \n4 \n0.588 \n'  5 \n0.594 ' \n[\n0.403\n] \n[\n0.402\n] \n[\n0.401\n] \n[\n0.400\n] \n[\n0.400\n] \nX\n6 \n= \n0.597 \n'\nX? \n= \n0.598 \n'\nXg \n= \n0.599 \n'\nX\n9 \n= \n0.600 \n'\nX\nio \n= \n0.600","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":34023,"to":34124}}}}],[679,{"pageContent":"Example 3.65 \nSection 3.7 Applications \n233 \n[\n0.4\n] \nand so on. It appears that the state vectors approach (or conver\ng\ne to\n) \nthe vector , \n0.6 \nimplying that eventually 40% of the toothpaste users in the survey will be using \nBrand A and 60% will be using Brand B. Indeed, it is easy to check that, once this \ndistribution is reached, it will never change. We simply compute \n[\n0.\n7\n0 0.20\n] \n[\n0.4\n] = \n[\n0.4\n] \n0.30 0.80 0.6 0.6 \nA state vector x with the property that Px = x is called a steady state vector. In \nChapter 4, we will prove that every Markov chain has a unique steady state vector. For \nnow, let's accept this as a fact and see how we can find such a vector without doing any \niterations at all. \nWe begin by rewriting the matrix equation Px = x as Px = Ix, which can in turn be \nrewritten as (I - P)x = 0. Now this is just a homogeneous system of linear equations \nwith coefficient matrix I -   P, so the augmented matr  ix is [I - PI OJ. In Example 3.64, \nwe have \n[I \n-\np \nI \nOJ \n= \n[\n1 \n-\n0.\n7\n0 \n-\n0.30 \nwhich reduces to \n-","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":34126,"to":34176}}}}],[680,{"pageContent":"with coefficient matrix I -   P, so the augmented matr  ix is [I - PI OJ. In Example 3.64, \nwe have \n[I \n-\np \nI \nOJ \n= \n[\n1 \n-\n0.\n7\n0 \n-\n0.30 \nwhich reduces to \n-\n0.20 \nI \nO\nJ \n[ \n0.30 \n1 \n-\n0.80 0 \n-\n-\n0.30 \n-\n0.20 l\no\n] \n0.20 0 \nSo, if our steady state vector is x = \n[\n::\n]\n, then x\n2 \nis a free  variable and the parametric \nsolution is \nIf we require x to be a probability vector, then we must have \n1 = X\n1 \n+ Xz = \nt\nt + t = �t \n3 \n2 \n[\n0.4\n] \nTherefore, x\n2 \n= t = 5 = 0.6 and x\n1 \n= 5 = 0.4, so x = \n, in agreement with our \n0.6 \niterative calculations above. (If we require x to contain the actual distribution, then \nin this example we must have x1 + x\n2 \n= 200, from which it follows that \nx = \n[ \n80\n] \n.) \n120 \nA psychologist places a rat in a cage with three compartments, as shown in Figure 3.21. \nThe rat has been trained to select a door at random whenever a bell is rung and to \nmove through it into the next compartment. \n(a) If the rat is initially in compartment 1, what is the probability that it will be in","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":34176,"to":34250}}}}],[681,{"pageContent":"move through it into the next compartment. \n(a) If the rat is initially in compartment 1, what is the probability that it will be in \ncompartment 2 after the bell has rung twice? three times? \n(b) In the long run, what proportion of its time will the rat spend in each compartment? \nSolution Let P = [p;\nj\nJ be the transition matr  ix for   this Markov chain. Then \nI I \n2 \nd \nP21 \n= \nP3\n1 \n= \n2, P\n12 \n= \nPn \n= \n3\n, \nP32 \n= \np\n23 \n= \n3\n, \nan \nP\n11 \n= \nP22 \n= \np\n33 \n= O","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":34250,"to":34287}}}}],[682,{"pageContent":"234 \nChapter 3  Matrices \nFigure 3.21 \n� \n(Why? Remember that pij is the probability of moving from j to i.\n) \nTherefore, \nand the initial state vector is \n(a) After one ring of the bell, we have \nContinuing (rounding to three decimal places), we find \n[\nl \nl \n3 \nx\n2 \n= Px\n1 \n= \n0 \n2 \n3 \nand \n[\nl \nI \n3 \nx\n3 \n= Px\n2 \n= \n0 \n2 \n3 \nm11 \n!\n]\n[!\n] \n[!\n] \n[\n03 33\n-\n= \n0.333 \n0.333 \n[ \nl \nl \n[ \n0 222 l \n� \n= \n0.389 \n18 \n0.389 \nTherefore, after two rings, the probability that the rat is in compartment 2  is ! \n= \n0.333, and  after three rings, the  probability that  the rat is in compartment  2  is \nfs \n= \n0.389. [Note that these questions could also be answered by computing \n(\nP\n2\n)\nn \nand \n(\nP\n3\n)\n2\n1\n.]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":34289,"to":34365}}}}],[683,{"pageContent":"Section 3.7 Applications \n235 \n(b) This question is asking for the steady state vector x as a probability vector. As we \nsaw above, x must be in the null space of I -P, so we proceed to solve the system \n_\nl O\nJ \n[ 1 \n-i 0 -----* 0 \n1  0 \n0 \n0 \n0 \n-� O\nJ \n-1 0 \n0  0 \nHence, ifx � \n[ \n:: l then x, � l idm and x, � l t, x, � l. Since x mu•t be ' prnb· \nability vector, we need 1 = x1 + x\n2 \n+ x\n3 \n= � t. Thus, t = i and \nwhich tells us that, in the long run, the rat spends �of its time in compartment 1 and \ni of its time in each of the other two compartments. \nlinear Economic Models \nWe  now revisit the economic models that we first encountered in Section 2.4 and \nrecast these models in terms of matrices. Example 2.33 illustrated the Leontief closed \nmodel. The system of equations we needed to solve was \nIn matrix form, this is the  equation Ex = x, where \nE=\n[\n�\n�: \n�\n�\n! \n1\n/\n2   1\n/\n3 \n1/2\n] \n[x\n1\n] \n1\n/\n4  and  x = x\n2 \n1\n/\n4 \nX\n3 \nThe matrix E  is called an exchange matrix and the vector x is called a price vector. \nIn general, if E = [ e;\nj","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":34367,"to":34427}}}}],[684,{"pageContent":"E=\n[\n�\n�: \n�\n�\n! \n1\n/\n2   1\n/\n3 \n1/2\n] \n[x\n1\n] \n1\n/\n4  and  x = x\n2 \n1\n/\n4 \nX\n3 \nThe matrix E  is called an exchange matrix and the vector x is called a price vector. \nIn general, if E = [ e;\nj\n], then e;\nj \nrepresents the fraction (or percentage) of industry j's \noutput that is consumed by industry i and \nX\n; is the  price charged by industry i for its \noutput. \nIn a closed economy, the sum of each column of E is 1. Since the entries of E are \nalso nonnegative, Eis a  stochastic matrix and the problem of finding a solution to the \nequation \nEx = x \n(1) \nis precisely the same as the problem of finding the steady state vector of a Markov \nchain!  Thus, to find a price vector x that satisfies Ex = x, we solve the equivalent \nhomogeneous equation \n(\nI -E\n)\nx =   0.  There will always be  infinitely many solu­\ntions; we seek a solution where the prices are all nonnegative and at least one price \nis positive.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":34427,"to":34476}}}}],[685,{"pageContent":"236 \nChapter 3  Matrices \nExample 3.66 \nThe Leontief open model is more interesting. In Example 2.\n3\n4\n, \nwe needed to solve the \nsystem \nx\n1 \n= 0.\n2\nx\n1 \n+ 0.5x\n2 \n+ \nO\n.\nl\nx3 + 10 \nXz= 0.\n4\nx\n1 \n+ 0.2X\n2 \n+ 0.2X3 + 10 \nx3= \nO\n.\nl\nx\n1 \n+ 0.\n3\nx\n2 \n+ 0.\n3\nx3 + \n3\n0 \nIn matrix form, we have \nwhere \nx =  Cx + d or \n(\nI \n-\nC\n)\nx = d \n[\n0.2  0.5 \nc = 0.\n4 \n0.2 \n0.1 \n0.\n3 \n0.1\n] \n[\nx\n1\n] \n[\n10\n] \n0.2 , \nX \n= Xz ,  d =  10 \n0.\n3 \nX3 \n3\n0 \n(2) \nThe matrix C is called the consumption matrix, x is the production vector, and d is \nthe demand vector. In general, if C = [c;\nj\n], x = [x;J,  and d = [d;J,  then c;\nj \nrepresents \nthe dollar value of industry i's output that is needed to produce one dollar's worth of \nindustry j's output, X; is the dollar value (price) of industry i's output, and d; is the  dol­\nlar value of the external demand for industry i's output. Once again, we are interested \nin finding a production vector x with nonnegative entries such that at least  one entry \nis positive. We call such a vector x a fe asible solution.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":34478,"to":34567}}}}],[686,{"pageContent":"in finding a production vector x with nonnegative entries such that at least  one entry \nis positive. We call such a vector x a fe asible solution. \nDetermine whether there is a solution to the Leontief open model determined by the \nfollowing consumption matrices: \n[\n1/\n4 \n(a) C = \n1/2 \nSolulion \n(a) \n1/\n3\n] \n1/\n3 \nWe have \nI-\nC = \n[\n� \n(b) C= [\n�\n�\n� \nO\nJ \n_ \n[\n1/\n4 \n1 \n1/2 \n1/\n3\n] \n1/\n3 \n1/2\n] \n2/\n3 \n[ \n3\n/\n4 \n-\n1/2 \n-1/\n3\n] \n2/\n3 \nso the equation \n(\nI \n-\nC\n)\nx = d becomes \n[ \n3\n/\n4 \n-\n1/2 \n-\n1/\n3\n] \n[\nX\n1\n] \n= [\nd\n1\n] \n2/\n3 \nx\n2 \nd\nz \nIn practice, we would row reduce the corresponding augmented matrix to determine \na solution. However, in this case, it is instructive to notice that the coefficient matr  ix \nI \n-\nC is invertible and then to apply Theorem \n3\n.\n7\n. We compute \n[\nX\n1\n] \n= [ \n3\n/\n4 \nX\n2 \n-\n1/2 \n-\n1/\n3\n]\n-\n1\n[\nd\n1\n] \n[\n2 \nl\n]\n[\nd\n] \n2/\n3 \nd\n2 \n-\n3\n/2 9/\n4 \nd\n: \nSince d\n1\n, \nd\n2\n, \nand all entries of \n(\nI \n-\nC)-\n1 \nare nonnegative, so are x1 and x2• Thus, we can \nfind a feasible solution for any nonzero demand vector.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":34567,"to":34712}}}}],[687,{"pageContent":"Theorem 3.34 \nSection 3.7 Applications \n231 \n(b) In this case, \nso that \n[ \n1\n/\n2 \nI \n-C = \n-1\n/\n2 \n-1\n/\n2\n] \n2\n/\n3 \nand \n(\nI -c)\n-\nl \n= \n[\n-\n4 \n-\n6 \nx =  ( I -C)-1d = \n[\n-\n4 \n-\n6 \n-\n6\n]\nd \n-\n6 \n-\n6\n] \n-\n6 \nSince all entries of \n(\nI - C\n)\n-\n1 are negative, this will not produce a feasible solution for \nany nonzero demand vector d. \n4 \nMotivated by Example 3.66, we have the following definition. (For two m X n \nmatrices A = \n[a\nij\n] and B = [b;\nj\n], we will write A 2 B if a\niJ \n2 b\n;\nJor all i and j. Similarly, \nwe may define A >  B, A ::::::  B, and so on. A matrix A is called nonnegative if A 2 0 \nand positive if A > 0.) \nDefinition \nA consumption matrix C is called productive if I -C is invertible \nand ( I -\nc)\n-\nl \n2 0. \nWe  now give three results that give criteria for a consumption matr  ix to be \nproductive. \nLet C be a   consumption matrix. Then C is pro  ductive if and only if there exists a \nproduction vector x 2 0 such that x > Cx. \nProof \nAssume that C is pro ductive. Then I -C is invertible and ( I - C  )-1 2 0. Let \nThen x = \n(\nI -\nc)\n-\n1\nj","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":34714,"to":34803}}}}],[688,{"pageContent":"production vector x 2 0 such that x > Cx. \nProof \nAssume that C is pro ductive. Then I -C is invertible and ( I - C  )-1 2 0. Let \nThen x = \n(\nI -\nc)\n-\n1\nj \n2 0 and (I -C)x = j > 0. Thus, x -Cx > 0 or,  equiva­\nlently, x > Cx. \nConversely,  assume that there exists a vector x 2 0 such  that x > Cx.  Since \nC 2 0 and C * 0, we have x > 0 by Exercise 35. Furthermore, there must exist a \nreal number ,\\ with 0 < ,\\ < 1 such that Cx < ,\\x. But then \nC\n2\nx = C(Cx) \n:::::: \nC(,\\x) = ,\\(Cx) < ,\\(\n,\\\nx) = ,\\\n2\nx \n� \nBy induction, it can be shown that 0 :::::: C\n\"\nx < ,\\\n\"\nx for all n 2 0. (Write out the de­\ntails of this induction proof.) Since 0 < ,\\ < 1, ,\\\n\" \napproaches 0 as n gets large. There­\nfore, as n ---+ \noo\n, ,\\\n\"\nx ---+ 0 and hence C\"x---+ 0. Since x > 0, we must have C\n\"\n---+ 0 as \nn ---+ \noo\n. \nNow consider the matrix equation \n( I \n-C)\n(\nI + C \n+ C\n2 \n+ \n·   ·   · \n+ C\n\"\n-1) =\nI -C\n\"","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":34803,"to":34859}}}}],[689,{"pageContent":"238 \nChapter 3  Matrices \ncorollarv  3.35 \nThe word corollary comes from \nthe Latin word corollarium, which \nrefers to a garland given as a re­\nward. Thus, a corollary is a little \nextra reward that follows from a \ntheorem. \nCorollarv  3.36 \nAs n \n� \noo\n, e\nn \n� \n0, so we have \n(I - C  )(I +  C  +  C\n2 \n+  ... ) = I -0 = I \nTherefore, I - C is invertible, with its inverse given by the infinite matrix series \nI +  C +  C\n2 \n+ \n...\n. Since all the terms in this series are nonnegative, we also have \n(I -\nc\n)-1 = r + \nc  +  c\n2 \n+ ... ::::  o \nHence, C is productive. \nRemarks \n• \nThe infinite series I +  C  +  C\n2 \n+  . \n. . \nis the matrix analogue of the geomet­\nric series 1 + x +  x\n2 \n+ \n...\n. You may be familiar with the fact that, for \nl\nx\nl \n< 1, \n1 + x +  x\n2 \n+  ... = 1\n/\n(\n1 -x\n)\n. \n• \nSince the vector Cx represents the amounts consumed by each industry, the  in­\nequality x > Cx means that there is some level of prod uction for which each industry \nis prod ucing more than it consumes. \n•","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":34861,"to":34922}}}}],[690,{"pageContent":"equality x > Cx means that there is some level of prod uction for which each industry \nis prod ucing more than it consumes. \n• \nFor an alternative approach to the first part of the proof of Theorem 3.34, see \nExercise 42 in Section 4.6. \nLet C be a consumption matrix. If the sum of each row of C is less than 1, then \nC is pro ductive. \nProof If \nthen Cx is a vector consisting of the row sums of C. If each row sum of C is less than \n1, then the condition x > Cx is satisfied. Hence, C is pro ductive. \nLet C be a consumption matrix. If the sum of each column of C is less than 1, then \nC is pro ductive. \nProof \nIf each column sum of C is less than 1, then each row sum of c\nT \nis less than 1. \nHence, C\nT \nis productive, by Corollary 3.35. Therefore, by Theorems 3.9(d) and 3.4, \nIt follows that (I - c\n)\n-l 2: 0 too and, thus, c is pro ductive. \nYou are asked to give alternative proofs of Corollaries 3.35 and 3.36 in Exercise 52 of \nSection 7.2.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":34922,"to":34945}}}}],[691,{"pageContent":"It follows that (I - c\n)\n-l 2: 0 too and, thus, c is pro ductive. \nYou are asked to give alternative proofs of Corollaries 3.35 and 3.36 in Exercise 52 of \nSection 7.2. \nIt follows from the definition of a consumption matrix that the sum of column \nj is the total dollar value of all the inputs needed to produce one dollar's worth of \nindustry j's output-that is, industry j's income exceeds its expenditures. We say that \nsuch an industry is profitable. Corollary 3.36 can therefore be rephrased to state that \na consumption matrix is pro ductive if all industries are profitable.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":34945,"to":34954}}}}],[692,{"pageContent":"P. H. Leslie, \"On the Use of \nMatrices in Certain Population \nMathematics;' Biometrika 33 \n(1945), pp. 183-212. \nExample 3.61 \nPopulation Growlh \nSection 3.7 Applications \n239 \nOne of the most popular models of population growth is a matrix-based model, first \nintroduced by P. H.   Leslie in 19\n4\n5. The Leslie model describes the growth of the fe­\nmale portion of a population, which is assumed to have a maximum lifespan. The \nfemales are divided into age classes, all of which span an   equal number of years. Using \ndata about the average birthrates and survival probabilities of each class, the model is \nthen able to determine the growth of the population over time. \nA certain species of German beetle, the Vollmar-Wasserman beetle (or VW beetle, \nfor short), lives for at most \n3 \nyears. We divide the female VW beetles into three age \nclasses of 1 year each: youths (0-1 year), juveniles (1-2 years), and adults (2-\n3 \nyears).","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":34956,"to":34978}}}}],[693,{"pageContent":"for short), lives for at most \n3 \nyears. We divide the female VW beetles into three age \nclasses of 1 year each: youths (0-1 year), juveniles (1-2 years), and adults (2-\n3 \nyears). \nThe youths do not lay eggs; each juvenile produces an average of four female beetles; \nand each adult produces an average of three females. \nThe survival rate for youths is 50% (that is, the probability of a youth's surviving to \nbecome a juvenile is 0.5)\n, \nand the survival rate for juveniles is 25%. Suppose we be  gin \nwith a population of 100 female VW beetles: \n4\n0 youths, \n4\n0 juveniles, and 20 adults. \nPredict the beetle population for each of the next 5 years. \nSolution After 1 year, the number of youths will be the number produced during \nthat ye ar: \n4\n0 x \n4 \n+ 20 x \n3 \n= 220 \nThe number of juveniles will simply be the number of youths that have survived: \n4\n0 x 0.5 = 20 \nLikewise, the number of adults will be the number of juveniles that have survived: \n4\n0 x 0.25 = 10 \nWe can combine these into a single matrix equation \n[\n�\ns \n4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":34978,"to":35014}}}}],[694,{"pageContent":"4\n0 x 0.5 = 20 \nLikewise, the number of adults will be the number of juveniles that have survived: \n4\n0 x 0.25 = 10 \nWe can combine these into a single matrix equation \n[\n�\ns \n4 \n0 \n0.25 \n°' Lxo � x,, whm x, � \n[ \n;n, the initilli populotion di'tcibution vcrtornnd x, � \n[ \n2\n:\n� \n-\nis the distribution after 1 year. We see that the  structure of the equation is exactly the \nsame as for Markov chains: x\nk\n+\ni \n= Lx\nk \nfor k= 0\n, \n1\n, \n2\n, \n... (although the interpretation \nis quite different). It follows that we  can iteratively compute successive population \ndistribution vectors. (It also follows that x\nk \n= L \nk\nx 0 \nfor k = 0\n, \n1\n, \n2\n, \n... , as for Markov \nchains, but we will not use this fact here.) \nWe compute \nx\n, \n� Lx\n, \n[\n�\ns \nx\n, \n� Lx\n, \n� \n[\n�\ns \n4 \n0 \n0.25 \n4 \n0 \n0.25 \n3\n]\n[\n220\n] \n[\n110\n] \n0 \n20 \n110 \n0   10 \n5 \n3\n][\n110\n]  [\n4\n55 \nl \n0 \n110 \n55 \n0 \n5 \n2\n7\n.5","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":35014,"to":35111}}}}],[695,{"pageContent":"240 \nChapter 3  Matrices \n4 \n0 \n0.2\n5 \n4 \n0 \n0.2\n5 \n3 \nl \n[\n455 \nl \n� \n��.\n5 \n[\n302.5 \nl \n22\n7\n.\n5 \n13.75 \n3\n][\n302.5 \nl \n[\n951.2 \nl \n0 \n22\n7\n.\n5 \n= 151.2 \n0 \n13.75 \n5\n6.88 \nTherefore, the model predicts that after \n5 \nyears there will be approximately \n95\n1 young female VW beetles, 1 5 1  juveniles, and 57 \nadults. (Note: You could argue \nthat we should have rounded to the nearest integer at each step-for example, 28 \nadults after step 3-which would have affected the subsequent iterations. We elected \nnot to do this, since the calculations are only approximations anyway and it is much \neasier to use a calculator or CAS if you do not round as you go.) \nThe matrix L in Example 3.67 is called a Leslie matrix. In general, if we have a \npopulation with n age classes of equal duration, L will be an n X n matrix with the \nfollowing structure: \nb\n, \nb\n2 \nb\n3 \nb\nn\n-\n1 \nb\nn \ns\n, \n0 0 0   0 \n0 \nS\n2 \n0 0   0 \nL= \n0 0 \nS\n3 \n0   0 \n0  0 0 \nS\nn\n-\n1 \n0 \nHere, b1\n, \nb\n2\n, \n••• are the birt\nh \nparameters (b; = the average numbers of females pro­\nduced by each female in class i\n) \nand s1\n, \ns\n2\n,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":35113,"to":35213}}}}],[696,{"pageContent":"b\n2 \nb\n3 \nb\nn\n-\n1 \nb\nn \ns\n, \n0 0 0   0 \n0 \nS\n2 \n0 0   0 \nL= \n0 0 \nS\n3 \n0   0 \n0  0 0 \nS\nn\n-\n1 \n0 \nHere, b1\n, \nb\n2\n, \n••• are the birt\nh \nparameters (b; = the average numbers of females pro­\nduced by each female in class i\n) \nand s1\n, \ns\n2\n, \n••• are the survival probabilities (s; = the \nprobability that a female in class i survives into class i + 1). \nWhat are we to make of our calculations? Overall, the beetle population appears \nto be increasing, although there are some fluctuations, such as a decrease from 250 to \n225 from year 1 to year 2. Figure 3.22 shows the change in the population in each of \nthe three age classes and clearly shows the growth, with fluctuations. \n4000 \ni:: 3000 \n0 \n·� \n:; \n§'2000 \nCl., \n1\n000 \nI \nAdults \nol..:::::::::+::::::::i:::::::: ___,_�::;..._.\"\"\"'*���=:::::::i:::::::::::+::::=:::+-_. \n0 \n2    4 \n6 \n8 \n1\n0 \nTime \n(\nin years\n) \nFigure 3.22","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":35213,"to":35284}}}}],[697,{"pageContent":"0.9 \n0.8 \n0.7 \nc \n0.6 \n0 \n-� \n:; \n0.. \n0.5 \n0 \n0.. \n4-< \n0 \n0.4 \nI \n... \nc \n0) \n(.) \nI \n.... \n0.3 \n0) \n0... \n0.2 \n0.1 \nFigure 3.23 \n5 \n10 \nTime \n(\nin years) \nSection 3.7 Applications \n241 \nJuveniles \nAdults \n15 \n20 \nIf, instead of plotting the actual population, we plot the relative population in \neach class, a different pattern emerges. To do this, we need to compute the fraction of \nthe population in each age class in each year; that is, we need to divide each distribu­\ntion vector by the sum of its components. For example, after 1 year, we have \n[\n220\n] \n[\n0.88\n] \n-\n1\n-x\n1 \n= \n_\nl\n_ \n2\n0 = 0.08 \n250 \n250 \n10 \n0.0\n4 \nwhich tells us that 88% of the population consists of youths, 8% is juveniles, and \n4\n% is \nadults. If we plot this type of data over time, we get a graph like the one in Figure \n3\n.\n2\n3, \nwhich shows clearly that the proportion of the population in each class is approaching \na steady state. It turns out that the steady state vector in this example is \n[\n0.\n7\n2\n] \n0.\n2\n4 \n0.0\n4 \nThat is, in the long run, \n7\n2% of the population will be youths, 2\n4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":35286,"to":35373}}}}],[698,{"pageContent":"a steady state. It turns out that the steady state vector in this example is \n[\n0.\n7\n2\n] \n0.\n2\n4 \n0.0\n4 \nThat is, in the long run, \n7\n2% of the population will be youths, 2\n4\n% juveniles, and \n4\n% \nadults. (In other words, the population is distributed among the three age classes in \nthe ratio 18:6:1.) We will see how to determine this ratio exactly in Chapter \n4\n. \nGraphs and Digraphs \nThere are many situations in which it is important to be able to model the inter­\nrelationships among a finite set of objects. For example, we might wish to describe \nvarious types of networks (roads connecting towns, airline routes connecting cit­\nies, communication links connecting satellites, etc.) or relationships among groups \nor individuals (friendship relationships in a society, predator-prey relationships in","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":35373,"to":35400}}}}],[699,{"pageContent":"242 \nChapter 3  Matrices \nA \nc \nA \nB \nFigure 3.24 \nB \nD \nc \nTwo representations of the same \ngraph \nD \nThe term vertex (vertices is the \nplural) comes from the Latin verb \nvertere, which means \"to turn:' In \nthe context of graphs (and geom­\netry), a vertex is a corner-a point \nwhere an edge \"turns\" into a dif­\nferent edge. \nan ecosystem, dominance relationships in a sport, etc.). Graphs are ideally suited to \nmodeling such networks and relationships, and it turns out that matrices are a useful \ntool in their study. \nA graph consists of a finite set of points (called vertices\n) \nand a finite set of \ned\ng\nes, each of which connects two (not necessarily distinct) vertices. We say that \ntwo vertices are ad\nj\nacent if they are the endpoints of an edge. Figure 3.24 shows an \nexample of the same graph drawn in two different ways. The graphs are the \"same\" \nin the sense that all we care about are the adjacency relationships that identify the \nedges.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":35402,"to":35436}}}}],[700,{"pageContent":"example of the same graph drawn in two different ways. The graphs are the \"same\" \nin the sense that all we care about are the adjacency relationships that identify the \nedges. \nWe can record the essential information about a graph in a matrix and use matrix \nalgebra to help us answer certain questions about the graph. This is particularly use­\nful if the graphs are large, since computers can handle the calculations very quickly. \nDefinition \nIf G is a graph with n vertices, then its adjacency matrix is the \nn X n matrix A [or A(G)] defined by \na \n.. \n= \n{ 1 if there is an edge between vertices i and \nj \n'1 \n0 otherwise \nFi\ngure 3.25 \nshows a graph and its associated adjacency matrix. \nV\nl \nV\n2 \n[ \n1 \nt\nl \nA= \n1 \n0 \n0  0 \nV4 \nV3 \nFigure 3.25 \nA graph with adjacency matrix A \nRemark Observe that the adjacency matrix of a graph is necessarily a \n� \nsymmetric matrix. (Why?) Notice also that a diagonal entry a;; of A is zero un­\nless there is a loop at vertex i. In some situations, a graph may have more than one","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":35436,"to":35474}}}}],[701,{"pageContent":"� \nsymmetric matrix. (Why?) Notice also that a diagonal entry a;; of A is zero un­\nless there is a loop at vertex i. In some situations, a graph may have more than one \nedge between a pair of vertices. In such cases, it may make sense to modify the \ndefinition of the adjacency matrix so that a\nij \nequals the number of edges between \nvertices i and \nj\n. \nWe define a path in a graph to be a sequence of edges that allows us to travel \nfrom one vertex to another continuously. The length of a path is the number of edges \nit contains, and we will refer to a path with k edges as a k-path. For example, in the \ngraph of Figure 3.25\n, v\n1 \nv\n3\nv\n2\nv\n1 \nis a 3-path, and v\n4\nv\n1 \nv\n2\nv\n2\nv\n1 \nv\n3 \nis a \n5\n-path. Notice that the \nfirst of these is closed (it begins and ends at the same vertex); such a path is called a \ncircuit. The second uses the edge between v\n1 \nand v\n2 \ntwice; a path that does not include \nthe same edge more than once is called a simple path.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":35474,"to":35517}}}}],[702,{"pageContent":"Figure 3.26 \nA digraph \nSection 3.7 Applications \n243 \nWe can use the powers of a graph's adjacency matrix to give us information about \nthe paths of various lengths in the graph. Consider the square of the adjacency matrix \nin \nFigure 3.25\n: \nWhat do the entries of A \n2 \nrepresent? Look at the (2, 3) entry. From the definition of \nmatrix multiplication, we know that \nThe only way this expression can result in a nonzero number is if at least one of the \nproducts a\n2\nk\na\nk\n3 \nthat make up the sum is nonzero. But a\n2\nk\na\nk\n3 \nis nonzero if and only if \nboth a\n2\nk \nand a\nk\n3 \nare nonzero, which means that there is an edge between v\n2 \nand v\nk \nas \nwell as an edge between v\nk \nand v\n3\n. Thus, there will be a 2-path between vertices 2 and \n3 (via vertex k). In our example, this happens for k \n= 1 and for k = 2, so \n(A\n2\n)\n2\n3 \n=  a\n2\n,\na\nl3 \n+ a\n22\na\n2\n3 \n+ a\n2\n3\na\n33 \n+ a\n2\n4\na\n4\n3 \n= l·l +  l·l +  l·O +  O·O \n= 2 \n� \nwhich tells us that there are two 2-paths between vertices 2 and 3. (Check to see that \nthe remaining entries of A \n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":35519,"to":35595}}}}],[703,{"pageContent":"(A\n2\n)\n2\n3 \n=  a\n2\n,\na\nl3 \n+ a\n22\na\n2\n3 \n+ a\n2\n3\na\n33 \n+ a\n2\n4\na\n4\n3 \n= l·l +  l·l +  l·O +  O·O \n= 2 \n� \nwhich tells us that there are two 2-paths between vertices 2 and 3. (Check to see that \nthe remaining entries of A \n2 \ncorrectly give 2-paths in the graph.) The argument we \nhave just given can be generalized to yield the following result, whose proof we leave \nas Exercise \n7\n2. \nExample 3.68 \nIf A is the adjacency matrix of a graph G, then the \n(\ni, j\n) \nentry of A\nk \nis equal to the \nnumber of k-paths between vertices i and j. \nHow many 3-paths are there between v\n1 \nand v\n2 \nin Figure 3.25\n? \nSolution We need the (1, 2) entry of A\n3\n, which is the dot product of row 1 of A \n2 \nand \ncolumn 2 of A. The calculation gives \n(A\n3\n)\n1\n2 \n= \n3 · 1  + 2 · 1  + 1·1 \n+ 0 · 0 = \n6 \nso there are six 3-paths between vertices 1 and 2, which can be easily checked. \nIn many applications that can be modeled by a graph, the vertices are ordered \nby some type of relation that imposes a direction on the edges. For example,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":35595,"to":35664}}}}],[704,{"pageContent":"In many applications that can be modeled by a graph, the vertices are ordered \nby some type of relation that imposes a direction on the edges. For example, \ndirected edges might be used to represent one-way routes in a graph that models \na transportation network or predator-prey relationships in a graph modeling an \necosystem. A graph with directed edges is called a digraph. Figure 3.26 shows an \nexample. \nAn easy modification to the definition of adjacency matrices allows us to use \nthem with digraphs.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":35664,"to":35671}}}}],[705,{"pageContent":"244 \nChapter 3  Matrices \nExample 3.69 \nD \ns \nF \nR N \nFigure 3.21 \nA tournament \nDefinition \nIf G is a digraph with n vertices, then its adjacency matrix is the \nn X n matrix A [or A(G)] defined by \nif there is an edge from vertex i to vertex j \notherwise \nThus, the adjacency matrix for the digraph in Figure 3.26 is \nNot surprisingly, the adjacency matrix of a digraph is not symmetric in general. \n(When would it be?) You should have no difficulty seeing that A \nk \nnow contains the \nnumbers of directed k-paths between vertices, where we insist that all edges along a \npath flow in the same direction. (See Exercise 72.) The next example gives an applica­\ntion of this idea. \nFive tennis players (Djokovic, Federer, Nadal, Roddick, and Safin) compete in a \nround-robin tournament in which each player plays every other player once. The \ndigraph in Figure 3.27 summarizes the results. A directed edge from vertex i to ver­\ntex j means that player i defeated player j. (A digraph in which there is exactly one","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":35673,"to":35698}}}}],[706,{"pageContent":"digraph in Figure 3.27 summarizes the results. A directed edge from vertex i to ver­\ntex j means that player i defeated player j. (A digraph in which there is exactly one \ndirected edge between every pair of vertices is called a tournament.) \nThe adjacency matrix for the digraph in Figure 3.27 is \n0  1 \n0 \n0 \n0 \nA= \n1 \n0 \n0  1 \n0 \n0  0  0 \n0 \n1 \n0  0  1 \n0  0 \nwhere the order of the vertices (and hence the rows and columns of A) is determined \nalphabetically. Thus, Federer corresponds to row 2 and column 2, for example. \nSuppose we wish to rank the five players, based on the results of their matches. One \nway to do this might be to count the number of wins for each player. Observe that the \nnumber of wins each player had is just the sum of the entries in the corresponding row; \nequivalently, the vector containing all the row sums is given by the product Aj, where \n1 \n1 \nj =   1 \n1 \n1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":35698,"to":35726}}}}],[707,{"pageContent":"In our case, we have \n0 \n1 \n0 \n0 0 \n1 \nAj = \n1 \n0 0 \n0 0 0 \n0 0 \n1 \nwhich produces the following ranking: \n1 \n1 \n0 \n0 \n1 \n0 0 \nSection 3.7 Applications \n245 \n3 \n3 \n2 \n1 \n1 \nFirst: Djokovic, Federer (tie) \nSecond: Nadal \nThird: Roddick, Safin (tie) \nAre the players who tied in this ranking equally strong? Djokovic might argue that \nsince he defeated Federer, he deserves first place. Roddick would use the same type \nof argument to break the tie with Safin. However, Safin could argue that he has two \n\"indirect\" victories because he beat Nadal, who defeated two others; furthermore, \nhe might note that Roddick has only one indirect victory (over Safin, who then \ndefeated Nadal). \nSince in a group of ties there may not be a player who defeated all the others in the \ngroup, the notion of indirect wins seems more useful. Moreover, an indirect victory \ncorresponds to a 2-path in the digraph, so we can use the square of the adjacency ma­\ntrix. To compute both wins and indirect wins for each player, we need the row sums","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":35728,"to":35766}}}}],[708,{"pageContent":"corresponds to a 2-path in the digraph, so we can use the square of the adjacency ma­\ntrix. To compute both wins and indirect wins for each player, we need the row sums \nof the matrix A + A \n2\n, which are given by \n0 \n1 \n0 0 0 \n2     2 \n0 0 \n1 1 1 \n0 \n1  1  1 \n(\nA + A\n2\n)j = \n1 \n0 0 \n1 \n0 \n+ \n0 \n1 \n0 \n1 \n2 \n0 0 0 \n0 \n0 \n0 \n0 0 \n0 0 \n0 0 \n1 \n0 0 0 \n0 \n1 \n2 2 \n3   1 \n8 \n0 \n2 2  2 \n1 \n7 \n1  1 \n0 \n2  2 \n6 \n0 \n0  1  0 \n1 1 \n2 \n0 \n1 \n0 \n1 \n3 \nThus, we would rank the players as follows: Djokovic, Federer, Nadal, Safin, Roddick. \nUnfortunately, this approach is not guaranteed to break all ties. \nI \nExercises 3.1 \nMarkov Chains \n[\n0.\n5  0.3\n] \nIn Exercises 1-4, let P = \nbe th e transition ma-\n0.\n5 \n0.7 \n[\n0.\n5\n] \ntrix for a Markov chain with two states. Let x\n0 \n= \nbe \n0.\n5 \nth e initial state vector for the population. \n1. Compute x\n1 \nand x\n2\n• \n2. What proportion of the state 1 population will be in \nstate 2 after two steps? \n3. What proportion of the state 2 population will be in \nstate 2 after two steps? \n4. Find the steady state vector.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":35766,"to":35858}}}}],[709,{"pageContent":"246 \nChapter 3  Matrices \nIn Emdm 5-8,  let P \n� [ ! ! I ] be  the tmn;;t;on ma-\ni,;x forn Ma,kovcha;n wUh th:\"�al\" Letx,, � [ :m be \nth e initial state vector for th e population. \n5. Compute x1 and x\n2\n. \n6. What proportion of the state 1 population will be in \nstate 1 after two steps? \n7. What proportion of the state 2 population will be in \nstate 3 after two steps? \n8. Find the steady state vector. \n9. Suppose that the weather in a particular region \nbehaves according to a Markov chain. Specifically, \nsuppose that the probability that tomorrow will be \na wet day is 0.662 if today is wet and 0.2\n5\n0 if today \nis dry. The probability that tomorrow will be a dry \nday is 0.\n750 if today is dry and 0.338 if today is wet. \n[This exercise is based on an actual study of rainfall \nin Tel Aviv over a 27-year period. See K. R. Gabriel \nand J. Neumann, ''A Markov Chain Model for Daily \nRainfall Occurrence at Tel Aviv;' Quarterly Journal of \nthe Royal Meteorological Society, 88 (1962), \npp. 90-\n95.]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":35860,"to":35889}}}}],[710,{"pageContent":"and J. Neumann, ''A Markov Chain Model for Daily \nRainfall Occurrence at Tel Aviv;' Quarterly Journal of \nthe Royal Meteorological Society, 88 (1962), \npp. 90-\n95.] \n(a) Write down the transition matrix for this Markov \nchain. \n(b) If Monday is a dry day, what is the probability that \nWednesday will be wet? \n( c) In the long run, what will the distribution of wet \nand dry days be? \n10. Data have been accumulated on the heights of children \nrelative to their parents. Suppose that the probabilities \nthat a tall parent will have a tall, medium-height, or \nshort child are 0.6, 0.2, and 0.2, respectively; the prob­\nabilities that a medium-height parent will have a tall, \nmed\nium-height, or short child are 0.1, 0.7, and 0.2, re­\nspectively; and the probabilities that a short parent will \nhave a tall, medium-height, or short child are 0.2, 0.4, \nand 0.4, respectively. \n(a) Write down the transition matrix for this Markov \nchain. \n(b) What is the probability that a short person will \nhave a tall grandchild?","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":35889,"to":35913}}}}],[711,{"pageContent":"and 0.4, respectively. \n(a) Write down the transition matrix for this Markov \nchain. \n(b) What is the probability that a short person will \nhave a tall grandchild? \n( c) If 20% of the current population is tall, 50% is of \nmedium height, and 30% is short, what will the \ndistribution be in three generations? \n( d) What proportion of the population will be tall, of \nmedium height, and short in the long run? \n11. A study of pifwn (pine) nut crops in the American \nsou\nthwest from 1940 to 1947 hypothesized that \nnut production followed a Markov chain. [See \nD. H. Thomas, ''A Computer Simulation Model of \nGreat Basin Shoshonean Subsistence and Settlement \nPatterns;' in D. L. Clarke, ed., Models in Archaeology \n(London: Methuen, 1972).] The data suggested that \nif one year's crop was good, then the probabilities that \nthe following year's crop would be good, fair, or poor \nwere 0.08, 0.0\n7\n, and 0.85, respectively; if one year's \ncrop was fair, then the probabilities that the follow-","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":35913,"to":35936}}}}],[712,{"pageContent":"the following year's crop would be good, fair, or poor \nwere 0.08, 0.0\n7\n, and 0.85, respectively; if one year's \ncrop was fair, then the probabilities that the follow-\ning year's crop would be good, fair, or poor were 0.09, \n0.11, and 0.80, respectively; if one year's crop was poor, \nthen the probabilities that the following year's crop \nwould be good, fair, or poor were 0.11, 0.0\n5\n, and 0.84, \nrespectively. \n(a) Write down the transition matrix for this Markov \nchain. \n(b) If the pifion nut crop was good in 1940, find the \nprobabilities of a good crop in the years 1941 \nthrough \n1945. \n(c) In the long run, what proportion of the crops will \nbe good, fair, and poor? \n12. Robots have been programmed to traverse the maze \nshown in Figure 3.28 and at each junction randomly \nchoose which way to go. \nFigure 3.28 \n(a) Construct the transition matrix for the Markov \nchain that models this situation. \n(b) Suppose we start with 15 robots at each junc-\ntion. Find the steady state distribution of robots.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":35936,"to":35963}}}}],[713,{"pageContent":"Figure 3.28 \n(a) Construct the transition matrix for the Markov \nchain that models this situation. \n(b) Suppose we start with 15 robots at each junc-\ntion. Find the steady state distribution of robots. \n(Assume that it takes each robot the same amount \nof time to travel between two adjacent junctions.) \n13. Let j denote a row vector consisting entirely of \nl\ns. Prove \nthat a nonnegative matrix Pis a stochastic matrix if \nand only if jP = j.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":35963,"to":35974}}}}],[714,{"pageContent":"14. (a) Show that the product of two 2 X 2 stochastic \nmatrices is also a stochastic matrix. \n(b) Prove that the product of two n X n stochastic \nmatrices is also a stochastic matrix. \n(c\n) \nIf a 2 X 2 stochastic matrix P is invertible, prove that \nP\n-\n1 \nis also a stochastic matrix. \nSuppose we want to know the average (or expected) number \nof steps it will ta ke to go from state i to state j in a Markov \nchain. It can be shown that the following computation \nanswers this question: Delete the jth row and the jth column \nof the transition matrix P to get a new matrix Q. (Keep \nthe rows and columns of Q labeled as they were in P.) The \nexpected number of steps from state i to state j is given by \nthe sum of the entries in the column of (I \n-\nQ)\n-\n1 \nlabeled i. \n15. In Exercise 9, if Monday is a dry day, what is the \nexpected number of days until a wet day? \n16. In Exercise 10, what is the expected number of genera­\ntions until a short person has a tall descendant?","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":35976,"to":36003}}}}],[715,{"pageContent":"expected number of days until a wet day? \n16. In Exercise 10, what is the expected number of genera­\ntions until a short person has a tall descendant? \n17. In Exercise 11, if the pifion nut crop is fair one year, what \nis the expected number of years until a good crop occurs? \n18. In Exercise 12, starting from each of the other junc­\ntions, what is the expected number of moves until a \nrobot reaches junction 4? \nLinear Economic Models \nIn Exercises 19-26, determine which of th e matrices are \nexchange matrices. For those that are exchange matrices, \nfi nd a nonnegative price vector that satisfi es Equation (1). \n[\n1 /2  1 /\n4\n] \n[\n1 /3 2/3\n] \n19. \nI \n20. \nI \n1 2 3/4 \n1\n/\n2  1 2 \n[ \n0.4 \n0.7\n] \n[ \n0.1 \n0.6\n] \n21. 22. \n0.6 \n0.4 \n0.9 \n0.4 \n[ 1\n/3 \n0 \n�\n] \n23. 1/3 \n3/2 \n1/3 \n-\n1/2 \n[\n1/2 \n24. \n0 \n1/2 \n1 \n1\n�\n3 \nl \n0 \n0 \n2/3 \n[ 03 \n0 02\n] \n[\n050 \n0.70 \n035\n] \n25. 0.3 \n0.\n5 \n0.3 \n26. 0.2\n5 \n0.30 \n0.25 \n0.4 \n0.5 \n0.\n5 \n0.25 \n0 \n0.40 \nIn Exercises 27-30, determine whether th e given consump-\ntion matrix is productive. \n[\n0\n2\n0 \n0.10 \n010\n] \n[\n0.2 \n0.3\n] \n27. \n28. 0.30 \n0.15 \n0.45 \n0.5 \n0.6","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":36003,"to":36107}}}}],[716,{"pageContent":"25. 0.3 \n0.\n5 \n0.3 \n26. 0.2\n5 \n0.30 \n0.25 \n0.4 \n0.5 \n0.\n5 \n0.25 \n0 \n0.40 \nIn Exercises 27-30, determine whether th e given consump-\ntion matrix is productive. \n[\n0\n2\n0 \n0.10 \n010\n] \n[\n0.2 \n0.3\n] \n27. \n28. 0.30 \n0.15 \n0.45 \n0.5 \n0.6 \n0.15 \n0.30 \n0.50 \nSection 3.7 Applications \n241 \n[\n02 \n0.4 \n0.1 \n04] \n[ 0 35 \n0.2\n5 \n�\n35: \n0.3 \n0.2 0.2 \n0.1 \n29. 0.1 5 \n0.55 \n30. \n0 \n0.4 \n0.5 \n0.3 \n0.45 \n0.30 \n0.60 \n0.5 \n0 \n0.2 \n0.2 \nIn Exercises 31-34, a consumption matrix C and a demand \nvector dare given. In each case, fi nd a feasible production \nvector x that satisfi es Equation (2). \n[\n1\n/\n2 \n31. c  = \n1 /2 \n[\n0.1 \n32. c  = \n0.3 \n33. c  = \n[\n�\n5 \n[\n01 \nCAS \n34. C = \n0 \n0.3 \n1\n/\n4\n] \n[\nl\n] \n1 /2 \n,d = \n3 \n0.4\n] d  = \n[\n2\n] \n0.2 \n, \n1 \n0.2 \n0.4 \n0 \n0.4 \n0.2 \n0.2 \n01\n] \n[\n'\n] \n0.2 , d  = 2 \n0.5 \n4 \n01\n] \n[\nu\n] \n0.2 , d  = 3.\n5 \n0.3 \n2.0 \n35. Let A be an n X n matrix, A 2 0. Suppose that \nAx < x for some x in ll�r, x 2 0. Prove that x > 0. \n36. Let A, B, C, and D be n X n matrices and x and y \nvectors in !R\nn\n. Prove the following inequalities: \n(a) If A 2 B 2 0 and C 2 D 2 0, then \nAC \n2 BD 2 0.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":36107,"to":36245}}}}],[717,{"pageContent":"36. Let A, B, C, and D be n X n matrices and x and y \nvectors in !R\nn\n. Prove the following inequalities: \n(a) If A 2 B 2 0 and C 2 D 2 0, then \nAC \n2 BD 2 0. \n(b) If A >  B and x 2 0, x * 0, then Ax > Bx. \nPopulation Growth \n37. A population with two age classes has a Leslie matrix \n[ \n2 \n�] . If the initial population vector is \nL\n-\n0.6 \nx\n0 \n= \n[ \n1 \n�\n], compute x\n1\n, x\n2\n, and x3. \n38. A population with three age classes has a Leslie matrix \nL\n�\nH \n1 \n� \nl \n· If the m;t;ru populahon \n0 \n0.\n5 \nvector is x\n0 \n= \n[ \n1\n:\n]\n. \ncompute x,\n. \nx,, ond x,.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":36245,"to":36293}}}}],[718,{"pageContent":"248 \nChapter 3  Matrices \n39. A population with three age classes has a Leslie matrix \n[ \n1   1   3 \nl \nL = 0. 7 \n0 \n0 .  If the initial population vector is \n0 \n0.\n5 \n0 \n[100\n] \nx\n0 \n= 100 ,  compute x1, x\n2\n, and x\n3\n. \n100 \n40. A population with four age classes has a Leslie matrix \nL \n-\n_ \n[\n�\n�\n.\n5 \n� \n�  �i \n0.\n7 \n0   0 \n0 \n0.3 0 \n. If the initial population \nvoctodn\no \n� \nrn l \nr\nn\nmpute \nx,. x,. \nM\nd x, \n41. A certain species with two age classes of 1 year's dura­\ntion has a survival probability of 80% from class 1 to \nclass 2. Empirical evidence shows that, on average, \neach female gives birth to five females per year. Thus, \ntwo possible Leslie matrices are \nL \n= \n[\n0 \n5] and L = \n[\n4   1\n] \nI \n0.8 0 \n2 \n0.8 0 \n(a) Starting with x\n0 \n= \neach case. \n[ \n� \n� \nl \ncompute x1, .•. , x1\n0 \nin \n(b) For each case, plot the relative size of each age \nclass \nover time (as in Figure 3.23). What do your \ngraphs suggest? \n42. Suppose the Leslie matrix for the VW beetle is L \n= \n[ �.! L \n2\n� l · sta,ting with AA a\nc\nb;tmy \nX\no, dete\nc\n­\nmine the behavior of this population.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":36295,"to":36388}}}}],[719,{"pageContent":"graphs suggest? \n42. Suppose the Leslie matrix for the VW beetle is L \n= \n[ �.! L \n2\n� l · sta,ting with AA a\nc\nb;tmy \nX\no, dete\nc\n­\nmine the behavior of this population. \n43. Suppose the Leslie matrix for the VW beetle is \nL = [ � \n� \n2\n� \nl · Investigate the effect of varying \n0  0.\n5 \n0 \nthe survival probability s of the young beetles. \nc\nA\ns 44. Wo odland caribou are found primarily in the western \nprovinces of Canada and the American northwest. \nThe average lifespan of a female is about 14 years. \nThe birth and survival rates for each age bracket are \ngiven in Table 3.4, which shows that caribou cows do \nnot give birth at all during their first 2 years and give \nbirth to about one calf per year during their middle \nyears. The mortality rate for young calves is very high. \nTable 3.4 \nA\nge \nBirth \nSurvival \n(years) \nRate Rate \n0-2 \n0.0 \n0.3 \n2-4 \n0.4 \n0.7 \n4-6 1.8 \n0.9 \n6-8 \n1.8 \n0.\n9 \n8-10 \n1.8 \n0.\n9 \n10-12 \n1.6 0.6 \n12-14 0.6 \n0.0 \nThe numbers of woodland caribou reported in \nJasper National Park in Alberta in 1990 are shown in","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":36388,"to":36449}}}}],[720,{"pageContent":"Rate Rate \n0-2 \n0.0 \n0.3 \n2-4 \n0.4 \n0.7 \n4-6 1.8 \n0.9 \n6-8 \n1.8 \n0.\n9 \n8-10 \n1.8 \n0.\n9 \n10-12 \n1.6 0.6 \n12-14 0.6 \n0.0 \nThe numbers of woodland caribou reported in \nJasper National Park in Alberta in 1990 are shown in \nTable 3.5. Using a CAS, predict the caribou population \nfor 1992 and 1994. Then project the population for the \nyears 2010 and 2020. What do you conclude? (What \nassumptions does this model make, and how could it \nbe improved?) \nTable 3.5 \nWoodland Caribou \nPopulation in Jasper \nNational Park. 1990 \nA\nge \n(years) Number \n0-2 \n10 \n2-4 \n2 \n4-6 \n8 \n6-8 \n5 \n8-10 \n12 \n10-12 0 \n12-14 \nSource: World Wildlife Fund Canada","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":36449,"to":36496}}}}],[721,{"pageContent":"Graphs and Digraphs \nIn Exercises 45-48, determine the adjacency matrix of th e \ngiven graph. \n45 \nV\nI \nV\n2 \n.--------. \nV4 \nV3 \n46. \nV\n1 \nV\n2 \n47. \nV4 \nV5 \nIn Exercises 49-52, draw a graph that has th e given adja-\ncency matrix. \n49. \n[\n� \n1  1 \n�] \n50. \n[\n; \n0 \nl\nl \n0 0 \n0 0 0 \n0 0 \n0 0 \n1  1 \n0 0 0 0 \n1  1 \n0 0 0 \n1  1 \n0 0 0 \n51. \n1 \n0 0 0 \n1 \n52. \n0 0 0 \n1  1 \n1 \n0 0 0 \n1 \n0 0 \n0 0 0 \n1 \n0 0 \nSection 3.7 Applications \n249 \nIn Exercises 53-56, determine th e adjacency matrix of th e \ngiven digraph. \nV4 \nV3 \n54. \nv, \nV4 \nV\n2 \n55. \n56. VJ \nV\n2 \nV4 \nV3 \nIn Exercises 57-60, draw a digraph that has th e given adja-\ncency matrix. \n57. \n[\n� \n1 \n0 \n;\n] \n58. \n[\n: \n1 \n0 \n�\n: \n0 0 0 0 \n1 \n0 0 0 \n0 0 \n1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":36498,"to":36591}}}}],[722,{"pageContent":"250 \nChapter 3  Matrices \n0  0 \n1 \n0 \n1 \n1 \n0  0 \n1 \n0 \n59. \n0  0  0 0 \n1 \n60. \n1 \n0 \n1 \n0  0 \n0 0 \n1 \n0 \n0 \n1 \n0  0 \n1 \n0  0  0 0 \n0  0 \n0 \n1 \n0  0 \n1 \n0  0  0 \nIn Exercises 61-68,  use powers of adjacency matrices to \ndetermine th e number of paths of th e specified length \nbetween the given vertices. \n61. Exercise 50, length 2, v\n1 \nand v\n2 \n62. Exercise \n5\n2, length 2, v\n1 \nand v\n2 \n63. Exercise 50, length 3, v\n1 \nand v\n3 \n64. Exercise 52, length 4, v\n2 \nand v\n2 \n65. Exercise 57, length 2, v\n1 \nto v\n3 \n66. Exercise 57, length 3, v\n4 \nto v\n1 \n67. Exercise 60, length 3, v\n4 \nto v\n1 \n68. Exercise 60, length 4, v\n1 \nto v\n4 \n69. Let A be the adjacency matrix of a graph G. \n(a) If row i of A is all zeros, what does this imply \nabout G? \n(b) If column j of A is all zeros, what does this imply \nabout G? \n70. Let A be the adjacency matrix of a digraph D. \n(a) If row i of A \n2 \nis all zeros, what does this imply \nabout D? \n(h) If column j of A\n2 \nis all zeros, what does this imply \nabout D? \n71. Figure 3.29 is the digraph of a tournament with six \nplayers, P\n1 \nto P\n6","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":36593,"to":36680}}}}],[723,{"pageContent":"2 \nis all zeros, what does this imply \nabout D? \n(h) If column j of A\n2 \nis all zeros, what does this imply \nabout D? \n71. Figure 3.29 is the digraph of a tournament with six \nplayers, P\n1 \nto P\n6\n. Using adjacency matrices, rank the \nplayers first by determining wins only and then by \nusing the notion of combined wins and indirect wins, \nas in Example 3.69. \np\n6 \nFigure 3.29 \n72. Figure 3.30 is a digraph representing a food web in \na small ecosystem. A directed edge from a to b indi­\ncates that a has b as a source of food. Construct the \nadjacency matrix A for this digraph and use it to an­\nswer the following questions. \nRodent \nFish \nBird \nFigure 3.30 \n(a) Which species has the most direct sources of food? \nHow does A show this? \n(b) Which species is a direct source of food for the \nmost other species? How does A show this? \n(c) If a eats band b eats c, we say that a has c as an \nindirect source of food. How can we use A to de­\ntermine which species has the most indirect food","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":36680,"to":36714}}}}],[724,{"pageContent":"most other species? How does A show this? \n(c) If a eats band b eats c, we say that a has c as an \nindirect source of food. How can we use A to de­\ntermine which species has the most indirect food \nsources? Which species has the most direct and \nindirect food sources combined? \n(d) Suppose that pollutants kill the plants in this food \nweb, and we want to determine the effect this \nchange will have on the ecosystem. Construct a \nnew adjacency matrix A\n* \nfrom A by deleting the \nrow and column corresponding to plants. Repeat \nparts (a) to (c) and determine which species are \nthe most and least affected by the change. \n(e) What will the long-term effect of the pollution be? \nWhat matrix calculations will show this? \n73. Five people are all connected by e-mail. Whenever \none of them hears a juicy piece of gossip, he or she \npasses it along by e-mailing it to someone else in the \ngroup according to Table 3.6. \n(a) Draw the digraph that models this \"gossip \nnetwork\" and find its adjacency matrix A. \nTable 3.6","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":36714,"to":36737}}}}],[725,{"pageContent":"passes it along by e-mailing it to someone else in the \ngroup according to Table 3.6. \n(a) Draw the digraph that models this \"gossip \nnetwork\" and find its adjacency matrix A. \nTable 3.6 \nSender \nRecipients \nAnn \nCarla, Ehaz \nBert \nCarla, Dana \nCarla \nEhaz \nDana \nAnn, Carla \nEhaz \nBert","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":36737,"to":36753}}}}],[726,{"pageContent":"(b) Define a step as the time it takes a person to e-mail \neveryone on his or her list. (Thus, in one step, \ngossip gets from Ann to both Carla and Ehaz.) If \nBert hears a rumor, how many steps will it take \nfor everyone else to hear the rumor? What matrix \ncalculation reveals this? \n(c) If Ann hears a rumor, how many steps will it take \nfor everyone else to hear the rumor? What matrix \ncalculation reveals this? \n(d) In general, if A is the adjacency matrix of a \ndigraph, how can we tell if vertex i is connected to \nvertex j by a path (of some length)? \n[The gossip network in this exercise is reminiscent \nof the notion of \"six degrees of separation\" (found in the \nplay and film by that name), which suggests that any \ntwo people are connected by a path of acquaintances \nwhose length is at most 6. The game \"Six Degrees of \nKevin Bacon\" more frivolously asserts that all actors are \nconnected to the actor Kevin Bacon in such a way.] \n74. Let A be the adjacency matrix of a graph G.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":36755,"to":36774}}}}],[727,{"pageContent":"Kevin Bacon\" more frivolously asserts that all actors are \nconnected to the actor Kevin Bacon in such a way.] \n74. Let A be the adjacency matrix of a graph G. \n(a) By induction, prove that for all n 2:: 1, the \n(\ni, j\n) \nentry of A\nn \nis equal to the number of n-paths \nbetween vertices i and j. \n(b) How do the statement and proof in part (a) have \nto be modified if G is a digraph? \n75. If A is the adjacency matrix of a digraph G, what does \nthe \n(\ni, j\n) \nentry of AA \nT \nrepresent if i * j? \nChapter Review \nKev Definitions and concepts \nChapter Review \n251 \nA graph is called bipartite if its vertices can be subdi­\nvided into two sets U and V such that every edge has one \nendpoint in U and the other endpoint in V For example, \nthe graph in Exercise 48 is bipartite with U =  { v\n1\n, v\n2\n, v\n3\n} \nand V = {v\n4\n, v\n5\n}. In Exercises 76-79, determine whether a \ngraph with the given adjacency matrix is bipartite. \n76. The adjacency matrix in Exercise 49 \n77. The adjacency matrix in Exercise \n5\n2 \n78. The adjacency matrix in Exercise","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":36774,"to":36819}}}}],[728,{"pageContent":"graph with the given adjacency matrix is bipartite. \n76. The adjacency matrix in Exercise 49 \n77. The adjacency matrix in Exercise \n5\n2 \n78. The adjacency matrix in Exercise \n5 1 \n0 0 0 \n1 \n0 0 0 \n1 \n0 \n1 \n0 0 \n79. \n0 0 \n1 \n0 \n1  1 \n1 \n0 \n1 \n0 0 \n1 \n0 \n1 \n0 0 \n80. (a) Prove that a graph is bipartite if and only if its \nvertices can be labeled so that its adjacency matrix \ncan be partitioned as \nA= \n[�--H�-] \n(b) Using the result in part (a), prove that a bipartite \ngraph has no circuits of odd length. \nbasis, 198 \nBasis Theorem, 202 \nFundamental Theorem of Invertible \nMatrices, 172, 206 \nmatrix, 138 \nmatrix addition, 140 \nmatrix factorization, 180 \nmatrix multiplication, 141 \nmatrix powers, 149 \nnegative of a matrix, 140 \nnull space of a matrix, 197 \nnullity of a matrix, 204 \nouter \nproduct, 147 \npartitioned matrices (block \ncolumn matrix (vector),  138 \ncolumn space of a \nmatrix, 195 \ncomposition oflinear \ntransformations, 219 \ncoordinate vector with respect to a \nbasis, 208 \ndiagonal \nmatrix, 139 \ndimension, 203 \nelementary matrix, 170","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":36819,"to":36878}}}}],[729,{"pageContent":"column space of a \nmatrix, 195 \ncomposition oflinear \ntransformations, 219 \ncoordinate vector with respect to a \nbasis, 208 \ndiagonal \nmatrix, 139 \ndimension, 203 \nelementary matrix, 170 \nidentity matrix, 139 \ninverse of a square \nmatrix, 163 \ninverse of a linear \ntransformation, 221 \nlinear combination of matrices, 154 \nlinear dependence/independence \nof matrices, 157 \nlinear transformation, 213 \nLU factorization, 181 \nmult\niplication), 145, 148 \npermutation matrix, 187","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":36878,"to":36900}}}}],[730,{"pageContent":"252 \nChapter 3  Matrices \nproperties of matrix algebra, \n1 5 4, \n1 58, 1  59, 167 \nrank of a matrix, 204 \nRank Theorem, 20\n5 \nrepresentations of matrix \nrow matrix (vector),  138 \nrow \nspace of a matrix, 195 \nsca\nlar matrix, 139 \nstandard matrix of a linear \ntransformation, 216 \nsubspace, 192 \nproducts, 146-148 \nscalar multiple of a matrix, 140 \nspan \nof a set of matrices, 1 56 \nsq\nuare matrix, 139 \nsy\nmmetric matrix, 1 5 1 \ntransp\nose of a matrix, 1 5 1 \nzero matrix, 141 \nReview Questions \n1. Mark each of the following statements true or false: \n(a) For any matrix A, both AA \nT \nand A\nT \nA are defined. \n(b) If A and B are matrices such that AB = 0 and \nA * 0, then B = 0. \n(c) If A, B, and X are invertible matrices such that \nXA = B, then X = A\n-\n1\nB. \n(d) The inverse of an elementary matrix is an elemen­\ntary matrix. \n(e) The transpose of an elementary matrix is an \nelementary matrix. \n(f) The product of two elementary matrices is an \nelementary matrix. \n(g) If A is an m X n matrix, then the null space of A is \na subspace of ll�r.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":36902,"to":36951}}}}],[731,{"pageContent":"elementary matrix. \n(f) The product of two elementary matrices is an \nelementary matrix. \n(g) If A is an m X n matrix, then the null space of A is \na subspace of ll�r. \n(h) Every plane in IR\n3 \nis a two-dimensional subspace \nof IR\n3\n. \n(i) The transformation T: IR\n2 \n---+ IR\n2 \ndefined by \nT(x) = \n-\nxis a linear transformation. \n(j) If T: IR\n4\n---+ IR\n5 \nis a linear transformation, then \nthere is a 4 X \n5 \nmatrix A such that T(x) = Ax for \nall x in the domain of T. \nInExercises2-7,  letA  = \n[\n� \n�\n] \nandB  = \n[\n� \n-� \n-\n�\n]\n. \nCompute th e indicated matrices, if possible. \n2. A\n2\nB \n3. A\n2\nB\n2 \n4. B\nT\nA\n-\n1\nB \n5. (BB\nT\n)\n-\n1 \n6. (B\nT\nB)\n-\n1 \n7. The outer product expansion of AA\nT \n8. If A is a matrix such that \nA\n-\n1 \n= \n[ \nl\n/\n/\n2 \n-\nl\n]\n, find A. \n-\n3 \n2 \n4 \n9. If A � [ � : \n= \n: ] ond X is a matdx such thot \nAX= [\n-\n� \n-\n�], findX. \n3 \n-\n2 \n10. If possible, express the matrix A  = \n[ \n1 \n! \n] as a prod-\nuct of elementary matrices. \n4 \n11. If A is a square matrix such that A \n3 \n= 0, show that \n(\nI -  A\n)\n-\n1 \n=I\n+ \nA \n+ \nA\n2\n• \n12. Find an LU faotmization of A � [: \n-\nl \n:J","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":36951,"to":37072}}}}],[732,{"pageContent":"[ \n1 \n! \n] as a prod-\nuct of elementary matrices. \n4 \n11. If A is a square matrix such that A \n3 \n= 0, show that \n(\nI -  A\n)\n-\n1 \n=I\n+ \nA \n+ \nA\n2\n• \n12. Find an LU faotmization of A � [: \n-\nl \n:J \n13. Find bases for the row space, column space, and null \nsp�<ofA � [� =� � � n \n14. Suppose matrices A and Bare row equivalent. Do they \nhave the same row space? Why or why not? Do A and \nB have the same column space? Why or why not? \n15. If A is an invertible matrix, explain why A and A\nT \nmust \nhave the same null space. Is this true if A is a nonin­\nvertible square matrix? Explain. \n16. If A is a square matrix whose rows add up to the zero \nvector, explain why A cannot be invertible. \n17. Let A be an m X n matrix with linearly independent \ncolumns. Explain why A\nT \nA must be an invertible \nmatrix. Must AA \nT \nalso be invertible? Explain. \n18. Find a linear transformation T: IR\n2 \n---+ IR\n2 \nsuch that \nr\n[\n�\n] \n[\n�\n] \nand r\n[ \n_�\nJ \n[\n�\n]\n. \n19. Find the standard matrix of the linear transformation \nT: IR\n2 \n---+ IR\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":37072,"to":37140}}}}],[733,{"pageContent":"T \nalso be invertible? Explain. \n18. Find a linear transformation T: IR\n2 \n---+ IR\n2 \nsuch that \nr\n[\n�\n] \n[\n�\n] \nand r\n[ \n_�\nJ \n[\n�\n]\n. \n19. Find the standard matrix of the linear transformation \nT: IR\n2 \n---+ IR\n2 \nthat corresponds to a counterclockwise \nrotation of 45° about the origin followed by a projec­\ntion onto the line y =  -2x. \n20. Suppose that T : !R\nn \n---+ !R\nn \nis a linear transformation \nand suppose that \nv\nis a vector such that T \n(\nv\n) \n* 0 but \nT\n2\n(\nv\n) \n= O (where T\n2 \n= T 0 T). Prove that \nv \nand T(\nv\n) \nare linearly independent.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":37140,"to":37194}}}}],[734,{"pageContent":"Almost every combination of \nthe adjectives proper, latent, \ncharacteristic, eigen and secular, \nwith the nouns root, number \nand value, has been used in \nthe literature fo r what we call a \nproper value. \n-Paul R. Halmos \nFinite Dimensional Ve ctor Sp aces \n(2nd edition) \nVan Nostrand, 1958, p. 102 \nVJ V\n2 \n------\nFigure 4.1 \nK4 \nEigenv\nalues \nand \nEigenvectors \n4.0 \nIntroduction:  A   ovnamical svstem on Graphs \nCAS \nWe saw in the last chapter that iterating matrix multiplication often produces inter­\nesting results. Both Markov chains and the Leslie model of population growth exhibit \nsteady states in certain situations. One of the goals of this chapter is to help you \nunderstand such behavior. First we will look at another iterative process, or dynami­\ncal system, that uses matrices. (In the problems that follow, you will find it helpful \nto use a CAS or a calculator with matrix capabilities to facilitate the computations.) \nOur example involves graphs (see Section 3.7). A complete graph is any graph in","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":37196,"to":37225}}}}],[735,{"pageContent":"to use a CAS or a calculator with matrix capabilities to facilitate the computations.) \nOur example involves graphs (see Section 3.7). A complete graph is any graph in \nwhich every vertex is adjacent to every other vertex. If a complete graph has n verti­\nces, it is denoted by Kw For example, Figure 4.1 shows a representation of K4• \nProblem 1 Pick any vector x in IR\n4 \nwith nonnegative entries and label the vertices \nof K4 with the components ofx, so that v\n1 \nis labeled with x1 , and so on. Compute the \nadjacency matrix A of K4 and relabel the vertices of the graph with the corresponding \ncomponents of Ax. Try this for several vectors x and explain, in terms of the graph, \nhow the new labels can be determined from the old labels. \nProblem 2 Now iterate the process in Problem 1. That is, for a given choice of x, \nrelabel the vertices as described above and then apply A again (and again, and again) \nuntil a pattern emerges. Since components of the vectors themselves will get quite","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":37225,"to":37240}}}}],[736,{"pageContent":"relabel the vertices as described above and then apply A again (and again, and again) \nuntil a pattern emerges. Since components of the vectors themselves will get quite \nlarge, we will scale them by dividing each vector by its largest component after each \niteration. Thus, if a computation results in the vector \nwe will replace it by \n.!.\n[\n�\n] \n[\n�\n·\n5 \n] \n4 \n1 \n0.25 \n1 \n0.25 \n253","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":37240,"to":37259}}}}],[737,{"pageContent":"254 \nChapter 4 \nEigenvalues and Eigenvectors \nNote that this process guarantees that the largest component of each vector will now \nbe 1. Do this for K\n4\n, then K\n3 \nand K\ns\n. Use at least ten iterations and two-decimal-place \naccuracy. What appears to be happening? \nProblem 3 You should have noticed that, in each case, the labeling vector is \napproaching a certain vector (a steady state label!). Label the vertices of the complete \ngraphs with this steady state vector and apply the adjacency matrix A one more time \n(without scaling). What is the relationship between the new labels and the old ones? \nProblem 4 Make a conjecture about the general case Kw What is the steady state \nlabel? What happens if we label K\nn \nwith the steady state vector and apply the adja-\nFigure 4.2 \ncency matrix A without scaling? \nFigure 4.3 \nFigure 4.4 \nThe German adjective eigen means \n\"own'' or \"characteristic of' Eigen­\nvalues and eigenvectors are charac­\nteristic of a matrix in the sense that \nthey contain important informa­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":37261,"to":37289}}}}],[738,{"pageContent":"Figure 4.3 \nFigure 4.4 \nThe German adjective eigen means \n\"own'' or \"characteristic of' Eigen­\nvalues and eigenvectors are charac­\nteristic of a matrix in the sense that \nthey contain important informa­\ntion about the nature of the \nmatrix. The letter,.\\ (lambda), the \nGreek equivalent of the English \nletter L, is used for eigenvalues \nbecause at one time they were also \nknown as latent values. The prefix \neigen is pronounced \"EYE-gun:' \nProblem 5 The Petersen graph is shown in Figure 4.2. Repeat the process in \nProblems 1 through 3 with this graph. \nWe will now explore the process with some other classes of graphs to see if they \nbehave the same way. The cycle C\nn \nis the graph with n vertices arranged in a cyclic \nfashion. For example, C\ns \nis the graph shown in Figure 4.3. \nProblem 6 Repeat the process of Problems 1 through 3 with cycles C\nn \nfor various \nodd values of n and make a conjecture about the general case. \nProblem 1 Repeat Problem 6 with even values of n. What happens?","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":37289,"to":37316}}}}],[739,{"pageContent":"n \nfor various \nodd values of n and make a conjecture about the general case. \nProblem 1 Repeat Problem 6 with even values of n. What happens? \nA bipartite graph is a complete bipartite graph (see Exercises 74-78 in Sec­\ntion 3.7) if its vertices can be partitioned into sets U and V such that every vertex in U \nis adjacent to every vertex in V, and vice versa. If U and Veach have n vertices, then \nthe graph is denoted by K\nn\n,\nn\n-\nFor example, K\n3\n,\n3 \nis the graph in Figure 4.4. \nProblem 8 Repeat the process of Problems 1 through 3 with complete bipartite \ngraphs K\nn\n,\nn \nfor various values of n. What happens? \nBy the end of this chapter, you will be in a position to explain the observations you \nhave made in this Introduction. \nIntroduction to  Eigenvalues and Eigenvectors \nIn Chapter 3, we encountered the notion of a steady state vector in the context of \ntwo applications: Markov chains and the Leslie model of population growth. For a","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":37316,"to":37343}}}}],[740,{"pageContent":"In Chapter 3, we encountered the notion of a steady state vector in the context of \ntwo applications: Markov chains and the Leslie model of population growth. For a \nMarkov chain with transition matrix P, a steady state vector x had the property that \nPx \n= \nx; for a Leslie matrix L, a steady state vector was a population vector x satisfying \nLx = rx, where r represented the steady state growth rate. For example, we saw that \n[\n0.7 \n0.3 \n0.2\n] \n[\n0.4\n] = \n[\n0.4\n] \n0.8  0.6    0.6 \nand [� 5 \n4 \n0 \n0.25 \nIn this chapter, we investigate this phenomenon more generally. That is, for a square ma­\ntrix A, we ask whether there exist nonzero vectors x such that Ax is just a scalar multiple \nof x. This is the eigenvalue problem, and it is one of the most central problems in linear \nalgebra. It has applications throughout mathematics and in many other fields as well. \nDefinition \nLet A be an n \nX n matrix. A \nscalar A is called an eigenvalue of A if","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":37343,"to":37373}}}}],[741,{"pageContent":"algebra. It has applications throughout mathematics and in many other fields as well. \nDefinition \nLet A be an n \nX n matrix. A \nscalar A is called an eigenvalue of A if \nthere is a nonzero vector x such that Ax = Ax. Such a vector xis called an eigenvec­\ntor of A corresponding to A.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":37373,"to":37379}}}}],[742,{"pageContent":"Example 4.1 \nExample 4.2 \nSection 4.1 \nIntroduction to Eigenvalues and Eigenvectors \n255 \nShow that x = \n[ \n�\n] is an eigenvector of A  = \n[ � \n�\n] and find the corresponding \neigenvalue. \nSolution We compute \nAx \n= \n[ \n� \n�\n] \n[ \n� \n] \n[\n:\n] \n=  4 \n[ \n� \n] \n= \n4x \nfrom which it follows that xis an eigenvector of A corresponding to the eigenvalue 4. \n4 \nShow that 5 is an eigenvalue of A  = \n[\n4\n1 \n�\n] and determine all eigenvectors corre-\nsponding to this eigenvalue. \nSolution We must show that there is a nonzero vector x such that Ax = Sx. But this \nequation is equivalent to the equation \n(\nA -SI\n)\nx = 0, so we need to compute the null \nspace of the matrix A -51. We find that \nA \n_ \nSJ= \n[\nI \n2\n] \n_ \n[\n5 O\nJ = \n[\n-4 2\n] \n4 3 0 5 4 -2 \nSince the columns of this matrix are clearly linearly dependent, the Fundamental \nTheorem of Invertible Matrices implies that its null space is nonzero. Thus, Ax = \nSx has a nontrivial solution, so 5 is an eigenvalue of A. We find its eigenvectors by \ncomputing the null space: \n[A-SI\nI\nO J\n=\n[\n-4 2\n1\n0\n]� \n[\nI \n-\nt\n1\n0\nJ \n4 \n-\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":37381,"to":37466}}}}],[743,{"pageContent":"Sx has a nontrivial solution, so 5 is an eigenvalue of A. We find its eigenvectors by \ncomputing the null space: \n[A-SI\nI\nO J\n=\n[\n-4 2\n1\n0\n]� \n[\nI \n-\nt\n1\n0\nJ \n4 \n-\n2 \n0 0   0  0 \nThus, if x = \n[\n:J is an eigenvector corresponding to the eigenvalue 5, it satisfies \nx\n1 \n-\nt \nx\n2 \n= 0, or x\n1 \n= \nt \nx\n2\n, so these eigenvectors are of the form \nThat is, they are the nonzero multiples of [ \nt\n] \n(or, equivalently, the nonzero multiples \nof \n[\n:Ji \n1 \n.+ \nThe set of all eigenvectors corresponding to an eigenvalue ,\\ of an n X n matrix A \nis just the set of nonzero vectors in the null space of A  -,\\I. It follows that this set of \neigenvectors, together with the zero vector in !R\nn\n, is the null space of A -  AI.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":37466,"to":37517}}}}],[744,{"pageContent":"256 \nChapter 4 \nEigenvalues and Eigenvectors \nExample 4.3 \nExample 4.4 \nDefinition \nLet A be an n x n matrix and let A be an eigenvalue of A. The \ncollection of all eigenvectors corresponding to A, together with the zero vector, is \ncalled the eigenspace of A and is denoted by EA-\nTherefore, in Example 4.2, E\n5 \n= \n{ \nt[ \n�\n] } \n. \nShow that A = 6 is an eigenvalue of A = \neigenspace. \nSolulion As in Example 4.2, we compute the null space of A -6I. Row reduction \nproduces \nA -6I \n= \n[\n-\n� \n-\n� \n-\n!] -----+ \n[\n� � -\n�\ni \n2    2 -4 \n0   0 0 \nfrom which we see that the null space of A -6I is nonzero. Hence, 6 is an eigenvalue \nof A, and the eigenvectors corresponding to this eigenvalue satisfy x\n1 \n+ x\n2 \n-\n2x3 \n= \n0\n, \nor x\n1 \n=   -x\n2 \n+ 2x3. It follows that \nIn IR\n2\n, we can give a geometric interpretation of the notion of an eigenvector. The \nequation Ax \n= \nAx says that the vectors Ax and x are parallel. Thus, x is an eigenvector \nof A if and only if A transforms x into a parallel vector [or, equivalently, if and only \nif T\nA \n(\nx\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":37519,"to":37581}}}}],[745,{"pageContent":"equation Ax \n= \nAx says that the vectors Ax and x are parallel. Thus, x is an eigenvector \nof A if and only if A transforms x into a parallel vector [or, equivalently, if and only \nif T\nA \n(\nx\n) \nis parallel to x, where T\nA \nis the matrix transformation corresponding to A] . \nFind the eigenvectors and eigenvalues of A = [ \n� \nO\n] geometrically. \n- 1 \nSolulion We recognize that A is the matrix of a reflection F in the x-axis (see \nExample 3.56\n)\n. The only vectors that F maps parallel to themselves are vectors parallel \nto the y-axis (i.e., multiples of [ �] ), w\n�\nich are reversed (eigenvalue -1), and vectors \nparallel to the x-axis (i.e., multiples of [ \n0\n] \n), which are sent to themselves (eigenvalue 1) \n(see Figure 4.5\n)\n. Accordingly, A \n= \n-1 and A = 1 are the eigenvalues of A, and the \ncorresponding eigenspaces are \nE\n_\n1 \n= \nspa\nn\n([\n�\n]\n) \nand \nE\n1 \n= \nspan\n([\n�\n]\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":37581,"to":37633}}}}],[746,{"pageContent":"The discussion is based on the \narticle \"Eigenpictures: Pictur­\ning the Eigenvector Problem\" by \nSteven Schonefeld in The College \nMathematics Jo urnal 26 (1996), \npp. 316-319. \nSection 4.1 \nIntroduction to Eigenvalues and Eigenvectors \n251 \ny \n3 \n-3 \nFigure 4.5 \nThe eigenvectors of a reflection \ny \n4 \n3 \n2 \n3 \n4 \nFigure 4.6 \nAnother way to think of eigenvectors geometrically is to draw x and Ax head-to­\ntail. Then x will be an eigenvector of A if and only if x and Ax are aligned in a straight \nline. In Figure 4.6, xis an eigenvector of A but y is not. \nIf xis an eigenvector of A corresponding to the eigenvalue A, then so is any non­\nzero multiple of x. So, if we want to search for eigenvectors geometrically, we need \nonly consider the effect of A on unit vectors. Figure 4.7(a) shows what happens when \nwe transform unit vectors with the matrix A = [ � \n�] \nof Example 4.\n[\n\\ �ny,\n]\nisplay \nthe results head-to-tail, as in Figure 4.6. We can see that the vector x = \n1 \n/ v'2 is an","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":37635,"to":37671}}}}],[747,{"pageContent":"we transform unit vectors with the matrix A = [ � \n�] \nof Example 4.\n[\n\\ �ny,\n]\nisplay \nthe results head-to-tail, as in Figure 4.6. We can see that the vector x = \n1 \n/ v'2 is an \neigenvector, but we also notice that there appears to be an eigenvector in the second \nquadrant. Indeed, this is the case, and it turns out to be the vector [ -\n� \nj �\n]\n.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":37671,"to":37686}}}}],[748,{"pageContent":"258 \nChapter 4 \nEigenvalues and Eigenvectors \nFigure 4.1 \nExample 4.5 \ny \ny \n(a) \n(b) \nIn Figure 4.\n7\n(b), we see what happens when we use the matrix A = \n[ \n1 \n-\n1 \nThere are no eigenvectors at all! \nWe now know how to find eigenvectors once we have the corresponding eigenval­\nues, and we have a geometric interpretation of them-but one question remains: How \ndo we first find the eigenvalues of a given matrix? The key is the observation that A is \nan eigenvalue of A if and only if the null space of A \n-\nAI is nontrivial. \nRecall from Section 3.3 that the determinant of a 2  X  2 matrix A = \n[\n;  �\n] \nis \nthe expression det A = ad \n-\nbe, and A is invertible if and only if det A is nonzero. \nFurthermore, the Fundamental Theorem oflnvertible Matrices guarantees that a ma­\ntrix has a nontrivial null space if and only if it is noninvertible-hence, if and only if \nits determinant is zero. Putting these facts together, we see that (for 2 X 2 matrices at","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":37688,"to":37721}}}}],[749,{"pageContent":"trix has a nontrivial null space if and only if it is noninvertible-hence, if and only if \nits determinant is zero. Putting these facts together, we see that (for 2 X 2 matrices at \nleast) A is an eigenvalue of A if and only if det(A -AI) = 0. This fact characterizes \neigenvalues, and we will soon generalize it to square matrices of arbitrary size. For the \nmoment, though, let's see how to use it with 2 X 2 matrices. \nFind all of the eigenvalues and corresponding eigenvectors of the matrix A \n[ \n� \n�] \nfrom Example 4.1. \nSolulion The preceding remarks show that we must find all solutions A of the equa­\ntion det(A \n-\nAI) \n= 0. Since \n[\n3  -\nA \ndet\n(\nA - AI\n) = \ndet \n1 \nl \n] \n= (\n3 - A\n)(\n3 - A\n) \n-\n1 \n= \nA\n2 \n- 6A  + 8 \n3 -\nA \nwe need to solve the quadratic equation A\n2 \n- 6A + 8 = 0. The solutions to this equa­\ntion are easily found to be A \n= \n4 and A = 2. These are therefore the eigenvalues of A.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":37721,"to":37765}}}}],[750,{"pageContent":"Section 4.1 \nIntroduction to Eigenvalues and Eigenvectors \n259 \nTo find the eigenvectors corresponding to the eigenvalue A =   4, we compute the \nnull space of A - 4I. We find \n[\n-\n1 \n[A - 4I \nl\noJ = \nl \n1\n1 \nO\nJ\n-+ \n[\n1 -1\n1 \nO\nJ \n-1 0    0   0  0 \nfrom which it follows that x = \n[\n:J is an eigenvector corresponding to A = 4 if and \nonly if x\n1 \n- x\n2 \n= 0 or x\n1 \n= x\n2\n• Hence, the eigenspace E\n4 \n= \n{ [\n:\n:\n]\n} \n= \n{ \nx\n2 \n[ \n�\n]\n} \n= \nspan\n(\n[\n�\n]\n)\n. \nSimilarly, for A =  2, we have \n[\nA - 2I I 0 \nl \n= \n[ \n�  �\nI \n� \nJ \n---+ \n[ \n� � \nI \n� \nJ \nso y = \n[\n;\nJ is an eigenvector corresponding to A = 2 if and only if y\n1 \n+ y\n2 \n= 0 or \ny\n1 \n= \n-Yi-\nThus, \nthe \neigen\nspa\nce E\n2 \n= \n{ \n[ \n�\n2\n]\n} \n= \n{\ny\n2\n[\n-\n�\n]\n} \n=\nspan\n([\n-\n�\n]\n)\n. \nFigure 4.8 shows graphically how the eigenvectors of A are transformed when \nmultiplied by A: an eigenvector x in the eigenspace E\n4 \nis transformed into 4x, and an \neigenvector yin the eigenspace E\n2 \nis transformed into 2y. As Figure 4.7(a) shows, the \neigenvectors of A are the only vectors in IR\n2 \nthat are transformed into scalar multiples","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":37767,"to":37892}}}}],[751,{"pageContent":"eigenvector yin the eigenspace E\n2 \nis transformed into 2y. As Figure 4.7(a) shows, the \neigenvectors of A are the only vectors in IR\n2 \nthat are transformed into scalar multiples \nof themselves when multiplied by A. \ny \nAy = 2y \n-4 \n-3  -2  -1 \n2 \n3 \n4 \n-1 \n-2 \n-3 \n-4 \nFigure 4.8 \nHow A transforms eigenvectors","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":37892,"to":37911}}}}],[752,{"pageContent":"260 \nChapter 4 \nEigenvalues and Eigenvectors \nExample 4.6 \nExample 4.1 \n.. \nI Exercises 4.1 \nRemark You will recall that a  polynomial equation with real coefficients (such as \nthe quadratic equation in Example 4.5) need not have real roots; it may have complex \nroots. (See Appendix C.) It is also possible to compute eigenvalues and eigenvectors \nwhen the entries of a matrix come from \"ll_\nP\n' \nwhere p is prime. Thus, it is importa nt to \nspecify the setting we intend to work in before we set out to compute the eigenvalues \nof a matr  ix. However, unless otherwise specified, the eigenvalues of a matrix wh ose \nentries are real numbers will be assumed to be real as well. \nInterpret the matr  ix in Example 4.5 as a matrix over \"11_\n3 \nand find its eigenvalues in \nthat field. \nSolulion The solution proceeds exactly as above, except we work modulo 3. Hence, \nthe quadratic equation A\n2 \n-\n6A + 8 = 0 becomes A\n2 \n+ 2 = 0. This equation is the \nsame as A\n2 \n= -2 = 1, giving A= 1 and A= -1 = 2 as the eigenvalues in Z\n3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":37913,"to":37944}}}}],[753,{"pageContent":"the quadratic equation A\n2 \n-\n6A + 8 = 0 becomes A\n2 \n+ 2 = 0. This equation is the \nsame as A\n2 \n= -2 = 1, giving A= 1 and A= -1 = 2 as the eigenvalues in Z\n3\n• (Check \nthat the same answer would be obtained by first reducing A modulo 3 to obtain \n[ \n� \n�\n] \nand then working with this matrix.) \nFind the eigenvalues of A = \n[ \n� \n-\n�\n] \n(a) over IR and (b) over the complex numbers C. \nSolulion \nWe must solve the equation \n[\n-\nA \nO = det (A \n-\nAI\n) \n= det \n1 \n-1\n] \n= ,\\\n2 \n+ 1 \n-\nA \n(a)  Over IR, there are no solutions, so A has no real eigenvalues. \n(b)  Over C, the solutions are A = i and A = -i. (See Appendix C.) \nIn the next section, we will extend the notion of determinant from 2 X 2 to \nn X n matrices, which in turn will allow us to find the eigenvalues of arbitrary square \nmatrices. (In fact, this isn't quite true-but we will at least be able to find a polynomial \nequation that the  eigenvalues of a given matrix must satisfy.) \n• \nIn Exercises 1-6, show that vis an eigenvector of A and find \n4.A = \n[\n! \n=\n�\nl \nv = \n[\n�\n]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":37944,"to":38003}}}}],[754,{"pageContent":"equation that the  eigenvalues of a given matrix must satisfy.) \n• \nIn Exercises 1-6, show that vis an eigenvector of A and find \n4.A = \n[\n! \n=\n�\nl \nv = \n[\n�\n] \nth e corresponding eigenvalue. \n1. A  = \n[\n� \n�\nl \nv = \n[\n�\n] \n2.A = \n[\n� \n�\nl\nv\n=\n[\n-\n�\n] \n3.A = \n[\n-\n! \n�\nl \nv = \n[ _\n�\n] \n[\n1 \n0 \n5.A = \n1 \n0 \n[\n: \n6.A = \n2 \n-;\n]\n.F\n[\n-\n:\n] \n-1\n] \n[\n-2\n] \n1 , v = \n1 \n0 \n1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":38003,"to":38072}}}}],[755,{"pageContent":"In Exercises 7-12, show that A is an eigenvalue of A and \nfind one eigenvector corresponding to this eigenvalue. \n7.A  = \n[\n� \n2\n] \nA=  3 \n-1 \n, \n8.A  = \n[\n� \n3\n]\n,\\\n=-1 \n2 \n, \n9.A  = \n[\n_\n� \n4\n] \nA=  1 \n5 \n, \nI\nO.A= \n[\n! \n-\n2\n] \n-\n7 \n, \nA= -6 \nH \n0 \n:\nJ\nA \n�\n-I \n11. A= 1 \n0 \n12. A\n� \n[: \n2 \n-I\n] \n� \n, \nA\n= \n2 \nIn Exercises 13-18, find the eigenvalues and eigenvectors of \nA geometrically. \n13.A  = \n[\n-\n� \n�\n] \n(reflection in the y-axis) \n14.A  = \n[\n� \n�\n] \n(reflection in the line y = x\n) \n15.A  = \n[\n� \n�\n] \n(projection onto the x-axis) \n16. A  = \n[\nil \n�\n] \n(projection onto the line through the \norigin �ith\n2\ndirection vector \n[\ni\n]) \n17. A  = \n[ \n� \n�\n] \n(stretching by a   factor of 2 horizontally \nand a factor of 3 vertically) \n18. \nA  = [ \n� \n-\n�\n] \n(counterclockwise rotation of 90° \nabout the origin) \nSection 4.1 \nIntroduction to Eigenvalues and Eigenvectors \n261 \nIn Exercises 19-22, th e unit vectors x in IR\n2 \nand their \nimages Ax under th e action of a 2 X 2 matrix A are drawn \nhead-to-tail, as in Figure 4.7. Estimate th e eigenvectors and \neigenvalues of A from each ''eigenpicture.\" \n19. \ny","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":38074,"to":38192}}}}],[756,{"pageContent":"2 \nand their \nimages Ax under th e action of a 2 X 2 matrix A are drawn \nhead-to-tail, as in Figure 4.7. Estimate th e eigenvectors and \neigenvalues of A from each ''eigenpicture.\" \n19. \ny \n20. \ny","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":38192,"to":38200}}}}],[757,{"pageContent":"262 \nChapter 4 \nEigenvalues and Eigenvectors \n21. \ny \n-2 \n22. \ny \nIn Exercises 23-26, use the method of Example 4.5 to find \nall of the eigenvalues of the matrix A. Give bases for each of \nth e corresponding eigenspaces. Illustrate th e eigenspaces and \nthe effect of multiplying eigenvectors by A as in Figure 4.8. \n23.A  = \n[\n� \n-\n�\n] \n24.A \n= \n[\n! �\n] \n29.A  = \n[\n�  �\n] [ \n0    1  + i\n] \n30.A  = \n1 \n-\ni 1 \nIn Exercises 31-34, find all of th e eigenvalues of the ma­\ntrix A over the indicated Z\np\n. \n31.A  = \n[\n� \n�\n] \nover Z\n3 \n32.A  = \n[\n� \n33. A = \n[\n! \n�\n]over Zs \n34. A = \n[\n: �]over Zs \n35. (a) Show that the eigenvalues of the 2 X 2 matrix \nA= \n[\n: �\n] \nare the solutions of the quadratic equation \nA\n2 \n- tr(A)A + det A = 0, where tr(A) is the trace \nof A. (See page 162.) \n(b) Show that the eigenvalues of the matrix A in \npart (a) are \nA =Ha + d ± \nV\n(\na \n-\nd\n)\n2 \n+ 4bc) \n( c) Show that the trace and determinant of the matrix A \nin part (a) are given by \ntr(A) = A\n1 \n+ A\n2 \nand det A = A\n1 \nA\n2 \nwhere A\n1 \nand A\n2 \nare the eigenvalues of A.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":38202,"to":38292}}}}],[758,{"pageContent":"part (a) are \nA =Ha + d ± \nV\n(\na \n-\nd\n)\n2 \n+ 4bc) \n( c) Show that the trace and determinant of the matrix A \nin part (a) are given by \ntr(A) = A\n1 \n+ A\n2 \nand det A = A\n1 \nA\n2 \nwhere A\n1 \nand A\n2 \nare the eigenvalues of A. \n36. Consider again the matr  ix A  in Exercise 3\n5\n. Give \nconditions on a, b, c, and d such that A has \n(a) two distinct real eigenvalues, \n(b) one real eigenvalue, and \n(c) no real eigenvalues. \n37. Show that the eigenvalues of the upper triangular \nmatrix \nA= \n[\n�  �\n] \nare A = a and A = d, and find the corresponding \n25. A = \n[\n� \n�\n] \n26. A \n= \n[ _\n� \n�\n] \neigenspaces. \n,E:S7 \n,E:S7\n38. \nLet a and b be real numbers. Find the eigenvalues and \nIn Exercises 27-30, find all of th e eigenvalues of th e matrix \ncorresponding eigenspaces of \nA over th e complex numbers C. Give bases for each of th e \ncorresponding eigenspaces. \n27. \nA \n= \n[ \n_ \n�  �\n] \n[\n2 \n-\n3\n] \n28. \nA  = \nl \nO \nover the  complex numbers.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":38292,"to":38367}}}}],[759,{"pageContent":"11 f \nDeterminants \nSection 4.2 Determinants \n263 \nHistorically, determinants preceded matrices-a curious fact in light of the way linear \nalgebra is taught today, with matrices before determinants. Nevertheless, determi­\nnants arose independently of matrices in the solution of many practical problems, \nand the theory of determinants was well developed almost two centuries before \nmatrices were deemed worthy of study in and of themselves. A snapshot of the his­\ntory of determinants is presented at the end of this section. \nRecall that the determinant of the 2 X 2 matrix \nA  = \n[\na\nll \na\n2\n1 \ndet \nA \n=  a\nll\na\n22 \n-  a\n1\n2\na\n2\n1 \nWe first encountered this expression when  we determined ways to compute the \ninverse of a matrix. In particular, we found that \nThe determinant of a matr  ix A is sometimes also denoted by \nI\nA \nI\n, \nso for the 2 X  2 \nmatrix A \n= \n[\na\n11 \na\n1\n2\n] \nwe may also write \na\n2\n1 \na\n22 \nWarning \nThis notation for the determinant is reminiscent of absolute value no-\n· \n. \n. \nI \na\nll \na\n1\n2 \nI \n. . \n[\na\nll \na\n1\n2\n]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":38369,"to":38441}}}}],[760,{"pageContent":"I\nA \nI\n, \nso for the 2 X  2 \nmatrix A \n= \n[\na\n11 \na\n1\n2\n] \nwe may also write \na\n2\n1 \na\n22 \nWarning \nThis notation for the determinant is reminiscent of absolute value no-\n· \n. \n. \nI \na\nll \na\n1\n2 \nI \n. . \n[\na\nll \na\n1\n2\n] \ntation. It 1s easy to mistake \n, the notat10n for determmant, for \n, \na\n2\n1 \na\n22 \na\n2\n1 \na\n22 \nthe notation for the matrix itself. Do not confuse these. Fortunately,  it will usually \nbe clear from the context which is intended. \nWe define the determinant of a 1 X 1 matrix A = [a] to be \ndet A= \nl\na\nl \n=a \n(Note that we really have to be careful with notation here: \nI \na \nI \ndoes not denote the \nabsolute value of a in this case.) How then should we define the determinant of a \n3 X 3 matrix? If you ask your CAS for the inverse of \nA= [� � ;] \ng     h l \nthe answer will be equivalent to \n[ \nei -fh ch -bi \nA\n-\n1 \n= \n� \nJg -  di \nai  -  cg \ndh -eg \nbg -ah \nbf-ce l \ncd -af \nae -bd \nwhere Li= aei -afh -bdi + bfg + cdh -ceg. Observe that \nLi  = aei -afh -   bdi + bfg + cdh -ceg \n=  a\n(\nei -fh)  -b(di -Jg\n) + c(dh  -eg) \n=  a \nI \n� \n�I \n-   b \nI \n� \n�I \n+ \nc \nI �  �\nI","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":38441,"to":38543}}}}],[761,{"pageContent":"264 \nChapter 4 \nEigenvalues and Eigenvectors \nExample 4.8 \nand that each of the entries in the matrix portion of A \n-\ni \nappears to be the determi­\nnant of a 2 X 2 submatrix of A. In fact, this is true, and it is the basis of the definition \nof the determinant of a 3 X 3 matrix. The definition is recursive in the sense that the \ndeterminant of a 3 X 3 matrix is defined in terms of determinants of 2 X 2 matrices. \nDefinition \n[a\nll \nLetA = a\n2\n1 \na\n13\n] \na\n2\n3 \n. Then the determinant of A is the scalar \na\n31 \na\n33 \n(1) \nNotice that each of the 2 X 2    determinants is obtained by deleting the row and col­\numn of A that contain the entry the determinant is being multiplied by. For example, \nthe first summand is a\n11 \nmultiplied by the determinant of the submatrix obtained by \ndeleting row 1 and column 1. Notice also that the plus and minus signs alternate in \nEquation ( 1). If we denote by A;\nj \nthe submatrix of a matrix A obtained by deleting row \ni and column j, then we may abbreviate Equation ( 1) as \ndet A  =  a\nll \ndet A","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":38545,"to":38586}}}}],[762,{"pageContent":"Equation ( 1). If we denote by A;\nj \nthe submatrix of a matrix A obtained by deleting row \ni and column j, then we may abbreviate Equation ( 1) as \ndet A  =  a\nll \ndet A\nll \n-  a\n1\n2 \ndet A\n1\n2 \n+ a\n13 \ndet A\n13 \n3 \n2: \n(\n-l\n)\n1\n+j\na\n1\nj \ndet A\n1\nj \nj =\nl \nFor any square matrix A, det A;\nj \nis called the (i, j)-minor of A. \nCompute the determinant of \nSolution We compute \ndet \nA \n= \n5\n1\n-\n� \n� \nI \n- ( -\n3) \nI \n� \n� \nI \n+ \n2 \nI \n� \n_ \n� \nI \n= 5\n(\n0 -\n(\n-2\n)) \n+  3\n(\n3 - 4\n) \n+ 2\n(\n-1 - 0\n) \n= 5\n(\n2\n) \n+  3\n( \n-1\n) \n+ 2\n(\n-1\n) \n= 5 \nWith a little practice, you should find that you can easily work out 2 X 2    determinants \nin yom head. Wciting out the secood line in the above solution is then \nunnecess:+ \nAnother method for calculating the determinant of a 3 X  3 matrix is analogous \nto the method for calculating the determinant of a 2 X  2 matrix. Copy the first two \ncolumns of A to the right of the matrix and take the products of the elements on the six","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":38586,"to":38678}}}}],[763,{"pageContent":"Example 4.9 \nSection 4.2 Determinants \n265 \ndiagonals shown below. Attach plus signs to the products from the downward-sloping \ndiagonals and attach minus signs to the products from the upward-sloping diagonals. \n(2) \n+ \nThis method gives \nIn Exercise 19, you are asked to check that this result agrees with that from Equa­\ntion ( 1) for a 3 X 3 determinant. \nCalculate the determinant of the matrix in Example 4.8 using the method shown in (2). \nSolution We  adjoin to A its  first two  columns and compute the six  indicated \nprod ucts: \n0 \n-10 -9 \n-2 \nAdding the three products at the bottom and subtracting the three products at the \ntop gives \ndet A = 0 + \n(\n-12\n) \n+ \n(\n-2\n) \n- 0 -\n(\n-10\n) \n-\n(\n-9\n) \n= 5 \nas before. \nWarning We  are about to define determinants for arbitrary square matrices. \nHowever, there is no analogue of the method in Example 4.9 for larger matrices. It is \nvalid only for 3 X 3 matrices. \nDelerminanls of n  x  n Malrices \nThe definition of the determinant of a 3 X  3 matrix extends naturally to arbitrary","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":38680,"to":38720}}}}],[764,{"pageContent":"valid only for 3 X 3 matrices. \nDelerminanls of n  x  n Malrices \nThe definition of the determinant of a 3 X  3 matrix extends naturally to arbitrary \nsquare matrices. \nDefiDiliOD \nLet A \n= \n[a;\n) \nbe an n x n matrix, where n  2: 2. Then the deter-\nminant of A is the scalar \nn \n_L(\n-l\n)\n1\n+\ni\na\n1\n1 \ndet \nA\n1\n1 \nj\n�\nI \n(3)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":38720,"to":38748}}}}],[765,{"pageContent":"266 \nChapter 4 \nEigenvalues and Eigenvectors \nTheorem 4.1 \nIt is convenient to combine a minor with its plus or minus sign. To this end, we define \nthe (i, j)-cofactor of A to be \nWith this notation, definition (3) becomes \nn \n<let A = \n2:a\n1\nj\nC\nij \nj =\nl \n(4) \nExercise 20 asks you to check that this definition correctly gives the formula for the \ndeterminant of a 2 X 2 matrix when n = 2. \nDefinition ( 4) is often referred to as cofactor expansion along the first row. It is \nan amazing fact that we get exactly the same result by expanding along any row (or \neven any column\n)\n! We summarize this fact as a theorem but defer the proof until the \nend of this section (since it is somewhat lengthy and would interrupt our discussion \nif we were to present it here). \nThe Laplace Expansion Theorem \nThe determinant of an n X n matr  ix A = [a;), where n 2: 2, can be computed as \nn \n2:a\n;\nj\nC\nij \nj =\nl \n(which is the cofactor expansion along the ith row) and also as \nn \n= \n2:a\ni\nj\nc\ni\nj \ni\n=\nl \n(the cofactor expansion along the jth column).","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":38750,"to":38798}}}}],[766,{"pageContent":"n \n2:a\n;\nj\nC\nij \nj =\nl \n(which is the cofactor expansion along the ith row) and also as \nn \n= \n2:a\ni\nj\nc\ni\nj \ni\n=\nl \n(the cofactor expansion along the jth column). \n(5) \n(6) \nSince C\nij \n= \n( -\nl\n)\ni\n+\nj \n<let A;\nj\n, each cofactor is plus or minus the corresponding minor, \nwith the correct sign given by the term (-1y+\nj\n. A quick way to determine whether \nthe sign is + or - is to  remember that the signs form a \"checkerboard\" pattern: \n+ + \n+ + \n+ + \n+ +","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":38798,"to":38840}}}}],[767,{"pageContent":"Example 4.10 \nPierre Simon Laplace (17 49-1827) \nwas born in Normandy, France, \nand was expected to become a \nclergyman until his mathematical \ntalents were noticed at school. \nHe made many important \ncontributions to calculus, \nprobability, and astronomy. He was \nan examiner of the young Napoleon \nBonaparte at the Royal Artillery \nCorps and later, when Napoleon was \nin power, served briefly as Minister \nof the Interior and then Chancellor \nof the Senate. Laplace was granted \nthe title of Count of the Empire \nin 1806 and received the title of \nMarquis de Laplace in 1817. \nExample 4.11 \nCompute the determinant of the matrix \nSection 4.2 Determinants \n261 \nby (a) cofactor expansion along the third row and (b) cofactor expansion along the \nsecond column. \nSolution \n(a) We compute \n=\n2\n1\n-\n� \n�\n1\n-\n(\n-\n1\n)\n1\n�  �\n1\n+\n3\n1\n� \n-\n�\n1 \n= 2\n(\n-\n6\n) + \n8 \n+ \n3\n(\n3\n) \n= \n5 \n(b)  In this case, we have \n=\n-\n(\n-\n3\n)\n1\n� \n�\n1\n+\n0\n1\n� \n�\n1\n-\n(\n-\n1\n)\n1\n�  �\nI \n=\n3\n(\n-\n1\n)+\n0\n+\n8 \n= 5 \nNotice that in part (b) of Example \n4\n. 1\n0 we needed to do fewer calculations than","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":38842,"to":38941}}}}],[768,{"pageContent":"� \n�\n1\n-\n(\n-\n1\n)\n1\n�  �\n1\n+\n3\n1\n� \n-\n�\n1 \n= 2\n(\n-\n6\n) + \n8 \n+ \n3\n(\n3\n) \n= \n5 \n(b)  In this case, we have \n=\n-\n(\n-\n3\n)\n1\n� \n�\n1\n+\n0\n1\n� \n�\n1\n-\n(\n-\n1\n)\n1\n�  �\nI \n=\n3\n(\n-\n1\n)+\n0\n+\n8 \n= 5 \nNotice that in part (b) of Example \n4\n. 1\n0 we needed to do fewer calculations than \nin part (a) because we were expanding along a column that contained a zero entry­\nnamely, a\n22\n; therefore, we did not need to compute C\n22\n• It follows that the Laplace \nExpansion Theorem is most useful when the matrix contains a row or column with \nlots of zer  os, since, by choosing to expand along that row or column, we minimize the \nnumber of cofactors we need to compute. \nCompute the determinant of \nSolution First, notice that column 3 has only one nonzero entry; we should there­\nfore expand along this column. Next, note that the \n+ \n/- pattern assigns a minus sign","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":38941,"to":39024}}}}],[769,{"pageContent":"268 \nChapter 4 \nEigenvalues and Eigenvectors \nExample 4.12 \nto the entry a\n2\n3 \n= 2. Thus, we have \ndet A = a\n13\nC\n13 \n+ a\n2\n3\nC\n2\n3 \n+ a\n33\nC\n33 \n+ a\n4\n3\nC\n4\n3 \n= O(C\n13\n) \n+ 2C\n2\n3 \n+ \nO(C\n33\n) \n+ \nO(C\n43\n) \n2  -3  1 \n-2  1   -1  3 \n-2 0 \nWe  now continue by  expanding along the  third row of the  determinant above \n(the third column would also be a good choice) to get \ndet A =  -2 \n( \n-2 \nI \n= \n� \n� \nI \n-\nI \n� \n� \nI\n) \n-2\n(\n-2\n(\n-8\n) \n-5\n) \n-2\n(\n11\n) \n=  -22 \n(Note that the +I -pattern for the 3 X 3 minor is not that of the original matrix but \nthat of a 3 X 3 matrix in general.) \n4 \nThe Laplace expansion is particularly useful when the matrix is (upper or lower) \ntriangular. \nCompute the determinant of \n2 \n-3 \n0 \n4 \n0 \n3 \n2 \n5 \n7 \nA= \n0    0 \n1 \n6 \n0 \n0 \n0  0 \n5 \n2 \n0    0 0 \n0 \n-1 \nSolution \nWe expand along the first column to get \n3 \n2  5 \n7 \ndet A = 2 \n0  1 \n6 \n0 \n0 \n0 \n5 \n2 \n0 \n0  0 \n-1 \n(We have omitted all cofactors corresponding to zero entries.) Now we expand along \nthe first column again: \n1  6 \n0 \ndet A = 2 · 3 0  5 \n2 \n0  0 -1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":39026,"to":39149}}}}],[770,{"pageContent":"3 \n2  5 \n7 \ndet A = 2 \n0  1 \n6 \n0 \n0 \n0 \n5 \n2 \n0 \n0  0 \n-1 \n(We have omitted all cofactors corresponding to zero entries.) Now we expand along \nthe first column again: \n1  6 \n0 \ndet A = 2 · 3 0  5 \n2 \n0  0 -1 \nContinuing to expand along the first column, we complete the calculation: \ndet A = 2 · 3 · 1 \nI \n� \n2\n1 \n= 2\n. \n3\n. \n1 \n. (\n5\n(\n-1\n) \n- 2\n. \n0\n) \n= 2\n. \n3\n. \n1\n. \n5\n. \n(\n-1\n) \n=  -30 \n-1 \n4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":39149,"to":39203}}}}],[771,{"pageContent":"Theorem 4.2 \nTheorem 4.3 \nSection 4.2 Determinants \n269 \nExample 4.12 should convince you that the determinant of a triangular matrix is the \nproduct of its diagonal entries. You are asked to give a proof of this fact in Exercise 21. \nWe record the result as a theorem. \nThe determinant of a triangular matrix is the product of the entries on its main \ndiagonal. Specifically, if A = [a;\n) \nis an n X n triangular matrix, then \n<let A \n= \na\n11\na\n22 \n· \n• \n·\na\n\"\" \nNole In general (that is, unless the matrix is triangular or has some other special \nform), computing a determinant by   cofactor expansion is not efficient. For example, \nthe determinant of a 3 X 3 matrix has 6 = 3! summands, each requiring two multipli­\ncations, and then five additions and subtractions are needed to finish off the calcula­\ntions. For an n X n matrix, there will be n! summands, each with n - 1   multiplications, \nand then n! -1 additions and subtractions. The total number of operations is thus \nT (n) = (n - l\n)\nn!  + n!  - 1 > n!","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":39205,"to":39235}}}}],[772,{"pageContent":"and then n! -1 additions and subtractions. The total number of operations is thus \nT (n) = (n - l\n)\nn!  + n!  - 1 > n! \nEven the fastest of supercomputers cannot calculate the determinant of a mod­\nerately large matr  ix using cofactor expansion. To  illustrate: Suppose we needed to \ncalculate a 50 X 50 determinant. (Matrices much larger than 50 X 50 are used to store \nthe data from digital images such as those transmitted over the Internet or taken by a \ndigital camera.) To calculate the determinant directly would require, in general, more \nthan 50! operations, and 50! \n= \n3 X   10\n6\n4\n. If we had a computer that could perform \na trillion (10\n1\n2\n) operations per second, it would take approximately 3  X   10\n52 \nsec­\nonds, or almost 10\n4\n5 \nyears, to finish the calculations. To put this in perspective, con­\nsider that astronomers estimate the age of the universe to be at least 10 billion ( 10\n1\n0\n) \nyears. Thus, on even a very fast supercomputer, calculating a 50 X 50 determinant by","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":39235,"to":39264}}}}],[773,{"pageContent":"sider that astronomers estimate the age of the universe to be at least 10 billion ( 10\n1\n0\n) \nyears. Thus, on even a very fast supercomputer, calculating a 50 X 50 determinant by \ncofactor expansion would take more than 10\n3\n0 \ntimes the age of the universe! \nFortunately,  there are better methods-and we now turn to developing more \ncomputationally effective means of finding determinants. First, we need to look at \nsome of the properties of determinants. \nProperties of Determinants \nThe most efficient way to compute determinants is to use row reduction. However, \nnot every elementary row operation leaves the determinant of a matrix unchanged. \nThe next theorem summarizes the main properties you need to understand in order \nto use row reduction effectively. \nLet A = \n[a\nij\n] be a square matrix. \na.  If A has a zero row (column), then <let A = 0. \nb. IfB is obtained by interchanging two rows (columns) of A, then <let B = -<let A. \nc.  If A has two identical rows (columns), then <let A = 0.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":39264,"to":39287}}}}],[774,{"pageContent":"a.  If A has a zero row (column), then <let A = 0. \nb. IfB is obtained by interchanging two rows (columns) of A, then <let B = -<let A. \nc.  If A has two identical rows (columns), then <let A = 0. \nd. If Bis obtained by multiplying a row (column) of A by k, then <let B = k <let A. \ne.  If A, B, and Care identical except that the ith row (column) of C is the sum of \nthe ith rows (columns) of A and B, then <let C = <let A +   <let B. \nf.  If Bis obtained by adding a multiple of one row (column) of A to another row \n(column), then <let B = <let A.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":39287,"to":39294}}}}],[775,{"pageContent":"210 \nChapter 4 \nEigenvalues and Eigenvectors \nExample 4.13 \nProof We  will prove (b) as Lemma 4.14 at the end of this section. The proofs of \nproperties (a) and (f) are left as exercises. We will prove the remaining properties in \nterms of rows; the corresponding proofs for columns are analogous. \n(c)  If A has two identical rows, swap them to obtain the matrix B. Clearly, B =A, so \n<let B =   detA. On the other hand, by (b), detB =   -detA. Therefore, detA =   -detA, \nso detA =   0. \n( d) Suppose row i of A is multiplied by k to prod uce B; that is, b\ni\nj = ka;\nj \nfor j = 1, ... , n. \nSince the cofactors C\ni\nj of the elements in the ith rows of A and B are identical (why?), \nexpanding along the ith row of B gives \nn n n \ndetB = �b\n;\nj\nC\ni\nj \n= �ka\nij\nc\nij \n= k �a\nij\nc\nij \n= k detA \nj =\nl \nj =\nl \nj =\nl \n(e) As in (d), the cofactors C\ni\nj of the elements in the  ith rows of A, B, and C are \nidentical. Moreover, c\ni\nj = a;\nj \n+ b;\nj \nfor j = 1, ... , n. We expand along the ith row of C \nto obtain \nn n n n \ndet C = �c\n;\nj\nC\ni\nj \n=�\n(\nau+ b\n;","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":39296,"to":39358}}}}],[776,{"pageContent":"i\nj of the elements in the  ith rows of A, B, and C are \nidentical. Moreover, c\ni\nj = a;\nj \n+ b;\nj \nfor j = 1, ... , n. We expand along the ith row of C \nto obtain \nn n n n \ndet C = �c\n;\nj\nC\ni\nj \n=�\n(\nau+ b\n;\n)Cu = �a\n;\nj\nC\niJ \n+ �buCu =  detA +   detB \nj =\nl \nj =\nl \nj =\nl \nj =\nl \nNotice that properties (b), ( d), and (    f) are related to elementary row operations. \nSince the echelon form of a square matr  ix is necessarily upper triangular, we can \ncombine these properties with Theorem 2 to calculate determinants efficiently. (See \nExploration: Counting Operations in Chapter 2, which shows that row reduction of \nan n X n matr  ix uses on the order of n\n3 \noperations, far fewer than the n ! needed for \ncofactor expansion.) The next examples illustrate the computation of determinants \nusing row reduction. \nCompute <let A if \n(a) \nA = \n[ 2\n0 \n3\n5 \n-\n3\n1] \n-4 -6 \n2 \n(b) A� \n[i -� \nj \nil \nSolution \n(a) \nUsing property (f) and then property (a), we have \n2 \n3 \n-1 \n2 \nR,+\n2\nR1 \n<let A= 0 \n5 \n3 0 \n-4 \n-6 \n2 \n0 \n3 \n-1 \n5 \n3 \n=O \n0    0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":39358,"to":39440}}}}],[777,{"pageContent":"Theorem 4.4 \nThe word lemma is de  rived from \nthe Greek verb lambanein, which \nmeans \"to grasp:' In mathematics, \na lemma is a \"helper theorem\" \nthat we \"grasp hold of\" and use \nto prove another, usually more \nimportant, theorem. \nSection 4.2 Determinants 211 \n(b) We reduce A to echelon form as follows (there are other possible ways to do   this): \n<let A= \nR\n3 -2\nR\n1 \nR\n4-S\nR\n1 \n0 \n3 \n2 \n5 \n2 \n0 \n4 \n-1 \n-4  5 3    0 -3 \n-3  6 \nR\n,<->\nR\n, \n0    2  -4 \n5  7 \n2    4    5 \n-3 \n5  -1  -3 \n-1 \n-4 \n2 \n1 \n5 \nR\n1tt\nR\n4 \nQ \n=  -3 \n1 \n0 \n0 \n0 \n2 \n4 \n3 \n=  -\n(\n-3\n) \n0 \n7 \n0  -1 \n2  -9 \nR\n,+4\nR\n, \n1    0 -1 2 \nR\n4+2\nR\n2 \nQ  - 1    2 -9 \n= 3 \n0 \n0 \n15  -33 \n0 \n0 \n0  -13 \n=  3 \n. \n1 \n. \n( \n-1\n) \n. \n15 \n. \n( \n-13\n) \n= 585 \n0 \n6 \n1 \n0 \n-1 \n2 \n5 \nRi/\n3 \n0 \n2 \n-4 \n5 \n=  -3 \n7 \n2    4    5 \n7 \n1 \n5 \n-1 \n-3 \n1 \n0 \n-1 \n2 \n-1 \n2 \n-9 \n4 \n7    3 \n2 \n-4 \n5 \nRemark By Theorem 4.3, we can also use  elementary column operations in the \nprocess of computing determinants, and we can \"mix and match'' elementary row and \ncolumn operations. For example, in Example 4. l 3(a), we could have started by adding","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":39442,"to":39572}}}}],[778,{"pageContent":"process of computing determinants, and we can \"mix and match'' elementary row and \ncolumn operations. For example, in Example 4. l 3(a), we could have started by adding \ncolumn 3 to column 1 to create a leading 1 in the upper left-hand corner. In fact, the \nmethod we used was faster, but in other examples column operations may speed up \nthe calculations. Keep this in mind when you work determinants by hand. \noe1erminan1s of Elemen1arv Malrices \nRecall from Section 3.3 that an elementary matr  ix results from performing an ele­\nmentary row operation on an identity matrix. Setting A = I\nn \nin Theorem 4.3 yields \nthe following theorem. \nLet E be an n X n elementary matr  ix. \na.  If E results from interchanging two rows of I\nn\n, then <let E = -1. \nb.  If E results from multiplying one row of I\nn \nby k, then <let E = k. \nc.  If E results from adding a multiple of one row of I\nn \nto another row, then \ndet E = 1. \nProof Since <let I\nn \n= 1, applying (b), (d), and (f) of Theorem 4.3 immediately gives","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":39572,"to":39596}}}}],[779,{"pageContent":"n \nby k, then <let E = k. \nc.  If E results from adding a multiple of one row of I\nn \nto another row, then \ndet E = 1. \nProof Since <let I\nn \n= 1, applying (b), (d), and (f) of Theorem 4.3 immediately gives \n(a), (b), and (c), respectively, of Theorem 4.4. \nNext, recall that multiplying a matrix B by an elementary matrix on th e left per­\nforms the corresponding elementary row operation on B. We can therefore rephrase \n(b), (d), and (f) ofTheorem 4.3 succinctly as the following lemma, the proof of which \nis straightforward and is left as Exercise 43.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":39596,"to":39609}}}}],[780,{"pageContent":"212 \nChapter 4 \nEigenvalues and Eigenvectors \nLemma 4.5 \nTheorem 4.6 \nLet B be an n X n matr  ix and let E be an n X n elementary matrix. Then \ndet\n(\nEB\n) \n= \n(\ndet E\n)(\ndet B\n) \nWe can use Lemma 4.5 to prove the main theorem of this section: a characteriza­\ntion of invertibility in terms of determinants. \nA square matrix A  is invertible if and only if det A *   0. \nProof Let A be an n X n matrix and let R be the reduced row echelon form of A. \nWe will show first that det A * 0 if and only if det R *   0. Let E\n1\n, E\n2\n, ••• , E\nr \nbe the \nelementary matrices corresponding to the elementary row operations that reduce \nA to R. Then \nTaking determinants of both sides and repeatedly applying Lemma 4.5, we obtain \n(\ndetE) \n· · · \n(\ndetE\n2\n)(\ndetE\n,\n)(\ndetA\n) \n= detR \nBy Theorem 4.4, the determinants of all the elementary matrices are nonzero. We \nconclude that det A * 0 if and only if det R *   0. \nNow suppose that A is invertible. Then, by the Fundamental Theorem oflnvertible \nMatrices, R = I\nn","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":39611,"to":39658}}}}],[781,{"pageContent":"conclude that det A * 0 if and only if det R *   0. \nNow suppose that A is invertible. Then, by the Fundamental Theorem oflnvertible \nMatrices, R = I\nn\n, so det R = 1 *   0. Hence, det A * 0 also. Conversely, if det A *   0, \nthen det R *   0, so  R cannot contain a zero row, by Theorem 4.3(a). It follows that \n...--\nR must be I\nn \n(why?), so A   is invertible, by the Fundamental Theorem again. \nDeterminants and Matrix Operations \nLet's now try to determine what relationship, if any, exists between determinants and \nsome of the basic matrix operations. Specifically, we would like to find formulas for \ndet(kA), det(A + B), det(AB), det(A \n-\ni \n), and det(A \nT\n) \nin terms of det A and det B . \n....,... Theorem 4.3(d) does not say that det(kA) = k det A.  The correct relationship \nbetween scalar multiplication and determinants is given by the following theorem. \nTheorem 4.1 \nIf A is an n x n matr  ix, then \ndet\n(\nkA\n) = \nk\nn \ndet A \nYou are asked to give a proof of this theorem in Exercise 44 .","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":39658,"to":39689}}}}],[782,{"pageContent":"Theorem 4.1 \nIf A is an n x n matr  ix, then \ndet\n(\nkA\n) = \nk\nn \ndet A \nYou are asked to give a proof of this theorem in Exercise 44 . \n....,... Unfortunately,  there  is  no  simple formula for  det(A + B), and  in general, \ndet(A + B) * det A +   det B. (Find two 2 X 2 matrices that verify this.) It therefore \ncomes as a pleasant surprise to find out that determinants are quite compatible with \nmatrix multiplication. Indeed, we have the following nice formula due to Cauchy.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":39689,"to":39702}}}}],[783,{"pageContent":"Theorem 4.8 \nExample 4.14 \nSection 4.2 Determinants \n213 \nAugustin Louis Cauchy (1789-1857) was born in Paris and studied engineering but switched \nto mathematics because of poor health. A brilliant and prolific mathematician, he published \nover 700 papers, many on quite difficult problems. His name can be found on many theorems \nand definitions in differential equations, infinite series, probability theory, algebra, and \nphysics. He is noted for introducing rigor into calculus, laying the foundation for the branch \nof mathematics known as analysis. Politically conservative, Cauchy was a royalist, and in 1830 \nhe followed Charles X into exile. He returned to France in 1838 but did not return to his post \nat the Sorbonne until the university dropped its requirement that faculty swear an oath of \nloyalty to the new king. \nIf A and B are n X n matrices, then \ndet\n(\nAB\n) \n= \n(\ndet A\n)(\ndet B\n) \nProof We consider two cases: A invertible and A not invertible.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":39704,"to":39728}}}}],[784,{"pageContent":"loyalty to the new king. \nIf A and B are n X n matrices, then \ndet\n(\nAB\n) \n= \n(\ndet A\n)(\ndet B\n) \nProof We consider two cases: A invertible and A not invertible. \nIf A is invertible, then, by the Fundamental Theorem oflnvertible Matrices, it can \nbe written as a product of elementary matrices-say, \nA= E,E\n2\n• ··\nE\nk \nThen AB = E\n1\nE\n2 \n·  ·  · EkB, so k applications of Lemma 4.5 give \nContinuing to apply Lemma 4.5, we obtain \ndet\n(\nAB\n) \n= det\n(\nE\n1\nE\n2 \n• • • \nE\nk\n)\ndet B = \n(\ndet A\n)(\ndet B\n) \nOn  the other hand, if A is not invertible, then neither is AB, by Exercise 47 \nin Section 3.3. Thus, by Theorem 4.6, det A = 0 and det(AB) = 0. Consequently, \ndet(AB) = (det A)(det B), since both sides are zero. \nApplying Theorem 4.8 to A = \n[\n2\n2 \n�]\nand B = \n[\n� \n�\n]\n, we find that \n[\n12 \nAB = \n16 \n�\n] \nand that det A = 4, det B = 3, and det(AB) = 12 = 4 · 3 =  (det A)(d  et B), as claimed. \n(Check these assertions!) \n4 \nThe next theorem gives a nice relationship between the determinant of an invertible \nmatrix and the determinant of its inverse.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":39728,"to":39798}}}}],[785,{"pageContent":"214 \nChapter 4 \nEigenvalues and Eigenvectors \nTheorem 4.9 \nIf A is invertible, then \n1 \ndet\n(\nA\n-\n1\n) \n=  --\ndetA \nProof Since A  is  invertible, AA \n-\nI = I, so  <let (AA \n-\nI\n) \n= <let I   = 1. Hence, \n......... \n(<let A)(det A\n-\n1\n) = 1, by Theorem 4.8, and since <let A  *  0 (why?   ), dividing by \n<let A yields the result. \nExample 4.15 \nVerify Theorem 4.9 for the matrix A  ofExample 4.14. \nTheorem 4.10 \nGabriel Cramer (1704-1752) was \na Swiss mathematician. The rule \nthat bears his name was published \nin 1750, in his treatise Introduction \nto the Analysis of Algebraic Curves. \nAs early as 1730, however, special \ncases of the formula were known \nto other mathematicians, including \nthe Scotsman Colin Maclaurin \n(1698-1746), perhaps the greatest \nof the British mathematicians who \nwere the \"successors of Newton:' \nSolulion We compute \nso \ndet A\n-\n1 \n= \n(\n�\n)\n(\n�\n)-\n(\n-\n�\n)\n(\n-\n�\n) \n= \n%-t \n= \n� \n= \nde\n�\nA \nRemark The beauty of Theorem 4.9 is that sometimes we do not need to know \nwh\na","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":39800,"to":39872}}}}],[786,{"pageContent":"were the \"successors of Newton:' \nSolulion We compute \nso \ndet A\n-\n1 \n= \n(\n�\n)\n(\n�\n)-\n(\n-\n�\n)\n(\n-\n�\n) \n= \n%-t \n= \n� \n= \nde\n�\nA \nRemark The beauty of Theorem 4.9 is that sometimes we do not need to know \nwh\na\nt the inverse of   a matrix is, but only that it exists, or to know what its determinant \nis. For the matrix A  in the last two examples, once we know that <let A = 4 *   0, we \nimmediately can deduce that A  is invertible and that <let A \n-\nl \n= �without actually \ncomputing A \n-\nl\n. \nWe now relate the determinant of a matrix A  to that of its transpose A\nT\n_ Since the \nrows of A\nT \nare just the columns of A, evaluating <let A\nT \nby expanding along the first \nrow is identical to evaluating <let A by expanding along its first column, which the \nLaplace Expansion Theorem allows us to do. Thus, we have the following result. \nFor any square matrix A, \ndetA = det A\nT \nCramer's  Rule and lhe Adioinl \nIn this section, we derive two useful formulas relating determinants to the solution","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":39872,"to":39928}}}}],[787,{"pageContent":"For any square matrix A, \ndetA = det A\nT \nCramer's  Rule and lhe Adioinl \nIn this section, we derive two useful formulas relating determinants to the solution \nof linear systems and the inverse of a matrix. The first of these, Cramer's Rule, gives \na formula for describing the solution of certain systems of n linear equations in n \nvariables entirely in terms of determinants. While this result is of little practical use \nbeyond 2 X 2 systems, it is of great theoretical importance. \nWe will need some new notation for this result and its proof. For an n X n ma­\ntrix A   and a vector b in !R\nn\n, let A;(b) denote the matrix obtained by replacing the ith \ncolumn of A by b. That is, \nColumn i \n-1.-\nA\n;\n(\nb\n) \n= \n[a\n1\n• \n··\nh\n···\na\nn\nl","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":39928,"to":39958}}}}],[788,{"pageContent":"Theorem 1.11 \nExample 4.16 \nSection 4.2 Determinants \n215 \nCramer's Rule \nLet A be an invertible n X n matrix and let b be a vector in !R\nn\n. Then the unique \nsolution x of the system Ax = b is given by \nX· = \nl \ndet\n(\nA\n;\n(\nb\n)) \ndet A \nfor i = 1, ... , n \nProof The columns of the identity matrix I = I\nn \nare the standard unit vectors e\n1\n, \ne\n2\n, .•• , e\nn\n- If Ax = b, then \nAI\n;\n(\nx\n) \n= A [ e\n1 \nx \n· · · \ne\nn\n] \n= \n[Ae\n1 \n•   •   • Ax \n· · · \nAe\nn\n] \n= \n[ a\n1 \n· · · \nb \n· · · \na\nn\n] = A\n;\n(\nb\n) \nTherefore, by Theorem 4.8, \n(\ndetA\n)(\ndetI\n;\n(\nx\n)) \n= det\n(\nAI\n;\n(\nx\n)) \n= det\n(\nA\n;\n(\nh\n)) \nNow \n1 \n0 \nX\n1 \n0   0 \n0 \n1 \nX\nz \n0   0 \ndetI\n;\n(\nx\n) \n= \n0   0 \nX; \n0   0 \n= X\n; \n0   0 \nX\nn\n-\n1 \n1   0 \n0  0 \nX\nn \n0 \nas can be seen by expanding along the ith row. Thus, (<let A)x; = det(A;(h)), and the \nresult follows by dividing by <let A (which is nonzero, since A is invertible). \nUse Cramer's Rule to solve the system \nSolution We compute \nX\n1 \n+ 2X\n2 \n= 2 \n-x\n1 \n+ 4x\n2 \n= 1 \ndet A \n= \n1\n-\n� \n!\nI\n= \n6,  det\n(\nA\n1\n(\nb\n)) \n=\nI\n�  !\nI\n= \n6,  and det\n(\nA\n2\n(\nb\n)) \n= \n1\n-\n� \n�\nI \nBy Cramer's Rule, \ndet\n(\nA\n1\n(\nb\n)) \n6 \nX\n1 \n= \n----\n= \n-\n= 1 \ndet A     6 \ndet\n(\nA\n2\n(\nb\n)) \nand  x\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":39960,"to":40150}}}}],[789,{"pageContent":"Solution We compute \nX\n1 \n+ 2X\n2 \n= 2 \n-x\n1 \n+ 4x\n2 \n= 1 \ndet A \n= \n1\n-\n� \n!\nI\n= \n6,  det\n(\nA\n1\n(\nb\n)) \n=\nI\n�  !\nI\n= \n6,  and det\n(\nA\n2\n(\nb\n)) \n= \n1\n-\n� \n�\nI \nBy Cramer's Rule, \ndet\n(\nA\n1\n(\nb\n)) \n6 \nX\n1 \n= \n----\n= \n-\n= 1 \ndet A     6 \ndet\n(\nA\n2\n(\nb\n)) \nand  x\n2 \n= \n---­\ndet A \n3 \n6 \n=3 \n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":40150,"to":40226}}}}],[790,{"pageContent":"216 \nChapter 4 \nEigenvalues and Eigenvectors \nRemark As noted previously, Cramer's Rule is computationally inefficient for \nall but small systems of linear equations because it involves the calculation of many \ndeterminants. The effort expended to compute just one of these determinants, using \neven the most efficient method, would be be  tter spent using Gaussian elimination to \nsolve the system directly. \nThe final result of this section is a formula for the inverse of a matrix in terms of \ndetermina\nnts.  This formula was hinted at by the formula for the inverse of a 3 X  3 \nmatrix, which was given without proof at the beginning of this section. Thus, we have \ncome full circle. \nLet's discover the formula for ourselves. If A is an invertible n X n matrix, its \ninverse is the (unique) matrix X that satisfies the equation AX = I. Solving for X one \ncolumn at a time, let \"i be the jth column of X. That is, \nX\n· = \n1 \nTherefore, Ax\nj \n= e\nj\n, and by Cramer's Rule, \nHowever, \na\n11 \na\ni\n2 \na\nz\n, \na\n22 \n<let \n(\nA\n;\n( \ne\n)\n) \n= \na\ni\n,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":40228,"to":40274}}}}],[791,{"pageContent":"column at a time, let \"i be the jth column of X. That is, \nX\n· = \n1 \nTherefore, Ax\nj \n= e\nj\n, and by Cramer's Rule, \nHowever, \na\n11 \na\ni\n2 \na\nz\n, \na\n22 \n<let \n(\nA\n;\n( \ne\n)\n) \n= \na\ni\n, \na\n1\n2 \na\nn\n, \na\nn\n2 \nwhich is the (j, i )-cofactor of A. \ndet\n(\nA/e\n)\n) \ndet A \nith column \n-J, \n0 \na\n1n \n0 \na\n2\nn \na \nJ\nn \n0 a\nnn \n= \n(\n-l\n)\nj\n+\ni\ndet A= C \nJI \nJI \nIt follows that x\nij \n= (1/det A)C\nj\ni\n• so A \n-\ni \n= X = \n(1/ <let A) [C\nj\ni\n] = (1\n/\ndet A) [C\nij\nf. \nIn words, the inverse of A is the transpose of the matrix of cofactors of A, divided by \nthe determinant of A. \nThe matrix \nis called the adjoint (or adjugate) of A and is denoted by adj A. The result we  have \njust proved can be stated as follows.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":40274,"to":40367}}}}],[792,{"pageContent":"Theorem 4.12 \nExample 4.11 \nLemma 4.13 \nSection 4.2 Determinants 211 \nLet A be an invertible n X n matrix. Then \n1 \nA\n-\n1 \n=--adj A \ndet A \nUse the adjoint method to compute the inverse of \nA=\n[\n�\n� \n-�i \n1 3  -3 \nSolution \nWe compute det A = \n-\n2 and the nine cofactors \nC\n11 \n= \n+\nI\n� \n4\n1 \n= -18 \n-3 \nC\n1\n2 \n= \n-\nI\n� \n4\n1 \n=  10 \n-3 \nC\n2\n1 \n= \n-\nI\n� \n-\n1\n1 \nC\n22 \n= \n+\nI\n� \n-\n1\n1 \n= 3 \n=  -2 \n-3 \n-3 \nC\n31 \n= \n+\nI\n� \n-1\n1 \nC\n3\n2 \n= \n-\nI\n� \n-1\n1 \n4 \n= \n10 \n=  -6 \n4 \nC\n1\n3 \n= \n+\nI\n� \nC\n2\n3 \n= \n-\nI\n� \nC\n33 \n= \n+\nI\n� \nThe adjoint is the transpose of the matrix of cofactors-namely, \nadj A = \n[\n-1\n: \n�� \n-\n�\nJ\nr\n=\n[\n-\n�\n� \n-\n� \n��\n1 \n10  -6  -2 \n4 \n-1 -2 \nThen \n�\nI \n= \n4 \n�\nI \n= -1 \n�\nI\n=  -2 \n[\n-1\n8 \n1 1 \nA\n-\n1 \n= \ndet A\nadj A = \n-2 \nl\n� \n3 \n-2 \n-1 \n��\n1 \n= \n[\n-\n� \n-\nt \n-\n:\n1 \n-2 \n-2 \nt \n1 \nwhich is the same answer we obtained (with less work) in Example 3.30. \nProof of lhe Laplace Expansion Theorem \nUnfortunately, there is no short, easy proof of the Laplace Expansion Theorem. The \nproof we give has the merit of being relatively straightforward. We break it down into \nseveral steps, the first of which is to prove that cofactor expansion along the first row","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":40369,"to":40545}}}}],[793,{"pageContent":"proof we give has the merit of being relatively straightforward. We break it down into \nseveral steps, the first of which is to prove that cofactor expansion along the first row \nof a matrix is the same as cofactor expansion along the first column. \nLet A be an n X n matrix. Then","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":40545,"to":40548}}}}],[794,{"pageContent":"218 \nChapter 4 \nEigenvalues and Eigenvectors \nLemma 4.14 \nProof We prove this lemma by induction on n. For n  = 1, the result is trivial. Now \nassume that the result is true for \n(\nn  -  1\n) \nX \n(\nn  -  1\n) \nmatrices; this is our induction \nhypothesis. Note that, by the definition of cofactor (or minor), all of the terms con­\ntaining au are accounted for by the summand au Cu. We can therefore ignore terms \ncontaining au. \nThe ith summand on the right-hand side of Equation (7) is a;\n1 \nC;\n1 \n= a;\n1 (\n-1\n) \ni\n+\nl \ndet A;\n1\n. Now we expand det A;\n1 \nalong the first row: \na\n1\n2 \na\n13 \na\n1\n1 \na\nln \na\n;\n-\n1,\n2 \na\n;\n-\n1,3 \na\n;\n-\n1,\nJ \na\n;\n-\n1,n \na\n;\n+\n1,\n2 \na\n;\n+\n1,3 \na\n;\n+\n1,\nJ \na\n;\n+J,\nn \na\nn\n2 \na\nn3 \na\nn\n) \na\nn,n \nThe \njth term in this expansion of det A;\n1 \nis a\n1\n1(\n-1\n)\n1\n+J\n-\nl \ndet A\n1\n;,\n11\n, \nwhere the nota­\ntion \nA\nk\nl\n,\nr\ns \ndenotes the submatrix of A obtained by deleting rows k and l and columns \nrand s. Combining these, we see that the term containing a;\n1\na\n11 \non the right-hand \nside of Equation \n( \n7) is \na\n;\n1 \n( \n- l\n)\n;\n+\n1\na\n1\n/\n- l\n)\n1\n+J\n-\nI \ndet A\n1\nu\n1 \n=  ( \n- l\n)\ni\n+J +\n1\na\n;\n1\na\n1\n1 \ndet A\n1\n;\n,1\n1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":40550,"to":40711}}}}],[795,{"pageContent":"rand s. Combining these, we see that the term containing a;\n1\na\n11 \non the right-hand \nside of Equation \n( \n7) is \na\n;\n1 \n( \n- l\n)\n;\n+\n1\na\n1\n/\n- l\n)\n1\n+J\n-\nI \ndet A\n1\nu\n1 \n=  ( \n- l\n)\ni\n+J +\n1\na\n;\n1\na\n1\n1 \ndet A\n1\n;\n,1\n1 \nWhat is the term containing a\ni\nl\na\n11 \non the left-hand side of Equation (7)? The \nfactor a\n1\n1 \noccurs in the jth summand, a\n11\nC\n1\ni  = \na\n1\n/-1\n)\n1\n+J \ndet A\n11\n. By the induction \nhypothesis, we can expand det A\n1\ni along its first column: \na\nz 1 \na\nz\n,\nJ\n-\n1 \na\nz\n,\nJ\n+\n1 \na\n2\nn \na\n31 \na\n3,\nj\n-\nI \na\n3,\nj\n+\nI \na\n3n \na\n;\n1 \na\n;\n,\nj\n-\n1 \na\n;\n,\nJ\n+\nl \na\n;\nn \na\nn\n! \na\nn,\nj\n-\n1 \na\nn,\n)+\n! \na\nnn \nThe ith term in this expansion of det A\n1\n1 \nis a\ni\nl \n(\n-l\n)\n(i\n-\nl\n)\n+\nl \ndet A\n1\n;\n,\n11\n, so the term con­\ntaining a;\n1\na\n1\ni on the left-hand side of Equation (7) is \n(-1\n)\n1\n+J \n(-1\n)\n(\n;\n-\ni\n)\n+ i \nd t A \n-\n(-l\n)\ni\n+J +i \nd t A \na\n1\nJ \na\n;\n1 \ne \n1\n;\n,1\n1 \n-\na\nil\na\n1\nJ \ne \n1\n;\n,1\n1 \nwhich establishes that the left- and right-hand sides of Equation (7) are equivalent. \nNext, we prove property (b) of Theorem 4.3. \nLet \nA \nbe an \nn \nX \nn \nmatr  ix and let B be obtained \nby interchanging \nany two rows \n(columns) of \nA. \nThen \ndet B = -det A","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":40711,"to":40928}}}}],[796,{"pageContent":"Section 4.2 Determinants \n219 \nProof Once again, the proof is by induction on n. The result can be easily checked \nwhen n  = 2, so assume that it  is true for \n(\nn - 1) X \n(\nn - 1) matrices. We will prove \nthat the result is true for n X n matrices. First, we prove that it holds when two adja­\ncent rows of A are interchanged-say, rows rand r +  1. \nBy Lemma 4.13, we can evaluate <let B by  cofactor expansion along its first col­\numn. The ith term in this expansion is (-1)\n1 \n+ \ni\nb;\n1 \n<let B;\n1\n. If i * rand i * r + 1, then \nb;\n1 \n= a;\n1 \nand B;\n1 \nis an \n(\nn - 1) X \n(\nn - 1) submatrix that is identical to A;\n1 \nexcept that \ntwo adjacent rows have been interchanged. \na\nll \na\n1\n2 \na\nl\nn \na\n;\n1 \na\n;\n2 \na;\nn \na\nr\n+\nl\n,\nl \na\nr\n+\nl\n,\n2 \na\nr\n+\nl\n,n \na\nr\nl \na\nr\n2 \na\nrn \na\nn\nl \na\nn\n2 \na\nnn \nThus, by the induction hypothesis, <let B;\n1 \n= -<let A;\n1 \nif i * rand i * r + 1. \nIf \ni \n= \nr, then \nb;\n1 \n= \na\nr\n+\nl\n,\nl \nan\nd B\ni\nl \n= \nA\nr\n+\nl\n,\nl\n· \na\nll \na\n1\n2 \nRow i-+ \na\nr\n+\nl\n,\nl \na\nr\n+\nl\n,\n2 \na\nr\nl \na\nr\n2 \na\nn\n] \na\nn\n2 \nTherefore, the rth summand in <let B is \na\nl\nn \na\nr\n+\nl\n,n \na\nrn \na\nnn \n( \n-l\n)\nr\n+\nI\nb\nr\nl \n<let B\nr\n! \n= \n( \n- l\n)\nr\n+\nI\na\nr\n+\nI,I \n<let A\nr\n+ \nI, I \n= -","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":40930,"to":41113}}}}],[797,{"pageContent":"r, then \nb;\n1 \n= \na\nr\n+\nl\n,\nl \nan\nd B\ni\nl \n= \nA\nr\n+\nl\n,\nl\n· \na\nll \na\n1\n2 \nRow i-+ \na\nr\n+\nl\n,\nl \na\nr\n+\nl\n,\n2 \na\nr\nl \na\nr\n2 \na\nn\n] \na\nn\n2 \nTherefore, the rth summand in <let B is \na\nl\nn \na\nr\n+\nl\n,n \na\nrn \na\nnn \n( \n-l\n)\nr\n+\nI\nb\nr\nl \n<let B\nr\n! \n= \n( \n- l\n)\nr\n+\nI\na\nr\n+\nI,I \n<let A\nr\n+ \nI, I \n= -\n( \n- l\n)\n(\nr\n+\nI\n)\n+\nI\na\nr\n+\nI,I \n<let A\nr\n+ \nI, I \nSimilarly, if i = r +  1, then b;\n1 \n= a\nr\n1\n, B;\n1 \n= A\nr\n1\n, and the (r + l)st summand in <let Bis \n(\n- l\n)\n(\nr\n+\nI\n)\n+\nI\nb\nr\n+\nI,I \ndet B\nr\n+\nI,I \n= \n(\n-l\n)\n'\na\nr\nl \ndet A\nr\nl \n= -\n(\n- l\n)\nr\n+\nl\na\nr\nl \ndet A\nr\nl \nIn other words, the rth and (r +   l)st terms in the first column cofactor expansion of \n<let B are the negatives of the ( r + 1 )st and rth terms, respectively, in the first column \ncofactor expansion of <let A. \nSubstituting all of these results into <let B and using Lemma 4.13 again, we obtain \nn \n�\n( \n)\ni\n+\nl \n<let B = \n,L,.; \n-1 b\n;\n1 \n<let B\n;\n1 \ni\n=l \nn \n� \n(\n-l\n)\ni\n+\nl\nb\n;\n1 \n<let B\n;\n1 \n+ \n(\n-l\n)\nr\n+\nl\nb\nr\nl \n<let B\nr1 \n+ \n(\n-l\n)\n(\nr\n+\nJ\n)\n+\nl\nb\nr\n+\nl\n,\nl \n<let B\nr\n+\nl\n,\nl \ni\n=l \ni\n;\"\nr\n,\nr\n+\nl \nn \n� \n(\n-l\n)\ni\n+\n1\na\n;\n1\n(\n-det A\n;\n1\n) \n-\n(\n-l\n)\n(\nr\n+\nl)\n+\nl\na\nr\n+\nl\n,\nl \ndet A\nr\n+\nl\n,\nl \n-\n(\n-l\n)\nr\n+\nl\na\nr1 \ndet A\nr1 \ni\n=l \ni\nt=\nr,r\n+\nl \nn \n�\n( \n)\ni\n+\nl \n-\n,L,.; \n- 1 \na;\n1 \n<let A\n;\n1 \ni\n=l \n-det A","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":41113,"to":41429}}}}],[798,{"pageContent":"280 \nChapter 4 \nEigenvalues and Eigenvectors \nA self-taught child prodigy, \nTakakazu Seki Kowa (1642-1708) \nwas descended from a family of \nsamurai warriors. In addition \nto discovering determinants, \nhe wrote about diophantine \nequations, magic squares, and \nBernoulli numbers (before \nBernoulli) and quite likely made \ndiscoveries in calculus. \nThis proves the result for n X n matrices if adjacent rows are interchanged. To \nsee that it  holds for arbitrary row interchanges, we need only note that, for example, \nrows rands, where r < s, can be swapped by performing 2(s -  r\n) - 1   interchanges of \nadjacent rows (see Exercise 67). Since the number of interchanges is odd and each one \nchanges the sign of the determinant, the net effect is a change of sign, as desired. \nThe proof for column interchanges is analogous, except that we expand along \nrow 1 instead of along column 1. \nWe can now prove the Laplace Expansion Theorem. \nProof of Theorem 4.1 Let B be the matrix obtained by moving row i of A to the top,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":41431,"to":41453}}}}],[799,{"pageContent":"row 1 instead of along column 1. \nWe can now prove the Laplace Expansion Theorem. \nProof of Theorem 4.1 Let B be the matrix obtained by moving row i of A to the top, \nusing i - 1   interchanges of adjacent rows. By Lemma 4.14, <let B = (-1) \ni\n-l \ndetA. But \nb\n1\nj \n= a;\nj \nand B\n1\nj \n= A;\nj \nfor j = 1, ... , n. \nThus, \na\n;\n1 \na;\nj \na\ni\nn \na\nll \na\n1\nj \na\nln \n<let B \n= \na\n;\n-\n1,1 \na;\n-\n1,\nj \na\ni\n-\n1,n \na;\n+\n1,1 \na;\n+\n1,\nj \na\ni\n+\nl,n \na\nnl \na\nn\nj \na\nnn \nn \ndet A = \n(\n-l\n)\ni\n-\n1 \ndet B = \n(\n-l\n)\ni\n-\n1\n2:\n(\n-1\n)\n1\n+\nj\nb\n1\nj\ndet B\n1\nj \nj�\nI \nn n \n= \n(\n-1\n)\n;\n-\ni \n2: \n(\n-1\n)\n1\n+\nj\na\nij \n<let A\nij \n= \n2: \n(\n-l\n)\ni\n+\nj\na\nij \n<let A\nij \nj�\nI \nj�\nI \nwhich gives the formula for cofactor expansion along row i. \nThe proof for column expansion is sim  ilar, invoking Lemma 4.13 so that we can \nuse column expansion instead of row expansion (see Exercise 68). \nA  Brief Hislorv of oe1erminan1s \nAs noted at the beginning of this section, the history of determinants predates that of \nmatrices. Indeed, determinants were first introduced, independently, by Seki in 1683","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":41453,"to":41589}}}}],[800,{"pageContent":"As noted at the beginning of this section, the history of determinants predates that of \nmatrices. Indeed, determinants were first introduced, independently, by Seki in 1683 \nand Leibniz in 1693. In 1748, determinants appeared in Maclaurin's Treatise on Alge­\nbra, which included a treatment of Cramer's Rule up to the 4 X 4 case. In 17 50, Cramer \nhimself proved the general case of his rule, applying it to curve fitting, and in 1772, \nLaplace gave a proof of his expansion theorem. \nThe term determinant was not coined until 1801, when it was used by Gauss. \nCauchy made the first use of determinants in the modern sense in 1812. Cauchy, in \nfact, was responsible for developing much of the early theory of determinants, in­\ncluding several important results that we have mentioned: the prod uct rule for de­\nterminants, the characteristic polynomial, and the notion of a diagonalizable matrix. \nDeterminants did not become widely known until 1841,  when Jacobi popularized","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":41589,"to":41600}}}}],[801,{"pageContent":"terminants, the characteristic polynomial, and the notion of a diagonalizable matrix. \nDeterminants did not become widely known until 1841,  when Jacobi popularized \nthem, albeit in the context of functions of several variables, such as are encountered in \na multivariable calculus course. (These types of determinants were called \"Jacobians\" \nby Sylvester around 1850, a term that is still used today.)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":41600,"to":41604}}}}],[802,{"pageContent":".. \nI \nExercises 4.2 \nSection 4.2 Determinants \n281 \nGottfried Wilhelm von Leibniz (1646-1716) was born in Leipzig and studied law, \ntheology, philosophy, and mathematics. He is probably best known for developing (with \nNewton, independently) the main ideas of differential and integral calculus. However, his \ncontributions to other branches of mathematics are also impressive. He developed the notion \nof a determinant, knew versions of Cramer's Rule and the Laplace Expansion Theorem before \nothers were given credit for them, and laid the foundation for matrix theory through work he \ndid on quadratic forms. Leibniz also was the first to develop the binary system of arithmetic. \nHe believed in the importance of good notation and, along with the familiar notation for \nderivatives and integrals, introduced a form of subscript notation for the coefficients of a \nlinear system that is essentially the notation we use today. \nBy the late 19th century, the theory of determinants had developed to the stage","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":41606,"to":41621}}}}],[803,{"pageContent":"linear system that is essentially the notation we use today. \nBy the late 19th century, the theory of determinants had developed to the stage \nthat entire books were devoted to it, including Dodgson's An Elementary Treatise on \nDeterminants in  1867 and Thomas Muir's monumental five -volume work, which \nappeared in the early 20th century. While their history is fascinating, today deter­\nminants are of theoretical more than practical interest. Cramer's Rule is a hopelessly \ninefficient method for solving a system of linear equations, and numerical methods \nhave replaced any use of determinants in the computation of eigenvalues. Determi­\nnants are used, however, to give students an initial understanding of the characteristic \npolynomial (as in Sections 4.1 and 4.3). \n• \nCompute the determinants in Exercises 1-6 using cofactor \n-4 \n1 \n3 \ncos e \nsin e \ntane \nexpansion along the first row and along the first column. \n9. \n2 \n-2 \n4 \n10. \n0 \ncose \n-\nsine \n1 \n0  3 \n0 \n1 \n-\n1 \n-\n1 \n0 0 \nsine \ncose \n1. 5 \n2. \n2 \n3 \n-\n2 \n0 \n2 \n-1 \n3    0 \na \nb","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":41621,"to":41671}}}}],[804,{"pageContent":"-4 \n1 \n3 \ncos e \nsin e \ntane \nexpansion along the first row and along the first column. \n9. \n2 \n-2 \n4 \n10. \n0 \ncose \n-\nsine \n1 \n0  3 \n0 \n1 \n-\n1 \n-\n1 \n0 0 \nsine \ncose \n1. 5 \n2. \n2 \n3 \n-\n2 \n0 \n2 \n-1 \n3    0 \na \nb \n0 0 a \n0 \n1 \n-1 \n0 \n1  1 \n0 \n11. 0 \na \nb \n12. \nb \nc d \n3. \n-1 \n0 \n4. \n1 \n0 \na \n0 b \n0 \ne \n0 \n0 \n-1 \n0 \n2 \n3 \n2 \n3 \n-1 \n0  3 \n2 \n0  3 \n-1 \n5. 2 \n3  1 \n6. \n4 \n5 \n6 \n2    5 2 \n6 \n1    0 \n2 \n2 \n13. \n14. \n3 \n2 \n7 \n8 \n9 \n0 0 0 0 \n-1 \n4 \n4  2 \n1 \n2 \n0 \n-3 \nCompute th e determinants in Exercises 7-15 using cofactor \nexpansion along any row or column that seems convenient. \n0  0  0 \na \n5  2  2 \n1 \n-1 \nb 0  0 c \n7. \n-1 \n2 \n8. \n2 \n0    1 \n15. \nd \nf \n0 \ne \n3  0  0 3  -2 \ng \nh \nj","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":41671,"to":41794}}}}],[805,{"pageContent":"282 \nChapter 4 \nEigenvalues and Eigenvectors \nIn Exercises 16-18, compute the indicated 3 X 3 determi­\nnants using the method of Example 4. 9. \n16. The determinant in Exercise 6 \n17. The determinant in Exercise 8 \n18. The determinant in Exercise 11 \n19. Verify that the method indicated in (2) agrees with \nEquation ( 1) for a 3 X 3 determinant. \n20. Verify that definition ( 4) agrees with the definition of a \n2 \nX 2 determinant when n = 2. \n21. Prove Theorem 4.2. [Hint: A proof by induction would \nbe appropriate here.] \nIn Exercises 22-25, evaluate the given determinant using \nelementary row and/or column operations and Theorem 4.3 \nto reduce th e matrix to row echelon form. \n22. The determinant in Exercise 1 \n23. The determinant in Exercise 9 \n24. The determinant in Exercise 13 \n25. The determinant in Exercise 14 \nIn Exercises 26-34, use properties of determinants to \nevaluate th e given determinant by inspection. Explain \nyour reasoning. \n1  1    1 \n3 \n1 \n0 \n26.  3 \n0 \n-2 \n27. 0 \n-2 \n5 \n2  2    2 \n0 \n0 \n4 \n0 \n0 \n1 \n2 \n3 \n-4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":41796,"to":41840}}}}],[806,{"pageContent":"In Exercises 26-34, use properties of determinants to \nevaluate th e given determinant by inspection. Explain \nyour reasoning. \n1  1    1 \n3 \n1 \n0 \n26.  3 \n0 \n-2 \n27. 0 \n-2 \n5 \n2  2    2 \n0 \n0 \n4 \n0 \n0 \n1 \n2 \n3 \n-4 \n28. 0 \n5 \n2 \n29. \n-\n3 \n-\n2 \n3 \n- 1 \n4 \n-1 \n5 \n2 \n2 \n3 \n4 \n1 \n3 \n30. 0 \n4 \n1 \n31. \n-2 \n0 \n-2 \n6  4 \n5 \n4 \n1 \n1  0  0 0 \n0 \n2 \n0  0 \n0 \n0  1 \n0 \n-3 \n0  0 0 \n32. \n33. \n0  1 0  0 0  0  0 \n4 \n0  0  0 1 0  0 0 \n1  0 1  0 \n0 0  1 \n34. \n1  1  0  0 \n0  0 \n1 \nFind the determinants in Exercises 35-40, assuming that \na b     c \nd \ne    f   = 4 \ng     h \n2a 2b 2c \n35. d e f \ng h \nd     e    f \n37. a b     c \ng     h \n2c b     a \n39. 2f e d \n2i h     g \na+ 2g \n40. 3d + 2g \ng \nb  + 2h \n3e + 2h \nh \n2a b/3 -c \n36. 2d e/3 -f \n2g h/3 -i \na  -  c     b     c \n38. d  - f     e    f \ng -  i h \nc  + 2i \n3f + 2i \n41. Prove Theorem 4.3(a). \n42. Prove Theorem 4.3(f). \n43. Prove Lemma 4.5. 44. Prove Theorem 4.7. \nIn Exercises 45 and 46, use Theorem 4.6 to find all values of \nk for which A is invertible. \n[\n1 \n-k \nk � 1: \n45. A = \nk \n+ 1 \n-8 \n46. A � \n[\n� \nk \n�] \n2 \nk","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":41840,"to":41960}}}}],[807,{"pageContent":"43. Prove Lemma 4.5. 44. Prove Theorem 4.7. \nIn Exercises 45 and 46, use Theorem 4.6 to find all values of \nk for which A is invertible. \n[\n1 \n-k \nk � 1: \n45. A = \nk \n+ 1 \n-8 \n46. A � \n[\n� \nk \n�] \n2 \nk \nIn Exercises 47-52, assume that A and Bare n X n matrices \nwith det A = 3 and det B = - 2. Find the indicated \ndeterminants. \n47. det\n(\nAB\n) \n48. det\n(\nA\n2\n) \n49. det\n(\nB\n-\n1\nA\n) \n50. det\n(\n2A\n) \n51. det\n(\n3B\nr\n) \n52. det\n(\nAA r) \nIn Exercises 53-56, A and B are n X n matrices. \n53. Prove that det(AB) = det(BA). \n54. If Bis invertible, prove that det(B\n-\n1\nAB) = det(A  ). \n55. If   A is idempotent (that is, A \n2 \n= A), find all possible \nvalues of det(A  ). \n56. A square matr  ix A  is called nilpotent if A\nm \n= 0 for \nsome m > 1. (The word nilpotent comes from the \nLatin nil, meaning \"nothing;' and potere, meaning \n\"to have power:' A nilpotent matrix is thus one that","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":41960,"to":42024}}}}],[808,{"pageContent":"becomes \"nothing\" -that is, the zero matrix-when \nraised to some power.) Find all possible values of \ndet(A) if A is nilpotent. \nIn Exercises 57-60, use Cramer's Rule to solve the given \nlinear system. \n57. x + y =  1 \nx-y=2 \n59.2x + y  + 3z =  1 \ny  +    z =  1 \nz =  1 \n58. 2x -y =   5 \nx  + 3y =  -1 \n60. \nx  + y  -  z =  1 \nx+y+z=2 \nx-y \n=  3 \nIn Exercises 61-64, use Theorem 4.12 to compute the in­\nverse of the coefficient matrix for th e given exercise. \n61. Exercise 57 \n62. Exercise 58 \n63. Exercise 59 \n64. Exercise 60 \n65. If A is an invertible n X n matrix, show that adj A is \nalso invertible and that \n1 \n(\nadj A\n)-\n1 \n=\n--\nA =   adj \n(\nA\n-\n1\n) \ndet A \n66. If A is an n X n matrix, prove that \ndet\n(\nadj A\n) \n= \n(\ndet A\n)\nn\n-\nI \n67. Verify that if r < s, then rows rand s of a matrix can \nbe interchanged by performing 2\n(\ns -  r\n) \n- 1   inter­\nchanges of adjacent rows. \n68. Prove that the Laplace Expansion Theorem holds for \ncolumn expansion along the jth column. \n69. Let A be a square matr  ix that can be partitioned as \nA = \n[\n-�-f-\n�\n-\n]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":42026,"to":42092}}}}],[809,{"pageContent":"changes of adjacent rows. \n68. Prove that the Laplace Expansion Theorem holds for \ncolumn expansion along the jth column. \n69. Let A be a square matr  ix that can be partitioned as \nA = \n[\n-�-f-\n�\n-\n] \nSection 4.2 Determinants \n283 \nwhere P and S are square matrices. Such a matrix is \nsaid to  be in block (upper) triangular fo rm. Prove that \ndet A = \n(\ndet P\n)(\ndet S\n) \n[Hint: Try a    proof by induction on the number of rows \nof P.] \n70. (a) Give an example to show that if A can be \npartitioned as \nA= \n[\n�- -f{j \nwhere P, Q, R, and S are all square, then it is not \nnecessarily true that \ndet A = \n(\ndet P\n)(\ndet S\n) \n-\n(\ndet Q\n)(\ndet R\n) \n(b) Assume that A is partitioned as in part (a) and \nthat Pis invertible. Let \nB = \n[ \n��-���-d-?-\n] \nCompute det (BA) using Exercise 69 and use the \nresult to show that \ndetA =   det P det\n(\nS -  RP\n-\n1\nQ\n) \n[The matrix S - RP\n-\n1\nQ is called the Schur com­\nplement of Pin A, after Issai Schur (1875-1941) , \nwho was born in Belarus but spent most of his \nlife in Germany. He is known mainly for his fun­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":42092,"to":42154}}}}],[810,{"pageContent":"-\n1\nQ\n) \n[The matrix S - RP\n-\n1\nQ is called the Schur com­\nplement of Pin A, after Issai Schur (1875-1941) , \nwho was born in Belarus but spent most of his \nlife in Germany. He is known mainly for his fun­\ndamental work on the representation theory of \ngroups, but he also worked in number theory, \nanalysis, and other areas.] \n(c) Assume that A is partitioned as in  part (a), that \nPis invertible, and that PR = RP. Prove that \ndetA =   det\n(\nPS - RQ\n) \nWriting Project \nWhich Came First: The Matrix or the Determinant? \nThe way in which matrices and determinants are taught today-matrices before \ndeterminants-bears little resemblance to the way these topics developed histori­\ncally. There is a brief history of determinants at the end of Section 4.2. \nWrite a report on the history of matrices and determinants. How did the nota­\ntions used for each evolve over time? Who were some of the key mathematicians \ninvolved and what were their contributions?","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":42154,"to":42181}}}}],[811,{"pageContent":"Write a report on the history of matrices and determinants. How did the nota­\ntions used for each evolve over time? Who were some of the key mathematicians \ninvolved and what were their contributions? \n1. Florian Cajori, A History of Mathematical Notations (New York: Dover, 1993). \n2. \nHoward Eves, An Introduction to th e History of Mathematics (Sixth Edition) \n(Philadelphia: Saunders College Publishing, 1990). \n3. Victor J. Katz, A History of Mathematics: An Introduction (Third Edition) \n(Reading, MA: Addison Wesley Longman, 2008). \n4. Eberhard Knobloch, Determinants, in Ivor Grattan-Guinness, ed., Compan­\nion Encyclopedia of th e History and Philosophy of th e Mathematical Sciences \n(London: Routledge, 2013).","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":42181,"to":42192}}}}],[812,{"pageContent":"Charles Lutwidge Dodgson (1832-\n1898) is much better known by his \npen name, Lewis Carroll, under \nwhich he wrote Alice's Adventures \nin Wonderland and Through the \nLooking Glass. He also wrote several \nmathematics books and collections \noflogic puzzles. \nThis vignette is based on the article \n\"Lewis Carroll's Condensation \nMethod for Evaluating Determinants\" \nby Adrian Rice and Eve Torrence in \nMath Horizons, November 2006, \npp. 12-15. For further details of \nthe condensation method, see \nDavid M. Bressoud, Proofs and \nConfirmations: The Story of  the \nAlternating Sign Matrix Conjecture, \nMAA Spectrum Series (Cambridge \nUniversity Press, 1999). \n284 \nVignette \nLewis Carroll's Condensation Method \nIn  1866,  Charles Dodgson-better  known by  his  pseudonym Lewis Carroll­\npublished his only mathematical research paper. In it,   he described a \"new and brief \nmethod\" for computing determinants, which he called \"condensation:' Although not","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":42194,"to":42219}}}}],[813,{"pageContent":"published his only mathematical research paper. In it,   he described a \"new and brief \nmethod\" for computing determinants, which he called \"condensation:' Although not \nwell known today and rendered obsolete by numerical methods for evaluating determi­\nnants, the condensation method is very useful for hand calculation. When calculators or \ncomputer algebra systems are not available, many students find condensation to be their \nmethod of choice. It requires only the ability to compute 2 X 2    determinants. \nWe require the following terminology. \nDefinition \nIf A is an n x n matrix with n  2: 3, the interior of A, denoted \nint(A), is the (n - 2\n) \nX (n - 2\n) \nmatrix obtained by deleting the first row, last \nrow, first column, and last column of A. \nWe will illustrate the condensation method for the 5 X 5 matrix \n2 \n3 \n-1 \n2 \n0 \n2 \n3  1 \n-4 \nA= \n2 \n-1 \n2 \n1    1 \n3    1 \n-\n1 \n2 \n-2 \n-4 \n0 \n2 \nBegin by setting A\n0 \nequal to the 6 X 6 matrix all of whose entries are 1. Then, we set \nA\n1 \n=A. It is useful to imagine A\n0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":42219,"to":42262}}}}],[814,{"pageContent":"2 \n3 \n-1 \n2 \n0 \n2 \n3  1 \n-4 \nA= \n2 \n-1 \n2 \n1    1 \n3    1 \n-\n1 \n2 \n-2 \n-4 \n0 \n2 \nBegin by setting A\n0 \nequal to the 6 X 6 matrix all of whose entries are 1. Then, we set \nA\n1 \n=A. It is useful to imagine A\n0 \nas the base of a pyramid with A\n1 \ncentered on top of \nA\n0\n. We are going to add successively smaller and smaller layers to the pyramid until \nwe reach a 1 X 1 matrix at the top-this will contain <let A. (Figure 4.9) \nAo \nFigure 4.9","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":42262,"to":42298}}}}],[815,{"pageContent":"Next, we \"condense\" A\n1 \ninto a 4 X 4 matrix A; whose entries are the determinants of \nall 2 X 2 submatrices of A \n1\n: \nI \n2 \n� \n11 \n3 \n-\n�\n11\n-\n� \n�I I� \n-\n�\nI \n2 \nI \n-\n�\n11\n-\n� \n� \n11 \n3 \n� \n11 \n� \n-\n�\n1 \n[\n-\nj \n11 \n-7 \n-\n:\n1 \nA; = \n2 2 \n7 \n1 \n-1 \n5 \n-4 \nI \n2 \n-\n1\n11\n-\n1    2\n11 \n2  1\n11\n1 \n-\n�\nI \n1 \n-1 \n6 \n3 \n1    1 -1 -1  2   2 \n1\n-\n! \n� \n11 \n1 \n-\n�\n11\n-\n�  �I I� \n-\n�\n1 \n1 \nNow we divide each entry of A; by the corresponding entry of int(A0) to get matrix \nA\n2\n. Since A0 is all 1 s, this means A\n2 \n= A;. \nWe repeat the procedure, constructing A� from the 2 X 2 submatrices of A\n2 \nand then \ndividing each entry of A� by the corresponding entry of int(A \n1\n), and so on. We obtain: \n1\n-\n� \n1\n�\nl \nI \n1\n� \n-\n;\n11\n-\n; \n-\n:\n1 \nA� = \n1\n-\n: \n_;\n11\n_; \n�\nI I \n� \n-\n!\nI \nA\n' \n4 \n1\n31 \n30 \n60\n/3 \n36/2 \n-4/-1 \n20\n11\n20 \n18 \n18 \n-27\n1 \n-29 \n1\n30  18\n11\n18  -29\n1 \n12 \n4    4 \n13 \n=\n�\n��� l = \n[\n�\n� \n�\n� \n26\n/\n2 \n12 4 \n= \n[\n-\n42 \n-\n96 \n-\n94 \n] \n350 \n, \n[ \n-42/7 \n-94/1 ] \n= \n[\n-6 \n-96/\n( \n-1\n) \n350\n/\n5 \n96 \n-94] \n70 \nA\n� \n= \n[\n1\n-\n:\n6 \n-�\n�\nI\n]\n= \n[8\n60\n4\n], \nA5 = [8604/\n18] = [478] \n-\n27\n] \n-29 , \n26 \n-\n27\n] \n-29 , \n13 \nAs can be checked by other methods, <let A = 478. In general, for an n X n matrix A,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":42300,"to":42521}}}}],[816,{"pageContent":"= \n[\n-6 \n-96/\n( \n-1\n) \n350\n/\n5 \n96 \n-94] \n70 \nA\n� \n= \n[\n1\n-\n:\n6 \n-�\n�\nI\n]\n= \n[8\n60\n4\n], \nA5 = [8604/\n18] = [478] \n-\n27\n] \n-29 , \n26 \n-\n27\n] \n-29 , \n13 \nAs can be checked by other methods, <let A = 478. In general, for an n X n matrix A, \nthe condensation method will produce a 1 X 1 matrix A\nn \ncontaining <let A. \nClearly, the method breaks down if the interior of any of the A; s contains a zero, \nsince we would then be trying to divide by zero to construct A;+ \n1\n. However, careful \nuse of elementary row and column operations can be used to eliminate the zeros so \nthat we can proceed. \n285","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":42521,"to":42573}}}}],[817,{"pageContent":"286 \nExploration \nGeometric Applications of Determinants \nThis exploration will reveal some of the amazing applications of determinants to \ngeometry. In particular, we will see that determinants are closely related to area and \nvolume formulas and can be used to prod uce the equations of lines, planes, and \ncertain other curves. Most of these ideas arose when the theory of determinants was \nbeing developed as a subject in its own right. \nThe Cross Product \nRecall from Exploration: The Cross Product in Chapter 1 that the cross product \nof u  � [ ::J ond v  � [ :: ] ;, thrndor\nn \nX v defined by \n[\nU\n2\nV\n3 \n-\nU\n3\nV\n2\n] \nU X V = \nU\n3\nV\n1 \n-\nU\n1\nV\n3 \nU\n1\nV\n2 \n-\nU\n2\nV\n1 \nIfwe write this cross product as (u\n2\nv\n3 \n-  u\n3\nv\n2\n)e 1  -(u\n1\nv\n3 \n-  u\n3\nv\n1\n)e\n2 \n+ (u\n1\nv\n2 \n-  u\n2\nv\n1\n)e\n3\n, \nwhere e\n1 , e\n2\n, and e\n3 \nare the standard basis vectors, then we see that the form of this \nformula is \nif we expand along the first column. (This is not a proper determinant, of course, \nsince e\n1 , e\n2\n, and e\n3 \nare vectors, not scalars; however, it gives a useful way of remem­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":42575,"to":42660}}}}],[818,{"pageContent":"formula is \nif we expand along the first column. (This is not a proper determinant, of course, \nsince e\n1 , e\n2\n, and e\n3 \nare vectors, not scalars; however, it gives a useful way of remem­\nbering the somewhat awkward cross product formula. It also lets us use properties of \ndeterminants to verify some of the properties of the cross product.) \nNow let's revisit some of the exercises from Chapter 1.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":42660,"to":42670}}}}],[819,{"pageContent":"y \nFioure 4.10 \n1. \nUse the determinant version of the cross product to compute u X   v. \n(\na\n)\nu � \n[\n}\n�\nH\nJ \n(\nb\n)\nu � \n[\n-\nl\n� \nm \n(o) \nu \n� \nn \nl \nv � \n[ \n=\n: \ni \n(\nd\n) \nu \n� \n[ \nl \n� \nm \n2. Ifu \n� \n[:\nl � \n[\n::land w \n� \n[\n:\nl\nhow that \n[\nU\n1 \nU  • \n( \nV X W) = det U2 \nU3 \n3. Use properties of determinants (and Problem 2 above, if necessary) to prove \nthe given property of the cross product. \n(\na\n) \nv X u = \n-\n(\nu X v\n) \n(\nc\n) \nu X u =  0 \n(\nb\n) \nu x 0  =  0 \n(\nd\n) \nu X kv  = k\n(\nu X v\n) \n(\ne\n) \nu X \n(\nv + w\n) \n= u Xv+ u X w \n(\nf\n) \nu · \n(\nu Xv\n) \n= 0 and v· \n(\nu Xv\n) \n= 0 \n(\ng\n) \nu · \n(\nv X w\n) \n= \n(\nu X v\n) \n· w \n(\nthe triple scalar product identity\n) \nArea and Volume \nWe can now give a geometric interpretation of the determinants of 2 X 2 and 3 X 3 \nmatrices. Recall that if u and v are vectors in IFR\n3\n, then the area A of the parallelogram \ndetermined by these vectors is given by A = \nII \nu X v \n11\n. (See Exploration: The Cross \nProduct in Chapter 1.) \n4.  Let u = \n[ \n�\n:] \nand v = \n[ \n:\n:]\n. Show that the area A of the parallelogram \ndetermined by u   and v is given by \n[H;nt w,;,, u and v\n\"\n' [ �: ] and [�:Ji","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":42672,"to":42814}}}}],[820,{"pageContent":"Product in Chapter 1.) \n4.  Let u = \n[ \n�\n:] \nand v = \n[ \n:\n:]\n. Show that the area A of the parallelogram \ndetermined by u   and v is given by \n[H;nt w,;,, u and v\n\"\n' [ �: ] and [�:Ji \n5.  Derive the area formula in Problem 4 geometrically, using Figure 4.10 as a \nguide. [Hint: Subtract areas from the large rectangle until the parallelogram remains.] \nWhere does the absolute value sign come from in this case? \n281","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":42814,"to":42831}}}}],[821,{"pageContent":"Figure 4.12 \n288 \nvxw \nh{ \nv \nFigure 4.11 \n6. \nFind the area of the parallelogram determined by u and v. \n(\na\n)\nu \n= \n[\n�\n],\nv = \n[\n-\n�\n] \n(\nb\n)\nu = \n[\n!\nl\nv \n= \n[\n�\n] \nGeneralizing from Problems 4-6, consider a parallelepiped, a three-dimensional \nsolid resembling a \"slanted\" brick, whose six faces are all parallelograms with oppo­\nsite faces parallel and congruent (Figure 4.11). Its volume is given by the area of its \nbase times its height. \n7.  Prove that the volume V of the parallelepiped determined by u, v, and w is \ngiven by the absolute value of the determinant of the 3 X 3 matrix [ u    v w] with u, \nv, and was its columns. [Hint: From Figure 4.11 you can see that the height h can be \nexpressed as h = \nll\nu\nll \ncos e, where e is the  angle between u and v X w. Use this fact to \nshow that V = \nl\nu · \n(\nv X w\n) \nI \nand apply the result ofFroblem 2.] \n8.  Show that the volume V of the tetrahedron determined by u, v, and w \n(Figure 4.12) is given by \nV =  i\nl\nu·\n(\nv X w\n)\nI","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":42833,"to":42894}}}}],[822,{"pageContent":"show that V = \nl\nu · \n(\nv X w\n) \nI \nand apply the result ofFroblem 2.] \n8.  Show that the volume V of the tetrahedron determined by u, v, and w \n(Figure 4.12) is given by \nV =  i\nl\nu·\n(\nv X w\n)\nI \n[Hint: From geometry, we know that the volume of such a solid is V = \nt \n(area of the \nbase) (height).] \nNow let's view these geometric interpretations from a transformational point of \nview. Let A be a 2 X 2 matrix and let P be the parallelogram determined by the vectors \nu and v. We will consider the effect of the matrix transformation T\nA \non the area of P. \nLet T\nA\n(\nP\n) \ndenote the parallelogram determined by T\nA\n(\nu\n) \n=Au and T\nA\n(v\n) \n=Av. \n9. \nProve that the area of T\nA\n(\nP\n) \nis given by I det Al (area of P\n)\n. \n10. \nLet A be a 3 X 3 matrix and let P be the parallelepiped determined by the \nvectors u, v, and w. Let T\nA\n(\nP\n) \ndenote the parallelepiped determined by T\nA\n(\nu\n) \n=\nAu, \nT\nA\n(v\n) \n= \nAv, and T\nA\n(\nw\n) \n= Aw. Prove that the volume of T\nA\n(\nP\n) \nis given by ldet A I \n(volume of P\n)\n.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":42894,"to":42976}}}}],[823,{"pageContent":"vectors u, v, and w. Let T\nA\n(\nP\n) \ndenote the parallelepiped determined by T\nA\n(\nu\n) \n=\nAu, \nT\nA\n(v\n) \n= \nAv, and T\nA\n(\nw\n) \n= Aw. Prove that the volume of T\nA\n(\nP\n) \nis given by ldet A I \n(volume of P\n)\n. \nThe preceding problems illustrate that the  determinant of a matrix captures what \nthe corresponding matrix transformation does to the area or volume of figures upon \nwhich the transformation acts. (Although we have considered only certain types of fig­\nures, the result is perfectly general and can be made rigorous. We will not do so here.)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":42976,"to":43010}}}}],[824,{"pageContent":"lines and Planes \nSuppose we are given two distinct points \n(\nx\n1\n, y\n1\n) \nand \n(\nx\n2\n, y\n2\n) \nin the plane. There is a \nunique line passing through these points, and its equation is of the form \nax+ by+  c  = 0 \nSince the two given points are on this line, their coordinates satisfy this equation. \nThus, \nax\n1 \n+ by\n1 \n+  c  = 0 \nax\n2 \n+ by\n2 \n+  c  = 0 \nThe three equations together can be  viewed as a system of linear equations in the vari­\nables a, b, and c. Since there is a nontrivial solution (i.e., the line exists), the coefficient \nmatrix \ncannot be invertible, by the Fundamental Theorem of Invertible Matrices. Conse­\nquently, its determinant must be zero, by Theorem 4.6. Expanding this determinant \ngives the equation of the line. \nThe equation of the line through the points \n(\nx\n1\n, y\n1\n) \nand \n(\nx\n2\n, y\n2\n) \nis given by \n11.  Use the method described above to find the equation of the line through the \ngiven points. \n(\na\n) (\n2, 3\n) \nand \n(\n-\n1, 0\n)    (\nb\n) (\n1, 2\n) \nand \n(\n4, 3\n) \n12.  Prove that the  three points \n(\nx\ni\n, \nY\ni\n)\n, \n(\nx\n2\n, y\n2\n)\n, and \n(\nx\n3\n, y\n3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":43012,"to":43103}}}}],[825,{"pageContent":"given points. \n(\na\n) (\n2, 3\n) \nand \n(\n-\n1, 0\n)    (\nb\n) (\n1, 2\n) \nand \n(\n4, 3\n) \n12.  Prove that the  three points \n(\nx\ni\n, \nY\ni\n)\n, \n(\nx\n2\n, y\n2\n)\n, and \n(\nx\n3\n, y\n3\n) \nare collinear (lie on \nthe same line) if and only if \nX\ni \nY\ni \n1 \nX\nz \nY\n2 \n1  = 0 \nX\n3 \nY\n3 \n13.  Show that the equation of the plane through the three noncollinear points \n(\nx\n1\n, \nY\n1\n, z\n1\n) , \n(\nx\n2\n, y\n2\n, z\n2\n) , \nand \n(\nx\n3\n, y\n3\n, z\n3\n) \nis given by \nx y z \nX\ni \nY\n1 \nZ\ni \nX\nz \nY\n2 \nZz \nX\n3 \nY\n3 \nZ\n3 \n=O \nWhat happens if the three points are collinear? [Hint: Explain what happens when \nrow reduction is used to evaluate the determinant.] \n289","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":43103,"to":43209}}}}],[826,{"pageContent":"y \n�+--+--+--+--+--+---+--+--+---. x \n-2 \n2  4 \n6 \nFigure 4.13 \n290 \n14.  Prove that the four points \n(\nx\n1\n, y\n1\n, z\n1\n), (\nx\n2\n, y\n2\n, z\n2\n)\n, \n(\nx\n3\n, y\n3\n, z\n3\n)\n, and (x\n4\n, y \n4\n, z\n4\n) are \ncoplanar (lie in the same plane) if and only if \nX1 \nY\n1 \nZ\n1 \nX\nz \nY\n2 \nZz \n=O \nX\n3 \nY\n3 \nZ\n3 \nX\n4 \nY4 \nZ\n4 \nCurve Filling \nWhen data arising from experimentation take the form of points \n(\nx, y) \nthat can be \nplotted in the plane, it is  often of interest to find a relationship between the variables x \nand y. Ideally, we would like to find a function whose graph passes through all of the \npoints. Sometimes all we want is an approximation (see Section 7.3), but exact results \nare also possible in certain situations. \n15.  From Figure 4.13 it appears as though we may be able to find a parabola \npassing through the points A( -1, 10), \nB(O, 5), and C\n(\n3, 2\n)\n. The equation of such a \nparabola is of the form y = \na + bx +  cx\n2\n. By substituting the given points into this \nequation, set up a   system of three linear equations in the variables \na\n, b, and c. Without","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":43211,"to":43295}}}}],[827,{"pageContent":"parabola is of the form y = \na + bx +  cx\n2\n. By substituting the given points into this \nequation, set up a   system of three linear equations in the variables \na\n, b, and c. Without \nsolving the system, use Theorem 4.6 to argue that it must have a unique solution. Then \nsolve the system to find the equation of the parabola in Figure 4.13. \n16.  Use the method of Problem 15 to find the polynomials of degree at most 2 \nthat pass through the following sets of points. \n(a) A(l, -1), B(2, 4), C\n(\n3, 3\n) \n(b) A(-1, -3),B(\nl\n, -1), C\n(\n3, 1) \n17. \nGeneralizing from Problems 15 and 16, suppose a\n1\n, a\n2\n, and \na\n3 \nare distinct \nreal numbers. For any real numbers b\n1\n,  b\n2\n, and b\n3\n, we want to show that there is a \nunique quadratic with equation of the form y \n= \na + bx +  cx\n2 \npassing through the \npoints \n(\na\n1\n,  b\n1\n)\n, \n(\na\n2\n,  b\n2\n)\n, and \n(a\n3\n,  b\n3\n)\n. Do this by demonstrating that the coefficient \nmatrix of the associated linear system has the determinant \na\n1 \na\ni \na\n2 \na� \n= (\na\n2 \n- a\n1\n)(\na\n3 \n- a\n1\n)(\na\n3 \n- a\nz\n) \na\n3 \na�","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":43295,"to":43383}}}}],[828,{"pageContent":"points \n(\na\n1\n,  b\n1\n)\n, \n(\na\n2\n,  b\n2\n)\n, and \n(a\n3\n,  b\n3\n)\n. Do this by demonstrating that the coefficient \nmatrix of the associated linear system has the determinant \na\n1 \na\ni \na\n2 \na� \n= (\na\n2 \n- a\n1\n)(\na\n3 \n- a\n1\n)(\na\n3 \n- a\nz\n) \na\n3 \na� \nwhich is necessarily nonzero. (Why?) \n18.  Let a\n1\n, a\n2\n, \na\n3\n, and \na\n4 \nbe distinct real numbers. Show that \na\n1 \na\ni \nai \na\nz \na� \na\n3 \n2 \n= (\na\n2 \n- a\n1\n)(\na\n3 \n- a\n1\n)(\na\n4 \n- a\n1\n)(\na\n3 \n- a\n2\n)(\na\n4 \n- a\n2\n)(\na\n4 \n- a\n3\n) * 0 \na\n3 \na� \na� \na\n4 \na\nz \n4 \na\n3 \n4 \nFor any real numbers b\n1\n, b\n2\n, b\n3\n, and b\n4\n, use this result to prove that there is a unique \ncubic with equation y = a + bx + cx\n2 \n+ dx\n3 \npassing through the four points \n(\na\n1\n, b\n1\n)\n, \n(\na\n2\n, b\n2\n)\n, (a\n3\n, b\n3\n)\n, and \n(a\n4\n, b\n4\n)\n. (Do not actually solve for \na\n, b, c, and d.\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":43383,"to":43538}}}}],[829,{"pageContent":"19.  Let \na 1 , a\n2\n, ••• , a\nn \nbe n real  numbers. Prove that \na\n1 \na\ni \nn\n-\n1 \na\n1 \na\n1 \na� \nn\n-\n1 \na\nz \na\n3 \na\n� \nn\n-\n1 \na\n3 \nII \n( a\nj \n-  a\n;\n) \nl 5'\ni\n<\nj\n-:5\nn \na\nn \na\nz \nn\n-\n1 \nn \na\nn \nw\nher\ne \nTI\n1\n:s\ni\n<J\n:s\nn \n(\na1 \n-a;\n) \nmeans the product of all terms of the form \n(\na\n1 \n-a;\n)\n, where \ni < j and both i and j are between 1 and n. \n[\nThe determinant of a matrix of this form \n(or its transpose) is called a Va ndermonde  determinant, named after the French \nmathematician A. T. Vandermonde (1735-1796).] \nDeduce that for any n points in the plane whose x-coordinates are all distinct, \nthere is a unique polynomial of degree n  -1 whose graph passes through the given \npoints. \n291","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":43540,"to":43623}}}}],[830,{"pageContent":"292 \nChapter 4 \nEigenvalues and Eigenvectors \n' \nExample 4.18 \nEigenvalues and Eigenvectors ot n  x  n Matrices \nNow that we have defined the determinant of an n X n matrix, we can continue our \ndiscussion of eigenvalues and eigenvectors in a general context. Recall from Section 4.1 \nthat A is an eigenvalue of A if and only if A -AI is noninvertible. By Theorem 4.6, this \nis true if and only if det(A -AI\n) \n= 0. To  summarize: \nThe eigenvalues of a square matr  ix A   are precisely the solutions A of the equation \ndet\n(\nA -AI)  = 0 \nWhen we expand det(A -AI\n)\n, we get a polynomial in A, called the characteristic \npolynomial of A. The equation det(A -AI\n) \n= 0 is called the characteristic equation \nof A. For example, if A = \n[\n:  �\n], \nits characteristic polynomial is \ndet\n(\nA -AI\n)  = \n= (\na  -  i\\\n)(\nd -  A\n) \n-  b e  = A\n2 \n-  (\na  + d\n)\ni\\  + \n(\nad -  b e\n) \nla-A b \nI \ne d -  i\\ \nIf A is n X n, its characteristic polynomial will be of degree n. According to the Fun­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":43625,"to":43673}}}}],[831,{"pageContent":"det\n(\nA -AI\n)  = \n= (\na  -  i\\\n)(\nd -  A\n) \n-  b e  = A\n2 \n-  (\na  + d\n)\ni\\  + \n(\nad -  b e\n) \nla-A b \nI \ne d -  i\\ \nIf A is n X n, its characteristic polynomial will be of degree n. According to the Fun­\ndamental Theorem of Algebra (see Appendix D), a polynomial of degree n with real \nor complex coefficients has at most n distinct roots. Applying this fact to the charac­\nteristic polynomial, we see that an n X n matrix with real or complex entries has at \nmost n distinct eigenvalues. \nLet's summarize the procedure we will follow (for now) to find the eigenvalues \nand eigenvectors ( eigenspaces) of a matrix. \nLet A be an n X n matrix. \n1. Compute the characteristic polynomial det(A -U) of A. \n2. Find the eigenvalues of A by solving the characteristic equation det (A -AI\n) \n= 0 \nfor i\\. \n3. For each eigenvalue A, find the null space of the matrix A  -   i\\I. This is \nthe eigenspace E\nA\n, the nonzero vectors of which are the eigenvectors of A \ncorresponding to A. \n4. Find a basis for each eigenspace.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":43673,"to":43712}}}}],[832,{"pageContent":"the eigenspace E\nA\n, the nonzero vectors of which are the eigenvectors of A \ncorresponding to A. \n4. Find a basis for each eigenspace. \nFind the eigenvalues and the corresponding eigenspaces of","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":43712,"to":43717}}}}],[833,{"pageContent":"Section 4.3 \nEigenvalues and Eigenvectors of n X n Matrices \n293 \nSolution \nWe follow the procedure outlined previously. The characteristic polynomial is \n1 \n0 \ndet\n(\nA -   AI\n) \n= \n-;\\ \n0 \n-;\\ \n1 \n2 \n-5 4  - ;\\ \nI\n-\nA 1 \nI \n1\n0 1 \nI \n= -;\\ \n-5 4  - ;\\ \n-\n2 4  - ;\\ \n= -;\\\n(;\\\n2 \n-  4;\\ + 5\n) \n-\n(\n-2\n) \n=  -A3 + 4A\n2 \n-\nSA + 2 \nTo find the eigenvalues, we need to solve the characteristic equation det(A - AI) = 0 \nfor A. The  characteristic polynomial factors as \n-\n(A -  1\n)\n2\n(\n;\\ - 2). (The Factor \nTheorem is helpful here; see Appendix D.)  Thus, the characteristic equation is \n-(\n;\\ -  1\n)\n2\n(\n;\\ - 2) = 0, which clearly has solutions ;\\ = 1 and ;\\ = 2. Since ;\\ = 1 is \na multiple root and A = 2   is a simple root, let us label them ;\\1 = ;\\\n2 \n= 1 and A3 = 2. \nTo find the eigenvectors corresponding to ;\\1 = ;\\\n2 \n= 1, we find the null space of \nn \n1 \n4 \n� \nJ \nn \n!\n] \nA -   II = \n-1 \n-1 \n-5 \n-5 \nRow reduction produces \nn \n1 \n0 \n�\n] \n[\n: \n0 \n-1 \n�\n] \n[A-I\nI OJ = \n-1 \n1 \n-----+ \n1 \n-1 \n-5 \n3 \n0    0 \n9--!>-","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":43719,"to":43817}}}}],[834,{"pageContent":"2 \n= 1, we find the null space of \nn \n1 \n4 \n� \nJ \nn \n!\n] \nA -   II = \n-1 \n-1 \n-5 \n-5 \nRow reduction produces \nn \n1 \n0 \n�\n] \n[\n: \n0 \n-1 \n�\n] \n[A-I\nI OJ = \n-1 \n1 \n-----+ \n1 \n-1 \n-5 \n3 \n0    0 \n9--!>-\n(We knew ;n odvan<ethot we mu\" get ot lmt one mo rn w. Why?) Thu', x � [ :: l' \nin the eigenspace E1 if and only if x1 - x\n3 \n= 0 and x\n2 \n- x\n3 \n= 0.   Setting the free variable \nx\n3 \n= t, we see that x1 = t and x\n2 \n= t, from which it follows that \nTo find the eigenvectors corresponding to A3 = 2, we find the null space of A -  21 \nby row reduction: \n[-2 \n[A -2IIOJ = � \n1 \n-2 \n-5 \n� \n�\ni \n----+ \n[\n� \n� \n=i \n�\ni \n2  0 \n0  0    0  0 \nSo x � [:\n:\nl ;, ;n the e;gmpoce E, ;fond only ;f x, � )x, ond x\n, \n� !x,. Sethng the \nfree variable x\n3 \n= t, we have","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":43817,"to":43894}}}}],[835,{"pageContent":"294 \nChapter 4 \nEigenvalues and Eigenvectors \nExample 4.19 \nwhere we have cleared denominators in the basis by multiplying through by the least \ncommon denominator 4. (Why is this permissible?) \n4 \nRemark Notice that in Example 4.18, A  is a 3 X 3 matrix but has only two distinct \neigenvalues. However, if we count multiplicities, A has exactly three eigenvalues \n(\nA = 1 \ntwice and A = 2 once). This is what the Fundamental Theorem of Algebra guarantees. \nLet us define the algebraic multiplicity of an eigenvalue to be its multiplicity as a root \nof the characteristic equation. Thus, A = 1 has algebraic multiplicity 2 and A =  2 has \nalgebraic multiplicity 1. \nNext notice that each eigenspace has a basis consisting of just one vector. In other \nwords, dim E\n1 \n= dim E\n2 \n= 1. Let us define the geometric multiplicity of an eigenvalue \nA to be dim E;\"' the dimension of its corresponding eigenspace. As you will see in \nSection 4.4, a comparison of these two notions of multiplicity is important.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":43896,"to":43918}}}}],[836,{"pageContent":"A to be dim E;\"' the dimension of its corresponding eigenspace. As you will see in \nSection 4.4, a comparison of these two notions of multiplicity is important. \nFind the eigenvalues and the corresponding eigenspaces of \nA=\n[\n-\n� \n� \n-\n�\n] \n1  0 -1 \nSolulion \nThe characteristic equation is \n-1 -  A \n0 = det\n(\nA -AI\n)  = \n3 \n1 \n0 \n-A \n0 \n1 \n-3 \n=-A \n1-1 -  A \n1 \n-1 -  A \nHence, the eigenvalues are A\n1 \n= A\n2 \n= 0 and A\n3 \n= -2. Thus, the eigenvalue 0 has  alge­\nbraic multiplicity 2 and the eigenvalue -2 has algebraic multiplicity 1. \nFor A\n1 \n= A\n2 \n= 0, we compute \n[A -OJ \nI\nOJ \n[A \nI \nO J \n0 \n0 \n0 \n-\n� \n�\ni \n� \n[\n� \n� \n-1 0 \n0  0 \n-\n1 O\nJ \n0  0 \n0  0 \n&om which it follow' th\n\"\nt'\"' eigenwdm x  � \n[\n:\n:\nl in E\n0 \n\"ti'fi\" x, � x,. Thecefoce, \nboth x\n2 \nand x\n3 \nare free. Setting x\n2 \n= s and x\n3 \n= t, we have \nFor A\n3 \n= -2, \n[A -(-2) I\nI\nO J [A + 2I\nI\nOJ","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":43918,"to":44010}}}}],[837,{"pageContent":"Theorem 4.15 \nExample 4.20 \nTheorem 4.16 \nSection 4.3 \nEigenvalues and Eigenvectors of n X n Matrices \n295 \nso x\n3 \n= tis free and x\n1 \n=  - x\n3 \n=  - t and x\n2 \n= 3x\n3 \n= 3t. Consequently, \nIt follows that ,\\\n1 \n= ,\\\n2 \n= 0 has geometric multiplicity 2 and ,\\\n3 \n= -2 has geometric \nmultiplicity 1. (Note that the algebraic multiplicity equals the geometric multiplicity \nfor each eigenvalue.) \n4 \nIn some situations, the eigenvalues of a   matrix are very easy to find. If A is a \ntriangular matrix, then so is A -AI, and Theorem 4.2 says that det(A -AI\n) \nis just \nthe product of the diagonal entries. This implies that the characteristic equa tion of \na triangular matr  ix is \n(\na\nll \n-\n.\\\n)(\na\n22 \n-  A\n)\n· · · \n(\na\nnn \n-  A )  = \n0 \nfrom which it follows immediately that the  eigenvalues are \nA\n1 \n=  a\n11\n, A\n2 \n=  a\n22\n, \n•.• \n, \nA\nn \n= a\nn\n,, . We summarize this result as a theorem and illustrate it with an example. \nThe eigenvalues of a triangular matrix are the entries on its main diagonal. \nThe eigenvalues of \nare ,\\\n1 \n= 2, ,\\\n2 \n= 1, ,\\\n3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":44012,"to":44085}}}}],[838,{"pageContent":",, . We summarize this result as a theorem and illustrate it with an example. \nThe eigenvalues of a triangular matrix are the entries on its main diagonal. \nThe eigenvalues of \nare ,\\\n1 \n= 2, ,\\\n2 \n= 1, ,\\\n3 \n= 3, and ,\\4 = -2, by Theorem 4.15. Indeed, the characteristic \npolynomial \nis just \n(\n2 -\n,\\)\n(\n1  -\n,\\)\n( 3 -\n,\\)\n(\n-2 \n-  A )\n. \nNote that diagonal matrices are a special case of Theorem 4.15. In fact, a diagonal \nmatrix is both upper and lower triangular. \nEigenvalues capture much importa  nt information about the behavior of a matrix. \nOnce we know the eigenvalues of a matrix, we can deduce a great many things without \ndoing any more work. The next theorem is one of the most important in this regard. \nA square matr  ix A   is invertible if and only if 0 is not an eigenvalue of A. \nProof Let A be a square matrix. By Theorem 4.6, A is invertible if and only if \n<let A *   0. But <let A * 0 is equivalent to <let (A -O\nJ\n)  *  0, which says that 0  is not a","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":44085,"to":44118}}}}],[839,{"pageContent":"Proof Let A be a square matrix. By Theorem 4.6, A is invertible if and only if \n<let A *   0. But <let A * 0 is equivalent to <let (A -O\nJ\n)  *  0, which says that 0  is not a \nroot of the characteristic equation of A (i.e., 0 is not an eigenvalue of A). \nWe can now extend the Fundamental Theorem of Invertible Matrices to include \nresults we have proved in this chapter.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":44118,"to":44124}}}}],[840,{"pageContent":"296 \nChapter 4 \nEigenvalues and Eigenvectors \nTheorem 4.11 \nTheorem 4.18 \nThe Fundamental Theorem of Invertible Matrices: Version 3 \nLet A be an n X n matrix. The following statements are equivalent: \na. A is invertible. \nb. Ax = b has a unique solution for every bin IJ�r. \nc. Ax = 0 has only the trivial solution. \nd. The reduced row echelon form of A is I\nn\n-\ne.  A   is a product of elementary matrices. \nf.  rank(A) =  n \ng.  nullity(A) = 0 \nh.  The column vectors of A are linearly independent. \ni.  The column vectors of A span IJ�r. \nj.  The column vectors of A form a basis for IJ�r. \nk. The row vectors of A are linearly independent. \n1. The row vectors of A span !R\nn\n. \nm. The row vectors of A form a basis for !R\nn\n. \nn.  det A * 0 \no.  0   is not an eigenvalue of A. \nProof The equivalence (a) � (n) is   Theorem 4.6, and we just proved (a) � (o) in \nTheorem 4.16. \nThere are nice formulas for the eigenvalues of the powers and inverses of a matrix.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":44126,"to":44156}}}}],[841,{"pageContent":"Proof The equivalence (a) � (n) is   Theorem 4.6, and we just proved (a) � (o) in \nTheorem 4.16. \nThere are nice formulas for the eigenvalues of the powers and inverses of a matrix. \nLet A be a square matrix with eigenvalue A and corresponding eigenvector x. \na.  For any positive  integer n, A\nn \nis an eigenvalue of A\nn \nwith  corresponding \neigenvector x. \nb. If A is invertible, then 1 /A is an eigenvalue of A -\ni \nwith corresponding eigenvector x. \nc.  If A is invertible, then for any integer n, A\nn \nis an eigenvalue of A\nn \nwith corre­\nsponding eigenvector x. \nProof \nWe are given that Ax = Ax. \n(a) \nWe proceed by induction on n. For n = 1, the result is just what has been given. \nAssume the result is true for n = k. That is, assume that, for some positive integer k, \nA \nk\nx \n= \nA \nk\nx. We must now prove the result for n = k \n+ \n1. But \nA\nk\n+\n1\nx \n= \nA\n(\nA\nk\nx\n) \n= A ( A\nk\nx\n) \nby the induction hypo  thesis. Using property (  d) ofTheorem 3.3, we have \nA(\nA\nk\nx)  = \nA\nk\n(\nAx\n)  = \nA\nk\n(\nAx\n)  = \nA\nk\n+\nI\nx \nThus, A\nk\n+\nl\nx  =  A\nk\n+\n1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":44156,"to":44232}}}}],[842,{"pageContent":"+ \n1. But \nA\nk\n+\n1\nx \n= \nA\n(\nA\nk\nx\n) \n= A ( A\nk\nx\n) \nby the induction hypo  thesis. Using property (  d) ofTheorem 3.3, we have \nA(\nA\nk\nx)  = \nA\nk\n(\nAx\n)  = \nA\nk\n(\nAx\n)  = \nA\nk\n+\nI\nx \nThus, A\nk\n+\nl\nx  =  A\nk\n+\n1\nx, as required. By induction, the result is true for all integers \nn 2: 1. \n(b) You are asked to prove this property in Exercise 13. \n(c) You are asked to prove this property in Exercise 14. \nThe next example shows one application of this theorem.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":44232,"to":44282}}}}],[843,{"pageContent":"Example 4.21 \nSection 4.3 \nEigenvalues and Eigenvectors of n X n Matrices \n291 \n[\no \n1\n1\n]\n1\n0\n[\n5\n1\n]\n. \nCompute \n2 \nSolution Let A = \n[ \n� \n�\n] \nand x \n= \n[ \n� \nl then what we want to find is A \n1\n0 \nx. T�e \neigenvalues of A are A\n1 \n= -1 and A\n2 \n= 2, with corresponding eigenvectors v\n1 \n= \n[ \n_ \n1\n] \nand v\n2 \n= \n[\n�\n]\n.That is, \n� \n(Check this.) Since {v\n1\n, v\n2\n} forms a basis for IR\n2 \n(why?), we can write x as a linear com­\nbination of v\n1 \nand v\n2\n. Indeed, as is easily checked, x = 3v\n1 \n+ 2v\n2\n. \nTheorem 4.19 \nTheorem 4.20 \nTherefore, using Theorem 4.18(a), we have \nA \n1\n0\nx \n= A \n1\n0\n(\n3v\n1 \n+ 2v\n2\n) = 3\n(\nA \n1\n0\nv\n1\n) \n+ \n2\n(\nA \n1\n0\nv\n2\n) \n= 3\n(\nA:\n0\n)\nv\n1 \n+ \n2\n(\nA�\n0\n)\nv\n2 \n= \n3\n(\n-1\n)\n1\n0\n[ \nl\n] \n+ \n2\n(\n2\n1\n0\n)\n[\n1\n] \n= \n[ \n3 \n+ \n2\n11\n] \n= \n[\n205\n1\n] \n-1 2 -3 + 2\n1\n2 \n4093 \nThis is certainly a lot easier than computing A \n1\n0 \nfirst; in fact, there are no matrix \nmultiplications at all! \nWhen it can be used, the method of Example 4.21 is quite general. We summarize \nit as the following theorem, which you are asked to prove in Exercise 42. \nSuppose the n X n matrix A  has eigenvectors v\n1\n, v\n2\n, •.. , v\nm \nwith corresponding","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":44284,"to":44446}}}}],[844,{"pageContent":"it as the following theorem, which you are asked to prove in Exercise 42. \nSuppose the n X n matrix A  has eigenvectors v\n1\n, v\n2\n, •.. , v\nm \nwith corresponding \neigenvalues A\n1\n, A\n2\n, ... , A\nm\n. If xis a  vector in !R\nn \nthat can be expressed as a linear \ncombination of these eigenvectors-say, \nthen, for any integer k, \nWarning The catch here is the \"if\" in the second sentence. There is absolutely \nno guarantee that such a linear combination is possible. The best possible situation \nwould be if there were a basis of !R\nn \nconsisting of eigenvectors of A; we will explore \nthis possibility further in the next section. As a step in that direction, however, we \nhave the following theorem, which states that eigenvectors corresponding to distinct \neigenvalues are linearly independent. \nLet A be an n X n matrix and let A \n1\n, A\n2\n, ... , A\nm \nbe distinct eigenvalues of A with cor­\nresponding eigenvectors v\n1\n, v\n2\n, ..• , v\nm\n. Then v\n1\n, v\n2\n, ..• , v\nm \nare linearly independent.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":44446,"to":44492}}}}],[845,{"pageContent":"298 \nChapter 4 \nEigenvalues and Eigenvectors \nProof The proof is indirect. We will assume that v\n1\n, v\n2\n, •.. , v\nm \nare linearly dependent \nand show that this assumption leads to a contradiction. \nIf v\n1\n, v\n2\n, •.• , v \nm \nare linearly dependent, then one of these vectors must be express­\nible as a linear combination of the previous ones. Let v\nk\n+ \n1 \nbe the first of the vectors \nV\n; \nthat can be so   expressed. In other words, v\n1\n, v\n2\n, •.. , v\nk \nare linearly independent, but \nthere are scalars c\n1\n, c\n2\n, •.• , c\nk \nsuch that \n(1) \nMultiplying both sides of Equation \n( \n1) by A from the left and using the fact that Av; = \nA;\nV\n; for each i, we have \nA\nk\n+\ni\nV\nk\n+\ni \n= Av\nk +\n1 \n= A (\nc\n1\nv\n1 \n+ c\n2\nv\n2 \n+ \n· · · \n+  c\nk\nv\nk\n) \n= \nC\n1\nAV\n1 \n+ \nC\n2\nAV\n2 \n+ \n... \n+  c\nk\nAv\nk \n= \nC\n1\nA\n1\nV\n1 \n+ \nC\n2\nA\n2\nV\n2 \n+ \n... \n+  c\nk\n,\\\nk\nv\nk \nNow we multiply both sides ofEquation (1) by A\nk\n+\nl \nto get \nWhen we subtract Equation \n(\n3\n) \nfrom Equation \n(\n2\n)\n, we obtain \n0 =  c\n1 \n(\n,\\\n1 \n-  A\nk\n+\n1\n)\nv\n1 \n+ \nc\n2\n(\nA\n2 \n-  A\nk\n+\n1\n)\nv\n2 \n+ \n· · · \n+  c\nk\n(\n,\\\nk \n-  A\nk\n+\n1\n)\nv\nk \nThe linear independence of v\n1\n, v\n2\n, •.• , v\nk \nimplies that \nc\n1 \n(\n,\\\n1 \n-  A\nk +","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":44494,"to":44671}}}}],[846,{"pageContent":"(\n3\n) \nfrom Equation \n(\n2\n)\n, we obtain \n0 =  c\n1 \n(\n,\\\n1 \n-  A\nk\n+\n1\n)\nv\n1 \n+ \nc\n2\n(\nA\n2 \n-  A\nk\n+\n1\n)\nv\n2 \n+ \n· · · \n+  c\nk\n(\n,\\\nk \n-  A\nk\n+\n1\n)\nv\nk \nThe linear independence of v\n1\n, v\n2\n, •.• , v\nk \nimplies that \nc\n1 \n(\n,\\\n1 \n-  A\nk +\n1\n)  =  c\n2\n(\nA\n2 \n-  A\nk +\n1\n)  = \n· · · \n=  c\nk\n(\n,\\\nk \n-  A\nk +\n1\n)  = 0 \n(\n2\n) \n(\n3\n) \nSince the  eigenvalues A; are all  distinct, the  terms in parentheses \n(\nA; -   ,\\\nk\n+\n1 )\n, \ni = 1, ... , k, are all nonzero. Hence, c\n1 \n=  c\n2 \n= \n· · · \n= ck = 0.   This implies that \nv\nk\n+\n1 \n=  c\n1\nv\n1 \n+ c\n2\nv\n2 \n+ \n· · · \n+  c\nk\nv\nk \n= Ov\n1 \n+ Ov\n2 \n+ \n· · · \n+ Ov\nk \n= 0 \nwhich is im  possible, since the  eigenvector \nV\nk\n+ \n1 \ncannot be  zero. Thus, we have a \ncontradiction, which means that our assumption that v\n1\n, v\n2\n, ... , v\nm \nare linearly \ndependent is false. It follows thatv\n1\n, v\n2\n, \n... \n, v\nm \nmust be linearly independent. \n.. \nI \nExercises 4.3 \nIn Exercises 1-12, compute (a) the characteristic polynomial \nof A, (b) th e eigenvalues of A, ( c) a basis for each eigenspace \nof A, and ( d) the algebraic and geometric multiplicity of \neach eigenvalue. \n1. A \n= \n[ \n1 3\n] \n-2 6 \n3.A � [� \n-\n� :J \n5.A � [\n-","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":44671,"to":44838}}}}],[847,{"pageContent":"of A, (b) th e eigenvalues of A, ( c) a basis for each eigenspace \nof A, and ( d) the algebraic and geometric multiplicity of \neach eigenvalue. \n1. A \n= \n[ \n1 3\n] \n-2 6 \n3.A � [� \n-\n� :J \n5.A � [\n-\n� -: :J \n2. A  = \n[ \n2 \nl\n] \n-1  0 \n4.A � [� : �] \n6.A � [� -\n� \n:J","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":44838,"to":44862}}}}],[848,{"pageContent":"Section 4.3 \nEigenvalues and Eigenvectors of n X n Matrices \n299 \n13. Prove Theorem 4.18(b). \n14. Prove Theorem 4.18(c). [Hint: Combine the proofs of \nparts (a) and (b) and see the fourth Remark following \nTheorem 3.9 (page 169).] \nIn Exercises 15 and 16, A is a 2 X 2 matrix with eigenvec­\ntors v\n1 \n= \n[ \n_ \n�\n] \nand v\n2 \n= \n[ \n�\n] \ncorresponding to eigenvalues \nA\n1 \n= \nt \nand A\n2 \n= 2, respectively, and x = \n[ \n�\n]\n. \n15. Find \nA \n1\n0\nx. \n16. Find A \nk\nx. What happens as k becomes large (i.e., k---+  oo)? \nIn Exercises 17 and 18, A is a 3 X 3 matrix with eigenvectors \nv\n, \n� [H v, � \n[J\nandv\n, \n� [ :J wm,pond;ngto \neigenvalues A\n1 \n= -L A\n2 \n= \nt\n, and A\n3 \n= 1, respectively, and \nFm \n17. FindA\n2\n0\nx. \n18. Find A \nk\nx. What happens as k becomes large (i.e., k---+  oo)? \n19. (a) Show that, for any square matrix A, A\nT \nand A have \nthe same characteristic polynomial and hence the \nsame eigenvalues. \n(b) Give an example of a 2 X 2 matrix A for which A\nT \nand A have different eigenspaces. \n20. Let A be a nilpotent matrix (that is, A\nm \n= 0 for some","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":44864,"to":44940}}}}],[849,{"pageContent":"same eigenvalues. \n(b) Give an example of a 2 X 2 matrix A for which A\nT \nand A have different eigenspaces. \n20. Let A be a nilpotent matrix (that is, A\nm \n= 0 for some \nm > 1). Show that A = 0 is the only eigenvalue of A. \n21. Let A be an idempotent matrix (that is, A \n2 \n= A). Show that \nA= OandA = 1    aretheonlypossibleeigenvaluesofA. \n22. If v is an eigenvector of A with corresponding eigen -\nvalue A and c is a scalar, show that vis an eigenvector \nof A -cI with corresponding eigenvalue A -c. \n23. (a) Find the eigenvalues and eigenspaces of \nA = \n[\n� \n�\n] \n(b) Using Theorem 4.18 and Exercise 22, find the eigen­\nvalues and eigenspaces of A \n-\ni\n, A -  2I, and A + 2I. \n24. Let A and B be n X n matrices with eigenvalues A and \nIL, respectively. \n(a) Give an example to show that A + IL need not be \nan eigenvalue of A + B. \n(b) Give an example to show that AIL need not be an \neigenvalue of AB. \n(c) Suppose A and IL correspond to the same eigen­\nvector x. Show that, in this case, A + IL is an eigen -","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":44940,"to":44973}}}}],[850,{"pageContent":"an eigenvalue of A + B. \n(b) Give an example to show that AIL need not be an \neigenvalue of AB. \n(c) Suppose A and IL correspond to the same eigen­\nvector x. Show that, in this case, A + IL is an eigen -\nvalue of A + B and AIL is an eigenvalue of AB. \n25. If A and B are two row equivalent matrices, do they \nnecessarily have the same eigenvalues? Either prove \nthat they do or give a counterexample. \nLet p(x) be th e polynomial \nThe com  panion matrix of p\n(\nx\n) \nis then X n matrix \n-a\nn\n-\nI \n-a\nn\n-\n2 \n-a\n1 \n-a\no \n1 \n0 0    0 \nC(p\n) \n= \n0 \n0 0 0 0 \n0 0 0 \n(4) \n26. Find the companion matrix of p\n(\nx\n) \n= x\n2 \n-7x + 12 and \nthen find the characteristic polynomial of C\n( \np\n)\n. \n27. Find the companion matrix of p\n(\nx\n) \n= x\n3 \n+ 3x\n2 \n-\n4x + 12 and then find the characteristic polynomial \nofC(p). \n28. (a) Show that the companion matrix C\n(\np\n) \nof p\n(\nx\n) \n= \nx\n2 \n+ ax +  b has characteristic polynomial \nA\n2 \n+  aA  + b. \n(b) Show that if A is an eigenvalue of the companion \nmatrix C\n(\np\n) \nin part (a), then \n[ \n�\n] \nis an eigenvector \nof C\n( \np\n) \ncorresponding to A.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":44973,"to":45061}}}}],[851,{"pageContent":"(\nx\n) \n= \nx\n2 \n+ ax +  b has characteristic polynomial \nA\n2 \n+  aA  + b. \n(b) Show that if A is an eigenvalue of the companion \nmatrix C\n(\np\n) \nin part (a), then \n[ \n�\n] \nis an eigenvector \nof C\n( \np\n) \ncorresponding to A. \n29. (a) Show that the companion matrix C\n(\np\n) \nof p \n(\nx\n) \n= \nx\n3 \n+ ax\n2 \n+ bx + c has characteristic polynomial \n-(A\n3 \n+ aA\n2 \n+ bA + c\n)\n. \n(b) Show that if A is an eigenvalue of the companion \nmatrix C\n(\np\n) \nin part (a), then [ �\n2 \nl is an eigenvector \nof C\n(\np\n) \ncorresponding to A. \n1 \n30. Construct a nontriangular 2 X 2 matrix with eigen­\nvalues 2 and 5. [Hint: Use Exercise 28.] \n31. Construct a nontriangular 3 X 3 matrix with eigen­\nvalues -2, 1, and 3. [Hint: Use Exercise 29.] \n32. (a) Use mathematical induction to prove that, for \nn 2 2, the companion matrix C\n(\np\n) \nof p\n(\nx\n) \n= x\nn \n+ \na\nn\n_\n1\nx\nn\n-\nI \n+ \n· · · \n+ a\n1\nx \n+ a\n0 \nhas characteristic \npolynomial ( -1) \nn\np (A). [Hint: Expand by cofactors \nalong the last column. You may find it helpful to \nintroduce the polynomial q\n(\nx\n) \n= \n(\np\n(\nx\n) \n-a\n0\n)/x.]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":45061,"to":45169}}}}],[852,{"pageContent":"300 \nChapter 4 \nEigenvalues and Eigenvectors \n(b) Show that if A is an eigenvalue of the companion \nmatrix C\n(\np\n) \nin Equation (4), then an eigenvector \ncorresponding to A is given by \nA\nn\n-\nI \n,t\nn\n-\n2 \nIf p\n(\nx\n) \n= x\nn \n+ a\nn\n_\n1\nx\nn\n-\n! \n+ ·   ·   · \n+ a\n1\nx \n+ a\n0 \nand A is a \nsquare matrix, we can define a square matrix p\n(\nA\n) by \np\n(A)  =A\nn\n+  a\nn\n_\n1\nA\nn\n-\ni \n+ \n·   ·   · +  a\n1\nA  +  a\n0\nI \nAn important theorem in advanced linear algebra says that \nif c\nA \n(\n,\\ ) \nis th e characteristic polynomial of the matrix A, \nthen c\nA \n(A) = 0 (in words, every matrix satisfies its charac­\nteristic equation). This is the celebrated Cayley-Hamilton \nTheorem, named after Arthur Cayley (1821-1895), \npictured below, and Sir William Rowan Hamilton (see \npage 2). Cayley proved this theorem in 1858. Hamilton \ndiscovered it, independently, in his work on quaternions, a \ngeneralization of the complex numbers. \nc \nj \n[\n1 -\n3\n1\n]\n. \n33. Verify the Cayley-Hamilton Theorem for A = \n2 \nThat is, find the characteristic polynomial c \nA \n( \n,\\ ) \nof A \nand showthat cA(A) = 0.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":45171,"to":45261}}}}],[853,{"pageContent":"generalization of the complex numbers. \nc \nj \n[\n1 -\n3\n1\n]\n. \n33. Verify the Cayley-Hamilton Theorem for A = \n2 \nThat is, find the characteristic polynomial c \nA \n( \n,\\ ) \nof A \nand showthat cA(A) = 0. \n34. Verify the Cayley-Hamilton Theorem for \nA\n� \n[\n� \n� \n:\nJ \nThe Cayley-Hamilton Theorem can be used to calculate \npowers and inverses of matrices. For example, if A is a 2 X 2 \nmatrix with characteristic polynomial c\nA\n(\n,\\\n) \n= ,\\\n2 \n+a,\\+ b, \nthen A \n2 \n+ aA + bl = 0, so \nA\n2 \n= -aA  -   bl \nand \nA\n3 \n= AA\n2 \n= A (-aA  -bl) \n=  -aA\n2 \n-  bA \n= -a\n(\n-aA  -bl) - bA \n= ( a\n2 \n-  b \n)\nA + ab I \nIt is easy to see that by continuing in this fashion we can \nexpress any positive power of A as a linear combina-\ntion of I and A. From A \n2 \n+ aA + bl = 0, we also obtain \nA(A +al\n) \n= -bl, so \nprovided b * 0. \n1 \na \nA\n-\n1 \n=--A -\n-\nI \nb b \n35. For the matrix A  in Exercise 33, use the Cayley­\nHamilton Theorem to compute A \n2\n, A \n3\n, and A \n4 \nby \nexpressing each as a linear combination of I and A. \n36. For the matrix A in Exercise 34, use the Cayley­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":45261,"to":45346}}}}],[854,{"pageContent":"Hamilton Theorem to compute A \n2\n, A \n3\n, and A \n4 \nby \nexpressing each as a linear combination of I and A. \n36. For the matrix A in Exercise 34, use the Cayley­\nHamilton Theorem to compute A \n3 \nand A \n4 \nby express­\ning each as a linear combination of I, A, and A \n2\n. \n37. For the matrix A in Exercise 33, use the Cayley­\nHamilton Theorem to compute A \n-\ni \nand A \n-\n2 \nby \nexpressing each as a linear combination of I and A. \n38. For the matrix A  in Exercise 34, use the Cayley­\nHamilton Theorem to compute A \n-\ni \nand A \n-\n2 \nby \nexpressing each as a linear combination of I, A, and A \n2\n. \n39. Show that if the square matrix A can be partitioned as \nA= \n[-i\nj-i\n-\n;\n-\n] \nwhere P and S are square matrices, then the character­\nistic polynomial of A is c\nA\n(\n,\\ ) \n= cp\n(\nA )\nc5\n(\n,\\ )\n.  [Hint: Use \nExercise 69 in Section 4.2.] \n40. Let A\n1\n, A\n2\n, ... , A\nn \nbe a complete set of eigenvalues (rep­\netitions included) of the n X n matrix A. Prove that \ndet\n(\nA\n) \n= A\n1 \nA\n2 \n•   •   ·\nA\nn \nand \ntr\n(\nA\n) \n= A\n1 \n+  A\n2 \n+ \n·   ·   · \n+  A\nn","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":45346,"to":45435}}}}],[855,{"pageContent":"40. Let A\n1\n, A\n2\n, ... , A\nn \nbe a complete set of eigenvalues (rep­\netitions included) of the n X n matrix A. Prove that \ndet\n(\nA\n) \n= A\n1 \nA\n2 \n•   •   ·\nA\nn \nand \ntr\n(\nA\n) \n= A\n1 \n+  A\n2 \n+ \n·   ·   · \n+  A\nn \n[Hint: The characteristic polynomial of A   factors as \ndet\n(\nA -AI\n) \n= ( - l\n)\nn\n(\n,\\ -  ,\\\n1\n)(\n,\\ -  A\n2\n) •   •   • (\n,\\ -  A\nn\n) \nFind the constant term and the coefficient of \nA\nn\n-\n! \non \nthe left and right sides of this equation.] \n41. Let A and B be n X n matrices. Prove that the sum of all \nthe eigenvalues of A + B is the sum of all the eigenval­\nues of A and B individually. Prove that the product of all \nthe eigenvalues of AB is the product of all the eigenval­\nues of A and B individually. (Compare this exercise with \nExercise 24.) \n42. Prove Theorem 4.19.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":45435,"to":45498}}}}],[856,{"pageContent":"Writing Proiect \nThe History of Eigenvalues \nSection 4.4 \nSimilarity and Diagonalization \n301 \nLike much oflinear algebra, the way the topic of eigenvalues is taught today does not \ncorrespond to its historical development.  Eigenvalues arose out of problems in sys­\ntems of differential equations before the concept of a matrix was even formulated. \nWrite a report on the historical development of eigenvalues. Describe the types \nof mathematical problems in which they originally arose. Who were some of the \nkey mathematicians involved with these problems? How did the terminology for \neigenvalues change over time? \n1. Thomas Hawkins, Cauchy and the Spectral Theory of Matrices, Historia Math­\nematica 2 (1975), pp. 1-29. \n2. Victor J. Katz, A History of Mathematics: An Introduction (Third Edition) (Read­\ning, MA: Addison Wesley Longman, 2008). \n3. Morris Kline, Mathematical Thought from Ancient to Modern Times (Oxford: \nOxford University Press, 1972). \nSimilarilv and Diauonalizalion","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":45500,"to":45518}}}}],[857,{"pageContent":"ing, MA: Addison Wesley Longman, 2008). \n3. Morris Kline, Mathematical Thought from Ancient to Modern Times (Oxford: \nOxford University Press, 1972). \nSimilarilv and Diauonalizalion \nAs you saw in the last section, triangular and diagonal matrices are nice in the sense \nthat their eigenvalues are transparently displayed. It   would be pleasant if we could \nrelate a given square matrix to a triangular or diagonal one in such a way that they \nhad exactly the same eigenvalues. Of course, we already know one procedure for con -\nverting a square matrix into triangular form-namely, Gaussian elimination. Unfor­\ntunately, this process does not preserve the eigenvalues of the matrix. In this section, \nwe consider a different sort of transformation of a matrix that does behave well with \nrespect to eigenvalues. \nSimilar Malrices \nDefinition \nLet A and B be n x n matrices. We say that A is similar to B if \nthere is an invertible n X n matrix P such that P\n-\n1 \nAP = B. If A is similar to B, \nwe write A � B.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":45518,"to":45537}}}}],[858,{"pageContent":"Similar Malrices \nDefinition \nLet A and B be n x n matrices. We say that A is similar to B if \nthere is an invertible n X n matrix P such that P\n-\n1 \nAP = B. If A is similar to B, \nwe write A � B. \nRemarks \n• \nIf A � B, we can write, equivalently, that A  = PBP\n-\n1 \nor AP= PB. \n• \nSimilarity is a relation on square matrices in the same sense that \"less than or \nequal to\" is a relation on the integers. Note that there is a direction (or order\n) \nimplicit \nin the definition. Just as a :::; b does not necessarily imply b :::; a, we should not assume \nthat A � B implies B � A. (In fact, this is true, as we will prove in the next theorem, \nbut it does not follow immediately from the definition.) \n• \nThe matrix P depends on A and B. It is not unique for a given pair of similar ma­\ntrices A and B. To see this, simply take A = B = I, in which case I� I, since p\n-\ni \nIP = I \nfor any invertible matrix P.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":45537,"to":45565}}}}],[859,{"pageContent":"302 \nChapter 4 \nEigenvalues and Eigenvectors \nExample 4.22 \nTheorem 4.21 \nTheorem 4.22 \nLet A = \n[\n0\n1 \n2\nJ and B = \n[ \n1 \n-1 \n-2 \n0\nJ. Then A �   B, since \n-1 \n[\n� \n-\n�\nJ \n[\n� \n-l\nJ \n[ \n3 \nl\nJ \n[\n1 \n1 \n-\n-1  -1 \n-\n1 \n-l\nJ\n[ \n1 O\nJ \n1 \n-2 -1 \n[ \n1 \n-l\nJ \nThus, AP = PB with P = \n1    1 \n. (Note that it  is not necessary to compute P\n-\n1\n. \nSee the first Remark before Example 4.22.\n) \n4 \nLet A, B, and C be n X n matrices. \na.  A   �A \nb. If A� B, then B �A. \nc.  If A � B and B � C, then A � C. \nProof (a) This property follows from the fact that r\n1\nAI =A. \n(b) If A � B, then P\n-\n1\nAP = B   for some invertible matrix P. As noted in the first \nRemark on the previous page, this is equivalent to PBP\n-\n1 \n=A. Setting Q = P\n-\n1\n, we \nhave Q\n-\n1\nBQ = (P\n-\n1\n)\n-\n1\nBP\n-\n1 \n= PBP\n-\n1 \n=A. Therefore, by definition, B �A. \n(c) You are asked to prove property (c) in Exercise 30. \nRemark Any relation satisfying the three properties of Theorem 4.21 is called \nan equivalence relation. Equivalence relations arise frequently in mathematics, and","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":45567,"to":45662}}}}],[860,{"pageContent":"Remark Any relation satisfying the three properties of Theorem 4.21 is called \nan equivalence relation. Equivalence relations arise frequently in mathematics, and \nobjects that are related via an equivalence relation usually share important properties. \nWe are about to see that this is true of similar matrices. \nLet A and B be n X n matrices with A �   B. Then \na.  det A =   det B \nb.  A   is invertible if and only if B is invertible. \nc. A and B have the same rank. \nd. A and B have the same characteristic polynomial. \ne. A and B have the same eigenvalues. \nf.  A\nm\n� B\nm \nfor all integers m 2::: 0. \ng. If A is invertible, then A\nm \n� B\nm \nfor all integers m. \nProof We prove (a) and (d) and leave the remaining properties as exercises. If A� B, \nthen P\n-\n1\nAP = B for some invertible matrix P. \n(a)  Taking determinants of both sides, we have \ndet B =   det\n(\nP\n-\n1\nAP\n) \n= \n(\ndet P\n-\n1\n)(\ndet A\n)(\ndet P\n) \n= \n(\n-\n1\n-)\n(\ndet \nA\n)(\ndet P\n) \n=  det A \ndet P \n(d)  The characteristic polynomial of Bis \ndet \n(\nB -  AI\n) \n=  det (p\n-\ni \nAP - AI\n) \n=  det\n(\nP\n-\n1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":45662,"to":45732}}}}],[861,{"pageContent":"det B =   det\n(\nP\n-\n1\nAP\n) \n= \n(\ndet P\n-\n1\n)(\ndet A\n)(\ndet P\n) \n= \n(\n-\n1\n-)\n(\ndet \nA\n)(\ndet P\n) \n=  det A \ndet P \n(d)  The characteristic polynomial of Bis \ndet \n(\nB -  AI\n) \n=  det (p\n-\ni \nAP - AI\n) \n=  det\n(\nP\n-\n1\nAP - AP\n-\n1\nIP\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":45732,"to":45781}}}}],[862,{"pageContent":"Example 4.23 \nExample 4.24 \nSection 4.4 \nSimilarity and Diagonalization \n303 \ndet\n(\nP\n-\n1\nAP - P\n-\n1\n(\nAI\n)\nP\n) \ndet\n(\nP\n-\n1\n(\nA -   AI\n)\nP\n) \n=  det\n(\nA -   AI\n) \nwith the last step following as in (a). Thus, det(B - i\\I) = det(A - i\\I); that is, the \ncharacteristic polynomials of B and A are the same. \nRemark \nTwo matrices may have properties (a) through ( e) (and more) in common \nand yet still not be similar. For example, A = [ \n� \n�\n] \nand B = [ \n� \n�\n] \nboth have de­\nterminant 1   and rank 2, are invertible, and have characteristic polynomial (1 - i\\)\n2 \nand eigenvalues i\\\n1 \n=  i\\\n2 \n=  1. But A  is not similar to B,   since P\n-\n1\nAP = P\n-\n1\nIP  = \nI * B for any invertible matrix P. \nTheorem 4.22 is most useful in showing that two matrices are not similar, since \nA and B cannot be similar if any of properties (a) through (e) fails. \n(a)A= [\n� \n�]andB = [� \n�\n]are not similar, since detA= -3but detB=3. \n(b) A = [ \n1 \n3\n] and B = [ \n1    1\n] \nare not similar, since the characteristic polyno-\n2  2 3 -1 \nmial of A is i\\\n2 \n- 3i\\ - 4   while that of Bis i\\\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":45783,"to":45860}}}}],[863,{"pageContent":"�]andB = [� \n�\n]are not similar, since detA= -3but detB=3. \n(b) A = [ \n1 \n3\n] and B = [ \n1    1\n] \nare not similar, since the characteristic polyno-\n2  2 3 -1 \nmial of A is i\\\n2 \n- 3i\\ - 4   while that of Bis i\\\n2 \n- 4. (Check this.) Note that A  and B do \nhave the same determinant and rank, however. \nDiagonalization \nThe best possible situation is when a square matrix is similar to a diagonal matr  ix. \nAs you are about to see, whether a matrix is diagonalizable is closely related to the \neigenvalues and eigenvectors of the matrix. \nDefinition \nAn n x n matrix A is diagonalizable if there is a diagonal matrix \nD such that A is similar to D-that is, if there is an invertible n X n matrix P such \nthat P\n-\n1\nAP = D. \nA= [ \nl \n3\n] is  diagonalizable since, if  P = [ \nl \n3\n] and  D = \n[\n4   0\n],  then \n2  2 \n1  -2 \n0  -1 \nP\n-\n1\nAP = D, as   can be easily checked. (Actually, it is faster to check the equivalent \nstatement AP = PD, since it does not require finding P\n-\n1\n.) \n-+","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":45860,"to":45909}}}}],[864,{"pageContent":"l \n3\n] and  D = \n[\n4   0\n],  then \n2  2 \n1  -2 \n0  -1 \nP\n-\n1\nAP = D, as   can be easily checked. (Actually, it is faster to check the equivalent \nstatement AP = PD, since it does not require finding P\n-\n1\n.) \n-+ \nExample 4.24 begs the question of where matrices P and D came from. Observe \nthat the  diagonal entries 4 and -1 of D are the eigenvalues of A, since they are the \nroots of its characteristic polynomial, which we found in Example 4.23(b ). The origin \nof matrix Pis less obvious, but, as we are about to demonstrate, its entries are obtained \nfrom the eigenvectors of A. Theorem 4.23 makes this connection precise.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":45909,"to":45931}}}}],[865,{"pageContent":"304 \nChapter 4 \nEigenvalues and Eigenvectors \nTheorem 4.23 \nExample 4.25 \nLet A be an n X n matrix. Then A is diagonalizable if and only if A has n linearly \nindependent eigenvectors. \nMore precisely, there exist an invertible matrix P and a diagonal matrix D such \nthat P\n-\n1\nAP = D   if and only if the columns of Pare n linearly independent eigen­\nvectors of A and the diagonal entries of D are the eigenvalues of A corresponding \nto the eigenvectors in P in the same order. \nProof Suppose first that A is similar to the diagonal matr  ix D via P\n-\n1\nAP = D  or, \nequivalently, AP = PD. Let the columns of P be \np1\n, \np\n2\n, ..• , \nP\nn \nand let the diagonal \nentries of D be A\n1\n, A\n2\n, •.. , A\nn\n. Then \n[\nA\n, \n0 \n]\nJ \nA\n[p\n1 \n... \nP\nnl \n[p\nl \np\n.\n] \n! \nA\nz \nP\nz \nP\nz \n0 \n(1) \nor \n[\nA\np\n1 \nA\np\nz \n... \nA\np\nn\n]  = \n[\nA\n1P1 \nA\nz\np\nz \nA\nn\nP\nn\n] \n(\n2\n) \nwhere the right-hand side is just the column-row representation of the product PD. \nEquating columns, we have \nA\np\n1 \n= A\n1P1\n, A\np\nz \n= A\nz\np\nz\n, \n·  · ·, \nA\np\nn \n= A\nn\nP\nn","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":45933,"to":46043}}}}],[866,{"pageContent":"or \n[\nA\np\n1 \nA\np\nz \n... \nA\np\nn\n]  = \n[\nA\n1P1 \nA\nz\np\nz \nA\nn\nP\nn\n] \n(\n2\n) \nwhere the right-hand side is just the column-row representation of the product PD. \nEquating columns, we have \nA\np\n1 \n= A\n1P1\n, A\np\nz \n= A\nz\np\nz\n, \n·  · ·, \nA\np\nn \n= A\nn\nP\nn \nwhich proves that the column vectors of P are eigenvectors of A wh  ose corresponding \neigenvalues are the diagonal entries of Din the same order. Since Pis invertible, its col­\numns are linearly independent, by the Fundamental Theorem oflnvertible Matrices. \nConversely, if A has n linearly independent eigenvectors \np1\n, \np\n2\n, .•. , \nP\nn \nwith cor­\nresponding eigenvalues A\n1\n, A\n2\n, .•• , A\nn\n, respectively, then \nA\np\n1 \n= A\n1P1\n, A\np\nz \n= A\nz\np\nz\n, \n·  ·  ·  , \nA\np\nn \n= A\nn\nP\nn \nThis implies Equation \n(\n2\n) \nabove, which is equivalent to Equation (1). Consequently, \nif we take P to be the n X n matrix with columns \np\n1\n, \np\n2\n, ... , \nP\nn\n' \nthen Equation (1) \nbecomes AP = PD. Since the columns of Pare linearly independent, the Fundamental \nTheorem oflnvertible Matrices implies that Pis invertible, so P\n-\n1\nAP = D; that is, A \nis diagonalizable.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":46043,"to":46155}}}}],[867,{"pageContent":"becomes AP = PD. Since the columns of Pare linearly independent, the Fundamental \nTheorem oflnvertible Matrices implies that Pis invertible, so P\n-\n1\nAP = D; that is, A \nis diagonalizable. \nIf possible, find a matr  ix P  that diagonalizes \nA \n� \n[\n� \n-\n� \n:\nJ \nSolulion We studied this matrix in Example 4.18, where we discovered that it has \neigenvalues A\n1 \n= A\n2 \n= 1 and A\n3 \n= 2. The eigenspaces have the following bases: \nFod\n,  �  A, � \n1,E\n, \nhas hasis \n[\n:J \nFod, \n� \n2, E\n, \nhas basis \n[ \nn","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":46155,"to":46191}}}}],[868,{"pageContent":"Example 4.26 \nSection 4.4 \nSimilarity and Diagonalization \n305 \nSince all other eigenvectors are just multiples of one of these two basis vectors, there \ncannot be three linearly independent eigenvectors. By Theorem 4.23, therefore, A is \nnot diagonalizable. \n4 \nIf possible, find a matrix P that diagonalizes \nA \n� \n[\n-\n� \n� \n=i\nl \nSolution This is the matrix of Example 4.19. There, we found that the  eigenvalues of \nA are A\n1 \n= A\n2 \n= 0   and ,\\\n3 \n= -2, with the following bases for the eigenspaces: \nFod\n, \n� \nA\n, \n� \n0, E\n0 \nh\" b\"i' p, \n� \nm ond p, \n� \nm \nFod, \n� \n-2, E_, h\" b\"i'\nP\n; \n� \n[\n-\n:J \nIt is straightforward to check that these three vectors are linearly independent. Thus, \nif we take \nthen Pis invertible. Furthermore, \nas can be easily checked. (If you are checking by hand, it is much easier to check the \nequivalent equation AP = PD.) \nRemarks \n• \nWhen there are enough eigenvectors, they can be placed into the columns of P \nin any order. However, the eigenvalues will come up on the diagonal of D in the same","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":46193,"to":46248}}}}],[869,{"pageContent":"Remarks \n• \nWhen there are enough eigenvectors, they can be placed into the columns of P \nin any order. However, the eigenvalues will come up on the diagonal of D in the same \norder as their corresponding eigenvectors in P. For example, if we had chosen \nthen we would have found","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":46248,"to":46253}}}}],[870,{"pageContent":"306 \nChapter 4 \nEigenvalues and Eigenvectors \nTheorem 4.24 \n• \nIn Example 4.26, you were asked to check that the eigenvectors p 1 , p\n2\n, and p\n3 \nwere linearly independent. Was it necessary to check this? We knew that {p\n1\n, p\n2\n} was \nlinearly independent, since it was a basis for the eigenspace E0• We also knew that the \nsets { p\n1 , p\n3\n} and { p\n2\n, p\n3\n} were linearly independent, by Theorem 4.20. But we could not \nconclude from this information that {p\n1\n, p\n2\n, p\n3\n} was linearly independent. The next \ntheorem, however, guarantees that linear independence is preserved when the bases of \ndifferent eigenspaces are combined. \nLet A be an n X n matrix and let A\n1\n, A\n2\n, ••• , A\nk \nbe distinct eigenvalues of A. If B; is \na basis for the eigenspace E;v then B \n= B\n1 U B\n2\nU ·  ·  · UB\nk \n(i.e., the total collection \nof basis vectors for all of the eigenspaces) is line arly independent. \nProof \nLet Bi = {v\n;1\n, v\n;\n2\n, •.• , v\n;\nn\n) for i = 1, ... , k. We have to show that","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":46255,"to":46311}}}}],[871,{"pageContent":"= B\n1 U B\n2\nU ·  ·  · UB\nk \n(i.e., the total collection \nof basis vectors for all of the eigenspaces) is line arly independent. \nProof \nLet Bi = {v\n;1\n, v\n;\n2\n, •.• , v\n;\nn\n) for i = 1, ... , k. We have to show that \nis linearly independent. Suppose some nontrivial linear combination of these vectors \nis the zero vector-say, \n(\nC\n11\nV\n11 \n+ \n.. \n'\n+ C\n1\nn\n,\nV\n1\nn\n) \n+ \n(\nC\n2\n1\nV\n2\n1 \n+ \n. \n' \n. \n+ C\n2\nn\n2\nV\n2\nn\n) \n+ \n. \n' \n. \n+ \n(c\nk !\nV\nk\nl \n+ \n... \n+ C\nkn\n,\nV\nkn\n) \n= \n0 \nDenoting the sums in parentheses by x\n1\n, x\n2\n,  ... x\nk\n, we can write Equation (3) as \nx\n1 \n+  x\n2 \n+ \n·   ·   · \n+ \nx\nk \n= 0 \n(3) \n(4) \n� \nNow each x, is in E\nA\n, (why?) and so either is an eigenvector corresponding to A; or \nis 0. But, since the eigenvalues A; are distinct, if any of the factors \nX\n; is an eigenvector, \nthey are linearly independent, by Theorem 4.20. Yet Equation ( 4) is a linear depen­\ndence relationship; this is a contradiction. We  conclude that Equation (3) must be \ntrivial; that is, all of its coefficients are zero. Hence, B is linearly independent.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":46311,"to":46414}}}}],[872,{"pageContent":"dence relationship; this is a contradiction. We  conclude that Equation (3) must be \ntrivial; that is, all of its coefficients are zero. Hence, B is linearly independent. \nThere is one case in which diagonalizability is automatic: an n X n matrix with \nn distinct eigenvalues. \nTheorem 4.25 \nIf A is an n X n matrix with n distinct eigenvalues, then A is diagonalizable. \nProof Let v1, v\n2\n,  ... , v\nn \nbe eigenvectors corresponding to the n distinct eigenvalues \n� \nof A. (Why could there not be more than n such eigenvectors?) By Theorem 4.20, \nv1, v\n2\n,  ... , v\nn \nare linearly independent, so, by Theorem 4.23, A is diagonalizable. \nExample 4.21 \nThe matrix \nhas eigenvalues \nA1 \n= 2, A\n2 \n= 5, and A\n3 \n= \n-1, by  Theorem 4.15. Since these are three \ndistinct eigenvalues for a 3 X 3 matrix, A is diagonalizable, by Theorem 4.25. (If we  actu­\nally require a matrix P such that p\n-\n1\nAP is diagonal, we must still compute bases for the \neigenspaces, as in Example 4.19 and Example 4.26 above.) \n4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":46414,"to":46448}}}}],[873,{"pageContent":"Lemma 4.26 \nTheorem 4.21 \nSection 4.4 \nSimilarity and Diagonalization \n301 \nThe final theorem of this section is an  important result that characterizes diagonaliz­\nable matrices in terms of the two notions of multiplicity that were introduced following \nExample 4.18. It gives precise conditions under which an n X n matrix can be diago­\nnalized, even when it has fewer than n eigenvalues, as in Example 4.26. We first prove a \nlemma that holds whether or not a matrix is diagonalizable. \nIf A is an n X n matrix, then the geometric multiplicity of each eigenvalue is less \nthan or equal to its algebraic multiplicity. \nProof Suppose A\n1 \nis an eigenvalue of A with  geometric multiplicity p; that  is, \ndim E\nA\n,\n= p. Specifically, let E\nA\n, \nhave basis B\n1 \n= {v\n1\n, v\n2\n, ..• , v\np\n}. Let Q be any invertible \nn X n matrix having v\n1\n, v\n2\n, .•. , v\np \nas its   first p columns-say, \nQ = (Y\n1 \n·   ·   · Yp Yp +l .  .  . Yn\nJ \nor, as a partitioned matr  ix, \nQ = [U \ni \nVJ \nLet \nwhere C is p X n.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":46450,"to":46496}}}}],[874,{"pageContent":"p\n}. Let Q be any invertible \nn X n matrix having v\n1\n, v\n2\n, .•. , v\np \nas its   first p columns-say, \nQ = (Y\n1 \n·   ·   · Yp Yp +l .  .  . Yn\nJ \nor, as a partitioned matr  ix, \nQ = [U \ni \nVJ \nLet \nwhere C is p X n. \nSince the columns of U are eigenvectors corresponding to A\n1\n, AU = A\n1 \nU. We \nalso have \nfrom which we obtain CU = I\nP\n, CV = 0, DU= 0, and DV = I\nn\n-\np · \nTherefore, \nQ\n-\n1\nAQ = \n[�\n]\nA \n[U \n:\nV J \n= \n[\n-�!.l:!!._l-�!.l:Y\n] \n= \n[�_1-�_f!_�-�!.l:YJ \n= \n[\n�-\n1\n-\n�!'.\nL\n��-�-\n] \nlD ' DA U ! DA V \nLA\n1\nDU\n:\nDA V \n0 ! DA V \nBy Exercise 69 in Section 4.2, it follows that \ndet\n(\nQ\n-\n1\nAQ \n-\nAI\n) \n= \n(\nA\n1 \n-\nA\n)\nP det\n(\nDA V \n-\nAI\n) \n(5) \nBut det ( Q\n-\n1\nAQ \n-\nAI) is the characteristic polynomial of Q\n-\n1\nAQ, which is the same \nas the characteristic polynomial of A, by Theorem 4.22(d). Thus, Equation (5) implies \nthat the algebraic multiplicity of A\n1 \nis at least p, its geometric multiplicity. \nThe Diagonalization Theorem \nLet A be an n X n matrix whose distinct eigenvalues are A \n1\n, A\n2\n, ... , A\nk\n. The following \nstatements are equivalent: \na. A is diagonalizable.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":46496,"to":46606}}}}],[875,{"pageContent":"The Diagonalization Theorem \nLet A be an n X n matrix whose distinct eigenvalues are A \n1\n, A\n2\n, ... , A\nk\n. The following \nstatements are equivalent: \na. A is diagonalizable. \nb. The union B of the bases of the eigenspaces of A (as in Theorem 4.24) contains \nn vectors. \nc.  The algebraic multiplicity of each eigenvalue equals its geometric multiplicity.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":46606,"to":46618}}}}],[876,{"pageContent":"308 \nChapter 4 \nEigenvalues and Eigenvectors \nProof (a) =? (b) If A is diagonalizable, then it has n linearly independent eigenvec­\ntors, by Theorem 4.23. If n; of these eigenvectors correspond to the eigenvalue A;, \n� \nthen B; contains at least n; vectors. (We already know that these n; vectors are linearly \nindependent; the only thing that might prevent them from being a basis for E\nA\n; \nis that \nthey might not span it.) Thus, B contains at least n vectors. But, by Theorem 4.24, \nB is a linearly independent set in !R\nn\n; hence, it contains exactly n vectors. \nExample 4.28 \nExample 4.29 \n(b) =? (c) Let the geometric multiplicity of A; be\nd\n; = dim E\nA\n; \nand let the algebraic \nmultiplicity of A; be m;. By Lemma 4.26, \nd\n; :s m; for i = 1, ... , k. Now assume that \nproperty (b) holds. Then we also have \nBut m\n1  +  m\n2 \n+ \n· · · \n+  m\nk \n= n, since the sum of the algebraic multiplicities of the \neigenvalues of A is just the degree of the characteristic polynomial of A-namely, n. \nIt follows that \nd\n1 \n+ \nd\n2 \n+ \n· · · \n+ \nd\nk \n= m","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":46620,"to":46667}}}}],[877,{"pageContent":"2 \n+ \n· · · \n+  m\nk \n= n, since the sum of the algebraic multiplicities of the \neigenvalues of A is just the degree of the characteristic polynomial of A-namely, n. \nIt follows that \nd\n1 \n+ \nd\n2 \n+ \n· · · \n+ \nd\nk \n= m\n1  +  m\n2 \n+ \n· · · \n+  m\nk\n, which implies that \n(6) \nUsing Lemma 4.26 again, we know that m; \n-\nd\n; 2: 0 for i = 1, ... , k, from which we \ncan deduce that each summand in Equation ( 6) is zero; that is, m; = \nd\n; for i = 1, ... , k. \n( c) =? (a) If the algebraic multiplicity m; and the geometric multiplicity \nd\n; are \nequal for each eigenvalue A; of A, then B has \nd\n1 \n+ \nd\n2 \n+ ·  ·  · \n+ \nd\nk \n= m\n1 \n+  m\n2 \n+ \n·  ·  · \n+  m\nk \n= n vectors, which are linearly independent, by Theorem 4.24. Thus, these are \nn linearly independent eigenvectors of A, and A is diagonalizable, by Theorem 4.23. \n(a) The matrix A = \n[\n� \n� \n�\ni from Example 4.18 has two distinct eigenvalues, \n2 \n-\n5 4 \nA\n1 \n= A\n2 \n= 1 and A3 = 2. Since the eigenvalue A\n1 \n= A\n2 \n= 1 has algebraic multiplicity","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":46667,"to":46741}}}}],[878,{"pageContent":"(a) The matrix A = \n[\n� \n� \n�\ni from Example 4.18 has two distinct eigenvalues, \n2 \n-\n5 4 \nA\n1 \n= A\n2 \n= 1 and A3 = 2. Since the eigenvalue A\n1 \n= A\n2 \n= 1 has algebraic multiplicity \n2 but geometric multiplicity 1, A is not diagonalizable, by the Diagonalization Theo­\nrem. (See also Example 4.25.) \n(b) The matrix A=[\n-\n� \n� \n-\n�\n] fromExample4.19 also hastwodistincteigen-\nl \n0 \n-\n1 \nvalues, A\n1 \n= A\n2 \n= 0 and A3 = \n-\n2. The eigenvalue 0 has algebraic and geometric mul­\ntiplicity 2, and the eigenvalue \n-\n2 has algebraic and geometric multiplicity 1. Thus, \nthis matrix is diagonalizable, by the Diagonalization Theorem. (This agrees with our \nfindings in Example 4.26.) \nWe conclude this section with an application of diagonalization to the computa­\ntion of the powers of a matrix. \nCompute A \n1\n0 \nif A = \n[\n� \n�\n]\n. \nSolution In Example 4.21, we found that this matrix has eigenvalues A\n1 \n= \n-\n1 \nand A\n2 \n= 2, with corresponding eigenvectors v\n1 \n= \n[ \n_ \n�\n] \nand v\n2 \n= \n[ \n�\n]\n. It follows","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":46741,"to":46815}}}}],[879,{"pageContent":"I \nExercises \n4.4 \nSection 4.4 \nSimilarity and Diagonalization \n309 \n(from any one of a number of theorems in this section) that A is diagonalizable and \nP-\n1\nAP = D\n, \nwhere \nl\nJ \n[\n-\n1 \nO\nJ \n2 \nand \nD = \n0 \n2 \nSolving for A\n, \nwe have A =  PDP-\n1 \nand, by Theorem \n4\n.22(f)\n, \nA\nn \n= PD\nn\nP-\n1 \nfor all \nn 2: 1. \nSince \n[\n(\n-\n0\n1\nr \nO\nJ \n2\nn \nwe have \nA\nn\n= \nP\nD\nn\np\n-\n1 \n= \n[\n_\n� \n�\nJ \n[\n(\n-\n0\n1\nr \n�\nn\nJ\n[ \n_\n� \n�\nr\nl \n[\n_\n� \n�\nJ \n[ \n( \n-\nO\nl\n)\nn \n0 \nJ \n[\nI \n2\nn \ni \n-\ni\nJ \n[ \n2\n(\n-\n1\n)\n\" \n+ \n2\n\" \n2\n(\n-\n1\n)\n•\n: \n+ \n2\n•\n+\n• \nH\nr\n'\n+ \n2\n· \nl \nr\n-\n1\nr\n:\n+\n2\n·\n+\n• \nSince we were only asked for A \n1\n0\n, \nthis is more than we needed. But now we can simply \nset n = 10 to find \n[\n2\n(\n-\n1\n)\n�\n+\n2\n1\n0 \nA\nl\nO \n= \n2\n(\n-\n1\n)\n11 \n+ \n2\n11 \n3 \n3 \n3\n4\n2 \n3\n4\n1 \n(\n-\n1\n)\n11 \n+ \n2\n1\n0\n] \n(-\n!\n)'\n: \n+ \n2\n\" \n� \n[\n682 68\n3\n] \nIn Exercises 1-4, show that A and Bare not similar matrices. \nIn Exercises 5-7, a diagonalization of the matrix A is given \nin th e form P-\n1\nAP = D. List the eigenvalues of A and bases \nfor the corresponding eigenspaces. \n1. \nA = \n[\n! \n�\nl \nB \n= \n[\n� \n�\nJ \n2A= \nB= \n[ \n2 \nl\nJ \n[ \n3 \n-\nl\nJ \n. \n-\n4 \n6 \n, \n-5 \n7 \n[\n: \n1 \n!\nJ\n.B\n�\n[-\n: \n0 \n3. A = \n2 \n4 \n0 \n3 \n4\n.A \n� \n[\n: \n2 \n-\n:\nJ\nB\n� \n[\n: \n-\n1 \n�\n] \n1 \n�\n] \n1 \n0 \n5. \n[ \n2 \n-\n1 \n-\n�\nJ\n[\n� \n-\n�\nJ\n[\n� \n�\nJ \n= \n[� \n�J \n6. \n[\n! \nl \nI \n1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":46817,"to":47118}}}}],[880,{"pageContent":"1. \nA = \n[\n! \n�\nl \nB \n= \n[\n� \n�\nJ \n2A= \nB= \n[ \n2 \nl\nJ \n[ \n3 \n-\nl\nJ \n. \n-\n4 \n6 \n, \n-5 \n7 \n[\n: \n1 \n!\nJ\n.B\n�\n[-\n: \n0 \n3. A = \n2 \n4 \n0 \n3 \n4\n.A \n� \n[\n: \n2 \n-\n:\nJ\nB\n� \n[\n: \n-\n1 \n�\n] \n1 \n�\n] \n1 \n0 \n5. \n[ \n2 \n-\n1 \n-\n�\nJ\n[\n� \n-\n�\nJ\n[\n� \n�\nJ \n= \n[� \n�J \n6. \n[\n! \nl \nI \n1 \n�m \n1 \nJ \n6 \n-\n1 \n0 \n1 \n0 \n-\n1 \n-\n2 \n1 \n0 \n3 \n-\n3 \n1 \n� \n[\n: \n0 \n-\n�\nl \n0 \n0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":47118,"to":47238}}}}],[881,{"pageContent":"310 \n7. \n[\n-\n! \n� [\n� \nChapter 4 \nEigenvalues and Eigenvectors \nI \nI \n3 \n�m \n0 \n_\n:\n] \n8 \n-� \n2 \n3 \n0 \n1 \n4 \n3 \n3 \n-1 \n-3 \n-3 3 \n0 \n-\n�l \n-\n2 \n0 \nIn Exercises 8-15, determine whether A is diagonalizable \nand, if so, find an invertible matrix P and a diagonal \nmatrix D such that P-\n1\nAP = D. \n[\n� \n�\n] \n[\n-\n3 \n�\n] \n8. A = \n9. A= \n-1 \n[\n� \n�\n] \n[\n: \n0 \nil \nIO.A = \n3 \n11. A= \n0 \n[\n� \n0 \n:\nJ \nH \n2 \nil \n12. A = 2 \n13. A= \n0 \n0 \n1 \nll \n0 0 \n;\n] \n15. A\n� \nl\n� \n0   0 \nj\n] \n3 \n2 2 \n0 \n1\n4\n.A = \n0 \n3 \n0 \n-\n2 \n0 0 0   0 \nIn Exercises 16-23, use th e method of Example 4.29 to \ncompute the indicated power of the matrix. \n[\n-\n4 \n:r \n[\n-\n� \n�r \n16. \n17. \n-\n3 \n18. \n[ \n4 \n-\n�\nr\n6 \n19. \n[\n� \n�r \n-1 \n20. \n[\n� \nn \n[\ni \n1 \n�\nr\n\" \n21. \n-1 \n0 \n-1 \n22. \n[\n: \n0 \nJ \n23. \n[\n� \nff \n1 \n-\n2 \n0 \nIn Exercises 24-29, find all (real) values ofkfor which A is \ndiagonalizable. \n2\n4\n.A = \n[\n� \n�\n] \n25. A= \n[\n� \n�\n] \n[\n� \n�\n] \n[\ni \n0 \n�\n] \n26. A = \n27. A= \n1 \n0 \n30. Prove Theorem 4.2\nl\n(c). \n31. Prove Theorem 4.22(b). \n32. Prove Theorem 4.22(c). \n33. Prove Theorem 4.22(e). \n34. Prove Theorem 4.22(f). \n35. Prove Theorem 4.22(g). \n36. If A and B are invertible matrices, show that AB and \nBA are similar.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":47240,"to":47429}}}}],[882,{"pageContent":"32. Prove Theorem 4.22(c). \n33. Prove Theorem 4.22(e). \n34. Prove Theorem 4.22(f). \n35. Prove Theorem 4.22(g). \n36. If A and B are invertible matrices, show that AB and \nBA are similar. \n37. Prove that if A and Bare similar matrices, then \ntr(A) = tr(B). [Hint: Find a way to use Exercise 45 \nfrom Section 3.2.] \nIn general, it is difficult to show that two matrices are simi­\nlar. However, if two similar matrices are diagonalizable, th e \ntask becomes easier. In Exercises 38-41, show that A and \nB are similar by showing that th ey are similar to the same \ndiagonal matrix. Then find an invertible matrix P such that \nP\n-\n1\nAP = B. \n38. A = \n[\n� \n39. A = \n[\n: \n40. A = \n[\n: \n41. A= \n[\n: \n-\n�\nl\nB = \n[\n� \n�\n] \n-\n3\n]\n,B =\n[\n-\nl  l\n] \n-\n2 \n-6 4 \n1 \n:\nJ\nB\n� [\n: \n-\n2 \n0 \n0 \n2\n] \n[\n-\n3 \n-1 \n� \n,B = : \n0 \n2 \n-\nS\ni \n2 \n-1 \n2 \n-\n4 \n-\n2 \n-\n�\nl \n5 \n4 \n42. Prove that if A is similar to B, then A\nT \nis similar to B\nT\n. \n43. Prove that if A is diagonalizable, so is A\nT\n. \n44. Let A be an invertible matrix. Prove that if A is diago­\nnalizable, so is A -\ni\n.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":47429,"to":47524}}}}],[883,{"pageContent":"5 \n4 \n42. Prove that if A is similar to B, then A\nT \nis similar to B\nT\n. \n43. Prove that if A is diagonalizable, so is A\nT\n. \n44. Let A be an invertible matrix. Prove that if A is diago­\nnalizable, so is A -\ni\n. \n45. Prove that if A is a diagonalizable matrix with only one \neigenvalue A, then A is of the form A = AI. (Such a \nmatrix is called a scalar matrix.) \n46. Let A and B be n X n matrices, each with n distinct \neigenvalues. Prove that A and B have the same eigen -\nvectors if and only if AB = BA . \n47. Let A and B be similar matrices. Prove that the alge­\nbraic multiplicities of the eigenvalues of A and B are \nthe same.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":47524,"to":47546}}}}],[884,{"pageContent":"Section 4.5 \nIterative Methods for Computing Eigenvalues \n311 \n48. Let A and B be similar matrices. Prove that the geo­\nmetric multiplicities of the eigenvalues of A and B are \nthe same. [Hint: Show that, if B = P\n-\n1\nAP, then every \neigenvector of Bis of the form p\n-\n1\nv for some eigen­\nvector v of A.] \n(a) Prove that it is not possible to find three linearly \nindependent vectors v\n1\n, v\n2\n, v\n3 \nin IR6 such that \nAv\n1 \n= \nv\n1\n, Av\n2 \n= \nv\n2\n, and Av\n3 \n= \nv\n3\n. \n(b) If A is diagonalizable, what are the dimensions of \nthe \neigenspaces E _\n1\n, E\n1\n, and E\n2\n? \n49. Prove that if A is a diagonalizable matrix such that \nevery eigenvalue of A is either 0 or 1, then A is idem­\npotent (that is, A\n2 \n=A\n)\n. \n52. LetA =\n[\n: �\n]\n. \n(a) Prove that A is diagonalizable if ( a  -  d\n)\n2 \n+ \n50. Let A be a nilpotent matrix (that is, A\nm \n= 0 for some \nm >  1\n)\n. Prove that if A is diagonalizable, then A must \nbe the zero matrix. \n4bc > 0 and is not diagonalizable if ( a  -  d\n)\n2 \n+ \n4bc < 0. \n(b) Find two examples to demonstrate that if","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":47548,"to":47623}}}}],[885,{"pageContent":"m \n= 0 for some \nm >  1\n)\n. Prove that if A is diagonalizable, then A must \nbe the zero matrix. \n4bc > 0 and is not diagonalizable if ( a  -  d\n)\n2 \n+ \n4bc < 0. \n(b) Find two examples to demonstrate that if \n51. Suppose that A is a 6 X 6 matrix with characteristic \npolynomial c\nA\n(\n,\\\n)  = (1 \n+ ,\\\n)\n(\n1 -,\\\n)\n2\n(\n2  -  ,\\\n)\n3\n• \n(\na  -  d\n)\n2 \n+ 4bc = 0, then A may or may not be \ndiagonalizable. \nIn 1824, the Norwegian math­\nematician Niels Henrik Abel \n(1802-1829) proved that a general \nfifth-degree (quintic) polynomial \nequation is not solvable by radicals; \nthat is, there is no formula for its \nroots in terms of its coefficients \nthat uses only the operations of \naddition, subtraction, multipli­\ncation, division, and taking nth \nroots. In a paper written in 1830 \nand published posthumously in \n1846, the French mathematician \nEvariste Galois (1811-1832) gave \na more complete theory that estab­\nlished conditions under which an \narbitrary polynomial equation can \nbe solved by  radicals. Galois's work \nwas instrumental in establishing","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":47623,"to":47676}}}}],[886,{"pageContent":"a more complete theory that estab­\nlished conditions under which an \narbitrary polynomial equation can \nbe solved by  radicals. Galois's work \nwas instrumental in establishing \nthe branch of algebra called group \ntheory; his approach to polynomial \nequations is now known as Galois \ntheory. \nIterative Methods tor Computing Eigenvalues \nAt this point, the only method we have for computing the eigenvalues of a matrix is \nto solve the characteristic equation. However, there are several problems with this \nmethod that render it impractical in all but small examples. The first problem is that \nit depends on the computation of a determinant, which is a very time-consuming \nprocess for large matrices. The second problem is that the characteristic equation is \na polynomial equation, and there are no formulas for solving polynomial equations \nof degree higher than 4 (polynomials of degrees 2, 3, and 4 can be solved using the \nquadratic formula and its analogues). Thus, we are forced to approximate eigenvalues","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":47676,"to":47693}}}}],[887,{"pageContent":"of degree higher than 4 (polynomials of degrees 2, 3, and 4 can be solved using the \nquadratic formula and its analogues). Thus, we are forced to approximate eigenvalues \nin most practical problems. Unfortunately, methods for approximating the roots of a \npolynomial are quite sensitive to roundoff error and are therefore unreliable. \nInstead, we bypass the characteristic polynomial altogether and take a different \napproach, approximating an eigenvector first and then using this eigenvector to find \nthe corresponding eigenvalue. In this section, we will explore several variations on \none such method that is based on a simple iterative technique. \nThe Power Method \nThe power method applies to an n X n matrix that has a dominant eigenvalue A\n1\n-\nthat is, an eigenvalue that is larger in absolute value than all of the other eigenvalues. \nFor example, if a matrix has eigenvalues -4, -3, 1, and 3, then -4 is the dominant \neigenvalue, since 4 = 1-41 > l-31 2 131 2 \nIll. On the other hand, a matrix with","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":47693,"to":47708}}}}],[888,{"pageContent":"For example, if a matrix has eigenvalues -4, -3, 1, and 3, then -4 is the dominant \neigenvalue, since 4 = 1-41 > l-31 2 131 2 \nIll. On the other hand, a matrix with \neigenvalues -4, -3, 3, and 4 has no dominant eigenvalue. \nThe power method proceeds iteratively to produce a sequence of scalars that con­\nverges to ,\\\n1 \nand a sequence of vectors that converges to the corresponding eigenvec­\ntor v\n1\n, the dominant eigenvector. For simplicity, we will assume that the matrix A is \ndiagonalizable. The following theorem is the basis for the power method.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":47708,"to":47719}}}}],[889,{"pageContent":"312 \nChapter 4 \nEigenvalues and Eigenvectors \nTheorem 4.28 \nLet \nA be an n X n diagonalizable matrix with dominant eigenvalue A 1 . Then there \nexists a nonzero vector x\n0 \nsuch that the sequence of vectors x\nk \ndefined by \nx\n1 \n= A\nX\no\n, x\n2 \n= Ax\n1\n, x\n3 \n= Ax\n2\n, ... , x\nk \n= Ax\nk\n_\n1\n, \n••• \napproaches a dominant eigenvector of A. \nProof \nWe may assume that the eigenvalues of A have been labeled so that \nI \nA\n1 I \n> \nI \nA\nz I \n2: \nI \nA\n3 I \n2: \n· · · \n2: \nI \nA\n\" \nI \nLet \nv1 , v\n2\n, ... , v\n\" \nbe the corresponding eigenvectors. Since v1, v\n2\n, ... , \nv\nn \nare linearly \n_. \nindependent (why?), they form a basis for !R\nn\n. Consequently, we can write x0 \nas a \nlinear combination of these eigenvectors-say, \nExample 4.30 \nXo \n= \nC\n1\nV\n1 \n+ \nC\n2\nV\n2 \n+ \n... \n+ \nC\nn\nV\nn \nNowx\n1 \n= Ax\n0\n,x\n2 \n= Ax\n1 \n= A\n(\nAx\n0) \n= A\n2\nx\n0\n,x\n3 \n= Ax\n2 \n= A\n(\nA\n2\nx\n0) \n= A\n3\nx\n0\n, and, generally, \nx\nk \n= A \nk\nX\no \nfor k 2: 1 \nAs we saw in Example 4.21, \nA\nk\nX\no \n= \nC\n1\nA\n�\nv\n1 \n+ \nC\n2\nA\n;\nv\n2 \n+ \n... \n+  c\nn\nA\n�\nv\nn \n= \nA\ni\n(\nc\n1\nv\n1 \n+ \nc\n2\n(�:\n)\n\\\n2 \n+ \n· · · \n+ \nc\n\"\n(\n��y\nv\n\"\n) \n(1) \nwhere we have used the fact that A\n1 \n* 0. \nThe \nfact that A 1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":47721,"to":47905}}}}],[890,{"pageContent":"x\nk \n= A \nk\nX\no \nfor k 2: 1 \nAs we saw in Example 4.21, \nA\nk\nX\no \n= \nC\n1\nA\n�\nv\n1 \n+ \nC\n2\nA\n;\nv\n2 \n+ \n... \n+  c\nn\nA\n�\nv\nn \n= \nA\ni\n(\nc\n1\nv\n1 \n+ \nc\n2\n(�:\n)\n\\\n2 \n+ \n· · · \n+ \nc\n\"\n(\n��y\nv\n\"\n) \n(1) \nwhere we have used the fact that A\n1 \n* 0. \nThe \nfact that A 1 \nis the dominant eigenvalue means that each of the fractions \nA\n2\n/ A\n1\n, A\n3\n/ A\n1\n, ... , A\nn\n/ A\n1\n, is less than 1 in absolute value. Thus, \nall go to zero as \nk � \noo. It follows that \nx\nk \n= \nA\nk\nX\no \n� \nA\n�\nc\n1\nv\n1 \nas\nk\n� \noo \n(2) \nNow, \nsince A 1 \n*  0 and v\n1 \n* 0, x\nk \nis approaching a nonzero multiple of\nv\n1 \n(that is, an \neig\nenvector corresponding to A 1) provided c\n1 \n* 0. (This is the required condition \non the initial vector x\n0\n: It must have a nonzero component c\n1 \nin the direction of the \ndomina\nnt eigenvector v 1 .\n) \nApproximate the dominant eigenvector of A = \n[\n� \n�\n] \nusing the method of \nTheorem 4.28. \nSolution We will take x\n0 \n= \n[ \n�\n] \nas the initial vector. Then \nx\n1 \n= Ax\n0 \n= \n[\n� \n�\n] \n[�\n] \n[\n�\n] \nx\n2 \n= \nAx\n1 \n= \n[\n� \n�\n] \n[\n�\n] \n[\n�\n] \nWe continue in this fashion to obtain the values ofx\nk \nin Table 4.1.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":47905,"to":48074}}}}],[891,{"pageContent":"Table 4.1 \nk 0 \n1 \nx\nk \n[\n�\n] \n[\n�\n] \nr\nk \n0.\n5\n0 \nl\nk \n1.00 \nSection 4.5 \nIterative Methods for Computing Eigenvalues \n313 \n7 \n6 \n5 \n4 \n3 \n2 \n2 \n[\n�\n] \n1.50 \n3.00 \ny \n3 \n[\n:\nJ \n0.83 \n1.67 \n4 \n5 \n6 \n[\n�\n�\n] \n[\n�\n�\n] \n[\n!\n�\n] \n1.10 \n0.95 \n1.02 \n2.20 \n1.91 \n2.05 \no \n0-/-t-----t------1---t-----t---r---r--\nx \n2  3  4  5  6  7 \nXo \nFigure 4.14 \n7 \n8 \n[\n!:\n] \n[\n171\n] \n1 70 \n0.99 \n1.01 \n1.98 \n2.01 \nFigure 4.14 shows what is happening geometrically. We know that the eigenspace \n� \nfor the dominant eigenvector will have dimension 1. (Why? See Exercise 46.) There­\nfore, it is a line through the origin in IR\n2\n. The first few iterates x\nk \nare shown along with \nthe directions they determine. It appears as though the iterates are converging on the \nline whose direction vector is \n[ \n�\n]\n. To confirm that this is the dominant eigenvector \nwe seek, we need only observe that the ratio r\nk \nof the first to the second component \nof x\nk \ngets very close to 1 as k increases. The second line in the body of Table 4.1 gives \nthese values, and you can see clearly that r\nk","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":48076,"to":48178}}}}],[892,{"pageContent":"k \nof the first to the second component \nof x\nk \ngets very close to 1 as k increases. The second line in the body of Table 4.1 gives \nthese values, and you can see clearly that r\nk \nis indeed approaching 1. We deduce that a \ndominant eigenvector of A is \n[ \n�\n]\n. \nOnce we have found a dominant eigenvector, how can we find the corresponding \ndominant eigenvalue? One approach is to observe that if an x\nk \nis approximately a \ndominant eigenvector of A for the dominant eigenvalue A\n1\n, then \nIt follows that the ratio l\nk \nof the first component of x\nk\n+ \n1 \nto that of x\nk \nwill approach A \n1 \nas\nk \nincreases. Table 4.1 gives the values of l\nk\n, and you can see that they are approach­\ning 2, which is the dominant eigenvalue.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":48178,"to":48213}}}}],[893,{"pageContent":"314 Chapter 4 \nEigenvalues and Eigenvectors \nTable 4.2 \nk 0 1 \nx\nk \n[\n�\n] \n[\n�\n] \nYk \n[\n�\n] \n[\n�\n·\n5\n] \nm\nk \n1 \n2 \n2 \nThere is a drawback to the method of Example 4.30: The components of the iter­\nates x\nk \nget very large very quickly and can cause significant roundoff errors. To avoid \nthis drawback, we can multiply each iterate by some scalar that reduces the magni­\ntude of its components. Since scalar multiples of the iterates x\nk \nwill still converge to a \ndominant eigenvector, this approach is acceptable. There are various ways to accom­\nplish it. One is to normalize each x\nk \nby dividing it by \nII \nxd\nl \n(i.e., to make each iterate \na unit vector). An easier method-and the one we will use-is to divide each x\nk \nby \nthe component with the maximum absolute value, so that the largest component is \nnow 1. This method is called scaling. Thus, if m\nk \ndenotes the component of x\nk \nwith \nthe maximum absolute value, we will replace x\nk \nby Yk = (1 / m\nk\n)\nx\nk\n. \nWe illustrate this approach with the calculations from Example 4.30. For x\n0\n, there","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":48215,"to":48276}}}}],[894,{"pageContent":"k \ndenotes the component of x\nk \nwith \nthe maximum absolute value, we will replace x\nk \nby Yk = (1 / m\nk\n)\nx\nk\n. \nWe illustrate this approach with the calculations from Example 4.30. For x\n0\n, there \nis nothing to do, since m\n0 \n= 1. Hence, \nWe then compute x\n1 \n= \n[ \n�\n] as before, but now we scale with m \n1 \n= 2 to get \nNow the calculations change. We take \nand scale to get \nY\n2 \n= \nC�\n5\n)[\n�\n·\n5\n] \n= \n[\n�\n.6\n7\n] \nThe next few calculations are summarized in Table 4.2. \nYou can now see clearly that the sequence of vectors Yk is converging to \n[ \n�\n]\n, a \ndominant eigenvector. Moreover, the sequence of scalars m\nk \nconverges to the corre­\nsponding dominant eigenvalue ,\\\n1 \n= 2. \n3 \n4 \n5 \n6 \n7 \n8 \n[ \n�\n·\n5\n] \n[\n�\n·\n67\n] \n[ \n1.83\n] \n1.67 \n[\n�\n·\n91\n] \n[ \n1.95\n] \n1.91 \n[\n�\n·\n98\n] \n[ \n1.99\n] \n1.98 \n[ \n�\n.67\n] \n[ \n�\n.83\n] \n[\n�\n.91\n] \n[\n�\n·\n95\n] \n[ \n�\n.98\n] \n[\n�\n·\n99\n] \n[ \n�\n.99\n] \n1.5 \n2 \n1.83 \n2 \n1.95 \n2 \n1.99","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":48276,"to":48406}}}}],[895,{"pageContent":"The Power Method \nExample 4.31 \nSection 4.5 \nIterative Methods for Computing Eigenvalues \n315 \nThis method, called the power method, is summarized below. \nLet A be a diagonalizable n X n matrix with a corresponding dominant eigen­\nvalue ,\\\n1\n. \n1. Let x0 = y0 be any initial vector in !R\nn \nwhose largest component is 1. \n2. Repeat the following steps for k = 1, 2, ... : \n(a) Compute x\nk \n= A\nYk\n-\n1\n· \n(b) Let m\nk \nbe the component of x\nk \nwith the largest absolute value. \n(c) Set y\nk\n= (\nl\n/\nm\nk\n)x\nk\n. \nFor most choices of x0, m\nk \nconverges to the dominant eigenvalue ,\\\n1 \nand \nY\nk \ncon-\nverges to a dominant eigenvector. \n4 \nUse the power method to approximate the dominant eigenvalue and a dominant \neigenvector of \n[\n-\n� \n5 \n-\n6\nl \nA= \n12 \n-12 \n-2 \n-2 \n10 \nSolution \nTaking as our initial vector \nwe compute the entries in Table 4.3. \n[ \n0.50\nl \nYou can see that the vectors \nYk \nare approaching \n1 and the scalars m\nk \nare \n-\n0.50 \napproaching 16. This suggests that they are, respectively, a dominant eigenvector and \nthe dominant eigenvalue of A. \nRemarks \n•","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":48408,"to":48486}}}}],[896,{"pageContent":"Yk \nare approaching \n1 and the scalars m\nk \nare \n-\n0.50 \napproaching 16. This suggests that they are, respectively, a dominant eigenvector and \nthe dominant eigenvalue of A. \nRemarks \n• \nIf the initial vector x0 has a zero component in the direction of the dominant \neigenvector v\n1 \n(i.e., if c = 0 in the proof of Theorem 4.28), then the power method \nTable 4.3 \nk    0 \nI \n2 \n3 \n4 \n5 \n6 \n7 \n[\n:\nl \n[ \n=\n:\nl \n[ \n-9 \n33 \nl \n[ \n862\nl \n[ \n812 \nl \n[ \n80\n3 \nl \n[ \n8.01\n-\n[ \n800] \nx\nk \n-19.33 \n17.31 \n16.25 \n16.05 \n16.01 \n16.00 \n11.67 \n-\n9.00 \n-8.20 \n-\n8.04 \n-\n8.01 \n-\n8.00 \nYk \n[\n:\nl \n[ \n=\n�\n�;\nl \n[ \n�\n4\n8\nl \n[ \n�s\no\nl \n[ \n�s\no\nl \n[ \n�s\no\nl \n[ \n�\ns\no\nl \n[ \n�s\no\nl \n1 1 \n-\n0.60 \n-\n0.52 \n-\n0.50 \n-\n0.50 \n-\n0.50 \n-\n0.50 \nm\nk \n1 \n6 \n-19.33 \n17.31 \n16.25 \n16.05 \n16.01 \n16.00","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":48486,"to":48611}}}}],[897,{"pageContent":"316 \nChapter 4 \nEigenvalues and Eigenvectors \nJohn William Strutt (1842-1919), \nBaron Rayleigh, was a British \nphysicist who made major contribu­\ntions to the fields of acoustics and \noptics. In 1871, he gave the first \ncorrect explanation of why the sky is \nblue, and in 1895, he discovered the \ninert gas argon, for which discovery \nhe received the Nobel Prize in 1904. \nRayleigh was president of the Royal \nSociety from 1905 to 1908 and \nbecame chancellor of Cambridge \nUniversity in 1908. He used \nRayleigh quotients in an 1873 paper \non vibrating systems and later in his \nbook The Theory of  Sound. \nExample 4.32 \nwill not converge to a dominant eigenvector. However, it is quite likely that during the \ncalculation of the subsequent iterates, at some point roundoff error will produce an x\nk \nwith a nonzero component in the direction of \nv\n1\n. The power method will then start to \nconverge to a multiple of \nv\n1\n. (This is one instance where roundoff errors actually help!)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":48613,"to":48643}}}}],[898,{"pageContent":"k \nwith a nonzero component in the direction of \nv\n1\n. The power method will then start to \nconverge to a multiple of \nv\n1\n. (This is one instance where roundoff errors actually help!) \n• The power method still works when there is a repeated dominant eigenvalue, \nor even when the matrix is not diagonalizable, under certain conditions. Details may \nbe found in most modern textbooks on numerical analysis. (See Exercises 21-24.\n) \n• For some matrices the power method converges rapidly to a dominant eigen­\nvector, while for others the convergence may be quite slow. A careful look at the proof \nof Theorem 4.28 reveals why. Since l,\\\n2\n/\n,\\\n1\n1 2: l,\\3\n/\n,\\\n1\n12': \n· · · \n2: l,\\\nn\n/\n,\\\n1\n1 , \nif l,\\\n2\n/\n,\\\n1\n1 \nis \nclose to zero, then (,\\\n2\n/ \n,\\\n1\n) \nk\n, .•• , (,\\\nn\n/ \n,\\\n1\n) \nk \nwill all approach zero rapidly. Equation \n(\n2\n) \nthen shows that x\nk \n= A\nk\nx0 will approach ,\\ic\n1\nv\n1 \nrapidly too. \nAs an illustration, consider Example 4.31. The eigenvalues are 16, 4, and 2, so \n,\\\n2\n/ \n,\\\n1 \n= 4\n/\n16 = 0.25. Since 0.25\n7","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":48643,"to":48718}}}}],[899,{"pageContent":"(\n2\n) \nthen shows that x\nk \n= A\nk\nx0 will approach ,\\ic\n1\nv\n1 \nrapidly too. \nAs an illustration, consider Example 4.31. The eigenvalues are 16, 4, and 2, so \n,\\\n2\n/ \n,\\\n1 \n= 4\n/\n16 = 0.25. Since 0.25\n7 \n= 0.00006, by the seventh iteration we should have \nclose to four-decimal-place accuracy. This is exactly what we saw. \n• \nThere is an alternative way to estimate the dominant eigenvalue ,\\\n1 \nof a ma­\ntrix A in conjunction with the power method. First, observe that if Ax = ,\\\n1\nx, then \n(\nAx\n) \n· x \n(\n,\\\n1\nx\n) \n· x  A\n1 \n(\nx · x\n) \nx · x \n= \nx · x \n= \nx · x \n=A\ni \nThe expression R (x) = ((Ax) · x) \n/ \n(x · x) is called a Rayleigh quotient. As we compute \nthe iterates x\nk\n, the successive Rayleigh quotients R (x\nk\n) should approach ,\\\n1\n. In fact, for \nsymmetric matrices, the Rayleigh quotient method is about twice as fast as the scaling \nfactor method. (See Exercises 17-20.) \nThe Shilled Power Melhod and lhe Inverse Power Melhod \nThe power method can help us approximate the dominant eigenvalue of a matrix, but","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":48718,"to":48783}}}}],[900,{"pageContent":"factor method. (See Exercises 17-20.) \nThe Shilled Power Melhod and lhe Inverse Power Melhod \nThe power method can help us approximate the dominant eigenvalue of a matrix, but \nwhat should we do if we want the other eigenvalues? Fortunately, there are several \nvariations of the power method that can be applied. \nThe shifted power method uses the observation that, if,\\ is an eigenvalue of A, \nthen ,\\ \n-\na is an eigenvalue of A \n-\nal for any scalar a (Exercise 22 in Section 4.3\n)\n. \nThus, if ,\\\n1 \nis the dominant eigenvalue of A, the eigenvalues of A \n-\n,\\if will be 0, \nA\n2 -\nA\n1\n, ,\\3 \n-\n,\\\n1\n, ... , A\nn \n-\n,\\\n1\n. We can then apply the power method to compute \n,\\\n2 \n-\n,\\\n1\n, and from this value we can find ,\\\n2\n. Repeating this process will allow us to \ncompute all of the eigenvalues. \nUse the shifted power method to compute the second eigenvalue of the matrix A = \n[\n� \n�\n] \nfrom Example 4.30. \nSolulion In Example 4.30, we found that ,\\\n1 \n= 2. To find ,\\\n2\n, we apply the power \nmethod to \nA \n-\n21 = \n[ \n-\n� \n_ \n�\n] \nWe take x\n0 \n= \n[ \n�\n]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":48783,"to":48850}}}}],[901,{"pageContent":"[\n� \n�\n] \nfrom Example 4.30. \nSolulion In Example 4.30, we found that ,\\\n1 \n= 2. To find ,\\\n2\n, we apply the power \nmethod to \nA \n-\n21 = \n[ \n-\n� \n_ \n�\n] \nWe take x\n0 \n= \n[ \n�\n]\n, but other choices will also work. The calculations are summarized \nin Table 4.4.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":48850,"to":48877}}}}],[902,{"pageContent":"Example 4.33 \nSection 4.5 \nIterative Methods for Computing Eigenvalues \n311 \nTable 4.4 \nk 0 \n1 \n2 \n3 \n4 \nx\nk \n[\n�\n] \n[\n-\n�\n] \n[ \n-\n�\n·\n5\n] \n[ \n-\n�\n·\n5\n] \n[ \n_\n�\n·\n5\n] \nY\nk \n[\n�\n] \n[\n-\n�\n.\n5\n] \n[\n-\n�\n.\n5\n] \n[\n-\n�\n.\n5\n] \n[\n-\n�\n.\n5\n] \nm\nk \n1 2 \n-3 -3 \n-3 \nOur choice of x0 has produced the eigenvalue - 3 after only two iterations. There­\nfore, A\n2 \n-\nA\n1 \n= \n-\n3, so A\n2 \n= \nA\n1 \n-  3 = 2 \n-\n3 = -1 is the second eigenvalue of \nA\n:._+ \nRecall from property (b) of Theorem 4.18 that if A is invertible with eigenvalue \nA, then A \n-\nl \nhas eigenvalue 1 /A. Therefore, if we apply the power method to A \n-\nl\n, its \ndominant eigenvalue will be the reciprocal of the smallest (in magnitude) eigenvalue \nof A. To use this inverse power method, we follow the same steps as in the power \nmethod, except that in step 2(a) we compute x\nk \n= A\n-\n1 \nY\nk\n-\nl\n· \n(In practice, we don't \nactually compute A \n-\ni \nexplicitly; instead, we solve the equivalent equation Ax\nk \n= \nYk\n-\ni \nfor x\nk \nusing Gaussian elimination. This turns out to be faster.) \nUse the inverse power method to compute the second eigenvalue of the matrix A = \n[ \n� \n�","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":48879,"to":49004}}}}],[903,{"pageContent":"k \n= \nYk\n-\ni \nfor x\nk \nusing Gaussian elimination. This turns out to be faster.) \nUse the inverse power method to compute the second eigenvalue of the matrix A = \n[ \n� \n�\n] \nfrom Example 4.30. \nSolution We start, as in Example 4.30, with \nXo \n= \ny\n0 \n= \n[ \n�\n]. To solve Ax\n1 \n= \ny0, \nwe use row reduction: \nThus, x\n1 \n= \n[\n�\n]\n,so y\n1 \n= \n[\n�\n]\n·\nThen we get x\n2 \nfrom Ax\n2 \n= \ny1\n: \n[ \n1 1 \nI \no \nJ \n[ \n1 o \nI \n0.5 \nJ \n[A \nI Y\ni\n] \n= \n2 0    1 \n� \n0 1 -0.5 \nHence, x\n2 \n= \n[ \n0\n·\n5\n]\n, and, by scaling, we get y\n2 \n= \n[ \n1\n]\n. Continuing, we get the \n-0.5 \n-1 \nvalues shown in Table 4.5, where the values m\nk \nare converging to -1. Thus, the \nsmallest eigenvalue of A is the reciprocal of -1 (which is also -1\n)\n. This agrees with \nour previous finding in Example 4.32.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":49004,"to":49092}}}}],[904,{"pageContent":"318 \nChapter 4 \nEigenvalues and Eigenvectors \nk \nTable 4.5 \n0 \n1 \n2 \n[\n�\n] \n[\n�] \n[\n-\n�\n:\n�\n] \n[\n�] \n[\n�\n] \n[ \n_�\n] \nO\n.\nS \nExample 4.34 \n3 \n4 \n5 \n6 \n7 8 \n9 \n[\n-\n0.S\n] \n[ \no\n.\ns \n] \n[ \no\n.\ns\n] \n[ \no\n.\ns \n] \n[ \no\n.\ns \n] \n[ \no\n.\ns \n] \n[ \no\n.\ns \n] \nl\n.\nS \n-\n0.83 \n-\n1.1 \n-\n0.9S \n-\n1.02 \n-\n0.99 \n-\n1.01 \n[\n-\n�\n.33\n] \n[\n-\n�\n.6\n] \n[\n-\n�\n.4S\n] \n[\n-\n�\n.\nS2\n] \n[\n-\n�\n.49\n] \n[\n-\n�\n.\nS\nl\n] \n[\n-\n�\n.\nS\nO\n] \nl\n.\nS \n-\n0.83 \n-\n1.1 \n-\n0.9S \n-\n1.02 \n-\n0.99 \n-\n1.01 \nThe Shilled Inverse Power Method \nThe most versatile of the variants of the power method is one that combines the two \njust mentioned. It can be used to find an approximation for any eigenvalue, provided \nwe have a close approximation to that eigenvalue. In other words, if a scalar a is given, \nthe shifted inverse power method will find the eigenvalue ,\\ of A that is closest to a. \nIf ,\\ is an eigenvalue of A and a  * ,\\, then A \n-\nal is invertible if a is not an \neigenvalue of A and 1 \n/ \n(,\\ \n-\na\n) \nis an eigenvalue of \n(\nA  -aI\n)\n-\n1\n. (See Exercise 4S .) If a \nis close to ,\\, then 1\n/\n(,\\ \n-\na\n) \nwill be a dominant eigenvalue of \n(\nA  -aI\n)\n-\n1\n. In fact, if \na is very close to ,\\, then 1 \n/ \n( ,\\ \n-\na\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":49094,"to":49274}}}}],[905,{"pageContent":"/ \n(,\\ \n-\na\n) \nis an eigenvalue of \n(\nA  -aI\n)\n-\n1\n. (See Exercise 4S .) If a \nis close to ,\\, then 1\n/\n(,\\ \n-\na\n) \nwill be a dominant eigenvalue of \n(\nA  -aI\n)\n-\n1\n. In fact, if \na is very close to ,\\, then 1 \n/ \n( ,\\ \n-\na\n) \nwill be much bigger in magnitude than the next \neigenvalue, so (as noted in the third Remark following Example 4.31) the conver­\ngence will be very rapid. \nUse the shifted inverse power method to approximate the eigenvalue of \nA=[-� \n-\n2 \nthat is closest to S. \nSolulion Shifting, we have \ns \n12 \n-\n2 \n-\n6\n] \n-\n12 \n10 \nA \n-\nSI = \n[ \n=: \n� \n-\n�\n�\ns\nl \n-\n2 \n-\n2 \nNow we apply the inverse power method with \nWe solve \n(\nA  -SI\n)\nx\n1 \n= \ny\n0 \nfor x\n1\n: \n[\n-\ns \n[\nA \n-\nSI \nI \ny\n0\n] \n= \n-\n4 \n-\n2 \ns \n7 \n-\n2 \n-\n6 \n-\n1\n2 \ns :]--� \n[\n: \n� \n� \n=\n�\n:\n:\n�\n] \n0  1 \n-\n0.39","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":49274,"to":49390}}}}],[906,{"pageContent":"Table 4.6 \nk \n0 \n1 \n[\n:\nJ \n[\n-\n061\n] \nx\nk \n-\n0.88 \n-\n0.39 \n[\n:\n] \n[ \n069\n-\nY\nk \n1.00 \n0.\n45 \nm\nk \n-\n0.88 \n2 \n[\n-\n041\n: \n-\n0.69 \n-\n0.35 \n[\n059\n] \n1.00 \n0.51 \n-\n0.69 \nSection 4.5 \nIterative Methods for Computing Eigenvalues \n319 \n3 \n4 \n5 \n6 \n7 \n[\n-\n047\n: \n[\n-\n049\n] \n[\n-\n050\n] \n[\n-\n050\n] \n[\n-\n050\n] \n-\n0.89 \n-\n0.95 \n-\n0.98 \n-\n0.99 \n-\n1.00 \n-\n0.44 \n-\n0.48 \n-\n0.49 \n-\n0.\n5\n0 \n-\n0.\n5\n0 \n[\n0 53\n] \n[ \n0 5 1 \nl \n[\n050\n] \n[ \n0 5\n0\n] \n[\n050\n-\n1.00 1.00 1.00 1.00 1.00 \n0.\n5\n0 0.\n5\n0 0.\n5\n0 0.\n5\n0 0.\n5\n0 \n-\n0.89 \n-\n0.95 \n-\n0.98 \n-\n0.99 \n-\n1.00 \nThis gives \n[\n-\n0.61\n] \nX\n1 \n= \n-\n0.88 \n, \n-\n0.39 \nm\n1 \n= \n-\n0.88, and \n1 \n[\n-\n0.61 \nl \n[\n0.69\n] \n-\n-\n-\n0.88 \n= 1 \n0.88 \n-\n0.39 \n0.45 \nWe continue in this fashion to obtain the values in Table 4.6, from which we deduce \nthat the eigenvalue of A closest to \n5 \nis approximately \n5 + 1\n/ \nm\n7 \n= \n5 + 1\n/ \n( \n-\n1) \n= 4, \nwhich, in fact, is exact. \n4 \nThe power method and its variants represent only one approach to the computa­\ntion of eigenvalues. In Chapter 5, we will discuss another method based on the QR \nfactorization of a matrix. For a more complete treatment of this topic, you can consult \nalmost any textbook on numerical methods.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":49392,"to":49585}}}}],[907,{"pageContent":"factorization of a matrix. For a more complete treatment of this topic, you can consult \nalmost any textbook on numerical methods. \n� Gerschgorin's Theorem \nWe owe this theorem to the \nRussian mathematician \nS. Gerschgorin (1901-1933), who \nstated it in 1931. It did not receive \nmuch attention until 1949, when it \nwas resurrected by Olga Taussky­\nTodd in a note she published in the \nAmerican Mathematica/ Monthly. \nIn this section, we have discussed several variations on the power method for ap­\nproximating the eigenvalues of a matrix. All of these methods are iterative, and the \nspeed with which they converge depends on the choice of initial vector. If only we had \nsome \"inside information\" about the location of the eigenvalues of a given matrix, \nthen we could make a judicious choice of the initial vector and perhaps speed up the \nconvergence of the iterative process. \nFortunately, there is a way to estimate the location of the eigenvalues of any","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":49585,"to":49602}}}}],[908,{"pageContent":"convergence of the iterative process. \nFortunately, there is a way to estimate the location of the eigenvalues of any \nmatrix. Gerschgorin's Disk Theorem states that the eigenvalues of a (real or complex) \nn X n matrix all lie inside the union of n circular disks in the complex plane. \nDefinition \nLetA = [a;\nj\n]bea(real or complex)nX nmatrix,and let r; denote \nthe sum of the absolute values of the off-diagonal entries in the ith row of A; that \nis, r; = \n2: la\nu\n·\nI The ith Gerschgorin disk is the circular disk D; in the complex \nj\n1'i \nplane with center a;; and radius r;. That is,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":49602,"to":49618}}}}],[909,{"pageContent":"320 \nChapter 4 \nEigenvalues and Eigenvectors \nExample 4.35 \nOlga Taussky-Todd (1906-1995) was born in Olmiitz in the Austro-Hungarian Empire \n(now Olmuac in the Czech Republic). She received her doctorate in number theory from the \nUniversity of Vienna in 1930. During World War II, she worked for the National Physical \nLaboratory in London, where she investigated the problem of flutter in the wings of super­\nsonic aircraft. Although the problem involved differential equations, the stability of an aircraft \ndepended on the eigenvalues of a related matrix. Taussky-Todd remembered Gerschgorin's \nTheorem from her graduate studies in Vienna and was able to use it to simplify the otherwise \nlaborious computations needed to determine the eigenvalues relevant to the flutter problem. \nTaussky-Todd moved to the United States in 1947, and ten years later she became the first \nwoman appointed to the California Institute of Technology. In her career, she produced over","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":49620,"to":49633}}}}],[910,{"pageContent":"Taussky-Todd moved to the United States in 1947, and ten years later she became the first \nwoman appointed to the California Institute of Technology. In her career, she produced over \n200 publications and received numerous awards. She was instrumental in the development of \nthe branch of mathematics now known as matrix theory. \nSketch the Gerschgorin disks and the eigenvalues for the following matrices: \n(a)A=\n[\n� \n-\n�\nJ \n(b) A = \n[\n� \n-\n�\n] \nSolulion (a) The two Gerschgorin disks are centered at 2 and \n-\n3 with radii 1 and2, \nrespectively. The characteristic polynomial of A is ,\\\n2 \n+ ,\\ \n-\n8, so the eigenvalues are \n,\\ = \n(\n-\n1 ± \nv1\n2 \n-\n4\n(\n-\n8\n))\n/\n2 \n= \n2.37, \n-\n3.37 \nFi\ngure 4.1 5 \nshows that the eigenvalues are contained within the two Gerschgorin \ndisks. \n(b) The two Gerschgorin disks are centered at 1 and 3 with radii I-3 I = 3 and 2, \nrespectively. The characteristic polynomial of A is ,\\ \n2 \n-\n4,\\ + 9, so the eigenvalues are \nA \n= \n(\n4 ± \nV\n(\n-\n4\n)\n2 \n-\n4\n(\n9\n))\n/\n2 = 2 ± iVs \n= \n2 + 2.23i, 2 \n-\n2.23i","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":49633,"to":49705}}}}],[911,{"pageContent":"respectively. The characteristic polynomial of A is ,\\ \n2 \n-\n4,\\ + 9, so the eigenvalues are \nA \n= \n(\n4 ± \nV\n(\n-\n4\n)\n2 \n-\n4\n(\n9\n))\n/\n2 = 2 ± iVs \n= \n2 + 2.23i, 2 \n-\n2.23i \nFigure 4.16 plots the location of the eigenvalues relative to the Gerschgorin disks. \nIm \n4 \n-4 \nFigure 4.15","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":49705,"to":49734}}}}],[912,{"pageContent":"Theorem 4.29 \nSection 4.5 \nIterative Methods for Computing Eigenvalues \n321 \nIm \n4 \n-4 \nFigure 4.16 \nAs Example 4.35 suggests, the eigenvalues of a matrix are contained within its \nGerschgorin disks. The next theorem verifies that this is so. \nGerschgorin's Disk Theorem \nLet A be an n X n (real or complex) matrix. Then every eigenvalue of A is contained \nwithin a Gerschgorin disk. \nProof Let A be an eigenvalue of A with corresponding eigenvector x. Let \nx\n; be \n_.... \nthe entry of x with the largest absolute value-and hence nonzero. (Why?) Then \nAx \n= \nAx, the ith row of which is \nRearranging, we have \nn \n2: \na\nij\nx\nJ \n= A\nx; \n1\n�1 \nbecause \nX\n; \n* 0. Taking absolute values and using properties of absolute value (see \nAppendix C), we obtain \n2:\na;1x1 \nI fi1\na\nij\nx\nJ\nI \n2: \nla;1x1\n1 \n2: \nlaullx1\nI \nI \nA \n-a;;\nI \n= \nJ\n*\ni \nJ\n*\ni \nJ\n*\ni \n:S 2: \nla;1\nI \nlx;\nI \n:S \nlx;\nI \nlx;\nI \n= r\n; \nX\n; \nJ\n*\ni \nbecause \nlx1\n1 \n:S \nlx;\nI \nforj * i. \nThis establishes that the eigenvalue A is contained within the Gerschgorin disk \ncentered at \na\n;; with radius r;.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":49736,"to":49828}}}}],[913,{"pageContent":"322 \nChapter 4 \nEigenvalues and Eigenvectors \nExample 4.36 \nRemarks \n• \nThere is a corresponding version of the preceding theorem for Gerschgorin \ndisks whose radii are the sum of the off-diagonal entries in the ith column of A. \n• \nIt can be shown that if k of the Gerschgorin disks are disjoint from the other \ndisks, then exactly k eigenvalues are contained within the union of these k disks. In \nparticular, if a single disk is disjoint from the other disks, then it must contain exactly \none eigenvalue of the matrix. Example 4.35(a) illustrates this. \n• \nNote that in Example 4.35(a), 0 is not contained in a Gerschgorin disk; that is, \n0 is not an eigenvalue of A. Hence, without any further computation, we can deduce \nthat the matrix A is invertible by Theorem 4.16. This observation is particularly use­\nful when applied to larger matrices, because the Gerschgorin disks can be determined \ndirectly from the entries of the matrix. \nConsider the matrix A = \n[ \n� 6 �\nl","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":49830,"to":49852}}}}],[914,{"pageContent":"ful when applied to larger matrices, because the Gerschgorin disks can be determined \ndirectly from the entries of the matrix. \nConsider the matrix A = \n[ \n� 6 �\nl \n· Gerschgorin's Theorem tells us that the eigen-\n2 0  8 \nvalues of A are contained within three disks centered at 2, 6, and 8 with radii 1, 1, \nand 2, respectively. See Figure 4. \nl \n7(a). Because the first disk is disjoint from the \nother two, it must contain exactly one eigenvalue, by the second Remark after Theo­\nrem 4.29. Because the characteristic polynomial of A has real coefficients, if it has \ncomplex roots (i.e., eigenvalues of A\n)\n, they must occur in conjugate pairs. (See Ap­\npendix D.) Hence there is a unique real eigenvalue between 1 and 3, and the union \nof the other two disks contains two (possibly complex) eigenvalues whose real parts \nlie between \n5 \nand 10. \nOn the other hand, the first Remark after Theorem 4.29 tells us that the same \nthree eigenvalues of A are contained in disks centered at 2, 6, and 8 with radii�, 1, \nand \nt,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":49852,"to":49877}}}}],[915,{"pageContent":"lie between \n5 \nand 10. \nOn the other hand, the first Remark after Theorem 4.29 tells us that the same \nthree eigenvalues of A are contained in disks centered at 2, 6, and 8 with radii�, 1, \nand \nt, \nrespectively. See Figure 4.\nl \n7(b ). These disks are mutually disjoint, so each con­\ntains a single (and hence real) eigenvalue. Combining these results, we deduce that \nA has three real eigenvalues, one in each of the intervals [1, 3], [\n5\n, 7], and [7.5, 8.5]. \n(Compute the actual eigenvalues of A to verify this.) \nIm \nIm \n4 4 \n-4 \n-4 \n(a) \n(\nb\n) \nFigure 4.11","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":49877,"to":49901}}}}],[916,{"pageContent":"Section 4.5 \nIterative Methods for Computing Eigenvalues \n323 \n. \n'� \n1 \nExercises \n4.5 \nIn Exercises 1-4, a matrix A is given along with an iterate \nx\n5\n, produced as in Example 4.30. \n(a)   Use these data to approximate a dominant eigenvector \nwhose first component is 1 and a corresponding dominant \neigenvalue. (Use three-decimal-place accuracy.) \n(b) Compare your approximate eigenvalue in part (a) with \nth e actual dominant eigenvalue. \n[\nl \n2\n] \n[ \n4443\n] \n1. A  = \n5 4 \n' \nX\ns \n= \n11109 \n2.A  = \n3.A  = \n[\n_\n� \n[\n� \n4\n] \n[ \n7811\n] \n-\n1 \n,\nX\ns \n= \n-\n3904 \nl\n] \n[\n144\n] \n1 \n'\nX\ns \n= \n89 \n4 A= \nx   = \n[\n1.5 \n0.\n5\n] \n[ \n60.62\n5\n] \n. \n2.0 3.0 \n' \n5 \n239.500 \nIn Exercises 5-8, a matrix A is given along with an iterate \nx\nk\n> produced using th e power method, as in Example 4.31. \n(a) Approximate the dominant eigenvalue and eigenvector \nby computing the corresponding m\nk \nand \nY\nk\n· \n(b) Verify that \nyou have approximated an eigenvalue and an eigenvector \nof A by comparing Ay\nk \nwith m\nk\nY\nk\n· \n5.A  = \n6.A  = \n7.A  = \n[\n_\n� \n[\n� \n[-� \n-\n3\n] \n[\n-\n3.667\n] \n1\n0 \n, x\ns\n= \n11.001 \n-\n�\nl \nX\n1\n0 \n= \n[\n�\n:\n!\n�\n�\n] \n� \n�\n1\n,\nX\ng \n= \n[\n1\n�\n:","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":49903,"to":50047}}}}],[917,{"pageContent":"k\n· \n(b) Verify that \nyou have approximated an eigenvalue and an eigenvector \nof A by comparing Ay\nk \nwith m\nk\nY\nk\n· \n5.A  = \n6.A  = \n7.A  = \n[\n_\n� \n[\n� \n[-� \n-\n3\n] \n[\n-\n3.667\n] \n1\n0 \n, x\ns\n= \n11.001 \n-\n�\nl \nX\n1\n0 \n= \n[\n�\n:\n!\n�\n�\n] \n� \n�\n1\n,\nX\ng \n= \n[\n1\n�\n:\n��\n�\n1 \n0 4 \n10.000 \n2 \n1 \n-\n1 \n-\n2\n1 \n[ \n3.415\n1 \n-\n3 \n, \nX\n1\n0 \n= \n2.914 \n1 \n-\n1.207 \nIn Exercises 9-14,  use the power method to approximate \nth e dominant eigenvalue and eigenvector of A. Use th e given \nin\nitial vector x0 , th e specified number of iterations k, and \nthree-decimal-place accuracy. \n[\n14 \n9. A= \n5 \n[\n-\n6 \n10. A\n= \n8 \nl\n�\nl \nXo \n= \n[\n�\nl \nk  = 5 \n-\n�l \nXo \n= \n[\n�l \nk  = 6 \n11.A  = \n[\n� \n�\nl\nX\no = \n[\n�l\nk \n= 6 \n[\n3.5 \n12.A  = \n1.5 \n13.A  = \n[\n4\n9\n8 \n1.\n5\n] \n[\nl\n] \n-0.5 'Xo \n= \n0 \n'\nk \n= 6 \n4 \n15 \n-\n4 \n: \nn\n�\n�\nuk\n�\n6 \nIn Exercises 15 and 16,  use th e power method to approxi­\nmate th e dominant eigenvalue and eigenvector of A to \ntwo-decimal-place accuracy. Choose any initial vector you \nlike (but keep the first Remark after Example 4. 31 in mind!) \nand apply th e method until th e digit in the second decimal \nplace of the iterates stops changing. \n[ \n12\n2 \n6\n0 \n16.A  = \n-\n6 6 \n-\n6\n1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":50047,"to":50221}}}}],[918,{"pageContent":"like (but keep the first Remark after Example 4. 31 in mind!) \nand apply th e method until th e digit in the second decimal \nplace of the iterates stops changing. \n[ \n12\n2 \n6\n0 \n16.A  = \n-\n6 6 \n-\n6\n1 \n-2 \n12 \nRayleigh quotients are described on page 316. In Exer-\ncises 17-20, to see how th e Rayleigh quotient method ap­\nproximates the dominant eigenvalue more rapidly than the \nordinary power method, compute the successive Rayleigh \nquotients R\n(\nx;\n) \nfor i = 1, ... , k for the matrix A in th e given \nexercise. \n17. Exercise 11 \n19. Exercise 13 \n18. Exercise 12 \n20. Exercise 14 \nThe matrices in Exercises 21-24 either are not diagonaliz­\nable or do not have a dominant eigenvalue (or both). Apply \nthe power method anyway with th e given initial vector x\n0\n, \nperforming eight iterations in each case. Compute the exact \neigenvalues and eigenvectors and explain what is happening. \n21.\nA \n= \n[� \n�\n],x0 \n= \n[\n�\n] \n22. \nA\n= \n[ \n_� \n�\n],x0 \n= \n[\n�\n] \n23. A \n� \n[ \n� � +• \n� \nm \n24.A \n� \n[\n� \n� \nn\n� \n� \nm","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":50221,"to":50293}}}}],[919,{"pageContent":"324 Chapter 4 \nEigenvalues and Eigenvectors \nIn Exercises 25-28, the power method does not converge to \nth e dominant eigenvalue and eigenvector.  Verify this, using \nth e given initial vector x\n0\n• Compute the exact eigenvalues \nand eigenvectors and explain what is happening. \n25.A=\n[\n=\n� \n�\nl\nXo\n=\n[\n�\n] \n26. A  = \n[ \n_ \n� \n� \nl \nXo \n= \n[ \n�\n] \n27.A \n_ \nn ; \n_\nn\n� \n_ \n[:\nJ \n42. p\n(\nx\n) \n= x\n2 \n-  x -3, a = 2 \n43. p\n(\nx\n) \n= x\n3 \n-2x\n2 \n+ 1, a = 0 \n44. p\n(\nx\n) \n= x\n3 \n-5x\n2 \n+ x + l, a = 5 \n45. Let A be an eigenvalue of A with corresponding \neigenvector x. If a  *  A and a is not an eigenvalue of \nA, show that 1 /\n(\nA -  a\n) \nis an eigenvalue of \n(\nA  -al\n)\n-\n1 \nwith corresponding eigenvector x. (Why must A  -al \nbe invertible?) \n46. If A has a dominant eigenvalue A\n1\n, prove that the ei­\ngenspace E\nA\n, \nis one-dimensional. \n2&A -\n[\n: \n�: \n�\n]\n·\n� -\n[:\n] \n�\nIn Exercises 47-50, draw th e Gerschgorin disks for the given \nmatrix. \nIn Exercises 29-32, apply th e shifted power method to \napproximate th e second eigenvalue of the matrix A in th e","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":50295,"to":50394}}}}],[920,{"pageContent":": \n�: \n�\n]\n·\n� -\n[:\n] \n�\nIn Exercises 47-50, draw th e Gerschgorin disks for the given \nmatrix. \nIn Exercises 29-32, apply th e shifted power method to \napproximate th e second eigenvalue of the matrix A in th e \ngiven exercise. Use the given initial vector x\n0\n, k iterations, \nand three-decimal-place accuracy. \n29. Exercise 9 \n31. Exercise 13 \n30. Exercise 10 \n32. Exercise 14 \nIn Exercises 33-36, apply the inverse power method to \napproximate, for the matrix A in the given exercise, the ei­\ngenvalue that is smallest in magnitude. Use the given initial \nvector x\n0\n, k iterations, and three-decimal-place accuracy. \n33. Exercise 9 \n34. Exercise 10 \n35. Exmi\" 7\n. \n� -\n[ \n_\n:\nJ \nk  -  5 \n36. Exercise 14 \nIn Exercises 37-40, use th e shifted inverse power method \nto approximate, for the matrix A in the given exercise, th e \neigenvalue closest to a. \n37. Exercise 9, a = 0 \n39. Exercise 7, a = 5 \n38. Exercise 12, a = 0 \n40. Exercise 13, a =  - 2 \nExercise 32 in Section 4.3 demonstrates that every poly­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":50394,"to":50439}}}}],[921,{"pageContent":"eigenvalue closest to a. \n37. Exercise 9, a = 0 \n39. Exercise 7, a = 5 \n38. Exercise 12, a = 0 \n40. Exercise 13, a =  - 2 \nExercise 32 in Section 4.3 demonstrates that every poly­\nnomial is (plus or min us) th e characteristic polynomial of \nits own companion matrix. Therefore, th e roots of a poly­\nnomial pare th e eigenvalues of C\n(\np\n)\n. Hence, we can use \nthe methods of this section to approximate th e roots of any \npolynomial when exact results are not readily available. In \nExercises 41-44, apply the shifted inverse power method to \nthe companion matrix C \n( \np\n) \nof p to approximate the root of \np closest to a to three decimal places. \n41. p\n(\nx\n) \n= x\n2 \n+ 2x -2, a = 0 \n47. u \n1 \n!\n] \n48. \n[\n� \n-i \n1 \n+ \n�i \n4 \n2i \n0 \n-2i \n[ \n4 � 3i \n2 \n-2 \n-l+  i \n0 0 \n49. \n1 +  i \n-i \n5  + 6i \n2; \nl \n1 \n-2i \n2i \n-5 -Si \n50. \n[\n; \nI \n0 \n�\n] \n2 \n4 \n! \n4 \nI \n6 \n6 \n0 \nI \n8 \n51. A square matrix is strictly  diagonally dominant if the \nabsolute value of each diagonal entry is greater than \nthe sum of the absolute values of the remaining entries \nin that row. (See Section 2.5.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":50439,"to":50519}}}}],[922,{"pageContent":"51. A square matrix is strictly  diagonally dominant if the \nabsolute value of each diagonal entry is greater than \nthe sum of the absolute values of the remaining entries \nin that row. (See Section 2.5.\n) \nUs  e Gerschgorin's Disk \nTheorem to prove that a strictly diagonally dominant \nmatrix must be invertible. [Hint: See the third Remark \nafter Theorem \n4.29.] \n52. If A is an n X n matrix, let 11 A 11 denote the maximum of \nthe sums of the absolute values of the rows of A; that is, \nll\nA\nll \n= \n1\n�'!,� c� \nl\na\niJ\nI). (See Section 7.2.\n) \nProve that \nif ,\\ is an eigenvalue of A, then I A I \n::; \nII A II . \n53. Let A be an eigenvalue of a stochastic matrix A \n(see Section 3.7). Prove that I A I \n::; \n1. [Hint: Apply \nExercise \n5\n2 to A\nr\n.] \n54. Prove that the eigenvalues of A  = \n[\n� \n� \n! 0 \n0  0 \n� \n�1 are \n3 \n2 \n� \n7 \nall real, and locate each of these eigenvalues within a \nclosed interval on the real line.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":50519,"to":50568}}}}],[923,{"pageContent":"' \nTheorem 4.30 \nTheorem 4.31 \nSection 4.6 \nApplications and the Perron-Frobenius Theorem \n325 \nApplications and the Perron-Frobenius Theorem \nIn this section, we will explore several applications of eigenvalues and eigenvectors. \nWe begin by revisiting some applications from previous chapters. \nMarkov Chains \nSection 3.7 introduced Markov chains and made several observations about the tran­\nsition (stochastic) matrices associated with them. In particular, we observed that if \nP is the transition matrix of a Markov chain, then P has a steady state vector x. That \nis, there is a vector x such that Px = x. This is equivalent to saying that P has 1 as an \neigenvalue. We are now in a position to prove this fact. \nIf Pis the n X n transition matrix of a Markov chain, then 1 is an eigenvalue of P. \nProof Recall that every transition matrix is stochastic; hence, each of its columns \nsums to 1. Therefore, if \nj \nis a row vector consisting of n \nl\ns, then \nj\nP = \nj\n. (See Exer­\ncise 13 in Section 3.7.\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":50570,"to":50597}}}}],[924,{"pageContent":"Proof Recall that every transition matrix is stochastic; hence, each of its columns \nsums to 1. Therefore, if \nj \nis a row vector consisting of n \nl\ns, then \nj\nP = \nj\n. (See Exer­\ncise 13 in Section 3.7.\n) \nTaking transposes, we have \np\nY\nf \n= (\nj\nP\nf \n= \nf \nwhich implies that \nj\nr is an eigenvector of p\nT \nwith corresponding eigenvalue 1. By \nExercise 19 in Section 4.3, P and p\nT \nhave the same eigenvalues, so 1 is also an eigen­\nvalue of P. \nIn fact, much more is true. For most transition matrices, every eigenvalue A sat­\nisfies IAI ::=::: 1 and the eigenvalue 1 is dominant; that is, if A  * 1, then IAI < 1. We need \nthe following two definitions: A matrix is called positive if all of its entries are posi­\ntive, and a square matrix is called regular if some power of it is positive. For example, \nA = \n[\n� \n�\n] \nis positive but B = \n[\n� \n�\n] \nis not. However, Bis regular, since B\n2 \n= \n[ \n1 \n! \n�\n] \nis positive. \nLet P be an n X n transition matrix with eigenvalue A. \na. \nI\nA\nI \n::=::: 1 \nb. If Pis regular and A  * 1, then IAI < 1.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":50597,"to":50657}}}}],[925,{"pageContent":"[\n� \n�\n] \nis not. However, Bis regular, since B\n2 \n= \n[ \n1 \n! \n�\n] \nis positive. \nLet P be an n X n transition matrix with eigenvalue A. \na. \nI\nA\nI \n::=::: 1 \nb. If Pis regular and A  * 1, then IAI < 1. \nProof As in Theorem 4.30, the trick to proving this theorem is to use the fact that p\nT \nhas the same eigenvalues as P. \n(a) Let x be an eigenvector of p\nT \ncorresponding to A and let x\nk \nbe the component of x \nwith the largest absolute value m. Then lx;I ::=::: lx\nk\nl = m for i = 1, 2, ... , n. Comparing \nthe kth components of the equation P\nT\nx = Ax, we have \nPlk\nX\n1 \n+ \nP\n2\nk\nX\n2 \n+ \n· · · \n+ \nP\nnk\nX\nn \n= \nAx\nk","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":50657,"to":50709}}}}],[926,{"pageContent":"326 \nChapter 4 \nEigenvalues and Eigenvectors \n(Remember that the rows of p\nT \nare the columns of P.) Taking absolute values, we obtain \nI \nA\nl\nm= \nI\nA\nI \nl\nx\nk\nl \n= \nI\nAx\nk\nl \n= \nl\np\nlk\nx\n1 \n+ \nP\n2\nk\nX\n2 \n+ \n· · · \n+ \nP\nnk\nx\nn\nl \n:S \nl\np\nlk\nx\nl \nI \n+ \nI\nP\n2\nk\nX\n2 \nI \n+ \n· · · \n+ \nI\nPnk\nX\nn \nI \n= \nP1k\nl\nx\n,\nI \n+ \nP\n2\nk\nl\nx\n2\nI \n+ \n· · · \n+ \nP\nnk\nl\nx\nn\nl \n(\n1\n) \n:S \nP\nlk\nm \n+ \nP\n2\nk\nm  + \n· · · \n+ \nP\nnk\nm \n= \n(\np\n,\nk \n+ \nP\n2\nk \n+ \n· · · \n+ \nP\nnk\n)\nm  =  m \nThe first inequality follows from the Triangle Inequality in IR, and the last equality \ncomes from the fact that the rows of p\nT \nsum to 1. Thus, IAlm :s m. After dividing by \nm, we have IAI :s 1, as desired. \n(b) We will prove the equivalent implication: If IAI = 1, then A  = 1. First, we show \nthat it is true when P (and therefore P\nT\n) is a positive matrix. If IAI = 1, then all of the \ninequalities in Equations \n( \n1\n) \nare actually equalities. In particular, \nEquivalently, \n(\n2\n) \nNow, since Pis positive,p;\nk \n> 0 for i = 1, 2, ... ,  n. Also, m -  lx;I 2: 0 for i = 1, 2, ... ,  n. \nTherefore, each summand in Equation \n(\n2\n) \nmust be zero, and this can happen only if","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":50711,"to":50857}}}}],[927,{"pageContent":"(\n2\n) \nNow, since Pis positive,p;\nk \n> 0 for i = 1, 2, ... ,  n. Also, m -  lx;I 2: 0 for i = 1, 2, ... ,  n. \nTherefore, each summand in Equation \n(\n2\n) \nmust be zero, and this can happen only if \nlx;I = m for i = 1, 2, ... ,  n. Furthermore, we get equality in the Triangle Inequality in \nIR if and only if all of the summands are positive or all are negative; in other words, \nthe p;\nk\nX; 's all have the same sign. This implies that \nwhere j is a row vector of n \nl\ns, as in Theorem 4.30. Thus, in either case, the eigenspace \nof p\nT \ncorresponding to A is E\nA \n= span(j\nr\n). \nBut, using the proof of Theorem 4.30, we see that j\nT \n= p\nT \n{ = A{, and, compar­\ning components, we find that A = 1. This handles the case where P is positive. \nIf Pis regular, then some power of Pis positive-say, P\nk\n. It follows that p\nk\n+\nI must \n� \nalso be positive. (Why?) Since A \nk \nand A \nk\n+ \n1 \nare eigenvalues of p\nk \nand p\nk\n+ \n1\n, respectively, \nby Theorem 4.18, we have just proved that A\nk \n= A\nk\n+\nl \n= 1. Therefore, A\nk\n(A -  1\n) \n= 0,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":50857,"to":50919}}}}],[928,{"pageContent":"k\n+\nI must \n� \nalso be positive. (Why?) Since A \nk \nand A \nk\n+ \n1 \nare eigenvalues of p\nk \nand p\nk\n+ \n1\n, respectively, \nby Theorem 4.18, we have just proved that A\nk \n= A\nk\n+\nl \n= 1. Therefore, A\nk\n(A -  1\n) \n= 0, \nwhich implies that A = 1, since A = 0 is impossible if IAI = 1. \nWe can now explain some of the behavior of Markov chains that we observed in \nChapter 3. In Example 3.64, we saw that for the transition matrix \np = \n[\n0.7    0.2\n] \n0.3    0.8 \n[\n0.6\n] \nand initial state vector Xo = \n, the state vectors x\nk \nconverge to the vector x = \n[\n0.4\n] \n0.4 \n, a steady state vector for P (i.e., Px = x\n)\n. We are going to prove that for regular \n0.6","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":50919,"to":50969}}}}],[929,{"pageContent":"Example 4.31 \nLemma 4.32 \nSection 4.6 Applications and the Perron-Frobenius Theorem \n321 \nMarkov chains, this always happens. Indeed, we will prove much more. Recall that \nthe state vectors x\nk \nsatisfy x\nk \n= P\nk\nx0. Let's investigate what happens to the powers p\nk \nas P becomes large. \n[\n0.\n7 \nThe transition matrix P = \n0.3 \n1\n0.\n7 -  A \n0 = det (P -AI) = \n0.3 \n0.2\n] \n0.8 \nhas characteristic equation \n0.2 \nI \n2 \n=  A \n-  l\n.\n5\nA \n+ \n0.\n5 \n= \n(\nA -  l\n)(\nA \n-\n0.\n5) \n0.8 -  A \nso its eigenvalues are A\n1 \n= 1 and A\n2 \n= 0.\n5\n. (Note that, thanks to Theorems 4.30 and \n4.31, we knew in advance that 1 would be an eigenvalue and the other eigenvalue \nwould be less than 1 in absolute value. However, we still needed to compute A\n2\n.\n) \nThe \neigenspaces are \nE\n1 \n= \nspan\n( \n[ \n�\n]\n) \nan\nd \nE\n0\n.\n5 \n= \nspan\n( \n[ \n_ \n�\n]\n) \nSo, taking Q  = \n[\n2\n3 \n1\n] , we know that Q\n-\n1\nPQ = \n[ \n1 \nO \n] =  D. From the method \n-1 0 0.\n5 \nused in Example 4.29 in Section 4.4, we have \np\nk\n= \nQD\nk\nQ\n-\ni \n= \n[\n2 \nl\n][\nl\nk \n0 \n]\n[\n2 \n3 \n-1 0 \n(\n0.\n5\n)\nk \n3 \n1 \n]\n-\n! \n-1 \nNow, as\nk� \noo, (0.\n5)\nk \n� \n0, so \nand p\nk \n� \n[ \n� \n_ \n�\n] \n[ \n� \n�\n] \n[ \n� \n1\n]\n-\nl \n= \n[\n0.4 0.4\n]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":50971,"to":51132}}}}],[930,{"pageContent":"-1 0 0.\n5 \nused in Example 4.29 in Section 4.4, we have \np\nk\n= \nQD\nk\nQ\n-\ni \n= \n[\n2 \nl\n][\nl\nk \n0 \n]\n[\n2 \n3 \n-1 0 \n(\n0.\n5\n)\nk \n3 \n1 \n]\n-\n! \n-1 \nNow, as\nk� \noo, (0.\n5)\nk \n� \n0, so \nand p\nk \n� \n[ \n� \n_ \n�\n] \n[ \n� \n�\n] \n[ \n� \n1\n]\n-\nl \n= \n[\n0.4 0.4\n] \n-1 \n0.6 0.6 \n(Observe that the columns of this \"limit matrix\" are identical and each is a steady \nstate vector for P.) Now let \nXo \n= \n[\na\n] \nbe any initial probability vector (i.e., a \n+ \nb = 1). \nThen \nb \nX \n= \np·\nv� \n� \nk \n[\n0.4 \nk \n·-u \n0.6 \n0.4\n] [\na\n] \n= \n[\n0.4a \n+ \n0.4b\n] \n= \n[\n0.4\n] \n0.6 b \n0.6a \n+ \n0.6b 0.6 \nNot only does this explain what we saw in Example 3.64, it also tells us that the state \n[\n0.4\n] \nveoto;s x, will oonmge to the steody stote vedo\nc \nx � \n0.6 \nfoe any choke of x,!\n4 \nThere is nothing special about Example 4.37. The next theorem shows that this \ntype of behavior always occurs with regular transition matrices. Before we can present \nthe theorem, we need the following lemma. \nLet P be a regular n X n transition matrix. If P is diagonalizable, then the dominant \neigenvalue A\n1 \n= 1 has algebraic multiplicity 1.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":51132,"to":51255}}}}],[931,{"pageContent":"328 \nChapter 4 \nEigenvalues and Eigenvectors \nTheorem 4.33 \nSee Finite Markov Chains by J. G. \nKemeny and J. L. Snell (New York: \nSpringer-Verlag, 1976). \nWe are taking some liberties with \nthe notion of a limit. Nevertheless, \nthese steps should be intuitively \nclear. Rigorous proofs follow from \nthe properties oflimits, which you \nmay have encountered in a calcu­\nlus course. Rather than get side­\ntracked with a discussion of matrix \nlimits, we will omit the proofs. \nProof The eigenvalues of P and \np\nT \nare the same. From the proof of Theorem 4.31 (b ), \nA\n1 \n= 1 has geometric multiplicity 1 as an eigenvalue of P\nr\n. Since Pis diagonalizable, \nso is P\nr\n, by Exercise 41 in Section 4.4. Therefore, the eigenvalue A\n1 \n= 1 has algebraic \nmultiplicity 1, by the Diagonalization Theorem. \nLet P be a regular n X n transition matrix. Then as k ---+ oo, \np\nk \napproaches an \nn X n matrix L whose columns are identical, each equal to the same vector x. \nThis vector xis a steady state probability vector for P.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":51257,"to":51293}}}}],[932,{"pageContent":"p\nk \napproaches an \nn X n matrix L whose columns are identical, each equal to the same vector x. \nThis vector xis a steady state probability vector for P. \nProof To simplify the proof, we will consider only the case where P is diagonaliz­\nable. The theorem is true, however, without this assumption. \nWe diagonalize Pas Q\n-\n1\nPQ =Dor, equivalently, P = QDQ\n-\n1\n, where \n0 \nFrom Theorems 4.30 and 4.31, we know that each eigenvalue A; either is 1 or satisfies \nI\nA;\nI \n< 1. Hence, as k ---+ oo, A7 approaches 1or0 for i = 1, ... ,  n. It follows that D\nk \nap­\nproaches a diagonal matrix-say, D*-each of whose diagonal entries is 1or0. Thus, \np\nk\n= QD\nk\nQ\n-\n1 \napproaches L = QD*Q\n-\n1\n. We write \nlim \np\nk \n= L \nk ---+ 'X! \nObserve that \nPL = P lim \np\nk \n= lim \npp\nk \n= lim \np\nk\n+\ni \n= L \nTherefore, each column of Lis an eigenvector of P corresponding to A\n1 \n= 1. To see \nthat each of these columns is a probability vector (i.e., L is a stochastic matrix), we \nneed only observe that, if j is the row vector with n 1 s, then \njL = j lim \np\nk \n= lim jP\nk","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":51293,"to":51354}}}}],[933,{"pageContent":"1 \n= 1. To see \nthat each of these columns is a probability vector (i.e., L is a stochastic matrix), we \nneed only observe that, if j is the row vector with n 1 s, then \njL = j lim \np\nk \n= lim jP\nk \n= lim j = j \nk ---+ 'X! k ---+ oc· k ---+ oc· \nsince p\nk \nis a stochastic matrix, by Exercise 14 in Section 3.7. Exercise 13 in Section 3.7 \nnow implies that L is stochastic. \nWe need only show that the columns of L are identical. The ith column of L is \njust Le;, where e; is the ith standard basis vector. Let v\n1\n, v\n2\n, ... , v\nn \nbe eigenvectors of \nP forming a basis of !R\nn\n, with v\n1 \ncorresponding to A\n1 \n= 1. Write \nfor scalars c\n1\n, c\n2\n, .•. , cw Then, by Theorem 4.19, \nBy Lemma 4.32, A\nj \n-=F 1 for j -=F 1, so, by Theorem 4.3\nl\n(b), \nI\nA\nj\nl \n< 1 for j -=F 1. Hence, \nA\nJ ---+ \n0 as k ---+ oo, for j -=F 1. It follows that \nLe\n; \n= lim P\nk\ne\n; \n= c\n1\nv\n1 \nk ---+ oc,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":51354,"to":51412}}}}],[934,{"pageContent":"Example 4.38 \nTheorem 4.34 \nSection 4.6 \nApplications and the Perron-Frobenius Theorem \n329 \nIn other words, column i of Lis an eigenvector corresponding to A\n1 \n= 1. But we have \nshown that the columns of L are probability vectors, so Le; is the unique multiplex of \nv\n1 \nwhose components sum to 1. Since this is true for each column of L, it implies that \nall of the columns of L are identical, each equal to this vector x. \nRemark Since L is a stochastic matrix, we can interpret it as the long range tran­\nsition matrix of the Markov chain. That is, L\nij \nrepresents the probability of being in \nstate i, having started from state j, if th e transitions were to continue indefinitely. The \nfact that the columns of L are identical says that the starting state does not matter, as \nthe next example illustrates. \nRecall the rat in a box from Example 3.65. The transition matrix was \nWe determined that the steady state probability vector was \nHence, the powers of P approach \n[\n� � �i \n-\n[0.250 0.250 \nL = 8 � 8 \n-","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":51414,"to":51442}}}}],[935,{"pageContent":"Recall the rat in a box from Example 3.65. The transition matrix was \nWe determined that the steady state probability vector was \nHence, the powers of P approach \n[\n� � �i \n-\n[0.250 0.250 \nL = 8 � 8 \n-\n0.375 0.375 \n�    �    � 0.375 0.375 \n0.250] \n0.375 \n0.375 \nfrom which we can see that the rat will eventually spend 25% of its time in compart­\nment 1 and 3 7  .5% of its time in each of the other two compartments. \nWe conclude our discussion of regular Markov chains by proving that the steady \nstate vector x is independent of the initial state. The proof is easily adapted to cover \nthe case of state vectors whose components sum to an arbitrary constant-say, s. In \nthe exercises, you are asked to prove some other properties of regular Markov chains. \nLet P be a regular n X n transition matrix, with x the steady state probability vector \nfor P, as in Theorem 4.33. Then, for any initial probability vector x0, the sequence \nof iterates x\nk \napproaches x. \nProof Let","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":51442,"to":51467}}}}],[936,{"pageContent":"330 \nChapter 4 \nEigenvalues and Eigenvectors \nExample 4.39 \nwhere x\n1 \n+ x\n2 \n+ ·  ·  · + X\nn \n= 1. Since x\nk \n= P\nk\nx0, we must show that lim P\nk\nx0 = x. Now, \nk-toc-\nby Theorem 4.33, the long range transition matrix is L = [x x \n· · · \nx] and lim p\nk\n= L. \nTherefore, \nlim p\nk\nx0 = \n( \nlim p\nk\n)\nx\n0 \n= Lx0 \nk -'>GO \nk-tGO \n= X\n1\nX + X\n2\nX + \n' ' ' \n+ X\nn\nX \n= (x\nl \n+ X\nz \n+ \n· · · \n+ X\nn\n)\nx = \nX \nPooulalion Growlh \nk--+oo \nWe return to the Leslie model of population growth, which we first explored in \nSe\nction 3.7. In Example 3.67 in that section, we saw that for the Leslie matrix \nr � [�s L �] \niterates of the population vectors began to approach a multiple of the vector \nIn other words, the three age classes of this population eventually ended up in the \nratio 18: 6: 1 . Moreover, once this state is reached, it is stable, since the ratios for the \nfollowing year are given by \nI\nF \n[� 5 \n4 \n0 \n0.25 \nl\n.5x \nand \nthe components are still in the ratio 27:9: 1.5 = 18:6: 1. Observe that 1.5 repre­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":51469,"to":51544}}}}],[937,{"pageContent":"following year are given by \nI\nF \n[� 5 \n4 \n0 \n0.25 \nl\n.5x \nand \nthe components are still in the ratio 27:9: 1.5 = 18:6: 1. Observe that 1.5 repre­\nsents the growth rate of this population when it has reached its steady state. \nWe can now recognize that xis an eigenvector of L corresponding to the eigen­\nvalue ,\\ = 1.5. Thus, the steady state growth rate is a positive eigenvalue of L, and an \neigenvector corresponding to this eigenvalue represents the relative sizes of the age \nclasses when the steady state has been reached. We can compute these directly, with­\nout having to iterate as we did before. \nFind the steady state growth rate and the corresponding ratios between the age classes \nfor the Leslie matrix L above. \nSolution We need to find all positive eigenvalues and corresponding eigenvectors of \nL. The characteristic polynomial of L is \n-,\\ \n4 \n3 \ndet\n(\nL -AI) = \n0.\n5 \n-A \n0 \n0 \n0.2\n5 \n-,\\ \n-,\\\n3 \n+ \n2,\\ + 0.375","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":51544,"to":51582}}}}],[938,{"pageContent":"Theorem 4.35 \nSection 4.6 Applications and the Perron-Frobenius Theorem \n331 \nso we must solve - ,\\\n3 \n+ 2,\\ + 0.37\n5 \n= 0 or, equivalently, 8,\\\n3 \n-16,\\ -  3 = 0. Factor­\ning, we have \n(\n2,\\ -  3\n)(\n4,\\\n2 \n+ 6,\\ + 1\n) \n= 0 \n(See Appendix D.) Since the second factor has only the roots \n(\n-3 +  Vs)/4 \n= \n-\n0.19 \nand \n(\n-3 -   Vs)/4 \n= \n-\n1.31, the only positive root of this equation is A  =\n�\n= 1.\n5\n. \nThe corresponding eigenvectors are in the null space of L \n-\nl \n.\n5\nI, which we find by \nrow reduction: \n[\n-\n1.\n5 \n[\nL \n-\n1.\n5\nII\nO\nJ = �\n-\n5 \n4 \n-\n1.\n5 \n0.2\n5 \n3 \n0 \n-\n1.5 \n0 \n1 \n0 \n-\n18 \nO\nJ \n-6 \n0 \n0 \n0 \nThu,, if x  � [ :: ] i\"n eigenvecto\nc \n'°\"\"ponding to A � 1.\n5\n, it \"ti'fi\" x, � I Bx, \nand x\n2 \n= 6x\n3\n• That is, \nHence, the steady state growth rate is 1.\n5\n, and when this rate has been reached, the age \nclasses are in the ratio 18: 6: 1, as we saw before. \nIn Example 4.39, there was only one candidate for the steady state growth rate: the \nunique positive eigenvalue of L. But what would we have done if L had had more than","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":51584,"to":51676}}}}],[939,{"pageContent":"In Example 4.39, there was only one candidate for the steady state growth rate: the \nunique positive eigenvalue of L. But what would we have done if L had had more than \none positive eigenvalue or none? We were also apparently fortunate that there was a \ncorresponding eigenvector all of whose components were positive, which allowed us \nto relate these components to the size of the population. We can prove that this situ­\nation is not accidental; that is, every Leslie matrix has exactly one positive eigenvalue \nand a corresponding eigenvector with positive components. \nRecall that the form of a Leslie matrix is \nb\n1 \nb\n2 \nb\n3 \nb\nn\n-\n1 \nb\nn \n5\n1 \n0 0 \n0   0 \n0 \n5\n2 \n0 \n0 \n0 \nL= \n(\n3\n) \n0 \n0 \n5\n3 \n0 \n0 \n0 \n0  0 \nSn\n-\nI \n0 \nSince the entries sj represent survival probabilities, we will assume that they are all \nnonzero (otherwise, the population would rapidly die out). We will also assume that \nat least one of the birth parameters b; is nonzero (otherwise, there would be no births","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":51676,"to":51724}}}}],[940,{"pageContent":"nonzero (otherwise, the population would rapidly die out). We will also assume that \nat least one of the birth parameters b; is nonzero (otherwise, there would be no births \nand, again, the population would die out). With these standing assumptions, we can \nnow prove the assertion we made above as a theorem. \nEvery Leslie matrix has a unique positive eigenvalue and a corresponding eigen­\nvector with positive components.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":51724,"to":51729}}}}],[941,{"pageContent":"332 \nChapter 4 \nEigenvalues and Eigenvectors \nOskar Perron (1880-1975) was a \nGerman mathematician who did \nwork in many fields of mathemat­\nics, including analysis, differential \nequations, algebra, geometry, and \nnumber theory. Perron's Theorem \nwas published in 1907 in a paper \non continued fractions. \nProof \nLet L be as in Equation \n(\n3\n)\n. The characteristic polynomial of Lis \n= (\n-l\n)\nn\nj(  A) \n(You are asked to prove this in Exercise 16.\n) \nThe eigenvalues of L are therefore the \nroots of \nf\n(\nA\n)\n. Since at least one of the birth parameters b; is positive and all of the \nsurvival probabilities s\nj \nare positive, the coefficients of \nf\n(\nA\n) \nchange sign exactly once. \nBy Descartes's Rule of Signs (Appendix D), therefore, \nf\n(\nA\n) \nhas exactly one positive \nroot. Let us call it A 1. \nBy direct calculation, we can check that an eigenvector corresponding to A1 is \ns\n1\n/ A\n1 \nS\n1\nS\n2 \nI \nA\nf \nS\n1\nS\n2\nS\n3\n/A\ni \n(You are asked to prove this in Exercise 18.\n) \nClearly, all of the components of x1 are \npositive.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":51731,"to":51800}}}}],[942,{"pageContent":"s\n1\n/ A\n1 \nS\n1\nS\n2 \nI \nA\nf \nS\n1\nS\n2\nS\n3\n/A\ni \n(You are asked to prove this in Exercise 18.\n) \nClearly, all of the components of x1 are \npositive. \nIn fact, more is true. With the additional requirement that two consecutive birth \nparameters b; and b;\n+\ni \nare positive, it turns out that the unique positive eigenvalue \nA1 of Lis dominant; that is, every other (real or complex) eigenvalue A of L satisfies \nIAI < A1. (It is beyond the scope of this book to prove this result, but a partial proof is \noutlined in Exercise 27 for readers who are familiar with the algebra of complex num­\nbers.) This explains why we get convergence to a steady state vector when we iterate \nthe population vectors: It is just the power method working for us! \nThe Perron-Frobenius Theorem \nIn the previous two applications, Markov chains and Leslie matrices, we saw that the \neigenvalue of interest was positive and dominant. Moreover, there was a correspond­\ning eigenvector with positive components. It turns out that a remarkable theorem","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":51800,"to":51836}}}}],[943,{"pageContent":"eigenvalue of interest was positive and dominant. Moreover, there was a correspond­\ning eigenvector with positive components. It turns out that a remarkable theorem \nguarantees that this will be the case for a large class of matrices, including many of \nthe ones we have been considering. The first version of this theorem is for positive \nmatrices. \nFirst, we need some terminology and notation. Let's agree to refer to a vector as \npositive if all of its components are positive. For two m X n matrices A = \n[a\nii\n] and \nB = [b\nii\n], we will write A 2: B if a\nii \n2: b\nij \nfor all i and j. (Similar definitions will apply \nfor A > B, A :s B, and so on.) Thus, a positive vector x satisfies x > 0. Let us define \nIAI = [la;\nj\nl\nl \nto be the matrix of the absolute values of the entries of A.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":51836,"to":51858}}}}],[944,{"pageContent":"Theorem 4.36 \nSection 4.6 \nApplications and the Perron-Frobenius Theorem \n333 \nPerron's Theorem \nLet \nA be a positive n X n matrix. Then A has a real eigenvalue A 1 \nwith the following \nproperties: \na. A\n1 \n> 0 \nb. A\n1 \nhas a corresponding positive eigenvector. \nc. If A is any other eigenvalue of A, then IAI \n:::; \nA 1 . \nIntuitively, we can see why the first two statements should be true. Consider the case \nof a 2 X 2 positive matrix A. The corresponding matrix transformation maps the first \nquadrant of the plane properly into itself, since all components are positive. If we re­\npeatedly allow A to act on the images we get, they necessarily converge toward some \nray in the first quadrant (Figure 4.18\n)\n. A direction vector for this ray will be a positive \nvector \nx, which must be mapped into some positive multiple of itself (say, A 1), \nsince A \nleaves the ray fixed. In other words, Ax= A\n1\nx, with x and A 1 \nboth positive. \nProof For some nonzero vectors x, Ax 2 Ax for some scalar A. When this happens, \nthen A\n(\nkx\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":51860,"to":51896}}}}],[945,{"pageContent":"since A \nleaves the ray fixed. In other words, Ax= A\n1\nx, with x and A 1 \nboth positive. \nProof For some nonzero vectors x, Ax 2 Ax for some scalar A. When this happens, \nthen A\n(\nkx\n) \n2 A\n(\nkx\n) \nfor all k > O; thus, we need only consider unit vectors x. In \nChapter 7, we will see that A maps the set of all unit vectors in !R\nn \n(the unit sphere\n) \ninto a \"generalized ellipsoid:' So, as x ranges over the nonnegative vectors on this unit \nsphere, there will be a maximum value of A such that Ax 2 Ax. (See Figure 4.19.\n) \nDe\nnote this number by A 1 \nand the corresponding unit vector by x\n1\n. \ny y y y \nFigure 4.18 \ny \nFigure 4.19","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":51896,"to":51926}}}}],[946,{"pageContent":"334 \nChapter 4 \nEigenvalues and Eigenvectors \nWe now show that Ax1 = A1x1. If not, then Ax1 > A1x1, and, applying A again, we \nobtain \nwhere the inequality is preserved, since A is positive. (See Exercise 40 and Section 3.7 \nExercise 36.\n) \nBut then y = (\nl\n/\nll\nAx1 \nll\n)Ax1 is a unit vector that satisfies Ay > A1y, so \nthere will be some A\n2 \n> A1 such that Ay 2: A\n2\ny. This contradicts the fact that A1 was \nthe maximum value with this property. Consequently, it must be the case that Ax1 = \nA1x1; that is, A1 is an eigenvalue of A. \nNow A is positive and x1 is positive, so A1x1 = Ax1 > 0. This means that A1 > 0 \nand x1 > 0, which completes the proof of (a) and (b). \nTo prove (c), suppose A is any other (real or complex) eigenvalue of A with cor­\nresponding eigenvector z. Then Az = Az, and, taking absolute values, we have \n(\n4\n) \nwhere the middle inequality follows from the Triangle Inequality. (See Exercise 40.\n) \nSince lzl > 0, the unit vector u in the direction of lzl is also positive and satisfies","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":51928,"to":51959}}}}],[947,{"pageContent":"(\n4\n) \nwhere the middle inequality follows from the Triangle Inequality. (See Exercise 40.\n) \nSince lzl > 0, the unit vector u in the direction of lzl is also positive and satisfies \nAu 2: IAlu. By the maximalityofAJrom the first part of this proof, we must have IAI ::::: A1. \nIn fact, more is true. It turns out that A1 is dominant, so IAI < A1 for any eigenvalue \nA i= A1. It is also the case that A1 has algebraic, and hence geometric, multiplicity 1. \nWe will not prove these facts. \nPerron's Theorem can be generalized from positive to certain nonnegative matri­\nces. Frobenius did so in 1912. The result requires a technical condition on the matrix. \nA square matrix A is called reducible if, subject to some permutation of the rows and \nthe same permutation of the columns, A can be written in block form as \n[\n� �\n] \nwhere B and D are square. Equivalently, A is reducible if there is some permutation \nmatrix P such that \nPAP\nT \n= \n[\n� \n�\n] \n(See page 187.\n) \nFor example, the matrix \n2 \n0     0 \n3 \n4 \n2 \n1 \n5 5 \nA= \n1 \n2 \n7 3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":51959,"to":51998}}}}],[948,{"pageContent":"[\n� �\n] \nwhere B and D are square. Equivalently, A is reducible if there is some permutation \nmatrix P such that \nPAP\nT \n= \n[\n� \n�\n] \n(See page 187.\n) \nFor example, the matrix \n2 \n0     0 \n3 \n4 \n2 \n1 \n5 5 \nA= \n1 \n2 \n7 3 \n0 \n6 \n0 0 \n2 \n0     0 \n7 \n2 \nis reducible, since interchanging rows 1 and 3 and then columns 1 and 3 produces \n7 \n2:   1 3 0 \n�--- -�-J __ � ____ ?. ____ ?. \n0     0 \ni \n2 1 3 \n0     0!6     2 1 \n0     0\n!\n1 7     2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":51998,"to":52041}}}}],[949,{"pageContent":"Section 4.6 Applications and the Perron-Frobenius Theorem \n335 \n(This is just PAP\nT\n, where \n0  0 \n1 \n0  0 \n0 \n1 \n0  0  0 \nP= \n1 \n0  0    0  0 \n0  0  0 0 \n0  0  0    0 \n� Check this!) \nA square matrix A that is not reducible is called irreducible. If A \nk \n> 0 for some \nk, then A is called primitive. For example, every regular Markov chain has a primitive \ntransition matrix, by definition. It is not hard to show that every primitive matrix is \n__.. \nirreducible. (Do you see why? Try showing the contrapositive of this.) \nTheorem 4.31 \nThe Perron-Frobenius Theorem \nSee Matrix Analysis by R. A. Horn \nand C. R. Johnson (Cambridge, \nEngland: Cambridge University \nPress, 1985). \nLet A be an irreducible nonnegative n X n matrix. Then A has a real eigenvalue A1 \nwith the following properties: \na. A1 >0 \nb. A1 has a corresponding positive eigenvector. \nc. If A is any other eigenvalue of A, then IAI ::=::: A1. If A is primitive, then this \ninequality is strict.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":52043,"to":52078}}}}],[950,{"pageContent":"with the following properties: \na. A1 >0 \nb. A1 has a corresponding positive eigenvector. \nc. If A is any other eigenvalue of A, then IAI ::=::: A1. If A is primitive, then this \ninequality is strict. \nd. If ,\\ is an eigenvalue of A such that IAI = ,\\ 1, then A is a (complex) root of the \nequation ,\\ \nn \n-,\\� = 0. \ne. A1 has algebraic multiplicity 1. \nThe interested reader can find a proof of the Perron-Frobenius Theorem in many \ntexts on nonnegative matrices or matrix analysis. The eigenvalue A1 is often called the \nPerron root of A, and a corresponding probability eigenvector (which is necessarily \nunique) is called the Perron eigenvector of A. \nlinear Recurrence Relalions \nThe Fibonacci numbers are the numbers in the sequence 0, 1, 1, 2, 3, 5, 8, 13, 21, ... , \nwhere, after the first two terms, each new term is obtained by summing the two terms \npreceding it. If we denote the nth Fibonacci number by \nf\nn\n, then this sequence is com­\npletely defined by the equations \nJ\no = 0, \nf\n1 = 1, and, for n 2: 2, \nJ\nn","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":52078,"to":52105}}}}],[951,{"pageContent":"preceding it. If we denote the nth Fibonacci number by \nf\nn\n, then this sequence is com­\npletely defined by the equations \nJ\no = 0, \nf\n1 = 1, and, for n 2: 2, \nJ\nn \n= \nf\nn\n-\n1 \n+ \nf\nn\n-\n2 \nThis last equation is an example of a linear recurrence relation. We will return to the \nFibonacci numbers, but first we will consider linear recurrence relations somewhat \nmore generally.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":52105,"to":52128}}}}],[952,{"pageContent":"336 \nChapter 4 \nEigenvalues and Eigenvectors \nExample 4.40 \nLeonardo of Pisa (1170-1250), pictured left, is better known by his nickname, Fibonacci, \nwhich means \"son of Bonaccio:' He wrote a number of important books, many of which have \nsurvived, including Liber abaci and Liber quadratorum. The Fibonacci sequence appears \nas the solution to a problem in Liber abaci: \"A certain man put a pair of rabbits in a place \nsurrounded on all sides by a   wall. How many pairs of rabbits can be produced from that \npair in a year if it is supposed that every month each pair begets a new pair which from the \nsecond month on becomes productive?\" The name Fibonacci numbers was given to the terms \nof this sequence by the French mathematician Edouard Lucas (1842-1891). \nDefinition \nLet \n(\nx\nn\n) \n= \n(\nx\n0\n, x\n1 , x\n2\n, ... \n) \nbe a sequence of numbers that is defined \nas follows: \n1. \nx\n0 \n= \na\n0\n, x\n1 \n= \na\n1\n, ... \n, x\nk\n-\nl \n= \na\nk\n_\n1\n, \nwhere \na\n0\n, a\n1\n, ... \n, a\nk\n-\nl \nare scalars. \n2. \nFor all \nn  2: \nk, \nX\nn \n= \nc\n1\nx\nn\n-\nl \n+  c\n2\nx\nn\n_\n2 \n+ \n· · · \n+  c\nk\nx\nn\n-\nk' \nwhere \nc\n1\n, c\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":52130,"to":52223}}}}],[953,{"pageContent":"as follows: \n1. \nx\n0 \n= \na\n0\n, x\n1 \n= \na\n1\n, ... \n, x\nk\n-\nl \n= \na\nk\n_\n1\n, \nwhere \na\n0\n, a\n1\n, ... \n, a\nk\n-\nl \nare scalars. \n2. \nFor all \nn  2: \nk, \nX\nn \n= \nc\n1\nx\nn\n-\nl \n+  c\n2\nx\nn\n_\n2 \n+ \n· · · \n+  c\nk\nx\nn\n-\nk' \nwhere \nc\n1\n, c\n2\n, \n.•. \n, c\nk \nare \nscalars. \nIf c\nk \n* 0, the equation in \n(\n2\n) \nis called a linear recurrence relation of order k. The \nequations in \n( \n1\n) \nare referred to as the initial conditions of the recurrence. \nThus, the Fibonacci numbers satisfy a linear recurrence relation of order 2. \nRemarks \n• \nIf, in order to define the nth term in a recurrence relation, we require the \n(\nn \n-\nk)th term but no term before it, then the recurrence relation has order k. \n• \nThe number of initial conditions is the order of the recurrence relation. \n• \nIt is not necessary that the first term of the sequence be called x\n0\n. We could \nstart \nat x1 \nor anywhere else. \n• \nIt is possible to have even more general linear recurrence relations by allowing \nthe coefficients \nC\n; to be functions rather than scalars and by allowing an extra, isolated","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":52223,"to":52328}}}}],[954,{"pageContent":"at x1 \nor anywhere else. \n• \nIt is possible to have even more general linear recurrence relations by allowing \nthe coefficients \nC\n; to be functions rather than scalars and by allowing an extra, isolated \ncoefficient, which may also be a function. An example would be the recurrence \nWe will not consider such recurrences here. \nConsider the sequence \n(\nx\nn\n) \ndefined by the initial conditions x\n1 \n= 1, x\n2 \n= 5 and the \nrecurrence relation x\nn \n= 5x\nn\n-\nl \n-6x\nn\n-\nl \nfor n 2: 2. Write out the first five terms of this \nsequence. \nSolulion We are given the first two terms. We use the recurrence relation to calcu­\nlate the next three terms. We have \nX\n3 \n= 5\nX\n2 \n-  6\nX\n1 \n=  5 \n• \n5  -  6 \n• 1 = 19 \nX\n4 \n=  5\nX\n3 \n-  6X\n2 \n=  5 \n. \n19 -  6 \n. \n5  = \n65 \nX\n5 \n= 5\nX\n4 \n-  6X\n3 \n= \n5·65  -6·19 = \n211 \nso the sequence begins 1, 5, 19, 65, 211, ....","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":52328,"to":52396}}}}],[955,{"pageContent":"Section 4.6 Applications and the Perron-Frobenius Theorem 331 \nClearly, if we were interested in, say, the \nl\nOO\nth term of the sequence in Exam­\nple 4.40, then the approach used there would be rather tedious, since we would have \nto apply the recurrence relation 98 times. It would be nice if we could find an explicit \nformula for \nX\nn as a function of n. We refer to finding such a formula as solving the re­\ncurrence relation. We will illustrate the process with the sequence from Example 4.40. \nTo begin, we rewrite the recurrence relation as a matrix equation. Let \nA = \n[\n� \n-\n�\nJ \nand introduce vectors \nx\nn \n= \n[\nx\n:\n:J for n 2: 2. Thus, \nx\n2 \n= \n[\n:\n:\nJ \n[\n�\nl \nX\n3 \n= \n[\n:\n:\nJ \n= \n[ \n1 \n� \nl \nx\n4 = \n[\n:\n: \nJ \n= \n[ \n�: \nl \nand so on. Now observe that, for n 2: 2\n, \nwe have \nA\nXn\n-\ni \n= \n[\n5 \n-\n6\nJ \n[\nXn\n-\nI\nJ \n= \n[\n5\nXn\n-\nI \n-\n6x\nn\n-\n2\nJ \n= \n[ \nXn \nJ \n= \nXn \n1 \n0 \nX\nn\n-\n2 \nX\nn\n-\n1 \nX\nn\n-\n1 \nNotice that this is the same type of equation we encountered with Markov chains and \nLeslie matrices. As in those cases, we can write \nWe now use the technique of Example 4.29 to compute the powers of A.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":52398,"to":52508}}}}],[956,{"pageContent":"Leslie matrices. As in those cases, we can write \nWe now use the technique of Example 4.29 to compute the powers of A. \nThe characteristic equation of A is \nA\n2 \n-\n5\n,\\ + 6 = 0 \nfrom which we find that the eigenvalues are ,\\\n1 \n= 3 and ,\\\n2 \n= 2. (Notice that the form \nof the characteristic equation follows that of the recurrence relation. If we write the \nrecurrence as \nX\nn \n-\n5\nxn\n-\nI + 6xn\n-\nz = 0, it is apparent that the coefficients are exactly \nthe same!) The corresponding eigenspaces are \nE\n3 \n= \nsp\nan\n( \n[ \n�\n]) \nan\nd \nE2 \n= \nsp\nan\n( \n[ \n�\n]) \nSettingP = \n[\n� \n�\nJ\n, \nwe know thatP\n-\n1\nAP = D = \n[\n� \n�J. ThenA = PDP\n-\n1 \nand \nA\nk \n= PD\nk\np\n-\n1 \n= \nIt now follows that \n[\n3  2\nJ \n[\n3\nk \nO\nJ \n[\n3  2\nJ\n-\n1 \n1 1 0 2\nk \n1 1 \n[\n3  2\nJ\n[\n3\nk \nO\nJ\n[ \n1 \n-\n2\nJ \n1 1 0 2\nk \n-1 3 \n-\n2\n(\n3\nk\n+\nI\n) \n+ 3\n(\n2\nk\n+\n1\n)\nJ \n-\n2\n(\n3\nk\n) \n+ \n3\n(\n2\nk\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":52508,"to":52636}}}}],[957,{"pageContent":"338 \nChapter 4 \nEigenvalues and Eigenvectors \n� \nfrom which we read off the solution x\nn \n= \n3\nn \n-\n2\nn\n. (To check our work, we could plug \nin n = 1, 2, ... , 5 to verify that this formula gives the same terms that we calculated \nusing the recurrence relation. Try it!) \nExample 4.41 \nJacques Binet (1786-1856) made \ncontributions to matrix theory, \nnumber theory, physics, and as­\ntronomy. He discovered the rule \nfor matrix multiplication in 1812. \nBinet's formula for the Fibonacci \nnumbers is actually due to Euler, \nwho published it in 1765; how­\never, it was forgotten until Binet \npublished his version in 1843. \nLike Cauchy, Binet was a royalist, \nand he lost his university posi­\ntion when Charles X abdicated in \n1830. He received many honors \nfor his work, including his elec­\ntion, in 1843, to the Academie des \nSciences. \nObserve that X\nn \nis a linear combination of powers of the eigenvalues. This is nec­\nessarily the case as long as the eigenvalues are distinct [as Theorem 4.38(a) will make","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":52638,"to":52674}}}}],[958,{"pageContent":"Sciences. \nObserve that X\nn \nis a linear combination of powers of the eigenvalues. This is nec­\nessarily the case as long as the eigenvalues are distinct [as Theorem 4.38(a) will make \nexplicit] . Using this observation, we can save ourselves some work. Once we have \ncomputed the eigenvalues ,\\\n1 \n= \n3 and ,\\\n2 \n= 2, we can immediately write \nwhere c\n1 \nand c\n2 \nare to be determined. Using the initial conditions, we have \n1  =  x\n1 \n=  c\n1\n3\n1 \n+  c\n2\n2\n1 \n= 3c\n1 \n+ 2c\n2 \nwhen n =  1 and \n5  = \nX\n2 \n= \nC\n1\n3\n2 \n+ \nC\n2\n2\n2 \n= 9c\n1 \n+ \n4c\n2 \nwhen n =  2. We now solve the system \n3c\n1 \n+ 2c\n2 \n=  1 \n9c\n1 \n+ 4c\n2 \n=  5 \nfor c\n1 \nand c\n2 \nto obtain c\n1 \n=  1 and c\n2 \n= -1. Thus, x\nn \n= \n3\nn \n-\n2\nn\n, as before. \nThis is the method we will use in practice. We now illustrate its use to find an \nexplicit formula for the Fibonacci numbers. \nSolve the Fibonacci recurrencef0 = \nO\n,f\n1 \n= 1, andf\nn \n= f\nn\n-\nl \n+ f\nn\n-\n2 \nfor n 2: 2. \nSolulion \nWriting the recurrence as \nf\nn \n-\nf\nn\n-\nl \n-\nf\nn\n-\n2 \n= 0, we see that the characteris­\ntic equation is ,\\ \n2 \n-,\\ -  1  =  0, so the eigenvalues are \n1  +\nVs \nA\n1 \n= \n----","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":52674,"to":52792}}}}],[959,{"pageContent":",f\n1 \n= 1, andf\nn \n= f\nn\n-\nl \n+ f\nn\n-\n2 \nfor n 2: 2. \nSolulion \nWriting the recurrence as \nf\nn \n-\nf\nn\n-\nl \n-\nf\nn\n-\n2 \n= 0, we see that the characteris­\ntic equation is ,\\ \n2 \n-,\\ -  1  =  0, so the eigenvalues are \n1  +\nVs \nA\n1 \n= \n----\n2 \nand \n1  -\nVs \nA\n2 \n=\n---\n2 \nIt follows from the discussion above that the solution to the recurrence relation has \nthe form \nfor some scalars c\n1 \nand c\n2\n. \nUsing the initial conditions, we find \n0  = f\no \n= \nC\n1\nA\n� \n+ \nC\n2\nA\n� \n= \nC\n1 \n+ \nC\n2 \nand \nSolving for c\n1 \nand c\n2\n, we obtain c\n1 \n=  1 /\nVs \nand c\n2 \nformula for the nth Fibonacci number is \n-1/\nVs\n. Hence, an explicit \n= \n_l \n(1 \n+ \nvs)\nn \n-\n_l (1  -   vs)\nn \nJ\nn \nVs 2 Vs 2 \n(\n5\n) \n4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":52792,"to":52894}}}}],[960,{"pageContent":"Theorem 4.38 \nSection 4.6 \nApplications and the Perron-Frobenius Theorem \n339 \nFormula \n(\n5\n) \nis a remarkable formula, because it is defined in terms of the irra­\ntional number \nVs \nyet the Fibonacci numbers are all integers! Try plugging in a few \nvalues for n to see how the \nVs \nterms cancel out to leave the integer values f\nn\n-\nFormula \n(\n5\n) \nis known as Binet's fo rmula. \nThe method we have just outlined works for any second order linear recur­\nrence relation whose associated eigenvalues are all distinct. When there is a repeated \neigenvalue, the technique must be modified, since the diagonalization method we \nused may no longer work. The next theorem summarizes both situations. \nLet X\nn \n= ax\nn\n-\nl \n+ bx\nn\n-\nz \nbe a recurrence relation that is satisfied by a sequence (x\nn\n). \nLet ,\\\n1 \nand ,\\\n2 \nbe the eigenvalues of the associated characteristic equation ,\\\n2 \n-\na,\\ \n-\nb = 0. \na. If A\n1 \n* A\n2\n, then X\nn \n= c\n1\nA7 + c\n2\nA� for some scalars c\n1 \nand c\n2\n. \nb. If A\n1 \n= A\n2 \n=A, then X\nn \n= c\n1\nA\nn \n+ c\n2\nn,\\\nn \nfor some scalars c\n1 \nand c\n2\n•","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":52896,"to":52978}}}}],[961,{"pageContent":"2 \n-\na,\\ \n-\nb = 0. \na. If A\n1 \n* A\n2\n, then X\nn \n= c\n1\nA7 + c\n2\nA� for some scalars c\n1 \nand c\n2\n. \nb. If A\n1 \n= A\n2 \n=A, then X\nn \n= c\n1\nA\nn \n+ c\n2\nn,\\\nn \nfor some scalars c\n1 \nand c\n2\n• \nIn either case, c\n1 \nand c\n2 \ncan be determined using the initial conditions. \nProof (a) Generalizing our discussion above, we can write the recurrence as x\nn \n= \nAx\nn\n_ \n1\n, where \n[ \nX\nn \n] \nx = \nn \nX\nn\n-\n1 \nand A= \n[\n� \n�\n] \nSince A has distinct eigenvalues, it can be diagonalized. The rest of the details are left \nfor Exercise 53. \n(b) We will showthat x\nn \n= c\n1\n,\\\nn \n+ c\n2\nn,\\\nn \nsatisfies the recurrence relation X\nn \n= ax\nn\n-\nl \n+ \nbx\nn\n-\nz \nor, equivalently, \nif ,\\\n2 \n-\na,\\ \n-\nb = 0. Since \nsubstitution into Equation \n( \n6\n) \nyields \nX\nn \n-\nax\nn\n-\nI \n-\nbx\nn\n-2 \n= \n(\nc\n1\nA\nn \n+ \nc\n2\nn,\\\nn\n) \n-\na\n(\nc\n1\nA\nn\n-\nI \n+ \nc\n2\n(\nn \n-\nl\n)\n,\\\nn\n-\nl\n) \n-\nb\n(\nc\n1\nA\nn\n-\nz \n+ c\n2\n(\nn \n-\n2\n)\n,\\\nn\n-\n2) \n= \nC\n1\n(\nA\nn \n-\na,\\\nn\n-\n1 \n-\nb,\\\nn\n-\n2) \n+ \nC\n2\n(\nn,\\\nn \n-  a(n \n-\nl\n)\nA\nn\n-\n1 \n-  b(n \n-\n2\n)\n,\\ \nn\n-\nZ) \n(\n6\n) \n= c\n1\n,\\\nn\n-2(\n,\\\n2 \n-\na,\\ \n-\nb\n) \n+ \nc\n2\nn,\\\nn\n-2(\n,\\\n2 \n-\na,\\ \n-\nb\n) \n+ \nc\n2\nA\nn\n-2(\na,\\ \n+ \n2b\n) \n= c\n1\nA\nn\n-\n2(\n0\n) \n+ c\n2\nn,\\\nn\n-\n2(\n0\n) \n+ c\n2\n,\\\nn\n-\n2(\na,\\ + 2b\n) \n= c\n2\nA \nn\n-\n2(\na,\\ + 2b\n) \nBut, since ,\\ is a double root of,\\ \n2 \n-\na,\\ \n-\nb = 0, we must have a\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":52978,"to":53259}}}}],[962,{"pageContent":"= c\n1\n,\\\nn\n-2(\n,\\\n2 \n-\na,\\ \n-\nb\n) \n+ \nc\n2\nn,\\\nn\n-2(\n,\\\n2 \n-\na,\\ \n-\nb\n) \n+ \nc\n2\nA\nn\n-2(\na,\\ \n+ \n2b\n) \n= c\n1\nA\nn\n-\n2(\n0\n) \n+ c\n2\nn,\\\nn\n-\n2(\n0\n) \n+ c\n2\n,\\\nn\n-\n2(\na,\\ + 2b\n) \n= c\n2\nA \nn\n-\n2(\na,\\ + 2b\n) \nBut, since ,\\ is a double root of,\\ \n2 \n-\na,\\ \n-\nb = 0, we must have a\n2 \n+ 4b = 0 and ,\\ = \na\n/\n2, using the quadratic formula. Consequently, a,\\ + 2b = a\n2\n/\n2 + 2b = \n-\n4b\n/\n2 + \n2b = 0, so","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":53259,"to":53344}}}}],[963,{"pageContent":"340 \nChapter 4 \nEigenvalues and Eigenvectors \nExample 4.42 \nTheorem 4.39 \nSuppose the initial conditions are x\n0 \n= rand x\n1 \n= s. Then, in either (a) or (b) there \nis a unique solution for c1 and c\n2\n. (See Exercise \n5\n4.) \nSolve the recurrence relation x\n0 \n= 1, x\n1 \n= 6, and X\nn \n= 6x\nn\n-\nl \n-\n9x\nn\n-\nl for n 2 2. \nSolulion The characteristic equation is A \n2 \n-\n6,\\ + \n9  = 0, which has A  = 3 as a \ndouble root. By Theorem 4.38(b), we must have X\nn \n=  c\n13\nn \n+  c\n2\nn3\nn \n= (c 1  + c\n2\nn)3\nn\n. \nSince 1  = x\n0 \n= c1 and 6 = x\n1 \n= (c\n1  + c\n2\n)3, we find that c\n2 \n= 1, so \nThe techniques outlined in Theorem 4.38 can be extended to higher order recurrence \nrelations. We state, without proof, the general result. \nLet X\nn \n= am\n-\niX\nn\n-\nl \n+ am\n-\nlX\nn\n-\nl + ·  ·  · +  a\n0\nX\nn\n-\nm be a recurrence relation of order \nm that is satisfied by a sequence (x\nn\n). Suppose the associated characteristic \npolynomial \nfactors as ( ,\\  -  A\n1\n)\nm\n1\n(\nA  -  A\n2\n)\nm\n2 •   •   • (\n,\\  -  ,\\\nk\n)\nm\n', where m 1  +  m\n2 \n+ ·  ·  · +  m\nk \n= m. \nThen x\nn \nhas the form \nX \n_ \n( \n\\ \nn \n+ \n\\ \nn \n+ \n2 \n\\ \nn \n+ \n... \n+ \nm\n1 \n-\nI \n\\ \nn","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":53346,"to":53473}}}}],[964,{"pageContent":"polynomial \nfactors as ( ,\\  -  A\n1\n)\nm\n1\n(\nA  -  A\n2\n)\nm\n2 •   •   • (\n,\\  -  ,\\\nk\n)\nm\n', where m 1  +  m\n2 \n+ ·  ·  · +  m\nk \n= m. \nThen x\nn \nhas the form \nX \n_ \n( \n\\ \nn \n+ \n\\ \nn \n+ \n2 \n\\ \nn \n+ \n... \n+ \nm\n1 \n-\nI \n\\ \nn\n) \n+ \n... \nn \n-\nC\n11\n1t\n1 \nC\n1\n2\nn\n1t\n1 \nC\n13\nn \nfl1 \nC\n1\nm\n1\nn \nflJ \n+ \n(\nc\nk1\nA\nJ: \n+  c\nk\n2\nnA\nJ: \n+  c\nk3\nn\n2\nA\nJ: \n+ \n· · · \n+  c\nk\nm\n,\nn\nm\n,\n-\nI\nA\nJ:\n) \nsvs1ems of  Linear Differenlial Equalions \nIn calculus, you learn that if x = x(t) is a differentiable function satisfying a differ­\nential equation of the form x' = kx, where k is a constant, then the general solution \nis x = Ce\nk\nt\n, where C is a constant. If an initial condition x(O) = x\n0 \nis specified, then, \nby substituting t =  0 in the general solution, we find that C = x\n0 • Hence, the unique \nsolution to the differential equation that satisfies the initial condition is \nx = xoe\nk\nt \nSuppose we have n differentiable functions of t-say, x\n1 , x\n2\n, ••• , X\nn\n-that satisfy a \nsystem of diff erential equations \nx{ \n= \na\n1 1\nx\n1 \n+ \na\n1\n2\nx\n2 \n+ \n· · · \n+  a\n1n\nx\nn \nx� \n= \na\nz1\nX\n1 \n+  a\nz\n2\nX\n2 \n+ \n· · · \n+  a\nzn\nX\nn","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":53473,"to":53630}}}}],[965,{"pageContent":"k\nt \nSuppose we have n differentiable functions of t-say, x\n1 , x\n2\n, ••• , X\nn\n-that satisfy a \nsystem of diff erential equations \nx{ \n= \na\n1 1\nx\n1 \n+ \na\n1\n2\nx\n2 \n+ \n· · · \n+  a\n1n\nx\nn \nx� \n= \na\nz1\nX\n1 \n+  a\nz\n2\nX\n2 \n+ \n· · · \n+  a\nzn\nX\nn \nWe can write this system in matrix form as x'  = Ax, where \n[x{\n(\nt\n) \nl \nx'(t)  = \nx��t\n) \n, \nx�\n(\nt\n) \nand \nNow we can use matrix methods to help us find the solution.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":53630,"to":53689}}}}],[966,{"pageContent":"Example 4.43 \nSection 4.6 Applications and the Perron-Frobenius Theorem \n341 \nFirst, we make a useful observation. Suppose we want to solve the following sys­\ntem of differential equations: \nX{ = 2\nX\n1 \nx;  =  5X\n2 \nEach equation can be solved separately, as above, to give \nX\n1 \n=  C\n1\ne\n2\n1 \nX\nz \n=  C\n2\ne\ns\nt \nwhere C\n1 \nand C\n2 \nare constants. Notice that, in matrix form, our equation x\n' \n= Ax has \na diagonal coefficient matrix \nA  = \n[\n�  �\n] \nand the eigenvalues 2 and 5 occur in the exponentials e\n2\n1 \nand e\n5\n1 \nof the solution. This \nsuggests that, for an arbitrary system, we should start by diagonalizing the coefficient \nmatrix, if possible. \nSolve the following system of differential equations: \nSolution Here the coefficient matrix is A  = \n[ \n1  2\n]\n, and we find that the eigenvalues \n3 2 \n[\n2\n] \n[\n-\n1\n] \nare ,\\\n1 \n= \n4 \nand ,\\\n2 \n= \n-\n1, with corresponding eigenvectors v\n1 \n= \n3 \nand v\n2 \n= \n1 \n, \nrespectively. Therefore, A is diagonalizable, and the matrix P that does the job is \nWe know that \nP= [v\n1 \nv\n2\n] \n= \n[\n� \n-\n�\n] \nP\n-\n1\nAP = \n[4 \nO\nJ \n=  D \n0 \n-\n1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":53691,"to":53791}}}}],[967,{"pageContent":"= \n-\n1, with corresponding eigenvectors v\n1 \n= \n3 \nand v\n2 \n= \n1 \n, \nrespectively. Therefore, A is diagonalizable, and the matrix P that does the job is \nWe know that \nP= [v\n1 \nv\n2\n] \n= \n[\n� \n-\n�\n] \nP\n-\n1\nAP = \n[4 \nO\nJ \n=  D \n0 \n-\n1 \nLet x =  Py (so that x\n' \n= Py'\n) \nand substitute these results into the original equation \nx\n' \n= Ax to get Py' = APy or, equivalently, \nThis is just the system \nwhose general solution is \ny\n'  =  P\n-\n1\nAPy =  D y \nY1 \n=  C\n1\ne\n4\n1 \nY2 \n=  C\n2\ne\n-\nt \nor","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":53791,"to":53853}}}}],[968,{"pageContent":"342 Chapter 4 \nEigenvalues and Eigenvectors \nTheorem 4.40 \nExample 4.44 \nTo find x, we just compute \nx =Py  = \n[\n� \nso \nx1 \n= 2C\n1\ne\n4\nt \n-  C\n2\ne\n-\nt \nand x\n2 \n= 3C\n1\ne\n4\n1 \n+  C\n2\ne\n-\nt\n. (Check that these values satisfy the \ngiven system.) \nRemark \nObserve that we could also express the solution in Example 4.43 as \nx =  C e\n4\n1\n[\n2\n] \n+ \nC e\n-1\n[\n-1\n] \n=  C e\n4\n1\nv   +  C  e\n-1\nv \nI \n3 \n2 l \n11 \n2 2 \nThis technique generalizes easily to n X n systems where the coefficient matrix is \ndiagonalizable. The next theorem, whose proof is left as an exercise, summarizes the \nsituation. \nLet A be an n X n diagonalizable matrix and let P  = \n[ v\n1 \nv\n2 \n•   •   • \nv\n\"\n] be such that \n0 \nThen the general solution to the system x'  = Ax is \nx =  C\n1\ne\nA\n1\nt\nv\n1 \n+  C\n2\ne\nA\ni\nr\nv\n2 \n+ \n·   ·   · \n+  C\n\"\ne\nA\"\n1\nV\n\" \nThe next example involves a biological model in which two species live in the \nsame ecosystem. It is reasonable to assume that the growth rate of each species de­\npends on the sizes of both populations. (Of course, there are other factors that govern","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":53855,"to":53955}}}}],[969,{"pageContent":"same ecosystem. It is reasonable to assume that the growth rate of each species de­\npends on the sizes of both populations. (Of course, there are other factors that govern \ngrowth, but we will keep our model simple by ignoring these.) \nIf x\n1 (\nt\n) \nand x\n2 \n(\nt\n) \ndenote the sizes of the two populations at time t, then x{(t) \nand \nx�\n(t) are their rates of growth at time t. Our model is of the form \nx{(t)  = ax\n1\n(t) \n+ \nbx\n2\n(t) \nx;(t)  = cx\n1 \n(t) \n+ \ndx\n2\n(t) \nwhere the coefficients a, b, c, and d depend on the conditions. \nRaccoons and squirrels inhabit the same ecosystem and compete with each other for \nfood, water, and space. Let the raccoon and squirrel populations at time t years be \ngiven by r \n(\nt\n) \nand s\n(\nt\n)\n, respectively. In the absence of squirrels, the raccoon growth \nrate is r'\n(\nt\n) \n= 2.5r\n(\nt\n)\n, but when squirrels are present, the competition slows the rac­\ncoon growth rate to r'\n(\nt\n) \n= 2.5r\n(\nt\n) \n-  s\n(\nt\n)\n. The squirrel population is similarly affected","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":53955,"to":54018}}}}],[970,{"pageContent":"rate is r'\n(\nt\n) \n= 2.5r\n(\nt\n)\n, but when squirrels are present, the competition slows the rac­\ncoon growth rate to r'\n(\nt\n) \n= 2.5r\n(\nt\n) \n-  s\n(\nt\n)\n. The squirrel population is similarly affected \nby the raccoons. In the absence of raccoons, the growth rate of the squirrel population \nis s'\n(\nt\n) \n= 2.5s\n(\nt\n)\n, and the population growth rate for squirrels when they are sharing \nthe ecosystem with raccoons is s'\n(\nt\n) \n= -0.25r(t) \n+ 2.Ss\n(\nt\n)\n. Suppose that initially","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":54018,"to":54059}}}}],[971,{"pageContent":"Section 4.6 \nApplications and the Perron-Frobenius Theorem \n343 \nthere are 60 raccoons and 60 squirrels in the ecosystem. Determine what happens to \nthese two populations. \nSolution Our system is x' = Ax, where \n[\nr\n( t\n)\n] \nx = x\n( t) = \ns(t\n) \n[ \n2.5 \nand A \n-\n-0.25 \n-I.\nO\nJ \n2.5 \nThe \neigenvalues of A are A 1 \n= 3 and A\n2 \n= 2, with corresponding eigenvectors v1 \n= \n[ \n-\n�\n] \nand v\n2 \n= \n[ \n�\n]\n. By Theorem 4.40, the general solution to our system is \nx\n(\nt) \n= \nC e\n3\nt\nv   +  C e\n2\nt\nv \n= \nC e\n3\nr\n[\n-2\n] \n+  C \ne\n2\nr\n[\n2\n] \n1 1 \n2 2 \n1 \n1 \n2 \n1 \n(\n7\n) \n. \nThe initial population vector is x\n( 0 ) = \n[\n: \n� \n� \n�\n] = \n[ \n:\n�\n]\n, so, setting t = 0 in Equa­\ntion \n(\n7\n)\n, we have \nSolving this equation, we find C\n1 \n= 15 and C\n2 \n= 45. Hence, \nx\n( t) = 15e\n3\nr\n[\n-\n�\n] \n+ \n45e\n2\nt\n[\n�\n] \nfrom which we find r(t\n) \n= -30e\n3\n1 \n+ 90e\n2\n1 \nand s(t\n) \n= 15e\n3\n1 \n+ 45e\n2\n1\n. Figure 4.20 shows \nthe graphs of these two functions, and you can see clearly that the raccoon population \n� \ndies out after a little more than 1 year. (Can you determine exactly when it dies out?) \n.+","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":54061,"to":54195}}}}],[972,{"pageContent":". Figure 4.20 shows \nthe graphs of these two functions, and you can see clearly that the raccoon population \n� \ndies out after a little more than 1 year. (Can you determine exactly when it dies out?) \n.+ \nWe now consider a similar example, in which one species is a source of food for \nthe other. Such a model is called a predator-prey model. Once again, our model will \nbe drastically oversimplified in order to illustrate its main features. \nFigure 4.20 \nRaccoon and squirrel populations","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":54195,"to":54204}}}}],[973,{"pageContent":"344 \nChapter 4 \nEigenvalues and Eigenvectors \nExample 4.45 \nRobins and worms cohabit an ecosystem. The robins eat the worms, which are their \nonly source of food. The robin and worm populations at time t years are denoted by \nr\n(\nt\n) \nand w\n(\nt\n)\n, respectively, and the equations governing the growth of the two popula­\ntions are \nr'(t)  =  w ( t\n) \n-12 \nw'( t)  = -r( t\n) \n+ \n10 \n(\n8\n) \nIf initially 6 robins and 20 worms occupy the ecosystem, determine the behavior of \nthe two populations over time. \nSolulion The first thing we notice about this example is the presence of the extra \nconstants, -12 and 10, in the two equations. Fortunately, we can get rid of them \nwith a simple change of variables. Ifwe let r(t\n) \n= x\n(\nt\n) + 10 and w\n(\nt\n) \n= \ny(\nt\n) + 12, then \nr'\n(\nt\n) \n= x'\n(\nt\n) \nand w'\n(\nt\n) \n= y'\n(\nt\n)\n. Substituting into Equations \n(\n8\n)\n, we have \nx'(t\n)  = \ny\n( t\n) \ny'\n( t)  = -x( t\n) \nwhich is easier to work with. Equations \n(\n9\n) \nhave the form x' \n[ \nO 1 \n] \n. Our \nnew initial conditions are \n-1 0 \n(\n9\n) \n=  Ax, where A  = \nx\n(O)  = r(O )  -10 =  6  -10 = -4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":54206,"to":54294}}}}],[974,{"pageContent":"x'(t\n)  = \ny\n( t\n) \ny'\n( t)  = -x( t\n) \nwhich is easier to work with. Equations \n(\n9\n) \nhave the form x' \n[ \nO 1 \n] \n. Our \nnew initial conditions are \n-1 0 \n(\n9\n) \n=  Ax, where A  = \nx\n(O)  = r(O )  -10 =  6  -10 = -4 \nand y (O)  =  w(O)  -12 = 20 -12 =  8 \nso x\n(O)  = \n[\n-\n:]. \nProceeding as in the last example, we find the eigenvalues and eigenvectors of A. \nThe characteristic polynomial is A\n2 \n+ 1, which has no real roots. What should we do? \nWe have no choice but to use the complex roots, which are A\n1 \n= \ni and A\n2 \n= \n-i. The \ncorresponding eigenvectors are also complex-namely, v\n1 \n= [�]and v\n2 \n= \n[ \n-\n�l By \nTheorem 4.40, our solution has the form \nFrom x(O) = [\n-\n:],we get \nwhose solution is C\n1 \n= \n-2 -4i and C\n2 \n= \n-2 + 4i. So the solution to system \n(\n9\n) \nis \nWhat are we to make of this solution? Robins and worms inhabit a real world­\nyet our solution involves complex numbers! Fearlessly proceeding, we apply Euler's \nformula \ne\n;\n1 \n= cos t +  i sin t","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":54294,"to":54365}}}}],[975,{"pageContent":"Section 4.6 \nApplications and the Perron-Frobenius Theorem \n345 \nIN.Si\\NC.T. \nT\\<?sERS AAE \n�n·tumn. \nCALVIN AND HOBBES© 1988 Watterson. Reprinted with permission of UNIVERSAL PRESS SYNDICATE. All rights reserved \n25 \n20 \n15 \n1\n0 \n5 \n(Appendix C) to get e \n-\ni\nt \n= cos ( \n-\nt) + i sin ( \n-\nt) = cos t \n-\ni sin t. Substituting, we have \nx\n(\nt\n) \n= \n(\n-\n2 \n-\n4i\n)(\ncos t + isin t\n)\nl�J + \n(\n-\n2 + 4i\n)(\ncos t \n-\nisin t\n)\n� �J \n[\n(\n-\n2cos t + 4sin t\n) \n+ i\n(\n-\n4cos t \n-\n2sin t\n)\n] \n(\n4cos t + 2sin t\n) \n+ i\n(\n-\n2cos t + 4sin t\n) \n+ \nl\n(\n-\n2cos t \n+ 4sin t\n) \n+ i\n(\n4cos t + 2sin t\n)\nJ \n_ \n( \n4 cos t + 2 sin t\n) \n+ i \n(\n2 cos t \n-\n4 sin t\n) \n= \n[\n-\n4 cos t + 8 sin t\n] \n8 cos t + 4 sin t \nThis gives x(t) \n= \n-\n4 cos t+ 8 sin t and y(t) = 8 cos t+ 4 sin t. Putting everything in \nterms of our original variables, we conclude that \nand \nr(t) = x(t) + 10 = \n-\n4 cos t + 8 sin t + 10 \nw(t) = y(t) + 12 = 8 cos t + 4 sin t + 12 \n1\n5 \n0 \n2 \n4  6  8  1\n0 \n1\n2 \n1\n4  16 \n3\n0 \nFigure 4.21 \nFigure 4.22 \nRobin and worm populations","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":54367,"to":54487}}}}],[976,{"pageContent":"346 \nChapter 4 \nEigenvalues and Eigenvectors \nExample 4.46 \nSo our solution is real after all! The graphs of r(t\n) \nand w\n(\nt\n) \nin Figure 4.21 show that \nthe two populations oscillate periodically. As the robin population increases, the \nworm population starts to decrease, but as the robins' only food source diminishes, \ntheir numbers start to decline as well. As the predators disappear, the worm popula­\ntion begins to recover. As its food supply increases, so does the robin population, \nand the cycle repeats itself. This oscillation is typical of examples in which the eigen­\nvalues are complex. \nPlotting robins, worms, and time on separate axes, as in Figure 4.22, clearly \nreveals the cyclic nature of the two populations. \nWe conclude this section by looking at what we have done from a different point \nof view. If x = x\n(\nt\n) \nis a differentiable function of t, then the general solution of the \nordinary differential equation x' = ax is x = cea\n1\n, where c is a scalar. The systems of","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":54489,"to":54516}}}}],[977,{"pageContent":"of view. If x = x\n(\nt\n) \nis a differentiable function of t, then the general solution of the \nordinary differential equation x' = ax is x = cea\n1\n, where c is a scalar. The systems of \nlinear differential equations we have been considering have the form x\n' \n= Ax, so if \nwe simply plowed ahead without thinking, we might be tempted to deduce that the \nsolution would be x = ce\nA\n1\n, where c is a vector. But what on earth could this mean? \nOn the right-hand side, we have the number e raised to the power of a matrix. This \nappears to be nonsense, yet you will see that there is a way to make sense of it. \nLet's start by considering the expression e\nA\n. In calculus, you learn that the func­\ntion ex has a power series expansion \nx\nz \nx\n3 \nex \n= 1 \n+  x  + \n-\n+ \n-\n+ \n... \n2! \n3! \nthat converges for every real number x. By analogy, let us define \nA\nz \nA\n3 \ne\nA \n= I + A \n+ \n-\n+ \n-\n+ \n· · · \n2! \n3! \nThe right-hand side is just defined in terms of powers of A, and it can be shown that \nit converges for any real matrix A. So now e\nA","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":54516,"to":54570}}}}],[978,{"pageContent":"A\nz \nA\n3 \ne\nA \n= I + A \n+ \n-\n+ \n-\n+ \n· · · \n2! \n3! \nThe right-hand side is just defined in terms of powers of A, and it can be shown that \nit converges for any real matrix A. So now e\nA \nis a matrix, called the exponential of A. \nBut how can we compute e\nA \nor e\nA\n1\n? For diagonal matrices, it is easy. \nCompute eD\nt \nfor D = \n[\n4 0\n]\n. \n0 -1 \nSolution \nFrom the definition, we have \nDt \n(Dt)\n2 \n(Dt)\n3 \ne \n= \nI +  Dt  + --\n+ \n--\n+ \n· · · \n2! \n3! \n0 \n] \n+ \n_i,\n[\n( 4t)\n2 \n-t \n2\n· 0 \n0 \n] \nI \n[\n(4t)\n3 \n( -t)\n2 \n+ \n31 0 \n[\n1  + \n(4t) \n+ tC4t\n0\n)\n2 \n+ t,C4t)\n3 \n+ \n· · · \no \n] \n1 \n+ C-t\n) \n+ tiC-t\n)\n2 \n+ t,C-t\n)\n3 \n+ \n· · · \n[ e;\nt \ne�\nt\n] \nThe matrix exponential is also nice if A is diagonalizable.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":54570,"to":54668}}}}],[979,{"pageContent":"Example 4.41 \nTheorem 4.41 \nSection 4.6 Applications and the Perron-Frobenius Theorem \n341 \nComputeeA for A = \n[\n� \n�\n]\n. \nSolution \nIn Example 4.43, we found the eigenvalues of A to be ,\\\n1 \n= 4 and ,\\\n2 \n= -1, \nwith corresponding eigenvectors v\n1 \n= \n[ \n�\n] and v\n2 \n= \n[ \n-\n�\n]\n, respectively. Hence, with \nP\n= [v\ni \nV\n2\n] \n= \n[\n2 \n-\nl\n]\n,we haveP\n-\n1\nAP=D= \n[\n4 0\n] .sinceA = PDP\n-\n1\n,we \n3 \n1 \n0 -1 \nhave A\nk \n= PD\nk\nP\n-\n1\n, so \nA\n1 \nA\n3 \neA = I \n+ \nA \n+ \n-\n+ \n-\n+ \n·   ·   · \n2! \n3! \n1 1 \n= P\nI\nP\n-\n1 \n+ PDP\n-\n1 \n+ \n-\nPD\n2\nP\n-\n1 \n+ \n-\nPD\n3\nP\n-\n1 \n+ \n·   ·   · \n2! \n3! \n( \nD\nz \nD\n3 \n) \n= P I \n+ \nD \n+ \n-\n+ \n-\n+ \n·   ·   · \np\n-i \n2! \n3! \n_!_\n[\n2e4 + 3e\n-\n1 \n5 3e4 \n-\n3e\n-\n1 \n0 \n]\n[\n2 -1\n]\n-\nI \ne\n-\n1 \n3 \n1 \n2e4 \n-\n2e\n-\n1\n] \n3e4 + 2e\n-\n1 \nWe are now in a position to show that our bold (and seemingly foolish) guess at \nan \"exponential\" solution of x' = Ax was not so far off after all! \nLet A be an n X n diagonalizable matrix with eigenvalues ,\\\n1\n, ,\\\n2\n, •.. , Aw Then the \ngeneral solution to the system x' = Ax is x = eA1c, where c is an arbitrary constant \nvector. If an initial condition x\n(\nO\n) \nis specified, then c = x\n(\nO\n)\n. \nProof","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":54670,"to":54841}}}}],[980,{"pageContent":"1\n, ,\\\n2\n, •.. , Aw Then the \ngeneral solution to the system x' = Ax is x = eA1c, where c is an arbitrary constant \nvector. If an initial condition x\n(\nO\n) \nis specified, then c = x\n(\nO\n)\n. \nProof \nLet P diagonalize A. Then A = PDP\n-\n1\n, and, as in Example 4.47, \nHence, we need to check that x' = Ax is satisfied by x = Pe01P\n-\n1\nc. Now, everything \nis constant except for e01, so \n(10) \nIf","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":54841,"to":54866}}}}],[981,{"pageContent":"348 \nChapter 4 \nEigenvalues and Eigenvectors \nFor example, see Linear Algebra \nby S. H. Friedberg, A. J. Insel, and \nL. E. Spence (Englewood Cliffs, \nNJ: Prentice-Hall, 1979). \nthen \n[\nI \n0 \ne\nDt \n= \ne\nA\n2\nt \n0 \n).,\n] \nTaking derivatives, we have \n0 0 \n0 \n0 \nd \n0 0 \n- (\ne\nA, t\n) \ndt \n�\nn·\n, \n� \n[\n! \n0 \nA\n1 \n0 \n0 \nA\nz\ne\nA\n, t \n0 \nJJ \n�r·' \nA\nn \n0 \n0 \ne\nA\n,\nt \n0 \nSubstituting this result into Equation \n(\n10\n)\n, we obtain \nJ,\n] \nx' = PDe\nDt\np\n-\n1\nc  =  PDP\n-\n1\nPe\nDt\np\n-\n1\nc  =  (PDP\n-\n1\n)(Pe\nDt\np\n-\n1\n) c  = Ae\nA\nt\nc  =  Ax \nas required. \nThe last statement follows easily from the fact that if x = x \n( \nt\n) \n= e\nA\nt \nc, then \nx\n(O)  =  e\nA\n·\nO\nc  =  e\n0\nc  = le=  c \nsince e\n0 \n= I. (Why?) \nIn fact, Theorem 4.41 is true even if A is not diagonalizable, but we will not prove \nthis. Computation of matrix exponentials for nondiagonalizable matrices requires the \nJordan normal form of a matrix, a topic that may be found in more advanced linear \nalgebra texts. \nIdeally, this short digression has served to illustrate the power of mathematics to","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":54868,"to":54984}}}}],[982,{"pageContent":"Jordan normal form of a matrix, a topic that may be found in more advanced linear \nalgebra texts. \nIdeally, this short digression has served to illustrate the power of mathematics to \ngeneralize and the value of creative thinking. Matrix exponentials turn out to be very \nimportant tools in many applications of linear algebra, both theoretical and applied. \nDiscrele Linear ovnamical svs1ems \nWe conclude this chapter as we began it-by looking at dynamical systems. Markov \nchains and the Leslie model of population growth are examples of discrete linear \ndynamical systems. Each can be described by a matrix equation of the form","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":54984,"to":54992}}}}],[983,{"pageContent":"Example 4.48 \nSection 4.6 \nApplications and the Perron-Frobenius Theorem \n349 \nwhere the vector xk records the state of the system at \"time\" k and A is a square matrix. \nAs we have seen, the long-term behavior of these systems is related to the eigenval­\nues and eigenvectors of the matrix A. The power method exploits the iterative nature \nof such dynamical systems to approximate eigenvalues and eigenvectors, and the \nPerron -Frobenius Theorem gives specialized information about the long-term behavior \nof a discrete linear dynamical system whose coefficient matrix A is nonnegative. \nWhen A is a 2 X 2 matrix, we can describe the evolution of a dynamical system \ngeometrically. The equation x\nk\n+I \n= Ax\nk \nis really an infinite collection of equations. \nBeginning with an initial vector XcJ, we have: \nX1 \n= A\nX\no \nx\n2 \n= Ax\n1 \nx\n3 \n= Ax\n2 \nThe set {\nX\no\n, x\n1\n, x\n2\n, .•. } is called a trajectory of the system. (For graphical purposes, we","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":54994,"to":55031}}}}],[984,{"pageContent":"Beginning with an initial vector XcJ, we have: \nX1 \n= A\nX\no \nx\n2 \n= Ax\n1 \nx\n3 \n= Ax\n2 \nThe set {\nX\no\n, x\n1\n, x\n2\n, .•. } is called a trajectory of the system. (For graphical purposes, we \nwill identify each vector in a trajectory with its head so that we can plot it as a point.) \nNote that x\nk \n= A \nk\nx\n0\n. \n[\n0.\n5 \n0 \n] \nLet A = . For the dynamical system x\nk\n+ \n1 \n= Ax\nk\n, plot the first five points \n0 0.8 \nin the trajectories with the following initial ve ctors: \n(a) \nX\no \n= \n[\nO\ns\n] \n[\n2.\n5\n] \n[\n1.2\n5\n] \nSolution (a) We compute x\n1 \n= A\nX\no \n= \n0 \n,   x\n2 \n= Ax\n1 \n= \n0 \n,   x\n3 \n= Ax\n2 \n= \n[\n0.62\n5\n] \n[\n0.3125\n] \n0 \n, \nX\n4 \n= Ax\n3 \n= \n0 \n. These are plotted in Figure 4.23, and the points are \nconnected to highlight the trajectory. Similar calculations produce the trajectories \nmarked \n(\nb\n)\n, \n(\nc\n)\n, and \n(\nd\n) \nin Figure 4.23. \ny \n(a) \nX3  X\n2 \nXj \nX\no \nx \n-2 \nX4 \n2 \n4   6 \n-2 \n(\nb) \n-4 \nFioure 4.23","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":55031,"to":55154}}}}],[985,{"pageContent":"350 \nChapter 4 \nEigenvalues and Eigenvectors \nIn Example 4.48, every trajectory converges to 0. The origin is called an attractor \nin this case. We can understand why this is so from Theorem 4.19. The matrix A in \nExample 4.48 has eigenvectors \n[ \n�\n] \nand \n[ \n�\n] \ncorresponding to its eigenvalues 0.5 and \n� \n0.8, respectively. (Check this.) Accordingly, for any initial vector \nExample 4.49 \nwe have \nBecause both \n(\n0.5\n)\nk \nand \n(\n0.8\n)\nk \napproach zero as k gets large, x\nk \napproaches 0 for any \nchoice ofx0• In addition, we know from Theorem 4.28 that because 0.8 is the domi­\nnant eigenvalue of A, x\nk \nwill approach a multiple of the corresponding eigenvector \n[ \n�\n] as long as c\n2 \n*  0 (the coefficient of Xo corresponding to \n[ \n�\n]\n). In other words, \nall trajectories except those that begin on the x-axis (where c\n2 \n=  O\n) \nwill approach the \ny-axis, as Figure 4.23 shows. \nDiscuss the behavior of the dynamical system x\nk\n+\ni \n= Ax\nk \ncorresponding to the \n[ \n0.65 -0.15\n] \nmatrix A = \n. \n-0.15 \n0.65","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":55156,"to":55219}}}}],[986,{"pageContent":"2 \n=  O\n) \nwill approach the \ny-axis, as Figure 4.23 shows. \nDiscuss the behavior of the dynamical system x\nk\n+\ni \n= Ax\nk \ncorresponding to the \n[ \n0.65 -0.15\n] \nmatrix A = \n. \n-0.15 \n0.65 \nSolution The eigenvalues of A are 0.5 and 0.8 with corresponding eigenvectors \n[ \n�\n] \n� \nand\n[\n-\n�\n] ,respectively.(Checkthis.) Hencefor an initialvector\nXo \n= c\n,\n[\n�\n] \n+ c\n2\n[\n-\n�\nl \nwe have \nk \n(  )\nk \n[ \n1\n]    (  )\nk \n[\n-\n1\n] \nx\nk \n= \nA \nXo \n= \nc\n1 \n0.5 \n1 \n+ \nc\n2 \n0.8 \n1 \nOnce again the origin is an attractor, because x\nk \napproaches 0 for any choice of Xo\n· \nIf \nc\n2 \n*  0, the trajectory will approach the line through the origin with direction vector \n[ \n-\n�\n]\n. Several such trajectories are shown in Figure 4.24. The vectors Xo where c\n2 \n=  0 \nare on the line through the origin with direction vector \n[ \n�\n]\n, and the correspond­\ning trajectory in this case follows this line into the origin.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":55219,"to":55307}}}}],[987,{"pageContent":"Example 4.50 \nSection 4.6 Applications and the Perron-Frobenius Theorem 351 \ny \nFigure 4.24 \nDiscuss the behavior of the dynamical systems x\nk\n+ \n1 \n= Ax\nk \ncorresponding to the \nfollowing matrices: \n(a) A = \n[\n� \n�\n] \n(b) A = \n[\n�\n.\n5 \n�\n-\nS\nJ \nSolution (a) The eigenvalues of A are \n5 \nand 3 with corresponding eigenvectors \n[ \n�\n] \nand \n[ \n-\n�\n]\n, respectively. Hence for an initial vector \nXo  =  c\n1 [ \n�\n] \n+ \nc\n2 \n[ \n-\n�\n]\n, we have \nAs k becomes large, so do both \n5\nk \nand 3\nk\n_ Hence, x\nk \ntends away from the origin. \nBecause the dominant eigenvalue of \n5 \nhas corresponding eigenvector \n[ \n�\n]\n, all trajec­\ntories for which c\n1 \n-=fa \n0 will eventually end up in the first or the third quadrant. Trajec­\ntories with c\n1 \n= 0 start and stay on the line y  = \n-\nx whose direction vector is \n[ \n-\nl\n]\n. \nSee Figure 4.25(a). \n1 \n(b) In this example, the eigenvalues are 1.5 and 0.\n5 \nwith corresponding eigenvectors \n[ \n�\n] \nand \n[ \n-\n�\n]\n, respectively. Hence,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":55309,"to":55401}}}}],[988,{"pageContent":"352 \nChapter 4 \nEigenvalues and Eigenvectors \nFigure 4.25 \nExample 4.51 \ny y \n20 \n-20 \n(a) \nIf c\n1 \n= 0, then x\nk \n= c\n2\n(\n0.\n5\n)\nk\n[ \n-\n�\n] \n---+ \n[ \n�\n] \nas k\n---+ \ncxi. But if c\n1 \n-=fa \n0, then \nand \nsuch trnjeoto;ies osymptotkally apprnooh the line y \n� \nx. See Egme 4.2\n5\n(b) \n4 \nIn Example 4.50(a), all points that start out near the origin become increasingly \nlarge in magnitude because \nl\n,t\nl > 1 for both eigenvalues; 0 is called \na repeller. In \nExample 4.\n50(b \n)\n, 0 is called a saddle point because the origin attracts points in some \ndirections and repels points in other directions. In this case, \nl\n-\\\n1\n1 \n< 1 and \nl\n-\\\n2\n1 \n> 1. \nThe next example shows what can happen when the eigenvalues of a real 2 X 2 \nmatrix are complex (and hence conjugates of one another). \nPlot the trajectory beginning with Xo = \n[ !\n] \nfor the dynamical systems x\nk\n+\n1 \n= Ax\nk \ncorresponding to the following matrices: \n(a) A= \n[0.\n5 \n0.5 \n-\n0.5\n] \n0.\n5 \n(b) A= \n[0.2 \n0.6 \n-\n1.2] \n1.4 \nSolulion The trajectories are shown in Figure 4.26( a) and (b ), respectively. Note that","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":55403,"to":55492}}}}],[989,{"pageContent":"k\n+\n1 \n= Ax\nk \ncorresponding to the following matrices: \n(a) A= \n[0.\n5 \n0.5 \n-\n0.5\n] \n0.\n5 \n(b) A= \n[0.2 \n0.6 \n-\n1.2] \n1.4 \nSolulion The trajectories are shown in Figure 4.26( a) and (b ), respectively. Note that \n(a) is a trajectory spiraling into the origin, whereas (b) appears to follow an elliptical \norbit.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":55492,"to":55515}}}}],[990,{"pageContent":"y \n-4 \nFigure 4.26 \nTheorem 4.42 \nIm \na\n+ \nb\ni \nr \nb \ne \nFigure 4.21 \n(a) \nSection 4.6 \nApplications and the Perron-Frobenius Theorem \n353 \ny \n-1\n0 \n(b) \nThe following theorem explains the spiral behavior of the trajectory in \nExample 4.S\nl\n(a). \n[\na \n-\nb\nJ \nLet A = \nb   a \n. The eigenvalues of A are ,\\ = a ± bi, and if a and b are not \nboth zero, then A can be factored as \nA= \n[\na \n-\nb\nJ \n= \n[\nr \nO\nJ \n[\nc\n�\ns() \nb   a 0 r  sm() \n-\nsin() \nJ \ncos() \nwhere r = IAI = Va\n2 \n+ b\n2 \nand() is the principal argument of a + bi. \nProof \nThe eigenvalues of A are \nA= \nt\n(\n2a ± �) = \nt\n(\n2a ± 2\\/b2\\/=l\n) \n=a± \nl\nb\nl\ni =a± bi \nby Exercise 35(b) in Section 4.1. Figure 4.27 displays a + bi, r, and(). It follows that \nA = \n[\na \n-\nb\nJ \n= r\n[\na\n/\nr \nb   a \nb\n/\nr \n-\nb/r\nJ \n= \n[\nr \nO\nJ \n[\nc\n�\ns() \na\n/\nr 0 r  sm() \n-\nsin() \nJ \ncos () \nRemark Geometrically, Theorem 4.42 implies that when A = [: �\nb \nJ \n-=F  0, \nthe linear transformation T ( x) = Ax is the composition of a rotation R = \n[ \nc\n�\ns () \n-\nsin() \nJ \nthrough the angle () followed by a scaling S = \n[ \nr \nO\nJ \nwith fac-\nsm () cos() \n0 r \ntor r (Figure 4.28). In Example 4.S\nl","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":55517,"to":55647}}}}],[991,{"pageContent":"[ \nc\n�\ns () \n-\nsin() \nJ \nthrough the angle () followed by a scaling S = \n[ \nr \nO\nJ \nwith fac-\nsm () cos() \n0 r \ntor r (Figure 4.28). In Example 4.S\nl\n(a), the eigenvalues are ,\\ = 0.\n5 \n± \nO\n.Si  so \nr = IAI = \\./2\n/\n2 \n= \n0.707 < 1, and hence the trajectories all spiral inward toward 0. \nThe next theorem shows that, in general, when a real 2 X 2 matrix has complex \n[\na \n-\nb\nJ \neigenvalues, it is similar to a matrix of the form \nb   a \n. For a complex vector \nx\n= \n[\n:\nJ \n[\n::�:\nJ \n= \n[\n:\nJ \n+ \n[\n�}","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":55647,"to":55697}}}}],[992,{"pageContent":"354 \nChapter 4 \nEigenvalues and Eigenvectors \nTheorem 4.43 \ny \nScaling /Ax = SRx \nRx \nRotation \n/ \nFigure 4.28 \nA rotation followed by a scaling \nwe define the real part, Re x, and the imaginary part, Im x, of x to be \nRe x=\n[\n:\n]    [\n:::\n] \nand Imx = \n[\n�\n]    [\n:::\n] \nLet A be a real 2  X  2 matrix with a complex eigenvalue A =  a  -   bi (where b  i=  0\n) \nand corresponding eigenvector x. Then the matrix P = [Re x Im x] is invertible \nand \nProof \nLet x =  u  + vi so that Rex =  u and Im x = v. From Ax = Ax, we have \nAu + Avi = Ax = Ax =  ( a  -bi\n)(\nu  + vi\n) \n=\nau+ avi -   bui  +  bv =(au+  bv\n) \n+\n(\n-bu  + av\n)\ni \nEquating real and imaginary parts, we obtain \nAu= au+ bv and Av= -bu  + av \nNow P  = \n[u\nl\nvJ, so \n[a -b\n] [\na \nP \nb a \n= \n[u\nl\nv\nl \nb \n-b\n] \na \n= \n[au\n+ \nbv\nl\n-b\nu \n+a\nv]\n= \n[Au\nl\nAv\n] \n= \nA[\nu\nl\nv\nl \n=AP \nTo show that P is invertible, it is enough to show that u and v are linearly indepen­\ndent. If u and v were not linearly independent, then it would follow that v = ku for \nsome (nonzero complex) scalar k, because neither u nor vis 0. Thus,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":55699,"to":55784}}}}],[993,{"pageContent":"dent. If u and v were not linearly independent, then it would follow that v = ku for \nsome (nonzero complex) scalar k, because neither u nor vis 0. Thus, \nx =  u  + vi =  u  + kui =  ( 1  + ki)\nu \nNow, because A is real, Ax = Ax implies that \nAx = Ax = Ax = Ax = Ax \nso x =  u  -vi is an eigenvector corresponding to the other eigenvalue A =  a  + bi. \nBut \nx = (1 + ki)\nu \n= (1 -\nk\ni)\nu","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":55784,"to":55797}}}}],[994,{"pageContent":"Section 4.6 \nApplications and the Perron-Frobenius Theorem \n355 \nbecause u is a real vector. Hence, the eigenvectors x and x of A are both nonzero \nmultiples of u and therefore are multiples of one another. This is impossible because \neigenvectors corresponding to distinct eigenvalues must be linearly independent by \nTheorem 4.20. (This theorem is valid over the complex numbers as well as the real \nnumbers.) \nThis contradiction implies that u and v are linearly independent and hence P is \ninvertible. It now follows that \nTheorem 4.43 serves to explain Example 4.\n5\nl\n(b). The eigenvalues of \nA= \n[\n0.2 \n0.6 \n-\n1.2\n] \n1.4 \nare 0.8 ±: 0.6i. For ,.\\ = 0.8 \n-\n0.6i, a corresponding eigenvector is \nFrom Theorem 4.43, it follows that for P = \nhave \n-\n1\n] \n[\n0.8 \nand C = \n0 \n0.6 \nA = PCP\n-\n1 \nand P\n-\n1\nAP = C \n-\n0.6\n] \n0.8 \n, we \nFor the given dynamical system x\nk\n+ \n1 \n= Ax\nk\n, we perform a change of variable. Let \nThen \nso \n� \nNowChas thesame eigenvaluesas A (why?)and\nl\n0.8 ±: 0.6i\nl \n= 1.Thus, the dynami­\ncal system \nY\nk\n+i \n= Cy\nk","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":55799,"to":55866}}}}],[995,{"pageContent":"] \n0.8 \n, we \nFor the given dynamical system x\nk\n+ \n1 \n= Ax\nk\n, we perform a change of variable. Let \nThen \nso \n� \nNowChas thesame eigenvaluesas A (why?)and\nl\n0.8 ±: 0.6i\nl \n= 1.Thus, the dynami­\ncal system \nY\nk\n+i \n= Cy\nk \nsimply rotates the points in every trajectory in a circle about \nthe origin by Theorem 4.42. \nTo \ndetermine a trajectory of the dynamical system in Example 4.5\nl\n(b), we it­\neratively apply the linear transformation T\n(\nx\n) \n= Ax = PCP\n-\n1\nx. The transformation \ncan be thought of as the composition of a change of variable (x to y), followed by the \nrotation determined by C, followed by the reverse change of variable (y back to x). We \nwill encounter this idea again in the application to graphing quadratic equations in \nSection \n5.5 and, more generally, as \"change of basis\" in Section 6.3. In Exercise 74 of \nSection 5.5,  you will show that the trajectory in Example 4.51 (b) is indeed an ellipse, \nas it appears to be from Figure 4.26(b).","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":55866,"to":55910}}}}],[996,{"pageContent":"Section 5.5,  you will show that the trajectory in Example 4.51 (b) is indeed an ellipse, \nas it appears to be from Figure 4.26(b). \nTo summarize then: If a real 2 X 2 matrix A has complex eigenvalues,.\\ = \na \n±: bi, \nthen the trajectories of the dynamical system x\nk\n+ I \n= Ax\nk \nspiral inward if 1,.\\1 < 1 \n(O is a spiral attractor), spiral outward if 1,.\\1 > 1 (O is a spiral repeller), and lie on a \nclosed orbit if 1,.\\1 = 1 (O is an orbital center).","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":55910,"to":55922}}}}],[997,{"pageContent":"356 \nVignette \nRanking Sports Teams and Searching \nthe Internet \nIn any competitive sports league, it is not necessarily a straightforward process to \nrank the players or teams. Counting wins and losses alone overlooks the possibility \nthat one team may accumulate a large number of victories against weak teams, while \nanother team may have fewer victories but all of them against strong teams. Which \nof these teams is better? How should we compare two teams that never play one \nanother? Should points scored be taken into account? Points against? \nDespite these complexities, the ranking of athletes and sports teams has become \na commonplace and much-anticipated feature in the media. For example, there are \nvarious annual rankings of U.S. college football and basketball teams, and golfers and \ntennis players are also ranked internationally. There are many copyrighted schemes \nused to produce such rankings, but we can gain some insight into how to approach","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":55924,"to":55938}}}}],[998,{"pageContent":"tennis players are also ranked internationally. There are many copyrighted schemes \nused to produce such rankings, but we can gain some insight into how to approach \nthe problem by using the ideas from this chapter. \nTo establish the basic idea, let's revisit Example 3.69. Five tennis players play one \nanother in a round-robin tournament. Wins and losses are recorded in the form of \na digraph in which a directed edge from i to j indicates that player i defeats player j. \nThe corresponding adjacency matrix A therefore has aiJ = 1 if player i defeats player \nj and has aiJ = 0 otherwise. \n0 \n1 \n0 \n5 \n2 \n0  0 1 \nA= \n1  0  0 \n1 \n0 \n0  0  0  0 \n1 \n0  0 \n1 \n0  0 \n4 \n3 \nWe would like \nto associate a ranking r; with player i in such a way that \nr\n; \n> r\n1 \nindicates that player i is ranked more highly than player j. For this","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":55938,"to":55969}}}}],[999,{"pageContent":"purpose, let's require that the r;'s be probabilities (that is, 0 :s r\n; \n:s 1 for all i, and \nr\n1 \n+  r\n2 \n+  r\n3 \n+  r\n4 \n+  r\n5 \n=  1 ) and then organize the rankings in a ranking vector \nFurthermore, let's insist that player i's ranking should be proportional to the sum of \nthe rankings of the players defeated by player i. For example, player 1 defeated players \n2, 4, and 5, so we want \nwhere a is the constant of proportionality. Writing out similar equations for the other \nplayers produces the following system: \nr\n1 \n=  a ( r\n2 \n+ \nr\n4 \n+ \nr\n5\n) \nr\n2 \n= \na(r\n3 \n+ \nr\n4 \n+ \nr\ns\n) \nr\n3 \n= a(r\n1 \n+ \nr\n4\n) \nObserve that we can write this system in matrix form as \n'\n1 \n0 \n1 \n0 \n'\n1 \nr\n2 \n0     0 \n1 \nr\n2 \nf\n3 \n= ll' 1 \n0     0 \n1 0 \nf\n3 \nor \nf\n4 \n0 \n0     0 \n0 \n1 \nf\n4 \nf\n5 \n0 \n0 \n1 \n0     0 \nf\n5 \nr  = aAr \n1 \nEquivalently, we see that the ranking vector r must satisfy Ar  =  -r. In other words, \nr is an eigenvector corresponding to the matrix A! \na \nFurthermore, A is a primitive nonnegative matrix, so the Perron-Frobenius Theo­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":55971,"to":56064}}}}],[1000,{"pageContent":"r is an eigenvector corresponding to the matrix A! \na \nFurthermore, A is a primitive nonnegative matrix, so the Perron-Frobenius Theo­\nrem guarantees that there is a unique ranking vector r. In this example, the ranking \nvector turns out to be \n0.29 \n0.27 \nr= \n0.22 \n0.08 \n0.14 \nso we would rank the players in the order 1, 2, 3, 5, 4. \nBy modifying the matrix A, it is possible to take into account many of the com­\nplexities mentioned in the opening paragraph. However, this simple example has \nserved to indicate one useful approach to the problem of ranking teams. \n351","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":56064,"to":56079}}}}],[1001,{"pageContent":"358 \nThe same idea can be used to understand how an Internet search engine such as \nGoogle works. Older search engines used to return the results of a search unordered. \nUseful sites would often be buried among irrelevant ones. Much scrolling was often \nneeded to uncover what you were looking for. By contrast, Google returns search re­\nsults ordered according to their likely relevance. Thus, a method for ranking websites \nis needed. \nInstead of teams playing one another, we now have websites linking to one an­\nother. We can once again use a digraph to model the situation, only now an edge from \ni to j indicates that website i links to (or refers to) website j. So whereas for the sports \nteam digraph, incoming directed edges are bad (they indicate losses), for the Internet \ndigraph, incoming directed edges are good (they indicate links from other sites). In \nthis setting, we want the ranking of website i to be proportional to sum of the rank­\nings of all the websites that link to i.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":56081,"to":56094}}}}],[1002,{"pageContent":"this setting, we want the ranking of website i to be proportional to sum of the rank­\nings of all the websites that link to i. \nUsing the digraph on page 356 to represent just five websites, we have \nr\n4 \n= a(r\n1 \n+  r\n2 \n+  r\n3\n) \nfor example. It is easy to see that we now want to use the transpose of the adjacency \n1 \nmatrix of the digraph. Therefore, the ranking vector r must satisfy A\nT \nr = -rand will \nthus be the Perron eigenvector of A\nr\n. In this example, we obtain \na \n0     0 \n1 \n0     0 0.14 \n0     0     0     0 \n0.08 \nA\nT\n= \n0 \n1 \n0     0 \n1 and \nr= \n0.22 \n1 1 \n0     0 0.27 \n1 \n0 0 0.29 \nso a search that turns up these five sites would list them in the order 5, 4, 3, 1, 2. \nGoogle actually uses a variant of the method described here and computes the rank­\ning vector via an iterative method very similar to the power method (Section 4.5\n)\n.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":56094,"to":56137}}}}],[1003,{"pageContent":"Section 4.6 \nApplications and the Perron-Frobenius Theorem \n359 \nI \nExercises 4.6 \nMarkov Chains \nWhich of th e stochastic matrices in Exercises 1-6 are regular? \n1. \n[ \n� \n�\n] \n2. \n[ \n� \n!\n] \n3. \n[\ni \n�\n] \n4. [\n! \n0 \n0 \n1 \n[\nO\nJ \n0 \n�\n'\n] \n[\n\"\n' \n5. 0.\n5 \n1 \n6. �.\n5 \n0.4 \n0 0.\n5 \n�\n] \n�\n] \n0 \n0 \nIn Exercises 7-9, Pis th e transition matrix of a regular \nMarkov chain. Find th e long range transition matrix L of P. \n[\nl \nl \n[ \n[\ni \ni\nJ \n3 \n7.P  = \n8.P = \nl \n2 \nI \n6 \n[ \n02 \n0.3 \n04\n] \n9. p  = 0.6 \n0.1 \n0.4 \n0.2 \n0.6 \n0.2 \n10. Prove that the steady state probability vector of a \nregular Markov chain is unique. [Hint: Use Theo­\nrem 4.33 or Theorem 4.34.] \nPopulation Growth \nIn Exercises 11-14, calculate th e positive eigenvalue and a \ncorresponding positive eigenvector of the Leslie matrix L. \n11.L  = \n[\nO \n0.\n5 \n13. L \n� [�s \n�\n] \n7 \n0 \n0.\n5 \n�\n] \n12.L=\n[\n1 \n0.\n5 \n14.L \n� \n[i \n5 \n0 \n2 \n3 \n�\n] \n15. If a Leslie matrix has a unique positive eigenvalue A\n1\n, \nwhat is the significance for the population if A\n1 \n> 1? \nA 1<l?A1\n=1? \n16. Verify that the characteristic polynomial of the Leslie","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":56139,"to":56261}}}}],[1004,{"pageContent":"5 \n0 \n2 \n3 \n�\n] \n15. If a Leslie matrix has a unique positive eigenvalue A\n1\n, \nwhat is the significance for the population if A\n1 \n> 1? \nA 1<l?A1\n=1? \n16. Verify that the characteristic polynomial of the Leslie \nmatrix Lin Equation (3) is \ncL\n(\nA\n) \n= \n(-l\n)\n\"\n(\nA\n\" \n-  b\n1\nA\n\"\n-\n1 \n-  b\n2\ns\n1\nA\n\"\n-\n2 \n-  b\n3\ns\n1\ns\n2\nA\n\"\n-\n3 \n-\n· · · \n-  b\nn\ns\n1\ns\n2 \n· · ·\nSn\n-\nI\n) \n[Hint: Use mathematical induction and expand \ndet(L -Al\n) \nalong the last column.] \n17. If all of the survival rates s; are nonzero, let \n1 \n0   0 0 \n0 \nS1 \n0 0 \nP= \n0  0 \nS1S\n2 \n0 \n0  0  0 \nS1S\n2\n· ··\ns\nn\n-\n1 \nCompute P\n-\n1\nLP and use it to find the characteris­\ntic polynomial of L. [Hint: Refer to Exercise 32 in \nSection 4.3.] \n18. \nVerify that an eigenvector of L corresponding to A 1 \nis \nS1\n/ A\n1 \nS1S\n2\n/ A\ni \nS 1 S\n2\nS3\n/ A\ni \n[ Hint: Combine Exercise 17 above with Exercise 32 in \nSection 4.3 and Exercise 46 in Section 4.4.] \nGA\ns \nIn Exercises 19-21, compute the steady state growth rate of \nthe population with th e Leslie matrix L from the given \nexercise. Then use Exercise 18 to help find th e corresponding","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":56261,"to":56375}}}}],[1005,{"pageContent":"GA\ns \nIn Exercises 19-21, compute the steady state growth rate of \nthe population with th e Leslie matrix L from the given \nexercise. Then use Exercise 18 to help find th e corresponding \ndistribution of th e age classes. \n19. Exercise 39 in Section 3.7 \n20. Exercise 40 in Section 3.7 \n21. Exercise 44 in Section 3.7 \nGA\ns \n22. Many species of seal have suffered from commercial \nhunting. They have been killed for their skin, blubber, \nand meat. The fur trade, in particular, reduced some \nseal populations to the point of extinction. Today, the \ngreatest threats to seal populations are decline of fish \nstocks due to overfishing, pollution, disturbance of \nhabitat, entanglement in marine debris, and culling \nby fishery owners. Some seals have been declared \nendangered species; other species are carefully \nmanaged. \nTable 4. 7 \ngives the birth and survival rates \nfor the northern fur seal, divided into 2-year age \nclasses. [The data are based on A. E. Yo rk and J. R.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":56375,"to":56399}}}}],[1006,{"pageContent":"managed. \nTable 4. 7 \ngives the birth and survival rates \nfor the northern fur seal, divided into 2-year age \nclasses. [The data are based on A. E. Yo rk and J. R. \nHartley, \"Pup Production Following Harvest of Female \nNorthern Fur Seals;' Canadian Journal of Fisheries and \nAquatic Science, 38 (1981), pp. 84-90.]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":56399,"to":56406}}}}],[1007,{"pageContent":"360 \nChapter 4 \nEigenvalues and Eigenvectors \nTable 4.1 \nA\nge (years) \nBirth Rate \nSurvival Rate \n0-2 \n0.00 \n0.91 \n2-4 \n0.02 0.88 \n4-6 \n0.70 0.85 \n6-8 \n1.53 \n0.80 \n8-10 \n1.67 \n0.74 \n10-12 \n1.65 \n0.67 \n12-14 \n1.56 \n0.59 \n14-16 \n1.45 0.49 \n16-18 \n1.22 \n0.38 \n18-20 \n0.91 \n0.27 \n20-22 \n0.70 0.17 \n22-24 \n0.22 \n0.15 \n24-26 \n0.00 0.00 \n(a) Construct the Leslie matrix L for these data and \ncompute the positive eigenvalue and a corre­\nsponding positive eigenvector. \n(b) In the long run, what percentage of seals will be in \neach age class and what will the growth rate be? \nExercise 23 shows that the long-run behavior of a popula­\ntion can be determined directly from the entries of its Leslie \nmatrix. \n23. The net reproduction rate of a population is defined as \n(b) Show that r = 1 if and only if ,\\\n1 \n= 1. (This repre­\nsents zero population growth.\n) \n[Hint: Let \n) \nb\nl \nb\nzS1 \nb\n3S 1 S\n2 \nb\nnS1 S\n2\n· \n.. \nSn\n-\n1 \ng\n(\n,\\ = \n-+ -\n+ \n--\n+ \n... \n+ \n----\n,\\ \n,\\\n2 \n,\\\n3 \n,\\\n\" \nShow that ,\\ is an eigenvalue of L if and only if \ng(,\\) = 1.]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":56408,"to":56499}}}}],[1008,{"pageContent":"= 1. (This repre­\nsents zero population growth.\n) \n[Hint: Let \n) \nb\nl \nb\nzS1 \nb\n3S 1 S\n2 \nb\nnS1 S\n2\n· \n.. \nSn\n-\n1 \ng\n(\n,\\ = \n-+ -\n+ \n--\n+ \n... \n+ \n----\n,\\ \n,\\\n2 \n,\\\n3 \n,\\\n\" \nShow that ,\\ is an eigenvalue of L if and only if \ng(,\\) = 1.] \n(c) Assuming that there is a unique positive eigen­\nvalue ,\\\n1\n, show that r < 1 if and only if the popu­\nlation is decreasing and r > 1 if and only if the \npopulation is increasing. \nA sustainable harvesting policy is a procedure that allows a \ncertain fraction of a population (represented by a population \ndistribution vector x) to be harvested so that the population \nreturns to x after one time interval (where a time interval \nis the length of one age class). If h is the fraction of each \nage class that is harvested, then we can express the harvest­\ning procedure mathematically as follows: If we start with \na population vector x, after one time interval we have Lx; \nharvesting removes hLx, leaving \nLx -   hLx = (1 -  h\n)\nLx \nSustainability requires that \n(1 -  h\n)\nLx = x \n24. If ,\\ \n1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":56499,"to":56561}}}}],[1009,{"pageContent":"a population vector x, after one time interval we have Lx; \nharvesting removes hLx, leaving \nLx -   hLx = (1 -  h\n)\nLx \nSustainability requires that \n(1 -  h\n)\nLx = x \n24. If ,\\ \n1 \nis the unique positive eigenvalue of a Leslie \nmatrix L and h is the sustainable harvest ratio, prove \nthat h =  1  -\n1 /\n,\\\n1\n. \ncAs \n25. (a) Find the sustainable harvest ratio for the wood­\nland caribou in Exercise 44 in Section 3.7. \n(b) Using the data in Exercise 44 in Section 3.7, \nreduce the caribou herd according to your answer \nto part (a). Verify that the population returns to its \noriginal level after one time interval. \n26. Find the sustainable harvest ratio for the seal in \nExercise 22. (Conservationists have had to harvest \nseal populations when overfishing has reduced the \navailable food supply to the point where the seals are \nin danger of starvation.) \nr = b\n1 \n+  b\n2\ns\n1 \n+ \nb\n3\ns\n1\ns\n2 \n+ \n· · · \n+  b\nn\ns\n1\ns\n2\n· · · \ns\nn\n-\nl \nwhere the b; are the birth rates and the s\nj \nare the \nsurvival rates for the population. \n�","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":56561,"to":56621}}}}],[1010,{"pageContent":"in danger of starvation.) \nr = b\n1 \n+  b\n2\ns\n1 \n+ \nb\n3\ns\n1\ns\n2 \n+ \n· · · \n+  b\nn\ns\n1\ns\n2\n· · · \ns\nn\n-\nl \nwhere the b; are the birth rates and the s\nj \nare the \nsurvival rates for the population. \n� \n27. Let L be a Leslie matrix with a unique positive eigen­\nvalue ,\\\n1\n. Show that if ,\\ is any other (real or com­\nplex) eigenvalue of L, then \n1\n,\\\n1 \n:s ,\\\n1\n. [Hint: Write,\\ = \nr(cos () +  i sin ()) and substitute it into the equation \n(a) Explain why r can be interpreted as the average \nnumber of daughters born to a single female over \nher lifetime. \ng(,\\) = 1, as in part (b) of Exercise 23. Use De Moivre's \nTheorem and then take the real part of both sides. The \nTriangle Inequality should prove useful.]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":56621,"to":56670}}}}],[1011,{"pageContent":"Section 4.6 Applications and the Perron-Frobenius Theorem \n361 \nThe Perron-Frobenius Theorem \nIn Exercises 28-31, find th e Perron root and the correspond­\ning Perron eigenvector of A. \n28.A  = \n[\n2\n1 \n0\n1\n] \n30.A \n� \n[\n: \n� \ni] \n29.A  = \n[\n2\n1 \n0\n3\n] \n31. A \n� \n[\n: \n0 \n� \nl \nIt can be shown that a nonnegative n X n matrix is irreduc­\nible \nif and only if (I  + A\n) \nn\n-\nt > 0. In Exercises 32-35, use \nth is criterion to determine whether the matrix A is irreduc­\nible. If A is reducible, find a permutation of its rows and \ncolumns that puts A into th e block form \n0 \n1 \n0 0 \n34.A  = \n1 \n0 \n0 \n0 \n0 \n0 \n1 \n0 \n0 \n0 \n0 \n1 \n0 \n0 \n1 \n0 \n0 \n33. A \n� \n[ \n! \n� \n� \n� \nl \n0 \n0 \n35.A  = \n1 \n0 \n0 \n1 \n0 \n0 \n0 \n0 \n0 0 0 \n0 0 \n0 0  1 \n1  0 0 \n0  1  1 \n36. (a) If A is the adjacency matrix of a graph G, show \nthat A is irreducible if and only if G is connected. \n(A graph is connected if there is a path between \nevery pair of vertices.) \n(b) Which of the graphs in Section 4.0 have an \nirreducible adjacency matrix? Which have a \nprimitive adjacency matrix?","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":56672,"to":56765}}}}],[1012,{"pageContent":"(A graph is connected if there is a path between \nevery pair of vertices.) \n(b) Which of the graphs in Section 4.0 have an \nirreducible adjacency matrix? Which have a \nprimitive adjacency matrix? \n37. Let G be a bipartite graph with adjacency matrix A. \n(a) Show that A is not primitive. \n(b) Show that if A is an eigenvalue of A, so is \n-\nA. \n[Hint: Use Exercise 80 in Section 3.7 and partition \nan eigenvector for A so that it is compatible with \nthis partitioning of A\n. \nUse this partitioning to find \nan eigenvector for \n-\nA.] \n38. A graph is called k-regular if k edges meet at each ver-\n(b) Show that if A is primitive, then the other eigen­\nvalues are all less thank in absolute value. [Hint: \nAdapt Theorem 4.31.] \n39. Explain the results of your exploration in Section 4.0 \nin light of Exercises 36-38 and Section 4.\n5\n. \nIn Exercise 40, th e absolute value of a matrix A  =   [ ai\nj\n] is \ndefined to be the matrix IAI  = \n[ \nl\nau\nl\n]. \n40. Let A and B be n X n matrices, x a vector in IJ�r, and c","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":56765,"to":56800}}}}],[1013,{"pageContent":"5\n. \nIn Exercise 40, th e absolute value of a matrix A  =   [ ai\nj\n] is \ndefined to be the matrix IAI  = \n[ \nl\nau\nl\n]. \n40. Let A and B be n X n matrices, x a vector in IJ�r, and c \na scalar. Prove the following matrix inequalities: \n(\na\n) \nl\ncA\nI \n= \nl\nc\nl \nI\nA\nI \n(c\n) \nI\nAx\nl \n:s \nI\nA\nI \nl\nx\nl \n(b) \nI\nA  +  B\nl \n::; \nI\nA\nI \n+ \nI\nB\nI \n(d) \nI\nAB\nI \n::; \nI\nA\nI I\nB\nI \n41. Prove that a 2 X 2 matrix A = \n[\na\n\" \na\nz 1 \nif and only if a 1\n2 \n= 0 or a\n2\n1 \n= 0. \na\n,\n2\n] \n· \nd 'bl \nIS re UC! e \na\nz\n2 \n42. Let A be a nonnegative, irreducible matrix such \nthat I \n-\nA is invertible and (\nI \n-\nA\n)-\n1 \n2 0. Let \nA\n1 \nand v\n1 \nbe the Perron root and Perron eigenvector \nof A. \n(a) Prove that 0 < A\n1 \n< 1. [Hint: Apply Exercise 22 \nin Section 4.3 and Theorem 4.18(b).] \n(b) Deduce from (a) that v\n1 \n> Av\n,\n. \nLinear Recurrence Relations \nIn Exercises 43-46, write out th e first six terms of th e \nsequence defined by th e recurrence relation with the given \ninitial conditions. \n43. \nX\no \n= \n1, \nX\nn \n= \n2x\nn\n-\nI \nfor n 2 1 \n44. a\n, \n= 128, a\nn \n=  a\nn\n_\n,\n/2 for n 2 2 \n45.\ny\n0 \n= \nO,\ny\n1 \n= \nl\n,\ny\nn \n= \nY\nn\n-\nI \n-\nY\nn\n-\nz \nfor\nn \n2 \n2 \n46. b\no \n= \n1, \nb\n, \n= \n1, \nb\nn","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":56800,"to":56968}}}}],[1014,{"pageContent":"initial conditions. \n43. \nX\no \n= \n1, \nX\nn \n= \n2x\nn\n-\nI \nfor n 2 1 \n44. a\n, \n= 128, a\nn \n=  a\nn\n_\n,\n/2 for n 2 2 \n45.\ny\n0 \n= \nO,\ny\n1 \n= \nl\n,\ny\nn \n= \nY\nn\n-\nI \n-\nY\nn\n-\nz \nfor\nn \n2 \n2 \n46. b\no \n= \n1, \nb\n, \n= \n1, \nb\nn \n= \n2b\nn\n-\nI \n+ \nb\nn\n-\n2 \nfor \nn \n2 \n2 \nIn Exercises 47-52, solve th e recurrence relation with the \ngiven initial conditions. \n47. \nX\no \n= 0, \nX\n1 \n= \n5, X\nn \n=  3\nX\nn\n-\nI \n+ 4X\nn\n-\n2 \nfor n 2 2 \n48. X\no \n= 0, \nX\n1 \n= \n1, X\nn \n= \n4X\nn\n-\nI \n-3X\nn\n-\n2 \nfor n 2 2 \ntex. Let G be a\nk\n-regular graph. \nEV \n52. \nThe recurrence relation in Exercise 45. Show that your \n(a) Show that the adjacency matrix A of G \nhas A = k as an eigenvalue. [Hint: Adapt \nTheorem 4.30.] \nsolution \nagrees with the answer to Exercise 45. \n53. Complete the proof of Theorem 4.38(a) by showing \nthat if the recurrence relation X\nn \n= ax\nn\n-\ni + bx\nn\n-\n2 \nhas","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":56968,"to":57101}}}}],[1015,{"pageContent":"362 \nChapter 4 \nEigenvalues and Eigenvectors \ndistinct eigenvalues ,\\1 * ,\\\n2\n, then the solution will be \nof the form \n[Hint: Show that the method of Example 4.40 works in \ngeneral.] \n54. (a) Show that for any choice of initial conditions \nx0 = rand x\n1 \n= s, the scalars c\n1 \nand c\n2 \ncan be \nfound, as stated in Theorem 4.38(a) and (b). \n(b) If the eigenvalues ,\\1 and ,\\\n2 \nare distinct and the \ninitial conditions are Xo  = 0, x\n1 \n= 1, show that \nX\nn \n= ( \nl \n)(,\\7 \n-\n,\\�). \nA\n1 \n-  A\nz \n55. The Fibonacci recurrence f,, = \nf\nn\n-\nI + \nf\nn\n-\n2 \nhas the \nassociated matrix equation x\nn \n= A\nX\nn\n_1, where \nX\nn \n= \nI \nf\nn \n] \na\nnd \nA \n= \n[ \nl \nl\n] \nLt\nn -\n1 \n1  0 \n(a) Withf0 = 0 andf\n1 \n= 1, use mathematical induc­\ntion to prove that \nA\n\" \n= \n[\nJ\nn\n+\nI \nf\nn \n] \nJ\nn \nf\nn\n-\n1 \nfor all n 2 1. \n(b) Using part (a), prove that \nf\nn\n+i\nf\nn\n-\n1 \n-\nf\nn\n2 \n= \n(\n-\n1\n)\n\" \nfor all n 2 1. [This is called Cassini's Identity, \nafter the astronomer Giovanni Domenico Cas­\nsini (1625   -1712). Cassini was born in Italy but, \non the invitation of Louis XIV, moved in 1669","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":57103,"to":57216}}}}],[1016,{"pageContent":"= \n(\n-\n1\n)\n\" \nfor all n 2 1. [This is called Cassini's Identity, \nafter the astronomer Giovanni Domenico Cas­\nsini (1625   -1712). Cassini was born in Italy but, \non the invitation of Louis XIV, moved in 1669 \nto France, where he became director of the Paris \nObservatory. He became a French citizen and \nadopted the French version of his name: Jean­\nDominique Cassini. Mathematics was one of his \nmany interests other than astronomy. Cassini's \nIdentity was published in 1680 in a paper sub­\nmitted to the Royal Academy of Sciences in Paris.] \n(c) An 8 X 8 checkerboard can be dissected as shown \nin Figure 4.29(a) and the pieces reassembled \nto form the \n5 \nX 13 rectangle in Figure 4.29(b). \n\\ \n\\ \n' \nv \n........ \n\\ \n� \nv \n\\ \n\\ \n' \n(a) \n1.......-\n_..... \n� \n.......... \n__.v\" \nv \n� \n_...... \nv \n(b) \nFigure 4.29 \nThe area of the square is 64 square units, but the \nrecta\nngle's area is 65 \nsquare units! Where did the \nextra square come from? [Hint: What does this \nhave to do with the Fibonacci sequence?]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":57216,"to":57266}}}}],[1017,{"pageContent":"The area of the square is 64 square units, but the \nrecta\nngle's area is 65 \nsquare units! Where did the \nextra square come from? [Hint: What does this \nhave to do with the Fibonacci sequence?] \n56. You have a supply of three kinds of tiles: two kinds \nof 1 X 2 tiles and one kind of 1 X 1 tile, as shown in \nFigure 4.30. \nDCJ \nFigure 4.30 \nLet t\nn \nbe the number of different ways to cover a \n1 \nX n rectangle with these tiles. For example, \nFigure 4.31 shows that t\n3 \n= \n5\n. \n(a) Find t\n1\n, ••• , t5 • \n� (Does t0 make any sense? If so, what is it?) \n(b) Set up a second order recurrence relation for t\n\"\n' \n(c) Using t\n1 \nand t\n2 \nas the initial conditions, solve the \nrecurrence relation in part (b). Check your answer \nagainst the data in part (a). \nFigure 4.31 \nThe five ways to tile a 1 X 3 rectangle \n57. You have a supply of 1 X 2 dominoes with which \nto cover a 2 X n rectangle. Let d\nn \nbe the number of \ndifferent ways to cover the rectangle. For example, \nFigure 4.32 shows that d\n3 \n= 3.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":57266,"to":57310}}}}],[1018,{"pageContent":"Section 4.6 \nApplications and the Perron-Frobenius Theorem \n363 \nEm \n/l� \n[8  BJ  ITIJ \nFigure 4.32 \nThe three ways to cover a 2 X 3 rectangle with 1 X 2 \ndominoes \n(a) Find d\n1\n, .•• , d5 . \n..-... (Does d0 make any sense? If so, what is it?) \n(b) Set up a second order recurrence relation for dw \n(c) Using d\n1 \nand d\n2 \nas the initial conditions, solve the \nrecurrence relation in part (b). Check your answer \nagainst the data in part (a). \n58. In Example 4.41, find eigenvectors v1 and v\n2 \ncorre-\n1 \n+\nVs \n1-\nVs \nsponding to A\n1 \n= \nand A\n2 \n= \n. With \nx\nk \n= \nI \nf\nk \n] \n, verif\ny fo\n:\nmula \n(2) in Section\n2 \n4.5. \nThat is, \nLtk\n-1 \nshow that, for some scalar c\n1\n, \n. \nx\nk \nhm k = \nC\n1\nV\n1 \nk->oo A\n1 \nsvstems of  Linear Differential Equations \nEV \nllili_ \nIn Exercises 59-64, find th e genera l solution to th e given \nsystem of differential equations. Then find the specific \nsolution that satisfies the initial conditions. (Consider \nall functions to be functions oft.) \n59. x' \n= \nx +  3y, x\n(\nO\n) \n= \n0 \ny' = 2x +  2y, y\n(\nO\n) \n= 5 \n60. x' = 2x \n-\ny, \nx\n(\nO\n) \n= 1 \ny\n' = \n-\nx + \n2y,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":57312,"to":57410}}}}],[1019,{"pageContent":"solution that satisfies the initial conditions. (Consider \nall functions to be functions oft.) \n59. x' \n= \nx +  3y, x\n(\nO\n) \n= \n0 \ny' = 2x +  2y, y\n(\nO\n) \n= 5 \n60. x' = 2x \n-\ny, \nx\n(\nO\n) \n= 1 \ny\n' = \n-\nx + \n2y, \ny\n(\nO\n) \n= 1 \n61. x; = X\n1 \n+ X\nz\n, X\n1\n(\n0\n) \n= 1 \nx\n� \n= X\n1 \n-\nX\nz\n, \nX\nz\n(\nO\n) \n= 0 \n62. \nYi \n= \nY1 \n-\nY\nz\n, \nY1\n(\n0\n) \n= 1 \nY\n� \n= \nY1 \n+ \nY\nz\n, \nY\nz\n(\nO\n) \n= 1 \n63. X1 = \ny \n-\nZ, \ny ' = X + Z, \nz' = x + y, \nx\n(\nO\n) \n= \ny\n(\nO\n) \n=  0 \nz\n(\nO\n) \n= -1 \n64. x' = \nx + 3\nz\n, \ny ' = X \n-\n2y + Z, \nz' = 3x + \nz, \nx\n(\nO\n) \n= 2 \ny\n(\nO\n) \n= 3 \nz\n(\nO\n) \n= 4 \n65. A scientist places two strains of bacteria, X and Y, in \na petri dish. Initially, there are 400 ofX and \n5\n00 of Y. \nThe two bacteria compete for food and space but do \nnot feed on each other. If x = x\n(\nt\n) \nand y = y (\nt\n) \nare \nthe numbers of the strains at time t days, the growth \nrates of the two populations are given by the system \nx' = \nl\n.2x \n-\n0.2y \ny' = \n-\n0.2x + I.Sy \n(a) Determine what happens to these two popu­\nlations by solving the system of differential \nequations. \n(b) Explore the effect of changing the initial populations \nbylettingx(\nO\n) = a and y(\nO","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":57410,"to":57569}}}}],[1020,{"pageContent":"-\n0.2x + I.Sy \n(a) Determine what happens to these two popu­\nlations by solving the system of differential \nequations. \n(b) Explore the effect of changing the initial populations \nbylettingx(\nO\n) = a and y(\nO\n) = b. Describe what hap­\npens to the populations in terms of a and b. \n66. Two species, X and Y, live in a symbiotic relationship. \nThat is, neither species can survive on its own and each \ndepends on the other for its survival. Initially, there \nare 15 ofX and 10 ofY. If x = x\n(\nt\n) \nand y = y (\nt\n) \nare the \nsizes of the populations at time t months, the growth \nrates of the two populations are given by the system \nx' = \n-\n0.Sx + 0.4y \ny' = 0.4x  -0.2y \nDetermine what happens to these two populations. \nIn Exercises 67 and 68, species X preys on species Y. The \nsizes of the populations are represented by x = x\n(\nt\n) \nand \ny = y (\nt\n)\n.  The growth rate of each population is governed \nby th e system of differential equations x\n' \n= Ax + b, where \nx = \n[;] \nand b is a constant vector. Determine what happens","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":57569,"to":57614}}}}],[1021,{"pageContent":"(\nt\n) \nand \ny = y (\nt\n)\n.  The growth rate of each population is governed \nby th e system of differential equations x\n' \n= Ax + b, where \nx = \n[;] \nand b is a constant vector. Determine what happens \nto the two populations for the given A and b and initial \nconditions x(\nO\n). (First show that there are constants a and b \nsuch that the substitutions x = u + a and y = v \n+ b convert \nthe system into an equivalent one with no constant terms.) \n67.A = \n[\n_� \nl\n] [\n-\n30] \n[\n20] \n1 \n,b = \n-\n10 \n,x\n(\nO\n)\n= \n30 \n1] \nb -\n[ \nO\nJ \nx \no \n-\n[10] \n- 1 \n' \n-\n40 \n' \n( ) \n-\n30 \n[\n-\n1 \n-1 \n68.A = \n69. Let x = x\n(\nt\n) \nbe a twice-differentiable function and \nconsider the second order differential equation \nx\n\" \n+ ax'  + bx= 0 \n(\n11\n) \n(a) Show that the change of variables y = x' and \nz = x allows Equation \n( \n11\n) \nto be written as a sys­\ntem of two linear differential equations in y and z.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":57614,"to":57694}}}}],[1022,{"pageContent":"364 \nChapter 4 \nEigenvalues and Eigenvectors \n(b) Show that the characteristic equation of the \nsystem in part (a) is ,\\\n2 \n+ a,\\ +  b = 0. \n70. Show that there is a change of variables that converts \nthe nth order differential equation \nx\n(\nn\n) \n+ \na\nn\n_\n1\nx\n(\nn\n-\nI\n) \n+ \n·   ·   · \n+ \na\n1\nx' \n+ \na\no \n=  0 \ninto a system of n linear differential equations whose \ncoefficient matrix is the companion matrix C\n( \np\n) \nof the \npolynomialp(,\\) = ,\\\n\" \n+ a\nn\n_\n1\n,\\\nn\n-\nI \n+ \n·   ·   · \n+ a 1 A + a0• \n[The notation x\n(\nk\n) \ndenotes the kth derivative of x. See \nExercises 26-32 in Section 4.3 for the definition of a \ncompanion matrix.] \nIn Exercises 71 and 72, use Exercise 69 to find the general \nsolution of the given equation. \n71. x\n\" \n-5x'  + 6x =  0 \n72. x\n\" \n+ 4x'  + 3x =  0 \nIn Exercises 73- 76, solve th e system of differential equations \nin th e given exercise using Theorem 4.41. \n73. \nExercise 59 \n74. Exercise 60 \n75. Exercise 63 \n76. Exercise 64 \nDiscrete linear Dvnamical svstems \nIn Exercises 77-84, consider the dynamical system \nx\nk\n+\n1 \n= Ax\nk\n.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":57696,"to":57779}}}}],[1023,{"pageContent":"73. \nExercise 59 \n74. Exercise 60 \n75. Exercise 63 \n76. Exercise 64 \nDiscrete linear Dvnamical svstems \nIn Exercises 77-84, consider the dynamical system \nx\nk\n+\n1 \n= Ax\nk\n. \n(a) Compute and plot \nX\no' \nx\n1\n, x\n2\n, x\n3 \nfor x\n0 \n= \n[ \n1\n1\n]\n. \n(b) Compute and plot \nX\no' \nx\n1\n, x\n2\n, x\n3 \nfor \nX\no \n= \n[\n0\n1 \n]\n. \n(c) Using eigenvalues and eigenvectors, classify th e origin as \nan attractor, repeller, saddle point, or none of these. \n( d) Sketch several typical trajectories of the system. \n77. A  = \n[2 \nl\n] \n78. A  = \n[0.5 -0.5] \n0 3 \n0 0.5 \nChapter Review \n79.A  = \n[\n_� \n-\n�\n] \n80.A  = \n[\n-\n� \n-\n�\nJ \n[ \n1.5 \n-\n�\n] \n[ \n0.1 \n0.9] \n81. A= \n82.A  = \n-1 0.5 \n0.5 \n[ \n0.2 \n0.4] \n84. A= \n[\nO \n-1.5] \n83.A  = \n-0.2 \n0.8 \n1.2 \n3.6 \nIn Exercises 85-88, the given matrix is of the form \n[a -b] \nA  = \nb a \n. In each case, A can be factored as th e \nproduct of a scaling matrix and a ro tation matrix. Find \nthe scaling factor r and the angle e of ro tation. Sketch th e \nfi rst four points of the trajectory for the dynamical system \nx\nk \n+ \n1 \n= Ax\nk \nwith x0 \n= \n[ \n�\n] \nand classify the origin as a spi­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":57779,"to":57896}}}}],[1024,{"pageContent":"the scaling factor r and the angle e of ro tation. Sketch th e \nfi rst four points of the trajectory for the dynamical system \nx\nk \n+ \n1 \n= Ax\nk \nwith x0 \n= \n[ \n�\n] \nand classify the origin as a spi­\nral attractor, spiral repeller, or orbital center. \n[ \n0 0.5] \n86.A  = \n-0.5 0 \n[ \n1\n1 \n85.A  = \n-\n�\n] \n87.A  = \n[ \n1 \nV3\n] \n-\nV3 \n1 \n88.A  = \n[\n-\nV3\n;2 -1/2 \n] \n1/2 -\nV3\n;2 \nIn Exercises 89-92, find an invertible matrix Panda ma-\n[ \na -b] \ntrix C of the form C  = \nb a \nsuch that A  = PCP -\ni\n. \nSketch th e fi rst six points of th e trajectory for th e dynamical \nsystem x\nk \n+ \n1 \n= Ax\nk \nwith \nX\no \n= \n[ \n�\n] \nand classify th e origin as \na spiral attractor, spiral repeller, or orbital center. \n[\n0.1 \n89.A  = \n0.1 \n-0.2] \n0.3 \n90.A  = \n[ \n_� \n�\n] \n91.A  = \n[\n� \n-\n�\n] \n92.A  = \n[\no  -\n1\n] \n1 \nV3 \n' \nKev Definitions and Concepts \nadjoint of a matrix, 276 \nalgebraic multiplicity of an \neigenvalue, 294 \ncharacteristic equation, 292 \ncharacteristic polynomial, 292 \ncofactor expansion, 266 \nCramer's Rule, 274-275 \ndeterminant, 263-265 \ndiagonalizable matrix, 303 \neigenvalue, 254","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":57896,"to":57998}}}}],[1025,{"pageContent":"eigenvalue, 294 \ncharacteristic equation, 292 \ncharacteristic polynomial, 292 \ncofactor expansion, 266 \nCramer's Rule, 274-275 \ndeterminant, 263-265 \ndiagonalizable matrix, 303 \neigenvalue, 254 \neigenvector, 254 \neigenspace, 256 \nGerschgorin disk, 319 \nGerschgorin's Disk Theorem, 321 \nLaplace Expansion Theorem, 266 \npower method (and its \nFundamental Theorem of Invertible \nMatrices, 296 \ngeometric multiplicity of an \neigenvalue, 294 \nvariants), 311-319 \nproperties of determinants, \n269-274 \nsimilar matrices, 301","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":57998,"to":58019}}}}],[1026,{"pageContent":"Review Questions \n1. Mark each of the following statements true or false: \n(a) For all square matrices A, det(\n-\nA) = \n-\n<let A. \n(b) If A and Bare n X n matrices, then det(AB) = \n<let (BA). \n(c) If A and Bare n X n matrices whose columns \nare the same but in different orders, then \n<let B = \n-\n<let A. \n( d) If A is invertible, then det(A \n-\ni\n) = <let A\nT\n. \n(e) If 0 is the only eigenvalue of a square matrix A, \nthen A is the zero matrix. \n(f) Two eigenvectors corresponding to the same \neigenvalue must be linearly dependent. \n(g) If an n X n matrix has n distinct eigenvalues, then \nit must be diagonalizable. \n(h) If an n X n matrix is diagonalizable, then it must \nhave n distinct eigenvalues. \n(i) Similar matrices have the same eigenvectors. \n(j) If A and B are two n X n matrices with the same \nreduced row echelon form, then A is similar to B. \n[ \n1 3 \n2. Let A =  3 5 \n7     9 \n�\n]\n. \n11 \n(a) Compute <let A by cofactor expansion along any \nrow or column. \n(h) Compute <let A by first reducing A to triangular \nform.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":58021,"to":58063}}}}],[1027,{"pageContent":"[ \n1 3 \n2. Let A =  3 5 \n7     9 \n�\n]\n. \n11 \n(a) Compute <let A by cofactor expansion along any \nrow or column. \n(h) Compute <let A by first reducing A to triangular \nform. \na b     c 3d 2e -4f f \n3. If d \ne \nf = 3, find 3a 2b  -   4c c . \ng    h \n3g 2h  -4i \n4. Let A and B be 4 X 4 matrices with <let A = 2 and \n<let B = \n-\n�. Find <let C for the indicated matrix C: \n(a) C=(AB)\n-\n1 \n(b) C=A\n2\nB(3A\nT\n) \n5. If A is a skew-symmetric n X n matrix and n is odd, \nprove that <let A = 0. \n-1 \n2 \n6. Find all values of k for which 1 \n1 \nk = 0. \n2 \n4     k\n2 \nIn Questions 7 and 8, show that x is an eigenvector of A and \nfi nd th e corresponding eigenvalue. \n7. \nx = \n[\n�\n]\n,A = \n[\n! \n�] \ns.\nF\nH\nJ\nA\n�\n[\n�: \n-\n60 \n-45] \n15 \n-\n32 \n18 \n-40 \nChapter Review 365 \n9. Let A = \n[\n-\n: \n-\n: \n-\n�\n]\n. \n0 \n0 \n-\n2 \n(a) Find the characteristic polynomial of A. \n(b) Find all of the eigenvalues of A. \n(c) Find a basis for each of the eigenspaces of A. \n(d) Determine whether A is diagonalizable. If A is \nnot diagonalizable, explain why not. If A is diago­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":58063,"to":58149}}}}],[1028,{"pageContent":"(b) Find all of the eigenvalues of A. \n(c) Find a basis for each of the eigenspaces of A. \n(d) Determine whether A is diagonalizable. If A is \nnot diagonalizable, explain why not. If A is diago­\nnalizable, find an invertible matrix P and a diago­\nnal matrix D such that P\n-\n1\nAP = D. \n10. If A is a 3 \nX \n3 diagonalizable matrix with eigenvalues \n-\n2, 3, and 4, find <let A. \n11. If A is a 2 X 2 matrix with eigenvalues A\n1 \n= \nt\n, A\n2 \n= -1, \n[\n�\n], \nV\nz \n= \n[\n-\n�\nl \nand corresponding eigenvectors v\n1 \n= \nfindA\n-\ns\n[\n�\nJ\n. \n12. If A is a diagonalizable matrix and all of its eigenvalues \nsatisfy I A I < 1, prove that A\nn \napproaches the zero ma­\ntrix as n gets large. \nIn Questions 13-15, determine, with reasons, whether A is \nsimilar to B. If A � B, give an invertible matrix P such that \nP\n-\n1 AP = B. \n13. A \n= \n[\n! \n�\n]\n,B = \n[\n� \n�\n] \n14 .A = \n[\n� \n�\n]\n,B = \n[\n� \n�\n] \n15. A \n� \n[\n� \ni :\nJ\n.\nB \n� \n[\n� \n� \n�\n] \n16. Let A = \n[ \n� \n�\n]\n. Find all values of k for which: \n(a) A has eigenvalues 3 and -1. \n(b) A has an eigenvalue with algebraic multiplicity 2.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":58149,"to":58242}}}}],[1029,{"pageContent":"[\n� \n�\n] \n14 .A = \n[\n� \n�\n]\n,B = \n[\n� \n�\n] \n15. A \n� \n[\n� \ni :\nJ\n.\nB \n� \n[\n� \n� \n�\n] \n16. Let A = \n[ \n� \n�\n]\n. Find all values of k for which: \n(a) A has eigenvalues 3 and -1. \n(b) A has an eigenvalue with algebraic multiplicity 2. \n(c) A has no real eigenvalues. \n17. If A \n3 \n= A, what are the possible eigenvalues of A? \n18. If a square matrix A has two equal rows, why must A \nhave 0 as one of its eigenvalues? \n19. If xis an eigenvector of A with eigenvalue A = 3, show \nthat xis also an eigenvector of A \n2 \n-\nSA + \n2I. What is \nthe corresponding eigenvalue? \n20. If A is similar to B with P\n-\n1\nAP = Band xis an eigen­\nvector of A, show that P\n-\n1\nx is an eigenvector of B.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":58242,"to":58298}}}}],[1030,{"pageContent":"... that sprightly  Scot of  Scots, Douglas, \nthat runs a-horseback up a hill \nperpendicular-\nFigure 5.1 \n-William Shakespeare \nHenry IV, Part I \nAct II, Scene IV \nShadows on a wall are projections \nOrthogonality \n5.0 \nIntroduction: Shadows on a  Wall \nIn this chapter, we will extend the notion of orthogonal projection that we encoun­\ntered first in Chapter 1 and then again in Chapter 3. Until now, we have discussed \nonly projection onto a single vector (or, equivalently, the one-dimensional subspace \nspanned by that vector). In this section, we will see if we can find the analogous for­\nmulas for projection onto a plane in IR\n3\n. Figure 5.1 shows what happens, for example, \nwhen parallel light rays create a shadow on a wall. A similar process occurs when a \nthree-dimensional object is displayed on a two-dimensional screen, such as a com­\nputer monitor. Later in this chapter, we will consider these ideas in full generality. \nTo begin, let's take another look at what we already know about projections. In","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":58300,"to":58321}}}}],[1031,{"pageContent":"puter monitor. Later in this chapter, we will consider these ideas in full generality. \nTo begin, let's take another look at what we already know about projections. In \nSection 3.6, we showed that, in IR\n2\n, the standard matrix of a projection onto the line \nthrough the origin with direction vector d = \n[ \n�\n:] \nis \nd\n,\nd\n2\n] \n= \n[ \nd\nl\n!(\nd\nl \n+ \nd\ni\n) \nd\ni \nd\n,\nd\n2!(\nd\nf \n+  d\ni\n) \nd\n,\nd\n2!(\nd\nl \n+  d\ni\n)\n] \nd\ni\n!(\nd\nf \n+  d\ni\n) \nHence, the projection of the vector v onto this line is just Pv. \nProblem 1 Show that P can be written in the equivalent form \n[ cos\n2 \n() \nP= \ncos() sin() \ncos() sin()\n] \nsin\n2 \n() \n� \n(What does() represent here?) \n366 \nProblem 2 Show that P can also be written in the form P = uu \nT\n, where u is a unit \nvector in the direction of d. \nProblem 3 Using Problem 2, find P and then find the projection of v = \n[ \n_\n!\n] \nonto the lines with the following unit direction vectors: \n(\na\n) \n\" \n= \n[ \n�:�J \n(\nb\n) \n\" \n= \n[\nf\nl \n(\nc\n) \n\" \n= \n[ \n-\nn \nProblem 4 Using the form P = uu\nT\n, show that (a) p\nT \n= P (i.e., Pis symmetric) \nand (b) P\n2 \n= P (i.e., Pis idempotent).","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":58321,"to":58431}}}}],[1032,{"pageContent":"Section 5.0 \nIntroduction: Shadows on a Wall \n361 \nProblem 5 Explain why, if P is a 2  X  2 projection matrix, the line onto which it \nprojects vectors is the column space of P. \nNow we will move into IR\n3 \nand consider projections onto planes through the \norigin. We will explore several approaches. \nFigure \n5\n.2 shows one way to proceed. If <!J' is a plane through the origin in IR\n3 \nwith \nnormal vector n and ifv is a vector in IR\n3\n, then p = projgp (v) is a vector in <!J' such that \nv -c n = p for some scalar c. \nn \nv \n-en \nFigure 5.2 \nProjection onto a plane \nProblem 6 Using the fact that n is orthogonal to every vector in <!/' , solve \nv -c n = p for c to find an expression for p in terms of v and n. \nProblem 1 Use the method of Problem 6 to find the projection of \nonto the planes with the following equations: \n(\na\n) \nx  + y +  z = 0 \n(\nb\n) \nx -2z = 0 \n(\nc\n) \n2x -3y +  z = 0 \nAnother approach to the problem of finding the projection of a vector onto a","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":58433,"to":58472}}}}],[1033,{"pageContent":"onto the planes with the following equations: \n(\na\n) \nx  + y +  z = 0 \n(\nb\n) \nx -2z = 0 \n(\nc\n) \n2x -3y +  z = 0 \nAnother approach to the problem of finding the projection of a vector onto a \nplane is suggested by Figure 5.3. We can decompose the projection of v onto <!J' into \nthe sum of its projections onto the direction vectors for <!/'. This works only if the \ndirection vectors are orthogonal unit vectors. Accordingly, let u1 and u\n2 \nbe direction \nvectors for <!J' with the property that \nll\nu\n1ll \n= \nll\nu\n2\nll \n= \n1 \nand \nu\n1 \n• \nu\n2 \n= \n0 \nFigure 5.3 \nI\nV \nI \nI \nI \nI \n'\"\"\" \nI U\n2 \nI \n.L.\n:....---::::\n� \np \n= \np \nI \n+ \nP\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":58472,"to":58530}}}}],[1034,{"pageContent":"368 \nChapter 5  Orthogonality \n• \nBy Problem 2\n, \nthe projections of v onto u1 and u\n2 \nare \np\n1 \n=  u\n1\nu\nf\nv and p\n2 \n=  u\n2\nu\nf\nv \nrespectively. To show that p1 + p\n2 \ngives the projection of v onto !JP, we need to show \nthat v \n-\n(\np\n1  +  p\n2\n) is orthogonal to !JP. It is enough to show that v \n-\n(\np\n1  +  p\n2\n) is \northogonal to both u1 and u\n2\n. (Why?) \nProblem 8 Show that u1 • (v -\n(\np\n1 + p\n2\n)) \n= 0 and u\n2 \n• (v -\n(\np\n1 + p\n2\n)) \n= 0. [Hint: \nUse the alternative form of the dot product, x\nT\ny \n= x · y, together with the fact that u1 \nand u\n2 \nare orthogonal unit vectors.] \nIt follows from Problem 8 and the comments preceding it that the matrix of the \nprojection onto the subspace !JP of IR\n3 \nspanned by orthogonal unit vectors u1 and u\n2 \nis \n(\n1\n) \nProblem 9 Repeat Problem 7, using the formula for P given by Equation \n(\n1\n)\n. \nUse the same v and use u1 and u\n2\n, as indicated below. (First, verify that u1 and u\n2 \nare \northogonal unit vectors in the given plane.) \n(\na\n) \nx + y  + z = 0 with u\n1 \n= \n[\n-\n�\nj�\n] \nand \nUz \n= \n[ \nl/� \n] \n1/\\/6 \n-1/\nV2 \n(\nb\n) \nx \n-\n2z = 0 with u\n1 \n= \n[\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":58532,"to":58643}}}}],[1035,{"pageContent":"2\n, as indicated below. (First, verify that u1 and u\n2 \nare \northogonal unit vectors in the given plane.) \n(\na\n) \nx + y  + z = 0 with u\n1 \n= \n[\n-\n�\nj�\n] \nand \nUz \n= \n[ \nl/� \n] \n1/\\/6 \n-1/\nV2 \n(\nb\n) \nx \n-\n2z = 0 with u\n1 \n= \n[\n2\n1:5\n] \nand u\n2 \n= \n[\n�\n] \nl/\nVs \n0 \n[ \nl/\nv'3\n] \n[ \n2/\n\\/6\n] \n(\nc\n) \n2x \n-\n3y + z = 0 with u\n1 \n= \n-\nl/v'3 and u\n2 \n= \n1/\\/6 \nl/\nv'3 \n-\n1/\n\\/6 \nProblem 10 Show that a projection matrix given by Equation ( 1\n) \nsatisfies proper­\nties (a) and (b) of Problem 4. \nProblem 11 Show that the matrix P of a projection onto a plane in IR\n3 \ncan be \nexpressed as \np = AA\nT \nfor some 3 X 2 matrix A. [Hint: Show that Equation \n(\n1\n) \nis an outer product expansion.] \nProblem 12 Show that if Pis the matrix of a projection onto a plane in IR\n3\n, then \nrank(P) = 2. \nIn this chapter, we will look at the concepts of orthogonality and orthogonal pro­\njection in greater detail. We will see that the ideas introduced in this section can be \ngeneralized and that they have many important applications . \nOrthogonalilV in IR n","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":58643,"to":58737}}}}],[1036,{"pageContent":"jection in greater detail. We will see that the ideas introduced in this section can be \ngeneralized and that they have many important applications . \nOrthogonalilV in IR n \nIn this section, we will generalize the notion of orthogonality of vectors in !R\nn \nfrom \ntwo vectors to sets of vectors. In doing so, we will see that two properties make the \nstandard basis { e\n1 , e\n2\n, ... , e\nn\n} of !R\nn \neasy to work with: First, any two distinct vectors in","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":58737,"to":58751}}}}],[1037,{"pageContent":"Example 5.1 \nz \ny \nx \nFigure 5.4 \nAn orthogonal set of vectors \nTheorem 5.1 \nSection 5.1 \nOrthogonality in lffi11 \n369 \nthe set are orthogonal. Second, each vector in the set is a unit vector. These two prop­\nerties lead us to the notion of orthogonal bases and orthonormal bases-concepts that \nwe will be able to fruitfully apply to a variety of applications. \nOrthogonal and Orthonormal Sels  of Veclors \nDefinilion A set of vectors {v\n1\n, v\n2\n, ••. , vd in !R\nn \nis called an orthogonal set if \nall pairs of distinct vectors in the set are orthogonal-that is, if \nv\ni\n·  v\nj \n= 0 whenever \ni * j for i,j = 1, 2, ... , k \nThe standard basis {e\n1\n, e\n2\n, •.• , e\nn\n} of !R\nn \nis an orthogonal set, as is any subset of it. As \nthe first example illustrates, there are many other possibilities. \nShow that {v\n1\n, v\n2\n, v\n3\n} is an orthogonal set in IR\n3 \nif \nSolution We must show that every pair of vectors from this set is orthogonal. This \nis true, since \nV\n1 \n• V\nz \n= 2 ( 0\n) \n+  1(1\n) \n+ \n( -1)(1) = 0 \nV\nz \n• V\n3 \n= 0 (1) \n+  1\n( -1\n) \n+ \n(1)(1) = 0 \nV\n1 \n• V\n3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":58753,"to":58825}}}}],[1038,{"pageContent":"3 \nif \nSolution We must show that every pair of vectors from this set is orthogonal. This \nis true, since \nV\n1 \n• V\nz \n= 2 ( 0\n) \n+  1(1\n) \n+ \n( -1)(1) = 0 \nV\nz \n• V\n3 \n= 0 (1) \n+  1\n( -1\n) \n+ \n(1)(1) = 0 \nV\n1 \n• V\n3 \n= 2 (1) \n+  1\n( -1\n) \n+ \n( -1)(1) = 0 \nGeometrically, the vectors in Example 5.1 are mutually \nFigure 5.4 shows. \nperpendicular, as \n4 \nOne of the main advantages of working with orthogonal sets of vectors is that \nthey are necessarily linearly independent, as Theorem 5.1 shows. \nIf {v\n1\n, v\n2\n, .•• , vd is an orthogonal set of nonzero vectors in !R\nn\n, then these vectors \nare linearly independent. \nProof \nIf c\n1\n, .•. , c\nk \nare scalars such that c\n1\nv\n1 \n+ ·  ·  · + c\nk\nv\nk \n= 0, then \n(c\n1\nv\n1 \n+ \n· · · \n+  c\nk\nv\nk\n) \n·v\ni \n= 0 · \nV\n; \n= 0 \nor, equivalently, \nc\n1\n(\nv\n1 \n·v\ni\n) \n+ \n· · · \n+  c\ni\n(\nv\ni \n·v\ni\n) \n+ \n· · · \n+  c\nk\n(\nv\nk \n·v\ni\n) = 0 \n(\n1\n) \nSince {v\n1\n, v\n2\n, ... , v\nk\n} is an orthogonal set, all of the dot products in Equation \n(\n1\n) \nare zero, except \nV; \n·\nv\ni\n. Thus, Equation \n( \n1\n) \nreduces to \nc\n;\n(\nv\ni \n· v\n;\n) = 0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":58825,"to":58963}}}}],[1039,{"pageContent":"310 \nChapter 5  Orthogonality \nExample 5.2 \nExample 5.3 \nNow, V; • V; *  0 because V; *  0 by hypothesis. So we must have \nC\n;  = 0. The fact that \nthis is true for all i = 1, ... , k implies that {v\n1\n, v\n2\n, ... , v\nk\n} is a linearly independent set. \nRemark Thanks to Theorem 5.1, we know that if a set of vectors is orthogonal, it \nis automatically linearly independent. For example, we can immediately deduce that \nthe three vectors in Example 5.1 are linearly independent. Contrast this approach \nwith the work needed to establish their linear independence directly! \nDefinition \nAn orthogonal basis for a subspace W of !R\nn \nis a basis of W that is \nan orthogonal set. \nThe vectors \nfrom Example 5.1 are orthogonal and, hence, linearly independent. Since any three \nlinearly independent vectors in IR\n3 \nform a basis for IR\n3\n, by the Fundamental Theorem \noflnvertible Matrices, it follows that {v\n1\n, v\n2\n, v\n3\n} is an orthogonal basis for IR\n3\n. \n4 \nRemark In Example 5.2, suppose only the orthogonal vectors v\n1 \nand v\n2 \nwere","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":58965,"to":59009}}}}],[1040,{"pageContent":"3\n, by the Fundamental Theorem \noflnvertible Matrices, it follows that {v\n1\n, v\n2\n, v\n3\n} is an orthogonal basis for IR\n3\n. \n4 \nRemark In Example 5.2, suppose only the orthogonal vectors v\n1 \nand v\n2 \nwere \ngiven and you were asked to find a third vector v\n3 \nto make {v\n1\n, v\n2\n, v\n3\n} an orthogonal \nbasis for IR\n3\n. One way to do this is to remember that in IR\n3\n, the cross product of two \nvectors v\n1 \nand v\n2 \nis orthogonal to each of them. (See Exploration: The Cross Product \nin Chapter 1.\n) \nHence we may take \nNote that the resulting vector is a multiple of the vector v\n3 \nin Example 5.2, as it must be. \nFind an orthogonal basis for the subspace W of IR\n3 \ngiven by \nSolution Section 5.3 gives a general procedure for problems of this sort. For now, \nwe will find the orthogonal basis by brute force. The subspace Wis a plane through \nthe origin in IR\n3\n. From the equation of the plane, we have x =  y -2z, so W consists \nof vectors of the form","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":59009,"to":59059}}}}],[1041,{"pageContent":"Section 5.1 \nOrthogonality in IR\" \n311 \nIt follow' that u � [ �] and v \n� [\n-\n�] \"e a b\";' foe W, but they Me nol mthogo­\nnal. It suffices to find another nonzero vector in W that is orthogonal to either one of \nthese. \nSuppo'e w � [ �] ;, a vedm ;n W that ;, mthogonal to u. Then x \n-\ny + 2z \n� \n0, \nsince w is in the plane W. Since u · \nw = 0, we also have x + y \n= 0. Solving the linear \nsystem \nx \n-\ny  + \n2z =  0 \nx + y \n=  0 \n� \nwe find that x = -z and y = z. (Check this.) Thus, any nonzero vector w of the form \nTheorem 5.2 \nwill do. To b\"pedfie, we \nw\nold toke w \n� [\n-\n:J-It ;\"\"Y to ehe<k that {u, w} ;, an \northogonal set in Wand, hence, an orthogonal basis for W, since dim W = 2. \nAnother advantage of working with an orthogonal basis is that the coordinates of \na vector with respect to such a basis are easy to compute. Indeed, there is a formula \nfor these coordinates, as the following theorem establishes. \nLet {v\n1\n, v\n2\n, ... , v\nk\n} be an orthogonal basis for a subspace W of !R\nn \nand let w be any","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":59061,"to":59106}}}}],[1042,{"pageContent":"for these coordinates, as the following theorem establishes. \nLet {v\n1\n, v\n2\n, ... , v\nk\n} be an orthogonal basis for a subspace W of !R\nn \nand let w be any \nvector in W. Then the unique scalars c\n1\n, ... , c\nk \nsuch that \nare given by \nW'V\n; \nC; \n= --\nfor i  = 1, ... , k \nV\n;\n' V\n; \nProof Since {v\n1\n, v\n2\n, ... , v\nk\n} is a basis for W, we know that there are unique scalars \nc\n1\n,  ... ,  c\nk \nsuch that w = c\n1\nv\n1 \n+ ·  ·  · +  c\nk\nv\nk \n(from Theorem 3.29\n)\n. To establish the \nformula for \nC\n;, we take the dot product of this linear combination with \nV; to obtain \nw \n· \nV\n; \n= (c\n1\nv\n1 \n+ \n· · · \n+ \nc\nk\nv\nk\n) \n• V\n; \n=  c\n1\n(\nv\n1 \n· \nv\n;\n) \n+ \n· · · \n+  c\n;\n(\nv\n; \n• \nv\n;\n) \n+ \n· · · \n+  c\nk\n(\nv\nk \n• v\n;\n) \n= \nC;\n(\nV\n;\n' v\n;\n) \nsince v\nj \n· V;  =  0 for j *  i. Since \nV; \n* \n0, V; • V\n; \n* \n0. Dividing by \nV; • V;, we obtain the \ndesired result.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":59106,"to":59223}}}}],[1043,{"pageContent":"312 \nChapter 5  Orthogonality \nExample 5.4 \nFind the coordinates of w = \n[ \n�\n] \nwith respectto the orthogonal basis B = { v\n1\n, v\n2\n, v\n3\n} \nof Examples 5.1 and 5.2. \n3 \nSolulion \nUsing Theorem 5.2, we compute \nThus, \nw\n· \nV\n1 \n2  +  2 -3 1 \nc    =\n--\n= =\n-\n] \nV\n1 \n• V\n1 \n4 +  1  +  1 \n6 \nw\n· \nV\n2 \n0  +  2  + \n3 5 \nC\n2 \n= \n--\n= \n= \n-\nV\n2\n\" V\n2 \n0  +  1   +   1 2 \nw\n· \nV\n3 \n1 -2  +  3 \n2 \nC\n3 \n= \n--\n= \nV\n3 \n• V\n3 \n1  +  1  +  1 \n3 \n� \n(Check this.) With the notation introduced in Section 3.5, we can also write the above \nequation as \nExample 5.5 \nCompare the procedure in Example 5.4 with the work required to find these \ncoordinates directly and you should start to appreciate the value of orthogonal bases. \nAs noted at the beginning of this section, the other property of the standard basis \nin !R\nn \nis that each standard basis vector is a unit vector. Combining this property with \northogonality, we have the following definition. \nDefinition \nA set of vectors in !R\nn \nis an orthonormal set if it is an orthogonal","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":59225,"to":59309}}}}],[1044,{"pageContent":"orthogonality, we have the following definition. \nDefinition \nA set of vectors in !R\nn \nis an orthonormal set if it is an orthogonal \nset of unit vectors. An orthonormal basis for a subspace W of !R\nn \nis a basis of W \nthat is an orthonormal set. \nRemark If S = { \nq1\n, ... , \nq\nd is an orthonormal set of vectors, then \nq\n; · \nq\nj \n= 0 for \ni \n-=fa \nj and \nll\nq;\nll \n= 1. The fact that each \nq\n; is a unit vector is equivalent to \nq\n; · \nq\n; = 1. \nIt follows that we can summarize the statement that S is orthonormal as \n{\no \nif i  * j \nq; \n. \nq\nj \n= \n1 if i = j \nShow that S = {\nq1\n, \nq\n2\n} is an orthonormal set in IR\n3 \nif \n[ l/v'3\n] \nq\ni\n= -l/v'3 \n[ l/v'6\n] \nand \nq\n2 \n= 2/v'6 \nl/v'3 \nl/v'6","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":59309,"to":59371}}}}],[1045,{"pageContent":"Example 5.6 \nTheorem 5.3 \nSolution We check that \nSection 5.1 \nOrthogonality in lffi\" \n313 \nq1 \n• \nq\n2 \n= 1\n/\\/18 - 2\n/\n\\/18 \n+ 1/\\/18 = 0 \nq\ni\n. \nq\ni \n= 1\n/\n3 + 1\n/\n3 + 1/3 = 1 \nq\n2\n·q\n2 \n= 1\n/\n6 + 4\n/\n6 + 1\n/\n6 = 1 \nIf we have an orthogonal set, we can easily obtain an orthonormal set from it: We \nsimply normalize each vector. \nConstruct an orthonormal basis for IR\n3 \nfrom the vectors in Example 5.1. \nSolution Since we already know that v1, v\n2\n, and v\n3 \nare an orthogonal basis, we nor­\nmalize them to get \n[ l l  [ l/V3] \nq\n3\n=\n11\n;\n1\nr\n3 \n= \n� \n-1 = -1\n10 \n3 \n1 \nl/\nV3 \nThen {\nq\n1, \nq\n2\n, \nq\n3\n} is an orthonormal basis for IR\n3\n. \nSince any orthonormal set of vectors is, in particular, orthogonal, it is linearly in -\ndependent, by Theorem 5.1. If we have an orthonormal basis, Theorem 5.2 becomes \neven simpler. \nLet {\nq\n1, \nq\n2\n, ... , \nq\nk} be an orthonormal basis for a subspace W of IR\n\" \nand let \nw \nbe \nany vector in W. Then \nw \n=  (\nw ·q1\n)\nq1 \n+ \n(\nw ·q\nz\n)\nq\nz \n+ \n· · · \n+ \n(\nw ·qk\n)\nqk \nand this representation is unique. \nProof \nApply Theorem \n5\n.2 and use the fact that \nq\n; · \nq","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":59373,"to":59490}}}}],[1046,{"pageContent":"\" \nand let \nw \nbe \nany vector in W. Then \nw \n=  (\nw ·q1\n)\nq1 \n+ \n(\nw ·q\nz\n)\nq\nz \n+ \n· · · \n+ \n(\nw ·qk\n)\nqk \nand this representation is unique. \nProof \nApply Theorem \n5\n.2 and use the fact that \nq\n; · \nq\n; = 1 for i = 1, ... , k. \nOrthogonal Malrices \nMatrices whose columns form an orthonormal set arise frequently in applications, as \nyou will see in Section 5.5. Such matrices have several attractive properties, which we \nnow examine.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":59490,"to":59526}}}}],[1047,{"pageContent":"314 \nChapter 5  Orthogonality \nTheorem 5.4 \nOrthogonal matrix is an unfortu­\nnate bit of terminology. \"Ortho­\nnormal matrix\" would clearly be a \nbetter term, but it is not standard. \nMoreover, there is no term for a \nnonsquare matrix with orthonor­\nmal columns. \nTheorem 5.5 \nExample 5.1 \nThe columns of an m X n matrix Q form an orthonormal set if and only if \nQ\nT\nQ =I\nn\n\" \nProof \nWe need to show that \nT \n{\nQ \n(\nQ  Q\n) ij \n= \n1 \nif i * j \nif i = j \nLet q; denote the ith column of Q (and, hence, the ith row of Q\nT\n)\n. Since the (i,j) \nentry of Q\nT \nQ is the dot product of the ith row of Q\nT \nand the )th column of Q, it \nfollows that \n( Q\nT\nQ\n)\n;j \n= q; ·q\nj \nby the definition of matrix multiplication. \nNow the columns Q form an orthonormal set if and only if \n{\no \nif i * J \nq; \n. \nq\ni \n= \n1 if i = j \nwhich, by Equation \n(\n2\n)\n, holds if and only if \nT \n_ \n{\nQ \n(\nQ  Q\n) ij \n-\n1 \nThis completes the proof. \nif i * j \nif i = j \nIf the matrix Q in Theorem 5.4 is a squar\ne \nmatrix, it has a special name. \nDefinition","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":59528,"to":59606}}}}],[1048,{"pageContent":"which, by Equation \n(\n2\n)\n, holds if and only if \nT \n_ \n{\nQ \n(\nQ  Q\n) ij \n-\n1 \nThis completes the proof. \nif i * j \nif i = j \nIf the matrix Q in Theorem 5.4 is a squar\ne \nmatrix, it has a special name. \nDefinition \nAn n x n matrix Q whose columns form an orthonormal set is \ncalled an orthogonal matrix. \n(\n2\n) \nThe most important fact about orthogonal matrices is given by the next theorem. \nA square matrix Q is orthogonal if and only if Q\n-\n1 \n=  Q\nT\n. \nProof By Theorem 5.4, Q is orthogonal if and only if Q\nT \nQ  =  I. This is true if and \nonly if Q is invertible and Q-\n1 \n=  Q\nT\n, by Theorem 3.13. \nShow that the following matrices are orthogonal and find their inverses: \n[\ncos {J \nand B = \nsin {J \n-\nsin {J\n] \ncos {J \nSolulion The columns of A are just the standard basis vectors for IR\n3\n, which are \nclearly orthonormal. Hence, A is orthogonal and \nA\n-\n' \n�\nA'\n� \n[� \n� \n�\n]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":59606,"to":59669}}}}],[1049,{"pageContent":"The word isometry literally means \n\"length preserving;' since it is \nderived from the Greek roots isos \n(\"equal\") and metron (\"measure\"). \nTheorem 5.6 \nSection 5.1 \nOrthogonality in lffi\" \n315 \nFor B, we check directly that \nBrB \n= \n[ \ncos\n() \nsin ()\n] \n[\ncos () \n-\nsin () \ncos ()  sin () \n-\nsin ()\n] \ncos () \n[ \ncos\n2 \n()  + sin\n2 \n() \n-\nsin () cos ()  + cos () sin () \n-\ncos () sin ()  + sin () cos ()\n] \nsin\n2 \n()  + cos\n2 \n() \nTherefore, B is orthogonal, by Theorem 5.5, and \n_\n1 \nT \n[ \nCOS() \nB  = B = \n-\nsin () \nsin ()\n] \ncos () \n[\n�  �\n] \n=I \nRemark Matrix A in Example 5. 7 is an example of a permutation matrix, a matrix \nobtained by permuting the columns of an identity matrix. In general, any n X n per­\nmutation matrix is orthogonal (see Exercise 25\n)\n. Matrix B is the matrix of a rotation \nthrough the angle() in IR\n2\n. Any rotation has the property that it is a length-preserving \ntransformation (known as an isometry in geometry). The next theorem shows that \nevery orthogonal matrix transformation is an isometry. Orthogonal matrices also","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":59671,"to":59737}}}}],[1050,{"pageContent":"transformation (known as an isometry in geometry). The next theorem shows that \nevery orthogonal matrix transformation is an isometry. Orthogonal matrices also \npreserve dot products. In fact, orthogonal matrices are characterized by either one of \nthese properties. \nLet Q be an n X n matrix. The following statements are equivalent: \na. Q is orthogonal. \nb. \nll\nQx\nll \n= \nll\nx\nll \nfor \nevery\nx\nin IR\n\"\n. \nc. Qx\n·\nQy =x\n·\ny for everyxand yin !R\n\"\n. \nProof We will prove that (a) ::::} (c) ::::} (b) ::::} (a). To do so, we will need to make use \nof the fact that if x and y are (column) vectors in IR\n\"\n, then x \n· \ny = x\nr\ny. \n(a) ::::} (c) Assume that Q is orthogonal. Then Q\nr\nQ =I, and we have \nQx · Qy = \n(\nQx\nf\nQy = xrQrQy = xrly \n= xry = x · y \n(c) ::::} (b) Assume that Qx \n· \nQy = x \n· \ny for every x and yin IR\n\"\n. Then, taking y = x, \nwe have Qx\n· \nQx = x\n·\nx, so \nll\nO\nx\nll \n= \nv'\nQx· \nQx = \nVX:-X \n= \nll\nx\nll\n. \n(b) ::::} (a) Assume that property (b) holds and let q; denote the ith column of Q.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":59737,"to":59807}}}}],[1051,{"pageContent":"Qy = x \n· \ny for every x and yin IR\n\"\n. Then, taking y = x, \nwe have Qx\n· \nQx = x\n·\nx, so \nll\nO\nx\nll \n= \nv'\nQx· \nQx = \nVX:-X \n= \nll\nx\nll\n. \n(b) ::::} (a) Assume that property (b) holds and let q; denote the ith column of Q. \nUsing Exercise 63 in Section 1.2 and property (b ), we have \nx·y = \ni\n(\nll\nx + \nr\nll\n2 \n-\nll\nx \n-\nr\nll\n2) \n= \ni\n(\nll\nQ\n(\nx \n+ \ny\n)\nll\n2 \n-\nll\nQ\n(\nx \n-\ny\n)\nll\n2\n) \n= \ni\n(\nll\nQx \n+ \nQ\nr\nll\n2 \n-\nll\nQx \n-\nQ\nr\nll\n2\n) \n= Qx·Qy \nfor all x and y in IR\n\"\n. [This shows that (b) ::::} (  c).] \nNow if e; is the ith standard basis vector, then q; = Qe;. Consequently, \n{\no \nif i  * j \nq; . % = Qe;\n. \nQe\nj \n= e; . ej = \n1 if i = j \nThus, the columns of Q form an orthonormal set, so Q is an orthogonal matrix.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":59807,"to":59904}}}}],[1052,{"pageContent":"316 \nChapter 5  Orthogonality \nTheorem 5.1 \nTheorem 5.8 \n.. \nI \nExercises 5.1 \nLooking at the orthogonal matrices A and Bin Example \n5\n.\n7\n, you may notice that \nnot only do their columns form orthonormal sets-so do their rows. In fact, every \northogonal matrix has this property, as the next theorem shows. \nIf Q is an orthogonal matrix, then its rows form an orthonormal set. \nProof \nFrom Theorem \n5\n.\n5\n, we know that Q\n-\n1 \n= Q\nT\n. Therefore, \n(\nQ\nT\n)\n-\n1 \n= \nco-\n1\n)-\n1 \n= Q = \n(\nQ\nT\n)\nY \nso Q\nT \nis an orthogonal matrix. Thus, the columns of Q\nT\n-which are just the rows of \nQ-form an orthonormal set. \nThe final theorem in this section lists some other properties of orthogonal \nmatrices. \nLet Q be an orthogonal matrix. \na. Q\n-\n1 \nis orthogonal. \nb. <let Q = ::t:: l \nc. If A is an eigenvalue of Q, then IAI = 1. \nd. If Q\n1 \nand Q\n2 \nare orthogonal n X n matrices, then so is Q\n1 \nQ\n2\n. \nProof We will prove property ( c) and leave the proofs of the remaining properties \nas exercises.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":59906,"to":59974}}}}],[1053,{"pageContent":"d. If Q\n1 \nand Q\n2 \nare orthogonal n X n matrices, then so is Q\n1 \nQ\n2\n. \nProof We will prove property ( c) and leave the proofs of the remaining properties \nas exercises. \n(c) Let A be an eigenvalue of Q with corresponding eigenvector v. Then Qv = AV, \nand, using Theorem \n5\n.6(b), we have \nJJ\nv\nJJ \n= \nJJ\nQv\nJJ \n= \nJJ\nAv\nJJ \n= \nI\nA\nI JJ\nv\nJJ \nSince \nJJ\nv\nJJ \n* 0\n, \nthis implies that IAI = 1. \n[\no \n-\n0\n1\n] \nRemark Property (c) holds even for complex eigenvalues. The matrix \n1 \nis orthogonal with eigenvalues i and \n-\ni, both of which have absolute value \n1 . \nIn Exercises 1-6, determine which sets of vectors are \northogonal. \nI. nHn \n[\n-\n�l \n2\n. \nuirn m \n,_ \n[ \nJf \n:H\n-\n!\nl \n·\n· \n[\n:H\n-\nH \nUl","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":59974,"to":60047}}}}],[1054,{"pageContent":"In Exercises 7-10, show that the given vectors form an ortho­\ngonal basis for IR\n2 \nor IR\n3\n•  Then use Theorem 5.2 to express \nwas a linear combination of these basis vectors. Give the \ncoordinate vector [w]8 of w with respect to the basis \nB = \n{v\n1\n, vJ \nof!R\n2 \nor B  = v\n1\n, v\n2\n, v\n3 \nof!R\n3\n• \n7.v\n1 \n= \n[ _\n�\n]\n,v\n2 \n= \n[\n�\nJ\nw  = \n[ _\n�\n] \n8\n.v\n1 \n= \n[\n�\n]\n,v\n2 \n= \n[\n-\n�J\nw \n= \n[\n�\n] \n9. v\n, \n� \n[ \n_ \n+' \n� \n[ \nl; \n� \n[ \n-\n:} \nw \n� \n[\n: \nl \n10. \nv\n, \n� \n[ \nl, \n� \n[ \n-J, \n� \n[ \n_n \nw  � \nm \nIn Exercises 11-15, determine whether the given orthogo­\nnal set of vectors is orthonormal. If it is not, normalize the \nvectors to form an orthonormal set. \n11. \n[\ni\nl \n[\n-\nn \n13.\n[\n!J\n[\n-\n!J\n[\nJ \n12\n' [\ni\nl \n[ \n-\ni\nJ \nl \n•\n. \n[\n-\niH!\nl \nU\nl \n[ \n1/\n2\n] \n[ \n0 \nl \n[ \nv'3/\n2\n] \n[ \n0 \nl \n1\n/2 \nv6/\n3 \n-v'3\n/\n6 \n0 \n15\n' \n-1/\n2 \n' \n1/v6 \n' \nv'3/6 \n' \n1\n/\\/2 \n1\n/2 \n-1\n/v6 -\nv'3\n/\n6    1/\\/2 \nIn Exercises 16-21, determine whether th e given matrix is \northogonal. If it is, find its inverse. \n[\no \n-\n0\n1\n] \n[ \n1/\\/2 1\n/\\/2\n] \n16. \n1 \n17. \n-1/\\/2 \n1\n/\\/2 \n18.\nu \n-\n! \n:i \n[\ncos e sin e \n19. \ncos\n2 \ne \nsin e \n20. \n[\n-\nl \nI \n-\n2 \nI \n2 \nI \n2 \n! \n2 \n-\ncos e \nsin e \n0 \n! \n2 \nI \n2 \nI \n2 \nI \n-\n2 \n-\nl\n] \n-\nsin\n2\ne \nl \n-\ncos e sin e","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":60049,"to":60278}}}}],[1055,{"pageContent":"[\no \n-\n0\n1\n] \n[ \n1/\\/2 1\n/\\/2\n] \n16. \n1 \n17. \n-1/\\/2 \n1\n/\\/2 \n18.\nu \n-\n! \n:i \n[\ncos e sin e \n19. \ncos\n2 \ne \nsin e \n20. \n[\n-\nl \nI \n-\n2 \nI \n2 \nI \n2 \n! \n2 \n-\ncos e \nsin e \n0 \n! \n2 \nI \n2 \nI \n2 \nI \n-\n2 \n-\nl\n] \n-\nsin\n2\ne \nl \n-\ncos e sin e \ncos e \nSection 5.1 \nOrthogonality in IR\" \n311 \n0 \n2\n/\n3 \n-2\n/\n3 \n1\n/\n3 \n0 \n1/\\/2 \n1\n/\\/2 \n0 \n22. Prove Theorem \n5\n.S(a). \n23. Prove Theorem \n5\n.S(b). \n24. Prove Theorem \n5\n.S(d  ). \n1\n/v6\n] \n1\n/v6 \n-1\n/v6 \n1\n/\\/2 \n25. Prove that every permutation matrix is orthogonal. \n26. If Q is an orthogonal matrix, prove that any matrix \nobtained by rearranging the rows of Q is also \northogonal. \n27. Let Q be an orthogonal 2 X 2 matrix and let x and y \nbe vectors in IR\n2\n. If e is the angle between x and y, \nprove that the angle between Qx and Qy is also e. \n(This proves that the linear transformations defined by \northogonal matrices are angle-preserving in IR\n2\n, a fact \nthat is true in general.) \n28. (a) Prove that an orthogonal 2 X 2 matrix must have \nthe form \n[\na \n-\nb\n] \nor \n[\na b\n] \nb \na b -a \nwhere \n[\n:\n] \nis a unit vector. \n(b) Using part (a), show that every orthogonal","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":60278,"to":60411}}}}],[1056,{"pageContent":", a fact \nthat is true in general.) \n28. (a) Prove that an orthogonal 2 X 2 matrix must have \nthe form \n[\na \n-\nb\n] \nor \n[\na b\n] \nb \na b -a \nwhere \n[\n:\n] \nis a unit vector. \n(b) Using part (a), show that every orthogonal \n2 X 2 matrix is of the form \n[\ncos e \nsin e \n-\nsin e\n] \ncos e \nwhere 0 :::::: e < 21T. \n[\ncos e \nor \nsin e \nsin e\n] \n-\ncos e \n(c) Show that every orthogonal 2 X 2 matrix corre­\nsponds to either a rotation or a reflection in IR\n2\n. \n(d) Show that an orthogonal 2 X 2 matrix Q cor­\nresponds to a rotation in IR\n2 \nif <let Q = 1 and a \nreflection in IR\n2 \nif <let Q = - 1. \nIn Exercises 29-32, use Exercise 28 to determine whether \nth e given orthogonal matrix represents a rotation or a \nreflection. If it is a rotation, give th e angle of rotation; if it is \na reflection, give the line of reflection. \n[\n1\n/\\/2 -1\n/\\/2\n] \n[ \n-1/\n2 \n29. \n1\n/\\/2 \n1/\\/2 \n30. \n-V3/\n2 \n[\n-1/\n2 \n31. \nV3/\n2 \nV3/\n2\n] \n1\n/\n2 \nV3/\n2\n] \n-1/2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":60411,"to":60494}}}}],[1057,{"pageContent":"318 \nChapter 5  Orthogonality \n33. Let A and B be n X n orthogonal matrices. \n(a) Prove that A(A r + Br\n)\nB = A \n+ B. \nwith a prescribed first vector x, a construction that is \nfrequently useful in applications.) \n(b) Use part (a) to prove that, if <let A  + <let B = 0, \nthen A  + B is not invertible. \n35. Prove that if an upper triangular matrix is orthogonal, \nthen it must be a diagonal matrix. \n34. Let x be a unit vector in !R\nn\n. Partition x as \n36. Prove that if n > m, then there is no m X n matrix A \nsuch that \nII \nAx \nII \n= \nll\nx\nll \nfor all x in !R\nn\n. \nLet \n37. Let B = {v\n1\n, ••• , v\nn\n} be an orthonormal basis for !R\nn\n. \n(a) Prove that, for any x and yin !R\nn\n, \nx \n· \ny \n= \n(\nx \n· v\n1\n)(\ny · v\n1\n) \n+ \n(\nx \n· v\n2)(\ny · v\n2) \n+ \n·   ·   · \nQ = \n[-�\n1\n--�--------------I�------------\n] \ny \ni \nI \n-\nC \n-\nx\nJyy\nr \n+ \n(\nx · v\nn\n)(\ny · v\nn\n) \n(This identity is called Parseval's Identity.) \n(b) What does Parseval's Identity imply about the \nrelationship between the dot products x · \ny \nand \n[x\nla\n· \n[\ny\n]\na? \nProve that Q is orthogonal. (This procedure gives a","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":60496,"to":60590}}}}],[1058,{"pageContent":"(b) What does Parseval's Identity imply about the \nrelationship between the dot products x · \ny \nand \n[x\nla\n· \n[\ny\n]\na? \nProve that Q is orthogonal. (This procedure gives a \nquick method for finding an orthonormal basis for !R\nn \nw\n_j_ \nis pronounced \"w perp:' \nw \nw \nI \ne \nFigure 5.5 \ne = w\n_j_ \nand w = e\n_j_ \nExample 5.8 \nOrthogonal Complements and \nOrthogonal Proieclions \nIn this section, we generalize two concepts that we encountered in Chapter 1. The no­\ntion of a normal vector to a plane will be extended to orthogonal complements, and \nthe projection of one vector onto another will give rise to the concept of orthogonal \nprojection onto a subspace. \nOrthogonal Complements \nA normal vector n to a plane is orthogonal to every vector in that plane. If the plane \npasses through the origin, then it is a subspace W of IR\n3\n, as is span(n). Hence, we have \ntwo subspaces of IR\n3 \nwith the property that every vector of one is orthogonal to every \nvector of the other. This is the idea behind the following definition.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":60590,"to":60631}}}}],[1059,{"pageContent":"3\n, as is span(n). Hence, we have \ntwo subspaces of IR\n3 \nwith the property that every vector of one is orthogonal to every \nvector of the other. This is the idea behind the following definition. \nDefiniliOD Let Wbe a subspace of !R\nn\n. We say that a vector v in !R\nn \nis orthogo­\nnal to W if v is orthogonal to every vector in W. The set of all vectors that are \northogonal to Wis called the orthogonal complement of W, denoted W_j_. That is, \nW_j_ = {v in !R\nn\n:v·w = 0 for all win W} \nIf Wis a plane through the origin in IR\n3 \nand e is the line through the origin perpen­\ndicular to W (i.e., parallel to the normal vector to W), then every vector v on e is \northogonal to every vector win W; hence, e = W_L. Moreover, W consists precisely \nof those vectors w that are orthogonal to every v on €; hence, we also have W = e_L. \nFigure 5.5 illustrates this situation.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":60631,"to":60653}}}}],[1060,{"pageContent":"Theorem 5.9 \nTheorem 5.10 \nSection 5.2 \nOrthogonal Complements and Orthogonal Projections \n319 \nIn Example 5.8, the orthogonal complement of a subspace turned out to be an­\nother subspace. Also, the complement of the complement of a subspace was the origi­\nnal subspace. These properties are true in general and are proved as properties (a) and \n(b) of Theorem 5.9. Properties (c) and (d) will also be useful. (Recall that the intersec­\ntion A n B of sets A and B consists of their common elements. See Appendix A.) \nLet W be a subspace of !R\nn\n. \na. w.\nL \nis a subspace of !R\nn\n. \nb. (W.\nL\n).\nL \n= W \nc. w n  w.\nL \n= {o} \nd. If W = span (W\n1\n, ... , W\nk\n), then \nV\nis in W.\nL \nif and only if \nV \n· W; = 0 for all \ni = 1, ... , k. \nProof (a) Since 0 · w = 0 for all win W, 0 is in W.\nL\n. Let u and v be in W.\nL \nand let \nc be a scalar. Then \nTherefore, \nso u \n+ v is in W .\nL\n. \nWe also have \nu · w = v · w = 0 \nfor all w in W \n(\nu \n+ v\n) \n·w = u · w \n+ v·w = 0 \n+  0 = 0 \n(\ncu\n) \n·w = c\n(\nu ·w\n) \n= c (O) = 0 \nfrom which we see that cu is in W \n.L","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":60655,"to":60723}}}}],[1061,{"pageContent":"Therefore, \nso u \n+ v is in W .\nL\n. \nWe also have \nu · w = v · w = 0 \nfor all w in W \n(\nu \n+ v\n) \n·w = u · w \n+ v·w = 0 \n+  0 = 0 \n(\ncu\n) \n·w = c\n(\nu ·w\n) \n= c (O) = 0 \nfrom which we see that cu is in W \n.L\n. It follows that W .\nL \nis a subspace of !R\nn\n. \n(b) We will prove this property as Corollary 5.12. \n(c) You are asked to prove this property in Exercise 23. \n(d) You are asked to prove this property in Exercise 24. \nWe can now express some fundamental relationships involving the subspaces \nassociated with an m X n matrix. \nLet A be an m X n matrix. Then the orthogonal complement of the row space of \nA is the null space of A, and the orthogonal complement of the column space of A \nis the null space of A\nT\n: \n(\nrow\n(\nA\n))\n.L = null\n(\nA\n) \nand \n(\ncol\n(\nA\n))\n.L = null\n(\nA\nT\n) \nProof If x is a vector in !R\nn\n, then x is in (row(A)) .L if and only if x is orthogonal to \nevery row of A. But this is true if and only if Ax = 0, which is equivalent to x being \nin null(A), so we have established the first identity. To prove the second identity, we","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":60723,"to":60787}}}}],[1062,{"pageContent":"every row of A. But this is true if and only if Ax = 0, which is equivalent to x being \nin null(A), so we have established the first identity. To prove the second identity, we \nsimply replace A by A\nT \nand use the fact that row(A\nT\n) = col(A). \nThus, an m X n matrix has four subspaces: row(A), null(A), col(A), and null(A\nT\n). \nThe first two are orthogonal complements in !R\nn\n, and the last two are orthogonal","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":60787,"to":60799}}}}],[1063,{"pageContent":"380 \nChapter 5  Orthogonality \nExample 5.9 \nnull(A\n) \nnull\n(\nA\nT\n) \n./ �. \n/ \nrow\n(\nA\n) \ncol(A\n) \n[ffim \nFigure 5.6 \nThe four fundamental subspaces \ncomplements in �m. The m X n matrix A defines a linear transformation from �\nn \ninto �m whose range is col(A). Moreover, this transformation sends null(A) to 0 in \n�m. Figure 5.6 illustrates these ideas schematically. These four subspaces are called \nthe fu ndamental subspaces of the m X n matrix A. \nFind bases for the four fundamental subspaces of \n[\n-\n; \n3 \n-\nI\n] \n-1 \n0 \nA= \n2 \n1 \n-2 \n1 6 1 \nand verify Theorem 5.10. \nSolulion In Examples 3.45, 3.47, and 3.48, we computed bases for the row space, \ncolumn space, and null space of A. We found that row(A) \n= \nspan (u\n1\n, u\n2\n, u3), where \nll\n1 \n= [1 0 \n0 -1 ], u\n2 \n= [ O \n2     0 3 ] , \nU\n3 \n= \n[ \n0     0     0 \n4 ] \nAlso, null(A) \n= \nspan(x\n1\n, x\n2\n), where \n-1 \n1 \n-2 \n-3 \nX\n1 \n= \n1 \n, \nX\nz \n= \n0 \n0 \n-4 \n0 \nTo show that (row(A))1- = null(A), it is enough to show that every \nU\n; is orthogonal to \n� \neach x\nj\n, which is an easy exercise. (Why is this sufficient?)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":60801,"to":60893}}}}],[1064,{"pageContent":"Example 5.10 \nSection 5.2 \nOrthogonal Complements and Orthogonal Projections \n381 \nThe column space of A is col(A) = span (a\n1\n, a\n2\n, a3), where \nWe still need to compute the null space of A\nr\n. Row reduction produces \n2 \n-\n3 \n4 \n0 \n1 \n0     0 \n1 \n0 \n1 \n-1 \n2 \n1 \n0 \n0 0 \n6 \n0 \n[Ari O] \n3 \n0 \n6 \n0 \n� \n0 0 \n3 \n0 \n1 1 \n-2 \n1 \n0 \n0     0     0     0 \n0 \n6 \n-1 \n3 \n0 \n0     0     0 0 \n0 \nSo, if y is in the null space of A\nr\n, then y\n1 \n-\ny4, y\n2 \n= \n-\n6y4, and y3 = \n-\n3y4. \nIt follows that \nnull\n(\nA\n'\n) \n� \n{ \n[ \n=:�]} \n� \nsp\n�\n( \n[ \n=m \nand it is easy to check that this vector is orthogonal to a\n1\n, a\n2\n, and a3. \nThe method of Example 5.9 is easily adapted to other situations. \nLet W be the subspace of IR\n5 \nspanned by \n1 \n-1 \n0 \n-\n3 \n1 \n-\n1 \nW\n1 \n= \n5 \n, \nW\nz\n= \n2 , \nW\n3\n= \n4 \n0 \n-2 \n-1 \n5 \n3 \n5 \nFind a basis for W\n_j_\n. \nSolution \nThe subspace W spanned by w\n1\n, w\n2\n, and w3 is the same as the column \nspace of \n1 \n-1 \n0 \n-\n3 \n1 \n-1 \nA= \n5 \n2 4 \n0 \n-2 \n-1 \n5 \n3 \n5","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":60895,"to":61034}}}}],[1065,{"pageContent":"382 \nChapter 5  Orthogonality \nv \nu \nFigure 5.1 \nv \n= proj0(\nv\n) + perp0(v\n) \nTherefore, by Theorem 5.10, Wl_ = (col(A))J_ = null(A\nT\n), and we may proceed as in \nthe previous example. We compute \nI\nA\n'\nl\no\nJ \n� \n[\n-\n: \n-3 5 \n2 \n-1 4 \n0     5   O\nJ \n[ \n1 0     0 3     4   O\nJ \n-2 3   0 \n� \n0 1 0 1 3   0 \n-1 5   0 \n0     0 1 0     2   0 \nHence, \ny is in W1-if and only if y1 \n= -3y\n4 \n-4y\n5\n, y\n2 \n= -y\n4 \n-3y\n5\n, and y\n3 \n= -2y\n5\n. It \nfollows that \n-3y\n4 \n-4y\n5 \n-3 \n-4 \n-y\n4 \n-3y\n5 \n-1 \n-3 \nwl_ = \n-2y\n5 \n=span \n0 \n-2 \nY\n4 \n1 \n0 \nY\ns \n0 \nand these two vectors form a basis for WJ_. \nOrthogonal Proieclions \nRecall that, in IR\n2\n, the projection of a vector v onto a nonzero vector u is given by \n(u•v) \nproj0\n(\nv\n) \n= --u \nu · u \nFurthermore, the vector perp0(v) = v \n-\nproj0(v) is orthogonal to proj0(v), and we \ncan decompose v as \nas shown in Figure 5.7. \nIf we let W = span(u), then w = proju(v) is in Wand w1-= perpu(v) is in W1-. We \ntherefore have a way of \"decomposing\" v into the sum of two vectors, one from Wand \nthe","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":61036,"to":61135}}}}],[1066,{"pageContent":"as shown in Figure 5.7. \nIf we let W = span(u), then w = proju(v) is in Wand w1-= perpu(v) is in W1-. We \ntherefore have a way of \"decomposing\" v into the sum of two vectors, one from Wand \nthe \nother orthogonal to W-namely, v = w + wl_. We now generalize this idea to !R\nn\n. \nDefinition \nLet W be a subspace of !R\nn \nand let {u\n1\n, ... , u\nk\n} be an orthogonal \nbasis for W. For any vector v in !R\nn\n, the orthogonal projection of v onto Wis \ndefined as \n( U\n1 \n• V \n) ( Uk\n· V \n) \nproj\nw\n(\nv\n) \n= \n--\nu\n, \n+ \n· · · \n+ \n--\nu\nk \nU\n1 \n• \nU\n1 \nUk \" Uk \nThe component of v orthogonal to W is the vector \nperp\nw\n(\nv\n) \n= v \n-\nproj\nw\n(\nv\n) \nEach summand in the definition of projw(v) is also a projection onto a single vec­\ntor (or, equivalently, the one-dimensional subspace spanned by it-in our previous \nsense). Therefore, with the notation of the preceding definition, we can write","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":61135,"to":61196}}}}],[1067,{"pageContent":"Example 5.11 \nSection 5.2 \nOrthogonal Complements and Orthogonal Projections \n383 \nFigure 5.8 \nP \n= \nP1 \n+ \nPz \nw \nSince the vectors \nU\n; are orthogonal, the orthogonal projection of v onto Wis the sum \nof its projections onto one-dimensional subspaces that are mutually orthogonal. Fig­\nure 5.8 illustrates this situation with W = span\n(\nu\n1\n, u\n2\n)\n, p = proj\nw\n(\nv\n)\n, p\n1 \n= proj01\n(\nv\n)\n, \nand p\n2 \n= proj0,\n(\nv\n)\n. \nAs a special case of the definition ofprojw(v), we now also have a nice geometric \ninterpretation of Theorem 5.2. In terms of our present notation and terminology, \nthat theorem states that if w is in the subspace W of !R\nn\n, which has orthogonal basis \n{v\n1\n, v\n2\n, •.• , vd, then \nThus, w is decomposed into a sum of orthogonal projections onto mutually orthogo­\nnal one-dimensional subspaces of W. \nThe definition above seems to depend on the choice of orthogonal basis; that is, \na different basis {u;, ... , u£} for W would appear to give a \"different\" projw(v) and","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":61198,"to":61252}}}}],[1068,{"pageContent":"The definition above seems to depend on the choice of orthogonal basis; that is, \na different basis {u;, ... , u£} for W would appear to give a \"different\" projw(v) and \nperpw(v). Fortunately, this is not the case, as we will soon prove. For now, let's be \ncontent with an example. \nLet \nW be tbe pl one in II' with equation x -y + \n2z \n� \n0, and let v \n� \n[ -: ] . Find tbe \northogonal projection of v onto Wand the component of v orthogonal to W. \nSolution \nIn Example 5.3, we found an orthogonal basis for W. Taking \nwe have \nll\n1 \n\"V = 2 \nU\nz \n• \nV = -2 \nU\n1 \n• U\n1 \n= 2 \nU\nz \n• U\nz \n= \n3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":61252,"to":61284}}}}],[1069,{"pageContent":"384 \nChapter 5  Orthogonality \nTheorem 5.11 \nTherefore, \nand \nproj\nw\n(\nv\n) \n= \n-\n1\n-\nu\n1 \n+ \n-\n2\n-\nu\n2 \n(u·v) (u·v) \nU\n1\n. \nU\n1 \nU\nz\n. \nU\nz \n� \ni\n[\n�\nl\n-\nf \n:\nJ \nU\nl \n[ \n3\n] \n[ �\n] \n[ �\n] \nperp\nw\n(\nv\n) \n= v \n-\nproj\nw\n(\nv\n) \n= \n-\n� \n-i \n-\n; \nIt is easy to see that projw(v) is in W, since it satisfies the equation of the plane. It \nis equally easy to see that perpw(v) is orthogonal to W, since it is a scalar multiple of \nthr nornal vootm \n[ \n-\n: \nl \nto W. ( S\" F igmo 5.9.\n) \nperpw(v) \nFigure 5.9 \nv \n= projw(\nv\n) + perpw(v) \nThe next theorem shows that we can always find a decomposition of a vector with \nrespect to a subspace and its orthogonal complement. \nThe Orthogonal Decomposition Theorem \nLet W be a subspace of IW and let v be a vector in !R\nn\n. Then there are unique \nvectors win Wand w_j_ in W_j_ such that \nv=w+w_j_ \nProof We need to show two things: that such a decomposition exists and that it is \nunique. \nTo \nshow existence, we choose an orthogonal basis {u1, ... ,  uk\n} for W Let \nw = projw(v) and let w_j_ = perpw(v). Then \nw + w\n_j_ \n= proj\nw\n(\nv\n) \n+ perp\nw\n(\nv\n) \n= proj\nw\n(\nv\n) \n+ \n(\nv \n-","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":61286,"to":61404}}}}],[1070,{"pageContent":"unique. \nTo \nshow existence, we choose an orthogonal basis {u1, ... ,  uk\n} for W Let \nw = projw(v) and let w_j_ = perpw(v). Then \nw + w\n_j_ \n= proj\nw\n(\nv\n) \n+ perp\nw\n(\nv\n) \n= proj\nw\n(\nv\n) \n+ \n(\nv \n-\nproj\nw\n(\nv\n)) \n= v","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":61404,"to":61435}}}}],[1071,{"pageContent":"Corollarv  5.12 \nSection 5.2 \nOrthogonal Complements and Orthogonal Projections \n385 \nClearly, w = projw(v) is in W, since it is a linear combination of the basis vectors \nu1, ... , uk. To show that wl. is in Wl., it is enough to show that wl. is orthogonal to \neach of the basis vectors \nU\n;, by Theorem \n5\n.9(d). We compute \nU\n; \n• \nwl. \n= U; \n• \nperp\nw\n(\nv\n) \n= u; · \n(\nv \n-\nproj\nw\n(\nv\n)) \n= u; · \n( \nv \n-\n(\n:\n1\n.\";Ju\n1\n-\n· · · \n-\n(\n�\nk\n.\";Ju\nk\n) \n= \nu- · \nv \n-\n--  (u · \nu) \n-  · · · \n-    --  (u · \nu) \n-\n· · · \n( \nU\n1 \n\"\nV\n) \n(\nU\n;\n\"\nV\n) \n' \nU\n1 \n·u\n1 \n' \nt \nu; · u; '   ' \n= u- ·v -0  -\n· · · \n-\n-\n'\n-\n(u-· u.) -\n· · · \n-  0 \n(\nu·v\n) \nz \nu\ni\n·u\ni \nz z \n= \nu\n;\n·v-u\n;\n·v=  0 \nsince u; · uj =  0 for j  -=F  i. This proves that wl. is in Wl. and completes the existence \npart of the proof. \nTo show the uniqueness of this decomposition, let's suppose we have another de­\ncomposition v = W\n1 \n+ wt, wherew\n1\nis in Wand wt is in wl.. Thenw+ w_L = W\n1 \n+ wt,so \nBut since w \n-\nw\n1 \nis in Wand wt \n-\nwl. is in Wl. (because these are subspaces), we \nknow that this common vector is in W n Wl. = {O} [using Theorem \n5","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":61437,"to":61559}}}}],[1072,{"pageContent":"1\nis in Wand wt is in wl.. Thenw+ w_L = W\n1 \n+ wt,so \nBut since w \n-\nw\n1 \nis in Wand wt \n-\nwl. is in Wl. (because these are subspaces), we \nknow that this common vector is in W n Wl. = {O} [using Theorem \n5\n.9(c)]. Thus, \nExample 5.11 illustrated the Orthogonal Decomposition Theorem. When W is \nthe subspace of IR\n3 \ngiven by the plane with equation x -y + 2z =  0, the orthogonal \ndernmpo,;tion of v � \n[ \n-! \n] \nwdh mped to W ;,. � w + w\", whe\nc\ne \nw \n� \np\nm\njw\n(\nv\n) \n� \nU\nl \nM\nd \nw\n\" \n� \npe\nc\npw\n(\nv\n) \n� \nHl \nThe uniqueness of the orthogonal decomposition guarantees that the definitions \nof projw(v) and perpw(v) do not depend on the choice of orthogonal basis. The \nOrthogonal Decomposition Theorem also allows us to prove property (b) of Theo­\nrem 5.9. We state that property here as a corollary to the Orthogonal Decomposition \nTheorem. \nIf Wis a subspace of !R\nn\n, then","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":61559,"to":61615}}}}],[1073,{"pageContent":"386 \nChapter 5  Orthogonality \nTheorem 5.13 \nCorollarv  5.14 \nProof If w is in Wand xis in W_L, then w · x =  0. But this now implies that w is in \n(W_L )_L. Hence, W C (W_L )_L. Now let v be in (W_L )_L. By Theorem 5.11, we can write \nv = w + w_L for (unique) vectors win Wand w_L in W_L. But now \n0  = v \n· \nw\n_L \n= \n( \nw + w\n_L\n) \n· \nw\n_L \n= w · \nw\n_L \n+ w\n_L \n· \nw\n_L \n=  0 \n+ w\n_L \n· \nw\n_L \n= w\n_L \n· \nw\n_L \nso w_L = 0. Therefore, v = w + w_L = w, and thus vis in W. This shows that ( W _L) _L C W \nand, since the reverse inclusion is also true, we conclude that (W_L)_L = W, as required. \nThere is also a nice relationship between the dimensions of Wand W_L, expressed in \nTheorem \n5.13. \nIf W is a subspace of !R\nn\n, then \ndim W + dim W \n_L \n= n \nProof Let {u\n1\n, ... , u\nk\n} be an orthogonal basis for Wand let {v\n1\n, ... , v1} be an orthog-\nonal basis for w_L. Then dim w = k and dim w_L = l. Let B = {u\n1\n, ... , \nUk\n,v\nl\n, ... , v1}. \nWe claim that Bis an orthogonal basis for !R\nn\n. \nWe first note that, since each u; is in Wand each v\nj \nis in W_L, \nU\n;\n'V","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":61617,"to":61688}}}}],[1074,{"pageContent":"1\n, ... , \nUk\n,v\nl\n, ... , v1}. \nWe claim that Bis an orthogonal basis for !R\nn\n. \nWe first note that, since each u; is in Wand each v\nj \nis in W_L, \nU\n;\n'V\nj \n=  0 for i = 1, .. .,k andj = 1, .. .,l \nThus, B is an orthogonal set and, hence, is linearly independent, by Theorem 5.1. \nNext, if vis a vector in !R\nn\n, the Orthogonal Decomposition Theorem tells us that v = \nw + w_L for some win Wand w_L in W_L. Since w can be written as a linear combina­\ntion of the vectors \nU\n; and w_L can be written as a linear combination of the vectors v\nj\n, v \ncan be written as a linear combination of the vectors in B. Therefore, B spans !R\nn \nalso \nand so is a basis for !R\nn\n. It follows that k + \nl = dim !R\nn\n, or \ndim w + dim w_L = n \nAs a lovely bonus, when we apply this result to the fundamental subspaces of a \nmatrix, we get a quick proof of the Rank Theorem (Theorem 3.26\n)\n, restated here as \nCorollary 5. 14. \nThe Rank Theorem \nIf A is an m X n matrix, then \nrank\n(\nA\n) \n+ nullity\n(\nA\n) \n= n","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":61688,"to":61740}}}}],[1075,{"pageContent":"matrix, we get a quick proof of the Rank Theorem (Theorem 3.26\n)\n, restated here as \nCorollary 5. 14. \nThe Rank Theorem \nIf A is an m X n matrix, then \nrank\n(\nA\n) \n+ nullity\n(\nA\n) \n= n \nProof In Theorem 5.13, take W = row(A). Then W_L = null(A), by Theorem 5.10, \nso dim W = rank(A) and dim W_L = nullity(A). The result follows. \nNote that we get a counterpart identity by taking W = col(A) [and therefore \nW_L = null(A\nT\n)J: \nrank\n(\nA\n) \n+ nullity\n(\nA r) =  m","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":61740,"to":61767}}}}],[1076,{"pageContent":"Section 5.2 \nOrthogonal Complements and Orthogonal Projections \n381 \nSections 5.1 and 5.2 have illustrated some of the advantages of working with \northogonal bases. However, we have not established that every subspace has an or­\nthogonal basis, nor have we given a method for constructing such a basis (except in \nparticular examples, such as Example 5.3\n)\n. These issues are the subject of the next \nsection. \nI \nExercises 5.2 \nIn Exercises 1-6, find the orthogonal complement W\n_j_ \nof W \nand give a basis for W \n_j_\n. \n1. W = \n{ \n[;] \n: 2x -  y \n=  0} \n2. \nW = \n{ \n[;] \n: 3x + 4y =  0} \n3. \nw � { \n[ \n� l x  + y  -  z � 0) \n4. \nW � { \n[\n:J 2x -  y  + 3z � 0) \n5.  W � { \n[ \n� l x � I, y � -I, z � 3t) \n6. \nW � {\n[\n�]x � \n�\nt, y � +.z � 21) \nIn Exercises 7 and 8, find bases for th e row space and null \nspace of A. Verify that every vector in row( A) is orthogonal \nto every vector in null(A). \n7. \nA= \n[ \n� \n-\n: _!\n] \n-1 -1 \n1 \n8. \nA \n= \n[\n-\n� \n�  -\n� \n� \n! \nl \n2 2 -2 0 1 \n-3 -1 3     4     5 \nIn Exercises 9 and 10, find bases for th e column space of","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":61769,"to":61839}}}}],[1077,{"pageContent":"to every vector in null(A). \n7. \nA= \n[ \n� \n-\n: _!\n] \n-1 -1 \n1 \n8. \nA \n= \n[\n-\n� \n�  -\n� \n� \n! \nl \n2 2 -2 0 1 \n-3 -1 3     4     5 \nIn Exercises 9 and 10, find bases for th e column space of \nA and the null space of A\nT \nfor th e given exercise. Verify \nthat every vector in col(A) is orthogonal to every vector \nin null(A \nT\n). \n9. Exercise 7 \n10. Exercise 8 \nIn Exercises 11-14, let W be th e subspace spanned by the \ngiven vectors. Find a basis for W\n_j_\n. \n4 2 \n6 \n2 2 \n14. \nW\n1 \n= \n-1 \n,W\nz \n= \n0 \n,W\n3 \n= \n2 \n1 \n-1 \n-1 \n- 3 \n2 \nIn Exercises 15-18, find the orthogonal projection of v onto \nth e subspace W spanned by th e vectors u;. (You may assume \nthat the vectors \nU\n; are orthogonal.)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":61839,"to":61901}}}}],[1078,{"pageContent":"388 \nChapter 5  Orthogonality \nIn Exercises 19-22,find the orthogonal decomposition of \nv with respect to W \nthat v = w + w\n'\n. Is it necessarily true that w\n' \nis in W J..? \nEither prove that it is true or find a counterexample. \n19. v = \n[\n-\n�\nl \nW =span(\n[�]\n) \n20.v � \n[ \nJ W � 'P\"•([\n:\nJ) \n26. Let {v\n1\n, ... , v\nn\n} be an orthogonal basis for !R\nn \nand let \nW = span(v\n1\n, ... , v\nk\n)\n· \nIs it necessarily true that Wl. = \nspan(v\nk\n+\nl\n' \n... , v\nn\n)? Either prove that it is true or find a \ncounterexample. \nIn Exercises 27-29, let W be a subspace of !R\nn\n, and let x be \na vector in !R\nn\n. \n21. F \n[\n-\nH \nw � 'P\"\"( \n[\n�\n].\n[\n-\n:\n]\n) \n27. Prove that xis in W if and only if projw(x) = x. \n28. Prove that xis orthogonal to W if and only if \nprojw(x) = 0. \n22. F \n[\n-\n; l \nW � 'P\"\" ( [J \n[\n-\nm \n29. Prove that projw(projw(x)) = projw(x). \n30. Let S = {v\n1\n, ... , v\nk\n} be an orthonormal set in !R\nn\n, and \nlet x be a vector in !R\nn\n. \n(a) Prove that \n23. Prove Theorem \n5\n.9(c). \n24. Prove Theorem \n5\n.9(d). \nll\nx\nll\n2 \n2: \nl\nx ·v\n1\nl\n2 \n+ \nl\nx ·v\n2\nl\n2 \n+ \n· · · \n+ \nl\nx ·v\nk\nl\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":61903,"to":62020}}}}],[1079,{"pageContent":"1\n, ... , v\nk\n} be an orthonormal set in !R\nn\n, and \nlet x be a vector in !R\nn\n. \n(a) Prove that \n23. Prove Theorem \n5\n.9(c). \n24. Prove Theorem \n5\n.9(d). \nll\nx\nll\n2 \n2: \nl\nx ·v\n1\nl\n2 \n+ \nl\nx ·v\n2\nl\n2 \n+ \n· · · \n+ \nl\nx ·v\nk\nl\n2 \n(This inequality is called Bessel's Inequality.) \n25. Let W be a subspace of !R\nn \nand v a vector in !R\nn\n. Suppose \nthat w and w\n' \nare orthogonal vectors with w in Wand \n(b) Prove that Bessel's Inequality is an equality if and \nonly if xis in span(S). \nExample 5.12 \nThe Gram-Schmidt Process \nand the OR Factorization \nIn this section, we present a simple method for constructing an orthogonal (or or­\nthonormal) basis for any subspace of !R\nn\n. This method will then lead us to one of the \nmost useful of all matrix factorizations. \nThe Gram-Schmidl Process \nWe would like to be able to find an orthogonal basis for a subspace W of !R\nn\n. The idea \nis to begin with an arbitrary basis {x\n1\n, ... , xd for Wand to \"orthogonalize\" it one \nvector at a time. We will illustrate the basic construction with the subspace W from \nExample","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":62020,"to":62087}}}}],[1080,{"pageContent":"n\n. The idea \nis to begin with an arbitrary basis {x\n1\n, ... , xd for Wand to \"orthogonalize\" it one \nvector at a time. We will illustrate the basic construction with the subspace W from \nExample \n5.3. \nLet W \n= \nspan(x\n1\n, x\n2\n), where \nConstruct an orthogonal basis for W. \nSolulion Starting with x\n1\n, we get a second vector that is orthogonal to it by taking \nthe component of x\n2 \northogonal to x\n1 \n(Figure 5.10).","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":62087,"to":62110}}}}],[1081,{"pageContent":"Section 5.3 \nThe Gram-Schmidt Process and the Q\nR \nFactorization \n389 \nFigure 5.10 \nConstructing v2 orthogonal to x\n1 \nAlgebraically, we set v1 = x\n1\n, so \nv\n2 \n= perp\nxJ\nx\nz\n) \n= x\n2 \n- proj\nx, \n(\nx\nz\n) \n=x -(\n�\n)x \n2 \nX\n1 \n•\nx\ni \nI \nw \nThen {v1, v\n2\n} is an orthogonal set of vectors in W. Hence, {v1, v\n2\n} is a linearly indepen-\ndent set and therefore a basis for W, since dim W = 2. \n4 \nRemark \nObserve that this method depends on the order of the original basis \nmtocs. Jn Exrunpl'5.12, if we had taken x\n, \n� \n[\n-\nf\nl \nand x, � \n[ \ni l we would have \n� \nobtained a different orthogonal basis for W. (Verify this.) \nThe generalization of this  method to more  than  two  vectors begins  as in \nExample 5.12. Then the process is to  iteratively construct the components of subse­\nquent vectors orthogonal to all of the vectors that have already been constructed. The \nmethod is known as the Gram-Schmidt Process. \nTheorem 5.15 \nThe Gram-Schmidt Process \nLet {x\n1\n, ... , xd be a basis for a subspace W of !R\nn \nand define the following: \n_ \n(\nv\n2 \n: \nx\nk\n)v\nz \n_ \n... \nV\nz \nV","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":62112,"to":62194}}}}],[1082,{"pageContent":"method is known as the Gram-Schmidt Process. \nTheorem 5.15 \nThe Gram-Schmidt Process \nLet {x\n1\n, ... , xd be a basis for a subspace W of !R\nn \nand define the following: \n_ \n(\nv\n2 \n: \nx\nk\n)v\nz \n_ \n... \nV\nz \nV\nz \nThen for each i = 1, ... , k, {v1,  ... , v;} is an orthogonal basis for W;. In particular, \n{v1, .•. , v\nk\n} is an orthogonal basis for W.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":62194,"to":62220}}}}],[1083,{"pageContent":"390 \nChapter 5  Orthogonality \nJorgen Pedersen Gram \n(1850-1916) was a Danish actuary \n(insurance statistician) who was \ninterested in the science of mea­\nsurement. He first published the \nprocess that bears his name in \nan 1883 paper on least squares. \nErhard Schmidt (1876-1959) was \na German mathematician who \nstudied under the great David \nHilbert and is considered one \nof the founders of the branch of \nmathematics known as functional \nanalysis. His contribution to the \nGram-Schmidt Process came in a \n1907 paper on integral equations, \nin which he wrote out the details \nof the method more explicitly than \nGram had done. \nExample 5.13 \nStated succinctly, Theorem 5 .15 says that every subspace of ll�r has an orthogonal \nbasis, and it gives an algorithm for constructing such a basis. \nProof We will prove by induction that, for each i = 1, ... , k, {v\n1\n, ... , v;} is an or­\nthogonal basis for W;. \nSince v\n1 \n= x\n1\n, clearly {v\ni\n} is an (orthogonal) basis for W\n1 \n= span(x\n1\n). Now assume \nthat, for some i < k, {v\n1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":62222,"to":62262}}}}],[1084,{"pageContent":"1\n, ... , v;} is an or­\nthogonal basis for W;. \nSince v\n1 \n= x\n1\n, clearly {v\ni\n} is an (orthogonal) basis for W\n1 \n= span(x\n1\n). Now assume \nthat, for some i < k, {v\n1\n, ... , v;} is an orthogonal basis for W;. Then \n= \n_ \n(\nV\nI\n• \nX\n;\n+\n1\n) \n_ \n(\nV\n2 \n• \nX\n;\n+\n1\n) \n_ \n... \n_ \n(\nV\n; • \nX\n;\n+\n1\n) \nV\n;\n+J X\n;\n+J \n. \nV\n1 \n. \nVz \n. \nV\n; \nV\n1 \nV\n1 \nV\nz \nV\nz \nV\n; \nV\n; \nBy the induction hypothesis, {v\n1\n, ... , v;} is an orthogonal basis for span(x\n1\n, ... , x;) = \nW;. Hence, \nV\n;\n+\n1 \n= \nX\n;\n+\n1 \n-proj\nw\n,\nC\nx\n;\n+\n1\n) \n= perp\nw,\nC\nX\n;\n+\n1\n) \nSo, by  the  Orthogonal Decomposition  Theorem, \nV\n;+ \n1 \nis  orthogonal to W;. By \ndefinition, v\n1\n,  ... , v; are linear combinations of x\n1\n, ... ,  X; and, hence, are in W;. \nTherefore, {v\n1\n, ... , V\n; +d is an orthogonal set of vectors in W;+i· \nMoreover, \nV\n; +i \n* \n0, since otherwise X\n;\n+\n1 \n= proj\nw\n,\nC\nX\n;\n+\n1\n)\n, which in turn implies \nthat X;+ \n1 \nis in W;. But this is impossible, since W; = span(x\n1\n, ... , X;) and {x\n1\n, ... , X;+ i} \nis line arly independent. (Why?) We conclude that {v\n1\n, ... , V;+d is a set  of i +  ! lin-\nearly independent vectors in W;+i· Consequently, {v\n1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":62262,"to":62411}}}}],[1085,{"pageContent":"1\n, ... , X;) and {x\n1\n, ... , X;+ i} \nis line arly independent. (Why?) We conclude that {v\n1\n, ... , V;+d is a set  of i +  ! lin-\nearly independent vectors in W;+i· Consequently, {v\n1\n, ... ,  V;+d is a basis for W;+1, \nsince dim W;+ \n1 \n=  i + 1. This completes the proof. \nIf we require an orthonormal basis for W, we simply need to normalize the \northogonal vectors produced by the Gram-Schmidt Process. That is, for each i, we \nreplace \nV\n; by  the unit vector q; =  (l/\nll\nv\n;\nll\n)\nv\n;\n. \nApply the Gram-Schmidt Process to construct an orthonormal basis for the subspace \nW = span(x\n1\n, x\n2\n, x\n3\n) of IR\n4\n, where \nSolulion First we note that {x\n1\n, x\n2\n, x\n3\n} is a linearly independent set, so it forms a basis \nfor W. We begin by setting v\n1 \n= x\n1\n. Next, we compute the component of x\n2 \northogo­\nnal \nto W1 = span(v\n1\n):","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":62411,"to":62464}}}}],[1086,{"pageContent":"Section 5.3 \nThe Gram-Schmidt Process and the Q\nR \nFactorization \n391 \nFor hand calculations, it is  a good idea to \"scale\" v\n2 \nat this point to eliminate fractions. \nWhen we are finished, we can rescale the orthogonal set we are constructing to obtain \nan orthonormal set; thus, we can replace each \nV\n; by any convenient scalar multiple \nwithout affecting the final result. Accordingly, we replace v\n2 \nby \nWe now find the component of x\n3 \northogonal to \nW\n2 \n= span\n(\nx\n1\n, xi\n) \n= span\n(\nv\n1\n, v\n2\n) \n= span\n(\nv\n1\n, v�\n) \nusing the orthogonal basis {v\n1\n, v�}: \nAgllin, we mrnle and \"'\"; \n� \n2v, \n� \n[\n-\n!l \n� We  now have an orthogonal basis {v\n1\n, v�, v\n�\n} for W.  (Check to make sure that \nthese vectors are orthogonal.) To  obtain an orthonormal basis, we normalize each \nvector: \n[\n-1\n1 \n[\n-1/V6\n] \n[\n-V6/6\n] \nq\n3 \n= \nC\n1\n:\n�\n11\n)\nv\n� \n= \n( �) \n� \n= \n1\n�\nV6 \n�/6 \n2 \n2\n/V6 \nV6/3 \nThen {q\n1\n, q\n2\n, q\n3\n} is an orthonormal basis for W.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":62466,"to":62560}}}}],[1087,{"pageContent":"392 \nChapter 5  Orthogonality \nExample 5.14 \nOne of the important uses of the Gram-Schmidt Process is to construct an orthogo­\nnal basis that contains a specified vector. The next example illustrates this application. \nFind an orthogonal basis for IR\n3 \nthat contains the vector \nSolulion \nWe first find any basis for IR\n3 \ncontaining v\n1\n• If we take \n� \nthen {v\n1\n, x\n2\n, x\n3\n} is clearly a basis for IR\n3\n. (Why?) We now apply the Gram-Schmidt \nProcess to this basis to obtain \nand finally \nThen {v\n1\n, v�, v\n�\n} is an orthogonal basis for IR\n3 \nthat contains v\n1\n. \nSimilarly, given a unit vector, we can find an orthonormal basis that contains it by \nusing the preceding method and then normalizing the resulting orthogonal vectors. \nRemark When the Gram-Schmidt Process is implemented on a computer, there \nis almost always some roundoff error, leading to a loss of orthogonality in the vec­\ntors q;. To avoid this loss of orthogonality, some modifications are usually made. The","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":62562,"to":62601}}}}],[1088,{"pageContent":"is almost always some roundoff error, leading to a loss of orthogonality in the vec­\ntors q;. To avoid this loss of orthogonality, some modifications are usually made. The \nvectors V; are normalized as soon as they are computed, rather than at the end, to \ngive the vectors q;, and as each q; is computed, the remaining vectors x\nj \nare modified \nto be orthogonal to q;. This procedure is known as the Modified Gram-Schmidt \nProcess. In practice, however, a version of the QR factorization is used to compute \northonormal bases. \nThe OB Factorization \nIf A is an m X n matrix with linearly independent columns (requiring that m 2 n\n)\n, \nthen applying the Gram-Schmidt Process to these columns yields a very useful fac­\ntorization of A into the product of a matr  ix Q with orthonormal columns and an","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":62601,"to":62615}}}}],[1089,{"pageContent":"Theorem 5.16 \nSection 5.3 \nThe Gram-Schmidt Process and the Q\nR \nFactorization \n393 \nupper triangular matrix R. This is the QR fa ctorization, and it has applications to the \nnumerical approximation of eigenvalues, which we explore at the end of this section, \nand to the problem ofleast squares approximation, which we discuss in Chapter 7. \nTo see how the QR factorization arises, let a\n1\n, ... , a\nn \nbe the (linearly independent) \ncolumns of A and let \nq\n1\n, ... , \nq\nn \nbe the orthonormal vectors obtained by applying the \nGram-Schmidt Process to A with normalizations. From Theorem 5.15, we know that, \nfor each i = 1, ... ,  n, \nW\n; \n= span\n(\na\n1\n, ... , a;\n) \n= span\n(\nq\n1\n, ... , \nq;\n) \nTherefore, there are scalars r\n1\n;, r\n2\n;, •.• , r;; such that \nThat is, \na\n, \n= r\nll\nq\n, \na\nz \n= r\n1\n2\nq\n1 \n+ \nY\nzz\nq\nz \nwhich can be written in matr  ix form as \n0 \nr\n1\nn\nl \nr\n�\nn \n= QR \nr\nnn \nClearly, the matrix Q has orthonormal columns. It   is also the case that the diago­\nnal entries of R are all nonzero. To see this, observe that if r;; =  0, then a; is a linear","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":62617,"to":62692}}}}],[1090,{"pageContent":"0 \nr\n1\nn\nl \nr\n�\nn \n= QR \nr\nnn \nClearly, the matrix Q has orthonormal columns. It   is also the case that the diago­\nnal entries of R are all nonzero. To see this, observe that if r;; =  0, then a; is a linear \ncombination of \nq\n1\n, ... , q;\n_\n1 \nand, hence, is in  W;\n_\n1\n. But then a; would be a linear com­\nbination of a\n1\n, ... , a;\n_ \n1\n, which is impossible, since a\n1\n, ... , a; are  linearly independent. \nWe conclude that r;; \n-=fa \n0 for i = 1, ... ,  n. Since R is upper triangular, it  follows that it \nmust be invertible. (See Exercise 23.\n) \nWe have proved the following theorem. \nThe QR Factorization \nLet A be an m X n matrix with linearly independent columns. Then A can be fac­\ntored as A = QR, where Q is an m X n matrix with orthonormal columns and R is \nan invertible upper triangular matr  ix. \nRemarks \n• \nWe can also arrange for the diagonal entries of R to be positive. If any r;; < 0, \nsimply replace q;  by -\nq\n; and r;; by -r;;. \n• \nThe requirement that A have linearly independent columns is a necessary one.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":62692,"to":62740}}}}],[1091,{"pageContent":"simply replace q;  by -\nq\n; and r;; by -r;;. \n• \nThe requirement that A have linearly independent columns is a necessary one. \nTo prove this, suppose that A is an m X n matrix that has a QR  factorization, as in The­\norem 5.16. Then, since R is invertible, we have Q = AR\n-\n1\n• Hence, rank(Q) = rank(A), \nby Exercise 61 in Section 3.5. But rank( Q) = n, since its columns are orthonormal and, \ntherefore, linearly independent. So rank(A) = n too, and consequently the columns of \nA are linearly independent, by the Fundamental Theorem.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":62740,"to":62752}}}}],[1092,{"pageContent":"394 \nChapter 5  Orthogonality \nExample 5.15 \n.. \nI \nExercises 5.3 \n• The QR factorization can be extended to arbitrary matrices in a  slightly \nmodified form. If A is m X n, it is possible to find a sequence of orthogonal matrices \nQ\n1 , .•. , \nO\nm\n-\nI \nsuch that \nO\nm\n-\nI\n··· \nQ\n2\nQ\n1\nA is an upper triangular m X n matrix R. Then \nA= QR, where Q = (\nO\nm\n-\nI\n··· \nQ\n2\nQ\n1)\n-\n1 \nis an orthogonal matrix. We will examine this \napproach in Exploration: The Modified QR Factorization. \nFind a QR factorization of \nA\n=\n[\n-\n�\n� \n�1 \n-1 0 1 \n1 \n2 \nSolulion The columns of A are just the vectors from Example 5.13. The orthonormal \nbasis for col(A) produced by the Gram-Schmidt Process was \nso \nq\n1 \n= \n[\n=\n:\n;:\n1\n, \nq\nz \n= \n[\n�\n�\n:\n:\n1\n, \nq\n3 \n= \n[\n-\n��:\n1 \n1/2 \nVs\n/ 10 \nV6\n/3 \n[ \n1/2 3\nVs\n/10 \n-1/2 3\nVs\n/10 \nQ \n= \n[\nq\ni \nq\nz \nq\n3\n] \n= \n-1\n/2 \nVs\n/10 \n1/2 \nVs\n/10 \n-\nv:\n/ 6\n1 \nV6\n/\n6 \nV6\n/3 \nFrom Theorem 5.16, A = QR for some upper triangular matrix R. To find R, we \nuse the fact that Q has orthonormal columns and, hence, Q\nT \nQ = I. Therefore, \nQ\nT\nA \n= \nQ\nT\nQR \n=\nIR = \nR \nWe compute \n[ 1/2 \nR = Q'A = \n3\nVs\n/1\n0 \n-\nV6\n/ 6 \n� \n[: \n1 \nVs \n0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":62754,"to":62900}}}}],[1093,{"pageContent":"use the fact that Q has orthonormal columns and, hence, Q\nT \nQ = I. Therefore, \nQ\nT\nA \n= \nQ\nT\nQR \n=\nIR = \nR \nWe compute \n[ 1/2 \nR = Q'A = \n3\nVs\n/1\n0 \n-\nV6\n/ 6 \n� \n[: \n1 \nVs \n0 \n-1/2 \n3\nVs\n/10 \n0 \n1/\n2 \nl \n3\nVs\n/2 \nV6\n/2 \n-1/2 \n[ \n1 \n1/2 \nVs\n/10 \n\\/5/\n10 \nl \n=: \nV6\n/6 \nV6\n/3 \n1 \n2 \n�\nl \n1 \n0 \nIn Exercises 1-4, the given vectors form a basis for IR\n2 \nor IR\n3\n• \nApply the Gram-Schmidt Process to obtain an orthogonal \nbasis. Then normalize this basis to obtain an orthonormal \nbasis. \n2. \nX\n1 \n= \n[ \n_ \n� \nl \nX\nz \n= \n[ \n�\n] \n3 \n.\n.. \n� \n[ \nJ \nx\n, \n� \n[ \nn \nx\n, \n� \nm","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":62900,"to":62997}}}}],[1094,{"pageContent":"Section 5.3 \nThe Gram-Schmidt Process and the Q\nR \nFactorization \n395 \nIn Exercises 5 and 6, the given vectors form a basis for a \nsubspace W of IR\n3 \nor IR\n4\n. Apply th e Gram-Schmidt Process \nto obtain an orthogonal basis for W \nIn Exercises 7 and 8, find the orthogonal decomposition of v \nwith respect to the subspace W \n7. \nv \n� \n[ \n-:} w\"' i• Emd\", \n8. \nF \n[\ni} Was in Exmise6 \nUse th e Gram-Schmidt Process to find an orth ogonal basis \nfor th e column spaces of th e matrices in Exercises 9 and 10. \n9. \n[\n: \n� \n�\n] \n11. Find an orthogonal basis for IR\n3 \nthat contains the \nveoto' \n[\n:J \n12. Find an orthogonal basis for IR\n4 \nthat contains the \nvectors \nIn Exercises 13 and 14, fill in th e missing entries of Q \nto make Q an orthogonal matrix. \n[ \n1/\n\\/2 \n1/\\/3 \n*\n] \n13. Q = \n0 1/\\/3 \n* \n-1/\n\\/2 \n1/\\/3 \n* \n[\n1/2 \n1/2 \n14. Q = \n1/2 \n1/2 \n2/\nVi4 \n* \nl/\nVi4 \n* \n0 \n* \n-3\n/\nVi4 \n* \n:\n] \n* \n* \nIn Exercises 15 and 16, fi nd a QR factorization of th e \nmatrix in th e given exercise. \n15. Exercise 9 \n16. Exercise 10","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":62999,"to":63080}}}}],[1095,{"pageContent":"* \n-1/\n\\/2 \n1/\\/3 \n* \n[\n1/2 \n1/2 \n14. Q = \n1/2 \n1/2 \n2/\nVi4 \n* \nl/\nVi4 \n* \n0 \n* \n-3\n/\nVi4 \n* \n:\n] \n* \n* \nIn Exercises 15 and 16, fi nd a QR factorization of th e \nmatrix in th e given exercise. \n15. Exercise 9 \n16. Exercise 10 \nIn Exercises 17 and 18, the columns of Q were obtained by \napplying the Gram-Schmidt Process to th e columns of A. \nFind the upper triangular matrix R such that A = QR. \n17.A = \nu \n18.\nA = \n[\n-\nj \n8 \n7 \n-2 \n2\n] \n[ \n' \n-1 \n'\nQ = \ni \n1 \n-\n-\n3 \n4 \n2/\nV6 \n3] \n[ \n1/v'6 \n-\n� \n,Q = \n-1\n�\nV6 \nl \n-\n!\n] \n3 \n� \n3 \n2 \n3 \n1/:3\n_ \n1/\\/3 \n1/\\/3 \n19. If A is an orthogonal matrix, find a QR factorization \nof A. \n20. Prove that A is invertible if and only if A = QR, where \nQ is orthogonal and R is upper triangular with nonzero \nentries on its diagonal. \nIn Exercises 21 and 22, use the method suggested by \nExercise 20 to compute A -\ni \nfor th e matrix A in the given \nexercise. \n21. Exercise 9 \n22. Exercise 15 \n23. Let A be an m X n matrix with linearly independent \ncolumns. Give an alternative proof that the upper \ntriangular matrix R in a QR factorization of A must","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":63080,"to":63175}}}}],[1096,{"pageContent":"exercise. \n21. Exercise 9 \n22. Exercise 15 \n23. Let A be an m X n matrix with linearly independent \ncolumns. Give an alternative proof that the upper \ntriangular matrix R in a QR factorization of A must \nbe invertible, using property (c) of the Fundamental \nTheorem. \n24. Let A be an m X n matr  ix with linearly independent \ncolumns and let A =  QR be a QR factorization of A. \nShow that A and Q have the same column space.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":63175,"to":63185}}}}],[1097,{"pageContent":"y \nfigure 5.11 \n396 \nExplorations \n0 \nThe Modified QR Factorization \nWhen the matrix A  does not have linearly independent columns, the Gram-Schmidt \nProcess as we have stated it does not work and so cannot be used to develop a gen­\neralized QR factorization of A. There is a modification of the Gram-Schmidt Process \nthat can be used, but instead we will explore a method that converts A into upper \ntriangular form one column at a time, using a sequence of orthogonal matrices. The \nmethod is analogous to that of LU factorization, in which the matrix L is formed \nusing a sequence of elementary matrices. \nThe first thing we need is the \"orthogonal analogue\" of an elementary matrix; \nthat is, we need to know how to construct an orthogonal matrix Q that will trans­\nform a given column of A-call it x-into the corresponding column of R-call it y. \nBy Theorem 5.6, it will be necessary that \nll\nx\nll = \nII \nQx\nll = ll\nr\nll\n. Figure 5.11 suggests a \nway to proceed: We can reflect x in a line perpendicular to x \n-\ny. If","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":63187,"to":63215}}}}],[1098,{"pageContent":"By Theorem 5.6, it will be necessary that \nll\nx\nll = \nII \nQx\nll = ll\nr\nll\n. Figure 5.11 suggests a \nway to proceed: We can reflect x in a line perpendicular to x \n-\ny. If \nis the unit vector in the direction ofx \n-\ny, then uj_ \n= [\n-\n�:\n]\nis orthogonal to u\n, \nand \nwe can use Exercise 26 in Section 3.6 to find the standard matrix Q of the reflection \nin the line through the origin in the direction of u J_. \n1. \nShow that Q = \n1 \n[\nl -\n2d\n2 \n-\n2d,d\n2 \n2. Compute Q for \n-\n2d\n1\nd\n2\n] \nT \nd\n2 \n=\nI \n-\n2uu. \n1 \n-\n2 \n2 \n(\na\n) \nu \n=[\nfl \n(\nb\n) \nx \n= [\n:\nJ\n,\nr \n= [\n�\n] \nWe can generalize the definition of Q as follows. If u is any unit vector in !R\nn\n, we \ndefine an n X n matrix Q as \nQ \n=\nI \n-\n2uur","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":63215,"to":63294}}}}],[1099,{"pageContent":"Alston Householder (1904-1993) \nwas one of the pioneers in the field \nof numerical linear algebra. He \nwas the first to present a systematic \ntreatment of algorithms for solving \nproblems involving linear systems. \nIn addition to introducing the \nwidely used Householder trans­\nformations that bear his name, he \nwas one of the first to advocate the \nsystematic use of norms in linear \nalgebra. His 1964 book The Th eory \nof  Matrices in Numerical Analysis is \nconsidered a classic. \nSuch a matrix is called a Householder matrix (or an elementary reflector). \n3.  Prove that every Householder matrix Q  satisfies the following properties: \n(a)  Q  is symmetric. (b)  Q is orthogonal. (c)  Q\n2 \n=I \n4. Prove that if Q is a Householder matrix corresponding to the unit vector \nu, then \n{\n-v  ifv is in span\n(\nu\n) \nQv = \nv \nifv ·u =  0 \n5.  Compute Q foe u � \n[ \n-�] <md wdfy Prnblem'3 ond 4. \n6.  Let x * \ny \nwith \nll\nx\nll \n= \nllrll \nand set u  =  (\n1/ \nll\nx \n-\nrll\n)\n(\nx \n-\ny\n). \nProve \nthat \nthe \ncorresponding Householder matr  ix Q   satisfies Qx = \ny","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":63296,"to":63352}}}}],[1100,{"pageContent":"5.  Compute Q foe u � \n[ \n-�] <md wdfy Prnblem'3 ond 4. \n6.  Let x * \ny \nwith \nll\nx\nll \n= \nllrll \nand set u  =  (\n1/ \nll\nx \n-\nrll\n)\n(\nx \n-\ny\n). \nProve \nthat \nthe \ncorresponding Householder matr  ix Q   satisfies Qx = \ny\n. [Hint: Apply Exercise 57 in \nSection 1.2 to the result in Problem 4.] \n7.  Find Q and verify Problem 6 for \nWe are now ready to perform the triangularization of an m X n matrix A, column \nby column. \n8. \nLet x be the first column of A and let \nShow that if Q\n1 \nis the Householder matr  ix given by Problem 6, then Q\n1\nA is a matrix \nwith the block form \nwhereA\n1 \nis \n(\nm -l\n)\nX\n(\nn -  1\n)\n. \nIf we repeat Problem 8 on the matrix A \n1\n, we use a Householder matr  ix P\n2 \nsuch that \nwhere A\n2 \nis (m  -  2\n) \nX \n(\nn -  2\n)\n. \n9. Set Q 2 \n= \n[ \n� :J. Show that Q\n2 \nis an orthogonal matrix and that \n391","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":63352,"to":63424}}}}],[1101,{"pageContent":"See G. H. Golub and C. F. Van \nLoan, Matrix Computations \n(Baltimore: Johns Hopkins \nUniversity Press, 1983). \n398 \n10.  Show that we can continue in this fashion to find a sequence of orthogonal \nmatrices Qi, ... , Q\nm\n-\ni such that Q\nm\n-\ni ·  ·  · Q\n2\nQiA = R   is an upper triangular m X n \nmatrix (i.e., r;1 = 0   if i > j\n)\n. \n11. \nDeduce that A = QR with Q = Qi Q\n2 \n·  ·  · Q\nm\n-\ni orthogonal. \n12. \nUse the method of this exploration to find a QR factorization of \n3 \n-4 \n-5 \nApproximating Eigenvalues \nwith the QR Algorithm \n3 \n-1 \nOne of the best (and most widely used) methods for numerically approximating the \neigenvalues of a matrix makes use of the QR factorization. The purpose of this ex­\nploration is to introduce this method, the QR algorithm, and to show it at  work in a \nfew examples. For a more complete treatment of this topic, consult any good text on \nnumerical linear algebra. (You will find it helpful to use a CAS to perform the calcula­\ntions in the problems below.)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":63426,"to":63465}}}}],[1102,{"pageContent":"numerical linear algebra. (You will find it helpful to use a CAS to perform the calcula­\ntions in the problems below.) \nGiven a square matrix A, the first step is to  factor it as  A =   QR (using whichever \nmethod is appropriate). Then we define Ai = RQ. \n1.  First prove that Ai is similar to A. Then prove that Ai has the same eigen­\nvalues as A. \n2. If A = [ \n� \n�\n]\n, find A\n1 \nand verify that it has the same eigenvalues as A. \nContinuing the algorithm, we factor Ai as Ai = QiR\n1 \nand setA\n2 \n= RiQi. Then we \nfactor A\n2 \n= Q\n2\nR\n2 \nand set A\n3 \n= R\n2\nQ\n2\n, and so on. That is, for k 2: 1, we compute Ak = \nQkRk \nand \nthen set Ak\n+\ni =  RkQk\n. \n3. \nProve that A\nk \nis similar to A for all k 2: 1. \n4. \nContinuing Problem 2, compute A\n2\n, A\n3\n, A\n4\n, and A5,   using two-decimal-place \naccuracy. What do you notice? \nIt can be shown that if the eigenvalues of A are all real and have distinct absolute \nvalues, then the matrices Ak approach an upper triangular matrix U. \n5.  What will be true of the diagonal entries of this matrix U?","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":63465,"to":63517}}}}],[1103,{"pageContent":"values, then the matrices Ak approach an upper triangular matrix U. \n5.  What will be true of the diagonal entries of this matrix U? \n6.  Approximate the eigenvalues of the following matrices by applying the QR \nalgorithm. Use  two-decimal-place accuracy and perform at least five iterations. \n(\na\n) \n[\n� \n�\n] \n(\nb\n) \n[\n�  �\n] \n(\nc\n) \n[ \n� � \n-\n�\n] \n(\nd\n) \n[ \n� \n2 \n-\n�\n] \n-4 0    1 -2  4    2 \n7.  Apply the QR algorithm to the matrix A = \n[ \n2 \nWhy? \n-1 \n3\n]\n.  What happens? \n-2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":63517,"to":63560}}}}],[1104,{"pageContent":"8. Shift  the eigenvalues of the  matrix  in  Problem 7  by replacing A with \nB  = A  + 0.9I. Apply the QR algorithm to B and then shift back by subtracting 0.9 \nfrom the (approximate) eigenvalues of B. Verify that this method approximates the \neigenvalues of A. \n9. Let Q0 =  Q and R 0  = R. First show that \nQ\nO\nQ\nI \n... \nQ\nk\n-\nl\nA\nk \n= AQ0Q\n1 \n... \nQ\nk\n-\n1 \nfor all k 2: 1. Then show that \n[Hint: Repeatedly use the same approach used for the first equation, working from \nthe \"inside out:'] Finally, deduce that \n(\nQ0Q\n1 \n·  ·  · Q\nk\n)(\nR\nk \n·  ·  · R\n1\nR0\n) \nis the QR factorization \nof A\nk\n+\ni\n. \n399","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":63562,"to":63606}}}}],[1105,{"pageContent":"400 \nChapter 5  Orthogonality \nExample 5.16 \nOrthogonal Diagonalizalion ot  svmmetric Matrices \nWe saw in Chapter 4 that a square matrix with real entries will not necessarily have real \n[\no \n-1\n] \neigenvalues. Indeed, the matrix \n1 0 \nhas complex eigenvalues i and - i. We also \ndiscovered that not all square matrices are diagonalizable. The situation changes \ndramatically if we restrict our attention to real symmetric matrices. As we will show \nin this section, all of the eigenvalues of a real symmetric matrix are real, and such a \nmatr  ix is always diagonalizable. \nRecall that a symmetric matrix is one that equals its own transpose. Let's begin by \nstudying the diagonalization process for a symmetric 2 X 2 matrix. \nIf possible, diagonalize the matrix A = \n[ \n1 2] \n. \n2 -2 \nSolulion The characteristic polynomial of A is A \n2 \n+ A -6 = (A + 3 )(A -  2\n)\n, from \nwhich we see thatA  has eigenvalues A1 = -3 and A\n2 \n= 2. Solving for the correspond­\ning eigenvectors, we find \nv\n1 \n= \n[ \n_\n�\n] \nand v\n2 \n= \n[\n�\n]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":63608,"to":63652}}}}],[1106,{"pageContent":"2 \n+ A -6 = (A + 3 )(A -  2\n)\n, from \nwhich we see thatA  has eigenvalues A1 = -3 and A\n2 \n= 2. Solving for the correspond­\ning eigenvectors, we find \nv\n1 \n= \n[ \n_\n�\n] \nand v\n2 \n= \n[\n�\n] \nrespectively. So A   is diagonalizable, and if we set P  = \n[ v\n1 \nv\n2\n], then we know that \np\n-\n1\nAP = \n[\n-\n� \n�\n] \n= D. \nHowever, we can do better. Observe that v1 and v\n2 \nare orthogonal. So, if we nor­\nmalize them to get the unit eigenvectors \nand then take \n[ \nl/\nVs\n] \nU\n1 \n= \n-2/\nVs \nand u\n2 \n= \n[2/\nVs\n] \nl/\nVs \n[ \nl/\nVs \n2/\nVs\n] \nQ =   [u\n1 \nUz\n] \n= \n-2/\nVs \nl/\nVs \nwe have Q\n-\n1\nAQ =  D also. But now Q is an orth ogonal matrix, since {u1, u\n2\n} is an \northonormal set of vectors. Therefore, Q\n-\n1 \n= Q\nT\n,  and we have Q\nT\nAQ = D. (Note that \nchecking is easy, since computing Q\n-\n1 \nonly involves taking a transpose!) \nThe situation in Example 5.16 is the one that interests us. It is important enough \nto warrant a new definition. \nD efi n  iii 0 n \nA square matrix A is orthogonally diagonalizable if there exists an \northogonal matrix Q and a diagonal matrix D such that Q\nT \nAQ = D.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":63652,"to":63750}}}}],[1107,{"pageContent":"to warrant a new definition. \nD efi n  iii 0 n \nA square matrix A is orthogonally diagonalizable if there exists an \northogonal matrix Q and a diagonal matrix D such that Q\nT \nAQ = D. \nWe are in   terested in finding conditions under which a matr  ix is orthogonally \ndiagonalizable. Theorem 5.17 shows us where to look.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":63750,"to":63757}}}}],[1108,{"pageContent":"Theorem 5.11 \nTheorem 5 .18 \nSection 5.4 \nOrthogonal Diagonalization of Symmetric Matrices \n401 \nIf A is orthogonally diagonalizable, then A is symmetric. \nProof If A  is orthogonally diagonalizable, then there exists an  orthogonal  ma­\ntrix Q and a diagonal matrix D such that Q\nT \nAQ = D. Since Q\n-\n1 \n= Q\nT\n, we have Q\nT \nQ = \nI= QQ\nT\n, so \nBut then \nsince every diagonal matrix is symmetric. Hence, A is symmetric. \nRemark Theorem 5.17 shows that the orthogonally diagonalizable matrices are \nall to be found among the symmetric matrices. It does not say that every symmetric \nmatr  ix must be orthogonally diagonalizable. However, it is a remarkable fact that this \nindeed is true! Finding a proof for this amazing result will occupy us for much of the \nrest of this section. \nWe next prove that we  don't need to worry about complex eigenvalues when work­\ning with symmetric matrices with real entries. \nIf A is a real symmetric matrix, then the eigenvalues of A are real.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":63759,"to":63788}}}}],[1109,{"pageContent":"We next prove that we  don't need to worry about complex eigenvalues when work­\ning with symmetric matrices with real entries. \nIf A is a real symmetric matrix, then the eigenvalues of A are real. \nRecall that the complex conjugate of a complex number z = a + bi is the number \nz = a -bi (see Appendix C). To show that z is real, we need to show that b = 0. One \nway to do this is to show that z = z, for then bi = -bi (or 2bi = O\n)\n, from which it \nfollows that b = 0. \nWe can also extend the notion of complex conjugate to vectors and matrices by, \nfor example, defining A to be   the matrix whose entries are the complex conjugates of \nthe entries of A; that is, if A =   [a;), then A = [a\nu\n]. The rules for complex conjugation \nextend easily to matrices; in particular, we have AB = AB for compatible matrices \nA and B. \nProof Suppose that A is an eigenvalue of A with corresponding eigenvector v. Then \n--\nAv = Av, and, taking complex conjugates, we have Av = Av. But then \nAv = Av = Av = Av = Av","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":63788,"to":63807}}}}],[1110,{"pageContent":"A and B. \nProof Suppose that A is an eigenvalue of A with corresponding eigenvector v. Then \n--\nAv = Av, and, taking complex conjugates, we have Av = Av. But then \nAv = Av = Av = Av = Av \nsince A is real. Taking transposes and using the fact that A is symmetric, we have \nv\nT\nA = v\nT\nA\nT \n= \n(\nAv\nf \n= \n(\nAv\nf \n= Av\nr \nTherefore, \nA\n(\nv\nr\nv\n) \n= v\nr\n(\nAv\n) \n= v\nr\n(\nAv\n) \n= \n(\nv\nr\nA\n)\nv = \n(\nA\nv\nr\n)\nv = \nA\n(\nv\nr\nv\n) \nor \n(\nA -\nA\n)(\nv\nr\nv\n) \n= o. \n[a\n,\n� \nb\n,\nil \n_\n_ \n[a\n,\n� \nb\n,\nil \nNow ifv = \n. \n,then v\n-. \n,so \na\nn \n+  b\nn\ni \na\nn \n-  b\nn\ni","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":63807,"to":63903}}}}],[1111,{"pageContent":"402 \nChapter 5  Orthogonality \nTheorem 5.19 \nExample 5.11 \nsince v * 0 (because it is an eigenvector). We  conclude that A - A = 0, or A =   A. \nHence, A is real. \nTheorem 4.20 showed that, for any square matrix, eigenvectors corresponding \nto distinct eigenvalues are linearly independent. For symmetric matrices, something \nstronger is true: Such eigenvectors are orthogonal. \nIf A is a symmetric matrix, then any two eigenvectors corresponding to distinct \neigenvalues of A are orthogonal. \nProof Let v\n1 \nand v\n2 \nbe eigenvectors corresponding to the distinct eigenvalues \nA\n1 \n* \nA\n2 \nso that Av\n1 \n= A\n1\nv\n1 \nand Av\n2 \n= A\n2\nv\n2\n. Using A\nT\n= A  and the fact that x · y = x\nT\ny \nfor any two vectors x and y in !R\nn\n, we have \n(\nv\nf\nA\nT)\nV\nz \n= v\ni\n(\nA\n2\nv\n2\n) \n(\nv\nf \nA\n)\nv\n2 \n= v\nf\n(\nAv\n2\n) \nA\n2\n(\nv\nf\nv\n2\n) \n= A\n2\n(\nv\n1 \n• v\n2\n) \nHence, (A\n1 \n- A\n2\n) (v\n1 \n• v\n2\n) = 0. But A\n1 \n- A\n2 \n* 0, so v\n1 \n• v\n2 \n= 0, as we wished to show. \nVerify the result of Theorem 5.19 for \nSolulion The characteristic polynomial of A is -A\n3 \n+ 6A \n2 \n-9A + 4 =  -(A -  4\n) \n· \n(A -   1)\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":63905,"to":64016}}}}],[1112,{"pageContent":"1 \n• v\n2\n) = 0. But A\n1 \n- A\n2 \n* 0, so v\n1 \n• v\n2 \n= 0, as we wished to show. \nVerify the result of Theorem 5.19 for \nSolulion The characteristic polynomial of A is -A\n3 \n+ 6A \n2 \n-9A + 4 =  -(A -  4\n) \n· \n(A -   1)\n2\n, from which it follows that the  eigenvalues of A are A\n1 \n= 4 and A\n2 \n= 1. The \ncorresponding eigenspaces are \n� \n(Check this.) We easily verify that \nfrom which it follows that every vector in £\n4 \nis orthogonal to every vector in £\n1\n. \n(Why?) \nRoman Note thot \n[\n-\n�] • \n[\n-\ni\n] � !. Thos, 'igenvedo;s conesponding to the \nsame eigenvalue need not be orthogonal.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":64016,"to":64060}}}}],[1113,{"pageContent":"Theorem 5.20 \nSp ectrum is a Latin word meaning \n\"image:' When atoms vibrate, they \nemit light. And when light passes \nthrough a prism, it spreads out � \ninto a spectrum -a band of \nrainbow colors. Vibration \nfrequencies correspond to the \neigenvalues of a certain operator \nand are visible as bright lines in the \nspectrum oflight that is emitted \nfrom a prism. Thus, we can liter-\nally see the eigenvalues of the atom \nin its spectrum, and for this rea-\nson, it is appropriate that the word \nsp ectrum has come to be applied \nto the set of all eigenvalues of a \nmatrix (or operator). \nSection 5.4 \nOrthogonal Diagonalization of Symmetric Matrices \n403 \nWe can now prove the main result of this section. It is called the Spectral Theo­\nrem, since the set of eigenvalues of a matrix is sometimes called the spectrum of the \nmatrix. (Technically, we should call Theorem 5.20 the Real Spectral Theorem, since \nthere is a corresponding result for matrices with complex entries.) \nThe Spectral Theorem","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":64062,"to":64087}}}}],[1114,{"pageContent":"matrix. (Technically, we should call Theorem 5.20 the Real Spectral Theorem, since \nthere is a corresponding result for matrices with complex entries.) \nThe Spectral Theorem \nLet A be an n X n real matrix. Then A is symmetric if and only if it is orthogonally \ndiagonalizable. \nProof We have already proved the \"if\" part as Theorem 5.17. To prove the \"only if\" \nimplication, we proceed by induction on n. For n = 1, there is nothing to do, since a \n1 \nX 1 matr  ix is already in diagonal form. Now assume that every k X k real symmet­\nric matrix with real eigenvalues is orthogonally diagonalizable. Let n = k \n+ \n1 and let \nA be an n X n real symmetric matrix with real eigenvalues. \nLet A\n1 \nbe one of the eigenvalues of A and let v\n1 \nbe a corresponding eigenvector. \nThen v\n1 \nis a real vector (why?)  and we can assume that v\n1 \nis a unit vector, since \notherwise we can normalize it and we will still have an eigenvector corresponding \nto A\n1\n. Using the Gram-Schmidt Process, we can extend v\n1 \nto an orthonormal basis","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":64087,"to":64115}}}}],[1115,{"pageContent":"1 \nis a unit vector, since \notherwise we can normalize it and we will still have an eigenvector corresponding \nto A\n1\n. Using the Gram-Schmidt Process, we can extend v\n1 \nto an orthonormal basis \n{v\n1\n, v\n2\n, ... , v\nn\n} of !R\nn\n. Now we form the matrix \nThen Q\n1 \nis orthogonal, and \nQ\nl \n= \n[v\n1 \nV\nz \n... \nv\nn\nJ \nIn a lecture he delivered at the University of Gottingen in 1905, the German mathematician \nDavid Hilbert (1862- 1943) considered linear operators acting on certain infinite-dimensional \nvector spaces. Out of this lecture arose the notion of a quadratic form in infinitely many \nvariables, and it was in this context that Hilbert first used the term sp ectrum to mean a \ncomplete set of eigenvalues. The spaces in question are now called Hilbert sp aces. \nHilbert made major contributions to many areas of mathematics, among them integral \nequations, number theory, geometry, and the foundations of mathematics. In 1900, at the \nSecond International Congress of Mathematicians in Paris, Hilbert gave an address entitled","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":64115,"to":64153}}}}],[1116,{"pageContent":"equations, number theory, geometry, and the foundations of mathematics. In 1900, at the \nSecond International Congress of Mathematicians in Paris, Hilbert gave an address entitled \n\"The Problems of Mathematics:' In it, he challenged mathematicians to solve 23 problems \nof fundamental importance during the coming century. Many of the problems have been \nsolved-some were proved true, others false-and some may never be solved. Nevertheless, \nHilbert's speech energized the mathematical community and is often regarded as the most \ninfluential speech ever given about mathematics.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":64153,"to":64159}}}}],[1117,{"pageContent":"404 \nChapter 5  Orthogonality \nsince v\nf\n(A\n1\nv\n1\n) = A\n1\n(\nv\nf\nv\n1\n) \n= A\n1\n(v\n1 \n·v\n1\n) = A\n1 \nand v\nf\n(A\n1\nv\n1\n) = A\n1\n(\nv\nf\nv\n1\n) \n= A\n1\n(v;·v\n1\n) = 0 \nfor i  of-1, because {v\n1\n, v\n2\n, •.. , v\nn\n} is an orthonormal set. \nBut \nso B is symmetric. Therefore, B has the block form \n� \nand A\n1 \nis symmetric. Furthermore, Bis similar to A (why?), so the characteristic poly­\nnomial of B is equal to the characteristic polynomial of A, by Theorem 4.22. By \nExercise 39 in Section 4.3, the characteristic polynomial of A\n1 \ndivides the character­\nistic polynomial of A. It follows that the eigenvalues of A \n1 \nare also eigenvalues of A \n..-... \nand, hence, are real. We also see that A\n1 \nhas real entries. (Why?) Thus, A\n1 \nis a k X  k \nreal symmetric matr  ix with real eigenvalues, so the induction hypothesis applies to it. \nHence, there is an orthogonal matrix P\n2 \nsuch that P\ni\nA\n1\nP\n2 \nis a diagonal matrix-say, \nD\n1\n. Now let \nExample 5.18 \nThen Q\n2 \nis an orthogonal \n(\nk  +  l\n)\nX\n(\nk  +  1\n) \nmatr  ix, and therefore so is Q =  Q\n1 \nQ\n2\n. \nConsequently, \nQ\nr\nAQ = \n(\nQ\n1\nQ\n2\nf\nA\n(\nQ\n1\nQ\nz\n) \n= \n(\nQ\nI\nQ\nf\n)\nA\n(\nQ","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":64161,"to":64285}}}}],[1118,{"pageContent":"i\nA\n1\nP\n2 \nis a diagonal matrix-say, \nD\n1\n. Now let \nExample 5.18 \nThen Q\n2 \nis an orthogonal \n(\nk  +  l\n)\nX\n(\nk  +  1\n) \nmatr  ix, and therefore so is Q =  Q\n1 \nQ\n2\n. \nConsequently, \nQ\nr\nAQ = \n(\nQ\n1\nQ\n2\nf\nA\n(\nQ\n1\nQ\nz\n) \n= \n(\nQ\nI\nQ\nf\n)\nA\n(\nQ\n1\nQ\nz\n) \n=  Q\nI\n(\nQ\nf\nAQ\n1\n)\nQ\nz \n=  Q\nI\nBQ\n2 \nwhich is a diagonal matrix. This completes the induction step, and we conclude that, \nfor all n 2 1, an n X n real symmetric matrix with real eigenvalues is orthogonally \ndiagonalizable. \nOrthogonally diagonalize the matrix \nSolution This is the matrix from Example 5.17. We have already found that the \neigenspaces of A are","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":64285,"to":64360}}}}],[1119,{"pageContent":"Section 5.4 \nOrthogonal Diagonalization of Symmetric Matrices \n405 \nWe need three orthonormal eigenvectors. First, we apply the Gram-Schmidt Process to \nto obtain \n[\n-\n�\n] \nand \nr\n-\ni\nl \nn\nl \nand \n[!\n] \nThe new wctm, which h\n.\n, been rn mtructed to be orthogoml to \n[\n-\n� \nl ;, ''ill ;n E\n, \n,._... \n(why?) and '° ;, o,thogonal to \n[\n: l Thu,, we  haw  th'\"  mutually mthogonal \nvectors, and all we need to do is normalize them and construct a matrix Q   with these \nvectors as its columns. We find that \n[1/\nv3 \nQ \n= \nl/\nv3 \nl/\nv3 \nand it is straightforward to verify that \n-1/\nv2 \n0 \nl/\nv2 \n-1/\nv6\n] \n2/\nv6 \n-1/\nv6 \nQ\n'\nAQ \n� \n[\n� \n� \n�\n] \nThe Spectral Theorem allows us to write a real symmetric matrix A in the form \nA = QDQ\nT\n, where Q is orthogonal and D is diagonal. The diagonal entries of D \nare just the eigenvalues of A, and if the columns of Q are the orthonormal vectors \nq\n1\n, ... , \nq\nn\n, then, using the column-row representation of the product, we have \nThis is called the spectral decomposition of A. Each of the terms A;q;qT is a rank 1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":64362,"to":64436}}}}],[1120,{"pageContent":"q\n1\n, ... , \nq\nn\n, then, using the column-row representation of the product, we have \nThis is called the spectral decomposition of A. Each of the terms A;q;qT is a rank 1 \nmatrix, by Exercise 62 in Section 3.5, and q;qT is actually the matrix of the projec­\ntion onto the su  bspace spanned by q;  . (See Exercise 25.) For this reason, the spectral \ndecomposition \nis sometimes referred to as the projection fo rm of the Sp ectral Theorem.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":64436,"to":64446}}}}],[1121,{"pageContent":"406 \nChapter 5  Orthogonality \nExample 5.19 \nExample 5.20 \nFind the spectral decomposition of the matrix A from Example 5.18. \nSolulion \nFrom Example 5.18, we have: \n[ \nl/VJ] \nq\ni\n=   l/\nVJ  , \nl/VJ \nTherefore, \n[-1\n/\\/6\n] \nq\n3 \n= \n2\n/\\/6 \n- 1  /\\/6 \n[1\n/\n3 \nl/ VJ] =   1\n/\n3 \n1/3 \n1/3 \n1/3 \n1/3 \n1\n/\n3] \n1/3 \n1/3 \nso \n[\nt t t\nl \n[ \n! 0 \n=4\nttt\n+ \n00 \nt t t \n-! 0 \nwhich can be easily verified. \n[ \n1\n/\n2 � -1�2] \n-1�2 0 1\n/\n2 \n[ \n1\n/\n6 \n- 1/\n3 \n1\n/\n6 \n- 1/\n3 \n2\n/\n3 \n- 1/\n3 \n1\n/\n6] \n- 1/\n3 \n1\n/6 \nIn this example, ,\\\n2 \n= ,\\\n3\n, so we could combine the last two terms A\n2\nq\n2\nqi + ,\\\n3\nq\n3\nq\nr \nto get \nThe rank 2 matrix \nq\n2\nqi + q\n3\nq\nr is the  matrix of a projection onto the two-dimensional \nsubspace (i.e., the plane) spanned by \nq\ni and \nq\n3\n. (See Exercise 26.) \n4 \nObserve that the  spectral decomposition expresses a symmetric matrix A explic­\nitly in terms of its eigenvalues and eigenvectors. This gives us a way of constructing a \nmatrix with given eigenvalues and (orthonormal) eigenvectors. \nFinda2 X 2matrixwitheigenvalues,\\\n1\n=3and,\\\n2 \n= -2andcorrespondingeigenvectors","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":64448,"to":64566}}}}],[1122,{"pageContent":"Section 5.4 \nOrthogonal Diagonalization of Symmetric Matrices \n401 \nSolution We  begin by normalizing the vectors to obtain an orthonormal basis \n{\nq\n1\n, q\n2\n}, \nwith \nNow, we compute the matrix A  whose spectral decomposition is \nA = A\n1\nq\n1\nq\nf \n+ A\n2\nq\nz\nqi \n= 3\n[\ni\n] \n[\nt \n�\n] \n- 2\n[\n-\ni\n] \n[\n-� \nt\nl \n= 3\n[\n!s \n!1\n] \n[ \n16 \n-\n�\n] \n25 \n- 2 \n25 \n1\n2 \n16 \n1\n2 \n25   25 \n-\n25 \n25 \n= \n[\n-J \n�\n] \n� \nIt is easy to check that A has the desired properties. (Do this.) \nI \nExercises \n5.4 \nOrthogonally diagonalize the matrices in Exercises 1-10 \nby finding an orth ogonal matrix Q and a diagonal \nmatrix D such that Q\nT \nAQ = D. \nI\n.\nA=\n[\n� :\nJ \n[ \n1 V2\n] \n3. \nA= \nV2 O \n2. \nA= \n[\n-\n1 3\n] \n3 \n-\n1 \n4. \nA= \n[ \n9 -2\n] \n-2 \n6 \n5. \nA = \n[ \n� : : \n] \n6. \nA = \n[ � ! �] \n7. \nA= \nu \n: \n-\n�\n] \n8. \nA= \n[\n� \n� \n;\n] \n9. \nA = \n[ \n� \n� \n� \n! \n] \nI \n0. \nA = \n[ \n� \n� \n� \n�: \n11. If b * 0, orthogonally diagonalize A = \n[\na\nb     a\nb\n]\n. \n12. If b  oF  0, orthogonally diogonolire A = \n[ \n� � n \n13. Let A and B be orthogonally diagonalizable n X n \nmatrices and let c be a   scalar. Use the Spectral \nTheorem to prove that the following matrices are","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":64568,"to":64723}}}}],[1123,{"pageContent":"[ \n� � n \n13. Let A and B be orthogonally diagonalizable n X n \nmatrices and let c be a   scalar. Use the Spectral \nTheorem to prove that the following matrices are \northogonally diagonalizable: \n(a) A+ B \n(b) cA \n(c) A\n2 \n14. If A is an invertible matrix that is orthogonally diago­\nnalizable, show that A\n-\n1 \nis orthogonally diagonalizable. \n15. If A and B are orthogonally diagonalizable and AB = \nBA, show that AB is orthogonally diagonalizable. \n16. If A is a symmetric matrix, show that every eigenvalue \nof A is nonnegative if and only if A = B\n2 \nfor some \nsymmetric matr  ix B. \nIn Exercises 17-20, find a spectral decomposition of th e \nmatrix in th e given exercise.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":64723,"to":64746}}}}],[1124,{"pageContent":"408 \nChapter 5  Orthogonality \n17. Exercise 1 \n19. Exercise 5 \n18. Exercise 2 \n20.  Exercise 8 \nIn Exercises 21 and 22, find a symmetric 2 X 2 matrix with \neigenvalues A\n1 \nand A\n2 \nand corresponding orthogonal \neigenvectors v\n1 \nand v\n2\n• \n21. A\n1 \n= -l,A\n2 \n=  2,v\n1 \n= \n[\n�\n]\n,v\n2 \n= \n[ \n_\n�\n] \n22\n.A\n1 \n= \n3, A\n2 \n= \n-\n3,v\n1 \n= \n[\n�\n]\n,v\n2 \n= \n[\n-\n�\n] \nIn Exercises 23 and 24, find a symmetric 3 X 3 matrix with \neigenvalues A\n1\n, A\n2\n, and ,\\\n3 \nand corresponding orthogonal \neigenvectors v\n1\n, v\n2\n, and v\n3\n. \n23 A\n,� \n1,A, \n� \n2, A, \n� \n3,v\n,  � \n[}\n, \n� \n[\n-\n:\n]\n. \nApplications \nQuadratic Forms \n25. \nLet q \nbe a unit vector in !R\nn \nand let W be the subspace \nspanned by \nq\n. Show that the orthogonal projection of a \nvector v onto W (as defined in Sections 1.2 and 5.2) is \ngiven by \nproj\nw\n(\nv\n) \n= \n(\nqq\nT)\nv \nand that the  matrix of this projection is thus \nqq \nT\n. \n[Hint: Remember that, for x and yin !R\nn\n, x · y = x\nT\ny.] \n26. Let {\nq\n1\n, ... , qd be an orthonormal set  of vectors in !R\nn \nand let W be the subspace spanned by this set. \n(a) Show that the matrix of the orthogonal projection","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":64748,"to":64871}}}}],[1125,{"pageContent":"n\n, x · y = x\nT\ny.] \n26. Let {\nq\n1\n, ... , qd be an orthonormal set  of vectors in !R\nn \nand let W be the subspace spanned by this set. \n(a) Show that the matrix of the orthogonal projection \nonto Wis given by \np   = \nq\n1\nq\nf \n+ \n... \n+ q\nk\nq\nl \n(b) Show that the  projection matrix Pin part (a) is \nsymmetric and satisfies P\n2 \n= P. \n(c) Let Q = \n[q\n1 \n· · · \nq\nk\n] be the n X k matrix whose \ncolumns are the orthonormal basis vectors of W. \nShow that P = QQ\nT \nand deduce that rank(P) =  k. \n27. Let A be an n X n real matrix, all of whose eigenvalues \nare real. Prove that there exist an orthogonal matrix Q \nand an upper triangular matrix T such that Q\nT \nAQ = T. \nThis very useful result is known as Schur's Triangular­\nization Theorem. [Hint: Adapt the proof of the Spec­\ntral Theorem.] \n28. Let A be a nilpotent matr  ix (see Exercise 56 in Sec­\ntion 4.2\n)\n. Prove that there is an orthogonal matrix Q \nsuch that Q\nT \nAQ is upper triangular with zeros on its \ndiagonal. [Hint: Use Exercise 27.] \nAn expression of the form \nax\n2 \n+ by\n2 \n+ cxy","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":64871,"to":64930}}}}],[1126,{"pageContent":"tion 4.2\n)\n. Prove that there is an orthogonal matrix Q \nsuch that Q\nT \nAQ is upper triangular with zeros on its \ndiagonal. [Hint: Use Exercise 27.] \nAn expression of the form \nax\n2 \n+ by\n2 \n+ cxy \nis called a quadratic fo rm in x and y. Similarly, \nax\n2 \n+ by\n2 \n+ cz\n2 \n+ dxy + exz + fyz \nis a quadratic form in x, y, and z. In words, a  quadratic form is a sum of terms, each of \nwhich has total degree two in the variables. Therefore, 5x\n2 \n-3y\n2 \n+ 2xy is a quadratic \nform, but x\n2 \n+ y\n2 \n+ xis not. \nWe can represent quadratic forms using matrices as follows: \nax\n2 \n+ by\n2 \n+ cxy  = [x y]\n[\nc\n;\n2 \nc\n�\n2\nJ\n[\n;\nJ","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":64930,"to":64978}}}}],[1127,{"pageContent":"and \nSection 5.5 Applications \n409 \nax\n2 \n+ by\n2 \n+ cz\n2 \n+ dxy + exz + fyz \n= \n[x y z] [d�2 \ne/2 \nd/2 e/2] \n[\nx] \nb f/2 y \nf/2 c z \n� \n(Verify these.) Each has the form x\nT \nAx, where the matrix A is symmetric. This obser­\nvation leads us to   the following general definition. \nExample 5.21 \nExample 5.22 \nDefinition \nA quadratic fo rm in n variables is a function f: !R\nn \n---+ IR of the \nform \nwhere A is a symmetric n X n matrix and xis in !R\nn\n. We refer to A as the matrix \nassociated with f \nWhat is the quadratic form with associated matrix A = \nSolution \nIf x = \n[ \n:\nJ then \n[ \n2 -3\n]\n? \n-3 \n5 \nObserve that the off-diagonal entries a\n1\n2 \n= \na\n2\n1 \n= -3 of A are combined to give \nthe coefficient -6 of x\n1\nx\n2\n• This is true generally. We can exp and a quadratic form in \nn variables x\nT \nAx as follows: \nx\nT\nAx = \na\n11\nx\n� \n+ a\n22\nx\n� \n+ \n· · · \n+ a\nnn\nx\n� \n+ 2: 2a\n;\nj\nx\ni\nx\nj \ni\n<\nj \nThus, if i * j, the coefficient of \nX\n;\nX\nj \nis 2a\nij\n. \nFind the matrix associated with the quadratic form \nf \n(x\n1\n, x\n2\n, x\n3\n) = \n2x\n� \n-  x\ni \n+ 5x\nf \n+ 6x\n1\nx\n2 \n-3x\n1\nx\n3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":64980,"to":65099}}}}],[1128,{"pageContent":"a\n11\nx\n� \n+ a\n22\nx\n� \n+ \n· · · \n+ a\nnn\nx\n� \n+ 2: 2a\n;\nj\nx\ni\nx\nj \ni\n<\nj \nThus, if i * j, the coefficient of \nX\n;\nX\nj \nis 2a\nij\n. \nFind the matrix associated with the quadratic form \nf \n(x\n1\n, x\n2\n, x\n3\n) = \n2x\n� \n-  x\ni \n+ 5x\nf \n+ 6x\n1\nx\n2 \n-3x\n1\nx\n3 \nSolution The coefficients of the squared terms x;2 go on the diagonal as a;;, and the \ncoefficients of the cross-product terms x;x\nj \nare split between a\nij \nand a\nj\ni\n· \nThis gives \n3 \n-1 \n0 \n-�]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":65099,"to":65167}}}}],[1129,{"pageContent":"410 \nChapter 5  Orthogonality \nso \nf\n(x\n,.\nx\n,\n,x\n,\n) \n� \n[x\n, \nx\n, \nx\n,\nJ\n[ \n_\n� \n-\n! \n-\n�\nJ\n[\n:\n:\nl \nas you can easily check. \nIn the case of a quadratic form f\n(\nx, y) in   two variables, the graph of z = j\n(\nx, y) is \na surface in IR\n3\n. Some examples are shown in Figure 5.12. \nObserve that the effect of holding x or y constant is to  take a cross section of \nthe graph parallel to the yz or xz planes, respectively. For the graphs in Figure 5.12, \nall of these cross sections are easy to identify.  For  example, in Figure 5.12(a), the \ncross sections we get by holding x or y constant are all parabolas opening upward, \nso f\n(\nx, y) 2 0 for all values of x and y. In Figure 5.12(c), holding x constant gives \nparabolas opening downward and holdingy constant gives parabolas opening upward, \nproducing a saddle point. \nz \nz \nx \ny \ny \nx \n(\na\n)  z \n= \n2\nx\n2 \n+ \n3\ny\n2 \n(b) \nz \n= \n-\n2\nx\n2  - 3\ny\n2 \nz \nz \ny \nx \n(\nc\n) \nz \n= \n2\nx\n2  - 3\ny\n2 \n(d) \nz \n= \n2\nx\n2 \nFigure 5.12 \nGraphs of quadratic forms f (x, y)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":65169,"to":65265}}}}],[1130,{"pageContent":"Theorem 5.21 \nSection 5.5 Applications \n411 \nWhat makes this type of analysis quite easy is the fact that these quadratic forms \nhave no cross-product terms. The matrix associated with such a quadratic form is a \ndiagonal matrix. For example, \n2x\n2\n-3y\n2\n= [x y]\n[\n� \n-\n�\nJ\n[;\nJ \nIn general, the matrix of a quadratic form is a symmetric matrix, and we saw in Sec­\ntion 5.4 that such matrices can always be diagonalized. We will now use this fact to \nshow that, for every quadratic form, we can eliminate the cross-product terms by \nmeans of a suitable change of variable. \nLet f\n(\nx\n) \n=  x\nT \nAx be a quadratic form in n variables, with A a symmetric n X n \nmatrix. By the Spectral Theorem, there is an orthogonal matrix Q that diagonalizes A; \nthat is, Q\nT \nAQ = D, where Dis a  diagonal matr  ix displaying the eigenvalues of A. We \nnow set \nX \n=  Qy Or, equivalently, y =  Q\n-\n1\nX \n=  Q\nT\nX \nSubstitution into the quadratic form yields \nx\nT\nAx =  (QyfA(\nQy\n) \n= \ny\nT\nQ\nT\nAQy \n= \ny\nT\nD\ny","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":65267,"to":65325}}}}],[1131,{"pageContent":"now set \nX \n=  Qy Or, equivalently, y =  Q\n-\n1\nX \n=  Q\nT\nX \nSubstitution into the quadratic form yields \nx\nT\nAx =  (QyfA(\nQy\n) \n= \ny\nT\nQ\nT\nAQy \n= \ny\nT\nD\ny \nwhich is a quadratic form without cross-product terms, since Dis diagonal. Further­\nmore, \nif the eigenvalues of A are A 1 , ... , A\nn\n, then Q can be chosen so that \nIfy = \n[y\nl \nbecomes \nY\nn\n] \nT\n, then, with respect to these new variables, the quadratic form \ny\nT\nDy = \nA\n1Y1\n2 \n+ \n· · \n. + A\nn\ny\n; \nThis process is called diagonalizing a quadratic fo rm. We have just proved the fol­\nlowing theorem, known as the Principal Axes Theorem. (The reason for this name \nwill become clear in the next subsection.) \nThe Principal Axes Theorem \nEvery quadratic form can be diagonalized. Specifically, if A is then X n symmet­\nric matr  ix associated with the quadratic form x\nT \nAx and if Q is an orthogonal \nmatrix such that Q\nT \nAQ = D is a diagonal matrix, then the change of variable \nx = Qy transforms the quadratic form x\nT \nAx into  the quadratic form y\nT\nD y,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":65325,"to":65392}}}}],[1132,{"pageContent":"T \nAx and if Q is an orthogonal \nmatrix such that Q\nT \nAQ = D is a diagonal matrix, then the change of variable \nx = Qy transforms the quadratic form x\nT \nAx into  the quadratic form y\nT\nD y, \nwhich has  no cross-product terms. If the  eigenvalues of A are A\n1 , •.. , A\nn \nand \ny \n= [y\n1 \nY\nn \nf\n, \nthen \nx\nT\nAx \n=  y\nT\nDy \n=  A\n1\ny\n� \n+ \n· · · \n+ \nA\nn\ny\n;","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":65392,"to":65430}}}}],[1133,{"pageContent":"412 \nChapter 5  Orthogonality \nExample 5.23 \nFind a change of variable that transforms the quadratic form \nf (x\n1\n, x\n2\n)  = \n5x\nf \n+ 4x\n1\nx\n2 \n+ 2x\n� \ninto one with no cross-product terms. \nSolulion The matrix off is \nA  = \n[\n� \n�\n] \nwith eigenvalues A\n1 \n= 6 and A\n2 \n= 1. Corresponding unit eigenvectors are \nq -\n[\n2\n/Vs\n] \nand q\n2 \n= \n[ \nl\n/Vs\n] \nI \n-\nl\n/Vs \n-\n2\n/Vs \n� \n(Check this.) Ifwe set \n= \n[\n2\n/Vs \nl\n/Vs\n] \nand D \n= \n[\n6 \no\n1\n] \nQ \nl\n/Vs \n-\n2\n/Vs \n0 \nthen Q\nT \nAQ = D. The change of variable x = Qy, where \nx = \n[\n::] and y \n= \n[\n;\n:] \nconverts f into \nf\n(\ny\n) \n= f\n(\ny\n1\n, y\ni\n} \n= \n[\nY\n1 \nY2 \nl [ \n� \n�\n] \n[;\n:] = \n6y\nf \n+ \nY\ni \nThe original quadratic form x\nT \nAx and the new one y\nT \nD y (referred to in the Princi­\npal Axes Theorem) are equal in the following sense. In Example 5.23, suppose we want \nto evaluate f(x) = x\nT \nAx at x = \n[ \n-\n�\n]\n. We have \nj\n(\n-\n1,3\n) \n= \n5\n(\n-\n1\n)\n2 \n+  4\n(\n-\n1\n)(\n3\n) \n+ 2\n(\n3\n)\n2 \n= 11 \nIn terms of the new variables, \n[\nY1\n] \n= \n= \nT\nX\n= \n[\n2\n/Vs \nl\n/Vs\n] \n[\n-\n1\n] \n= \n[ \nl\n/Vs\n] \ny\n2 \ny Q \nl\n/Vs \n-\n2\n/Vs \n3 -7 \n/Vs \nso \nf(\ny\nl\n, y\ni\n} \n= 6y\nf \n+ y� \n= \n6\n(\n1\n/Vs\n)2 \n+ \n(\n-7 \n/Vs\n)2 \n= \n55/5 \n= \n11 \nexactly as before.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":65432,"to":65639}}}}],[1134,{"pageContent":"(\n-\n1\n)(\n3\n) \n+ 2\n(\n3\n)\n2 \n= 11 \nIn terms of the new variables, \n[\nY1\n] \n= \n= \nT\nX\n= \n[\n2\n/Vs \nl\n/Vs\n] \n[\n-\n1\n] \n= \n[ \nl\n/Vs\n] \ny\n2 \ny Q \nl\n/Vs \n-\n2\n/Vs \n3 -7 \n/Vs \nso \nf(\ny\nl\n, y\ni\n} \n= 6y\nf \n+ y� \n= \n6\n(\n1\n/Vs\n)2 \n+ \n(\n-7 \n/Vs\n)2 \n= \n55/5 \n= \n11 \nexactly as before. \nThe Principal Axes Theorem has some interesting and important consequences. \nWe will consider two of these. The first relates to the possible values that a  quadratic \nform can take on. \nDefinition A quadratic form f\n(\nx\n) \n= x\nT\nAx is classified as one of the following: \n1. positive de.finite if f\n(\nx\n) > 0 \nfor all x \n-=fa \n0 \n2. positive semidefinite if f\n(\nx\n) \n::=:: 0 for all x \n3. negative de.finite if j\n(\nx) < 0 for all x \n-=fa \n0 \n4. negative semidefinite if f(x) :s 0 for all x \n5\n. indefinite if j\n(\nx\n) \ntakes on both positive and negative values","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":65639,"to":65744}}}}],[1135,{"pageContent":"Theorem 5.22 \nExample 5.24 \nSection 5.5 Applications \n413 \nA symmetric matrix A is called positive definite,  positive semidefinite,  nega­\ntive definite,  negative semidefinite, or indefinite if the associated quadratic form \nf(x) = x\nT \nAx has the corresponding property. \nThe quadratic forms in parts (a), (b ), (c), and (d) of Figure 5.12 are positive definite, \nnegative definite, indefinite, and positive semidefinite, respectively. The Principal Axes \nTheorem makes it easy to tell if a quadratic form has one of these properties. \nLet A be an n X n symmetric matrix. The quadratic form f(x) = x\nT \nAx is \na.  positive definite if and only if all of the eigenvalues of A are positive. \nb.  positive semidefinite if and only if all of the eigenvalues of A are nonnegative. \nc.  negative definite if and only if all of the eigenvalues of A are negative. \nd.  negative semidefinite if and only if all of the eigenvalues of A are non positive.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":65746,"to":65764}}}}],[1136,{"pageContent":"c.  negative definite if and only if all of the eigenvalues of A are negative. \nd.  negative semidefinite if and only if all of the eigenvalues of A are non positive. \ne.  indefinite if and only if A has both positive and negative eigenvalues. \nYou are asked to prove Theorem 5.22 in Exercise 27. \nClassify f(x, y, z) = 3.x\n2 \n+ 3y\n2 \n+ 3z\n2 \n- 2xy - 2xz - 2yz as positive definite, negative \ndefinite, indefinite, or none of these. \nSolution \nThe matrix associated with f is \n[\n-\n� \n-\n� \n=\n�\n] \n-1  -1 \n3 \nwhich has eigenvalues 1, 4, and 4. (Verify this.) Since all of these eigenvalues are posi­\ntive,f is a positive definite quadratic form. \nIf a quadratic form f(x) =  x\nT \nAx is positive definite, then, since f(O) = 0, the \nminimum value of f(x) is 0 and it occurs at the origin. Similarly, a negative definite \nquadratic form has a maximum at the origin. Thus, Theorem 5.22 allows us to solve \ncertain types of maxima/minima problems easily, without resorting to calculus. A type","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":65764,"to":65795}}}}],[1137,{"pageContent":"quadratic form has a maximum at the origin. Thus, Theorem 5.22 allows us to solve \ncertain types of maxima/minima problems easily, without resorting to calculus. A type \nof problem that falls into this category is the constrained optimization problem. \nIt is often important to know the maximum or minimum values of a quadratic \nform subject to certain constraints. (Such problems arise not only in mathematics \nbut also in statistics, physics, engineering, and economics.) We will be interested in \nfinding the extreme values of f(x) \n= \nx\nT \nAx subject to the constraint that \nII \nx\nii \n= \n1. \nIn the case of a quadratic form in two variables, we can visualize what the problem \nmeans. The graph of z = f(x, y) is a surface in IR\n3\n, and the constraint \nll\nx\nll \n= 1 restricts \nthe point (x, y) to the unit circle in the xy-plane. Thus, we are considering those \npoints that lie simultaneously on the surface and on the unit cylinder perpendicular","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":65795,"to":65820}}}}],[1138,{"pageContent":"ll\nx\nll \n= 1 restricts \nthe point (x, y) to the unit circle in the xy-plane. Thus, we are considering those \npoints that lie simultaneously on the surface and on the unit cylinder perpendicular \nto the xy plane. These points form a curve  lying on the surface, and we want the high­\nest and lowest points on this curve. Figure 5.13 shows this situation for the quadratic \nform and corresponding surface in Figure 5.12(c).","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":65820,"to":65828}}}}],[1139,{"pageContent":"414 \nChapter 5  Orthogonality \nTheorem 5.23 \nz \nFigure 5.13 \nThe intersection of z  = 2x\n2 \n- 3y\n2 \nwith the \ncylinder x\n2 \n+ y\n2 \n= 1 \ny \nIn this case, the maximum and minimum values of f(x, y) = 2x\n2 \n- 3y\n2 \n(the high­\nest and lowest points on the curve of intersection) are 2 and -3, respectively, which \nare just the eigenvalues of the associated matr  ix. Theorem 5.23 shows that this is \nalways the case. \nLet f(x) = x\nT \nAx be a quadratic form with associated n X n symmetric matrix A. \nLet the eigenvalues of A be A\n1 \n:::::: A\n2\n:::::: ···:::::: Aw Then the following are true, subject \nto the constraint II xi\ni \n= 1: \na. A\ni \n2: f(x) 2: A\nn \nb.  The maximum value of f(x) is A \n1\n, and it occurs when x is a unit eigenvector \ncorresponding to A\n1\n. \nc.  The minimum value of f(x) is A\nn\n, and it occurs when x is a unit eigenvector \ncorresponding to Aw \nProof As usual, we begin by orthogonally diagonalizing A. Accordingly, let Q be an \northogonal matrix such that Q\nT \nAQ is the  diagonal matrix","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":65830,"to":65882}}}}],[1140,{"pageContent":"corresponding to Aw \nProof As usual, we begin by orthogonally diagonalizing A. Accordingly, let Q be an \northogonal matrix such that Q\nT \nAQ is the  diagonal matrix \nThen, by the Principal Axes Theorem, the change of variable x = Qy gives x\nT \nAx = \ny\nT\nDy. Now note that y = Q\nT\nx implies that \nsince Q\nT\n=  Q\n-\n1\n. Hence, using x · x = x\nT\nx, we see that \nllrll \n= \nWr \n= \n� \n= \nll\nx\nll \n= 1. Thus, if xis a unit vector, so  is the corresponding y, and the values of x\nT \nAx \nand y\nT\nDy are the same.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":65882,"to":65917}}}}],[1141,{"pageContent":"Example 5.25 \nSection 5.5 Applications \n415 \n(a) To prove property (a), we observe that ify = \n[y\n1 \n· ·\n· \nY\nn\n]\nT\n, then \nf \n(\nx\n) \n= x\nT\nAx = y\nT\nD y \n= \nA1\nYl \n+ A\nz\ny\ni \n+ \n· · · \n+ A\nn\ny\n; \n:::; \nA\n1\ny\nf \n+ \nA1\nY\ni \n+ \n· · · \n+ A\n1\ny\n; \n= A\n1 \n(\ny\nf \n+ \nY\ni \n+ \n· · · \n+ y\n;\n) \n= A\n1llrll\n2 \n= A\n1 \nThus, f(x) s A\n1 \nfor all x such that \nll\nx\nll \n= 1. The proof that f(x)  2: \nA\nn \nis similar. \n(See Exercise 37.) \n(b) If q\n1 \nis a unit eigenvector corresponding to A\n1\n, then Aq\n1 \n= A\n1\nq\n1 \nand \nf\n(\nq\n1\n) \n= \nq\nf\nA\nq\n1 \n= \nq\nf \nA\n1\nQ\n1 \n= A\n1\n(\nq\nf\nq\n1\n) \n= A\n1 \nThis shows that the  quadratic form actually takes on the value A\n1\n, and so, by prop­\nerty (a), it is the maximum value of f(x) and it occurs when x = \nq\n1\n. \n(c) You are asked to prove this property in Exercise 38. \nFind the maximum and minimum values of the quadratic form f(x\n1\n, x\n2\n) = 5xi + \n4x\n1\nx\n2 \n+ 2xi subject to the constraint xi + xi = 1, and determine values of x\n1 \nand x\n2 \nfor which each of these occurs. \nSolution In Example 5.23, we found thatfhas the associated eigenvalues A\n1 \n= 6 and \nA\n2 \n= 1, with corresponding unit eigenvectors \n_ \n[\n2\n/Vs\n] and","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":65919,"to":66069}}}}],[1142,{"pageContent":"1 \nand x\n2 \nfor which each of these occurs. \nSolution In Example 5.23, we found thatfhas the associated eigenvalues A\n1 \n= 6 and \nA\n2 \n= 1, with corresponding unit eigenvectors \n_ \n[\n2\n/Vs\n] and \n_ \n[ \n1 /Vs\n] \nQi \n-\n1 /Vs \nQ\nz -\n-\n2\n/Vs \nTherefore, the maximum value off is 6 when x\n1 \n= 2\n/Vs \nand x\n2 \n= 1 /Vs. The mini­\nmum value off is 1 when x\n1 \n= 1\n/Vs \nand x\n2 \n= \n-\n2\n/Vs. \n(Obser ve that these extreme \nvalues occur twice-in opposite directions-since -q\n1 \nand -q\n2 \nare also unit eigen­\nvectors for A\n1 \nand A\n2\n, respectively.) \nGraphing Quadratic Equations \nThe general form of a quadratic equation in two variables x and y is \nax\n2 \n+ by\n2 \n+ cxy + dx \n+ \ney  + f = 0 \nwhere at least one of a, b, and c    is nonzero. The graphs of such quadratic equations are \ncalled conic sections (or conics), since they can be obtained by taking cross sections \nof a (double) cone (i.e., slicing it with a plane). The most important of the conic sec­\ntions are the ellipses (with circles as a special case), hyperbolas, and parabolas. These","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":66069,"to":66136}}}}],[1143,{"pageContent":"of a (double) cone (i.e., slicing it with a plane). The most important of the conic sec­\ntions are the ellipses (with circles as a special case), hyperbolas, and parabolas. These \nare called the nondegenerate conics. Figure 5.14 shows how they arise. \nIt is also possible for a cross section of a cone to result in a single point, a straight \nline, or a pair of lines. These are called degenerate conics. (See Exercises 59-64.) \nThe graph of a nondegenerate conic is said to be in standard position relative to \nthe coordinate axes if its equation can be expressed in one of the forms in Figure 5.15.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":66136,"to":66142}}}}],[1144,{"pageContent":"416 \nChapter 5  Orthogonality \ny \nb \n-b \na>b \ny \ny \n= ax\n2\n, \na> 0 \nFigure 5.15 \nCircle \nEllipse \nPa  rabola \nFigure 5.14 \nThe nondegenerate conics \nx\n2 \ny\n2 \nEllipse or Circle: 2 + 2 = l; \na\n, b  >  0 \na \nb \ny \nx\n2 \n_\n.i_\n_ \na\n2 \nb\n2 \n-\n1\n, \na\n, b > \n0 \ny \ny \n= ax\n2\n, \na< 0 \ny \nb \n-b \na<b \nHyperbola \ny \nb \n-b \nr\n_ \nx\n2 \n-\nb\n2 \na\n2 \n- 1\n, a\n, b > \n0 \nParabola \ny \nx = a\ny\n2\n, \na> 0 \ny \na \n-a \na=b \nNondegenerate conics in standard position \nH\ny\nperbola \ny \nx = a\ny\n2\n, \na< 0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":66144,"to":66234}}}}],[1145,{"pageContent":"Example 5.26 \nExample 5.21 \nSection 5.5 Applications \n411 \nIf possible, write each of the following quadratic equations in the form of a conic in \nstandard position and identify the resulting graph. \n(a) 4x\n2 \n+ 9y\n2 \n= 36 \n(b) 4x\n2 \n- 9y\n2 \n+ 1 = 0 \n(c) 4x\n2 \n- 9y = 0 \nSolulion (a)  The equation 4x\n2 \n+ 9y\n2 \n= 36 can be  written in the form \nx\n2 \ny\nz \n-+-= 1 \n9    4 \nso its   graph is an ellipse intersecting the x-axis at (±3, 0) and the y-axis at \n(\nO, ±2). \n(b) The equation 4x\n2 \n- 9y\n2 \n+ 1 = 0 can be written in the form \ny\n2 \nx\n2 \n1\n-\n1\n= 1 \n9 \n4 \nso its graph is a hyperbola, opening up and down, intersecting the y-axis at \n(\nO, ±\nt\n). \n(c) The equation 4x\n2 \n- 9y = 0 can be written in the form \n4 \ny = -x\n2 \n9 \nso its graph is a parabola opening upward. \nIf a quadratic equation contains too many terms to be written in one of the forms \nin Figure 5.15, then its graph is not in standard position. When there are additional \nterms but no xy term, the graph of the conic has been translated out of standard \nposition.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":66236,"to":66300}}}}],[1146,{"pageContent":"in Figure 5.15, then its graph is not in standard position. When there are additional \nterms but no xy term, the graph of the conic has been translated out of standard \nposition. \nIdentify and graph the conic whose equation is \nx\n2 \n+ 2y\n2 \n- 6x + Sy + 9 \n= \n0 \nSolulion \nWe begin by grouping the x and y terms separately to get \n(\nx\n2 \n-\n6x\n) \n+ \n(\n2y\n2 \n+ Sy\n) \n= \n-9 \nor \n(\nx\n2 \n- 6x\n) \n+ 2\n(\ny\n2 \n+ 4y\n) \n= \n-9 \nNext, we complete the squares on the two expressions in parentheses to obtain \n(\nx\n2 \n- 6x + 9\n) \n+ 2\n(\ny\n2 \n+ 4y + 4\n) \n= -9 + 9 +  S \nor \n(\nx - 3\n)\n2 \n+ 2\n(\ny + 2\n)\n2 \n= S \nWe now make the substitutions x' = x -3 and y' = y + 2, turning the above equa­\ntion into \n(\nx'\n)\n2 \n+ 2\n(\ny'\n)\n2 \n= S \n(\nx'\n)\n2 \n(\ny'\n)\n2 \nor \n--+ --=I \ns \n4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":66300,"to":66388}}}}],[1147,{"pageContent":"418 \nChapter 5  Orthogonality \nExample 5.28 \nThis is the equation of an ellipse in standard position in the x' y'  coordinate sy  stem, \nintersecting the x' -axis at (±2\\/2, O\n) \nand the y'-axis at \n(\nO, ±2). The origin in the x'y' \ncoordinate system is at x = 3, y = -2, so the ellipse has been translated out of stan­\ndard position 3 units to the right and 2 units down. Its graph is shown in Figure 5.16. \ny   y\n' \n2 \nx \n-2 \n-2 \nx\n' \n-4 \nFigure 5.16 \nA translated ellipse \nIf a quadratic equation contains a cross-product term, then it represents a conic \nthat has been rotated. \nIdentify and graph the conic whose equation is \n5x\n2 \n+ 4xy + 2y\n2 \n= 6 \nSolulion The left-hand side of the equation is a quadratic form, so we   can write it in \nmatr  ix form as x\nT \nAx = 6, where \nA  = \n[\n� \n�\n] \nIn Example 5.23, we found that the  eigenvalues of A are 6 and 1, and a matrix Q  that \northogonally diagonalizes A is \n[\n2/Vs \nQ = \nl/Vs \nl/Vs] \n-2/Vs \nObserve that <let Q = -1. In this example, we will interchange the columns of this","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":66390,"to":66437}}}}],[1148,{"pageContent":"orthogonally diagonalizes A is \n[\n2/Vs \nQ = \nl/Vs \nl/Vs] \n-2/Vs \nObserve that <let Q = -1. In this example, we will interchange the columns of this \nmatrix to make the determinant equal to + 1. Then Q will be the matrix of a rotation, \nby Exercise 28 in Section 5.1. It is always possible to rearrange the columns of an \n........... \northogonal matr  ix Q   to make its determinant equal to + 1. (Why?) We set \ninstead, so that \n[ \nl/Vs 2/Vs\n] \nQ \n= \n-2/Vs l/Vs","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":66437,"to":66455}}}}],[1149,{"pageContent":"y \n3 \nx\n' \nFigure 5.11 \nA rotated ellipse \nExample 5.29 \nSection 5.5 Applications \n419 \nThe change ofvariablex =  Qx' converts the given equation into the form (x')\nT\nDx' = 6 \nby means of a rotation. If x'  = \n[\n;:\n]\n, then this equation is just \n(x')\n2 \n+ 6(y'\n)\n2 \n= 6 \nor \n(x')\n2 \n+ \n(\ny '\n)\n2 \n=  1 \n6 \nwhich represents an ellipse in the x' y ' coordinate system. \nTo graph this ellipse, we need to know which vectors play the roles of e; = \n[ \n�\n] \nand e� = \n[ \n�] in the new coordinate system. (These two vectors locate the positions \nof the x' and y' axes.) But, from x =   Qx', we  have \nQe; = \n[ \nl\n/Vs \n2\n/Vs\n] \n[\nl\n] \n-\n2\n/Vs \nl\n/Vs \n0 \n[ \nl\n/Vs\n] \n-\n2\n/Vs \nand \n1 \n[ \nl\n/Vs \n2\n/Vs\n] \n[\nQ\n] \n[\n2\n/Vs\n] \nQe\nz \n= \n-\n2\n/Vs \nl\n/Vs \n1 \n= \nl\n/Vs \nThese are just the columns q1 and q\n2 \nof Q,  which are the eigenvectors of A! The fact \nthat these are orthonormal vectors agrees perfectly with the fact that the change of \nvariable is just a rotation. The graph is sh own in Figure 5.17. \n4 \nYou can now see why the Principal Axes Theorem is   so named. If a real  symmet­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":66457,"to":66555}}}}],[1150,{"pageContent":"variable is just a rotation. The graph is sh own in Figure 5.17. \n4 \nYou can now see why the Principal Axes Theorem is   so named. If a real  symmet­\nric matrix A  arises as the coefficient matrix of a quadratic equation, the eigenvectors \nof A give the directions of the principal axes of the corresponding graph. \nIt is possible for the graph of a conic to be both rotated and translated out of stan­\ndard position, as illustrated in Example 5.29. \nIdentify and graph the conic whose equation is \n2 2 \n2\n8 \n4 \n5x \n+ 4xy + 2y \n-\n-x \n-\n-y + 4 = 0 \nVs    Vs \nSolution The strategy is to eliminate the cross-product term first. In matr  ix form, \nthe equation is x\nT\nAx +Bx+ 4 = 0, where \nA = \n[ \n� \n�\n] \nand  B = \n[ \n-\n� \n-\n�\n] \nThe cross-prod uct term comes from the quadratic form x\nT \nAx, which we diagonalize \nas in Example \n5\n.28 by setting x =   Qx', where \nThen, as in   Example \n5\n.28, \n[ \nl\n/Vs \n2\n/Vs\n] \nQ = \n-\n2\n/Vs \nl\n/Vs \nx\nT\nAx = \n(\nx'\nf\nDx' = (x'\n)\n2 \n+ 6(y'\n)\n2 \nBut now we also have \nI \n[ \n28 \n4 \n] \n[ \nl\n/Vs \n2\n/Vs\n] \n[\nx'\n] \nBx\n= BQx \n= \n-\nVs \n-\nVs \n-\n2\n/Vs \nl\n/Vs \ny\n' \n= \n-\n4x\n'","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":66555,"to":66655}}}}],[1151,{"pageContent":"Then, as in   Example \n5\n.28, \n[ \nl\n/Vs \n2\n/Vs\n] \nQ = \n-\n2\n/Vs \nl\n/Vs \nx\nT\nAx = \n(\nx'\nf\nDx' = (x'\n)\n2 \n+ 6(y'\n)\n2 \nBut now we also have \nI \n[ \n28 \n4 \n] \n[ \nl\n/Vs \n2\n/Vs\n] \n[\nx'\n] \nBx\n= BQx \n= \n-\nVs \n-\nVs \n-\n2\n/Vs \nl\n/Vs \ny\n' \n= \n-\n4x\n' \n-\n12y\n'","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":66655,"to":66717}}}}],[1152,{"pageContent":"420 \nChapter 5  Orthogonality \ny \n\\ \n2 \ny\n' \nx \n-2 \n-1 \ny\n\" \n-1 \n-2 \n� \n-4 \nx\n\" \nx\n' \nFigure 5.18 \nThus, in terms of x' and y', the given equation becomes \n(x')\n2 \n+ \n6(y'\n)\n2 \n-4x'  -12y' +  4  =  0 \nTo  bring the  conic represented by this equation into standard position, we need \nto translate the x'y' axes. We  do so by completing the squares, as in Example 5.27. \nWe have \nor \n((\nx'\n)\n2 \n-4x'  +  4\n) \n+ \n6((y'\n)\n2 \n-2y'  + \n1 )  = -4 \n+  4 \n+ \n6  =  6 \n(\nx' -  2 )\n2 \n+ \n6(y' -   1 )\n2 \n=  6 \nThis gives us the translation equations \nx\n\" \n= x' -  2 \nand y\n\" \n= y '  -\nIn the x\n\"\ny\n\" \ncoordinate system, the equation is simply \n(x\n\"\n)\n2 \n+ 6(\ny\n\"\n)\n2 \n=  6 \nwhich is the equation of an ellipse (as   in Example 5.28\n)\n. We can sketch this ellipse by \nfirst rotating and then translating. The resulting graph is shown in Figure 5.18. 4 \nThe general form of a quadratic equation in three variables x, y, and z is \nax\n2 \n+  by\n2 \n+ cz\n2 \n+  dxy +  exz  + fyz + gx +  hy  +  iz  + j  =  0 \nwhere at least one of a, b, ... ,f is nonzero. The graph of such a quadratic equation","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":66719,"to":66808}}}}],[1153,{"pageContent":"ax\n2 \n+  by\n2 \n+ cz\n2 \n+  dxy +  exz  + fyz + gx +  hy  +  iz  + j  =  0 \nwhere at least one of a, b, ... ,f is nonzero. The graph of such a quadratic equation \nis called a quadric surfa ce (or quadric). Once again, to recognize a quadric we need","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":66808,"to":66816}}}}],[1154,{"pageContent":"x \nx\n2 \ny\n2 \nz\n2 \nEllip\nsoid: \n:::2 \n+ \nb2 \n+ \n:::2 \n= \n1 \na \nc \nz \ny \nSection 5.5 Applications \n421 \nx\n2 \ny\n2 \nz\n2 \nH\ny\nperboloid of one sheet: --,, \n+ \nb2 \n-\n:::2 \n= I \na\n\"' \nc \nz \nx\n2 \ny\n2 \nz\n2 \nH\ny\nperboloid \nof two \nsheets\n: \n-;;, \n-+--t\n2 \n-\nc\n2 \n= \n-\nI \nx\n2 \ny\n2 \nElliptic cone: \nz\n2 \n= \n-;;, \n+ \nb2 \nz \nx \nx\n2 \ny\n2 \nElliptic paraboloid: \nz = \n-;;, + \nb\n2 \nz \nx \nFigure 5.19 \nQuadric surfaces \ny \nz \ny \ny \nx \nx\n2 \ny\n2 \nH\ny\nperbolic paraboloid: \nz = \n-;;,  - p \nz","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":66818,"to":66919}}}}],[1155,{"pageContent":"422 \nChapter 5  Orthogonality \nExample 5.30 \nto put it into standard position. Some quadrics in standard position are shown in \nFigure 5.19; others are obtained by permuting the variables. \nIdentify the quadric surface whose equation is \n5x\n2 \n+ lly\n2 \n+ 2z\n2 \n+ 16xy + 20xz -4yz = 36 \nSolulion \nThe equation can be written in matrix form as x\nT \nAx = 36, where \n8 \n11 \n-2 \n10] \n-2 \n2 \nWe  find the eigenvalues of A to be 18,  9, and -9, with corresponding orthogonal \neigenvectors \nrespectively. We normalize them to obtain \nand form the orthogonal matr  ix \n[\n! -! =l\nl \nNote that in order for Q   to be the matrix of a rotation, we require <let Q = 1, which \nis true in this case. (Otherwise, <let Q = -1, and swapping two columns changes the \nsign of the determinant.) Therefore, \nand, with the change of variable x = Qx', we get x\nT\nAx = (x')Dx' = 36, so \n18(x')\n2 \n+ 9(\ny\n')\n2 \n-9(z')\n2 \n= 36 \n(x')\n2 \n(\ny\n')\n2 \n(z')\n2 \nor \n--+ --  -  --= 1 \n2 4 4 \nFrom Figure 5.19, we recognize this equation as the equation of a hyper  boloid of one","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":66921,"to":66977}}}}],[1156,{"pageContent":"T\nAx = (x')Dx' = 36, so \n18(x')\n2 \n+ 9(\ny\n')\n2 \n-9(z')\n2 \n= 36 \n(x')\n2 \n(\ny\n')\n2 \n(z')\n2 \nor \n--+ --  -  --= 1 \n2 4 4 \nFrom Figure 5.19, we recognize this equation as the equation of a hyper  boloid of one \nsheet. The x', y ' , and z' axes are in the directions of the eigenvectors \nq\n1 , \nq\n2\n, and \nq\n3\n, \nrespectively. The graph is shown in Figure 5.20.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":66977,"to":67009}}}}],[1157,{"pageContent":"I \nExercises 5.5 \nQuadratic Forms \nFigure 5.20 \nA hyperboloid of one sheet in \nnonstandard position \nSection 5.5 Applications \n423 \nz \nz\n' \nWe can also identify and  graph quadrics that have been translated out of sta  ndard \nposition using the \"complete-the-squares method\" of Examples 5.27 and 5.29. You \nwill be asked to do so   in the exercises. \nIn Exercises 1-6, evaluate th e quadratic form f\n(\nx\n) \n= x\nT \nAx \nfor the given A and x. \nIn Exercises 7-12, find the symmetric matrix A associated \nwith the given quadratic form. \n7. x\nl \n+ 2x\nf \n+ 6x\n1\nx\n2 \n1. A= \n2.A  = \n3.A  = \n4.A  = \n5.A  = \n6.A  = \n[\n� \n!\nl\nx = \n[;] \n[ \n� \n-\n� \nl \nx = \n[\n:\n:] \n[ \n_\n� \n-\n!\nl\nx  = \n[\n!\n] \nu \n0 \n-\n}\n� \n[\n�\n] \n2 \nu \n0 \n-\n}\n� \n[\n-\n:\n] \n2 \n[\n: \n2 \n:Jx\n�\nm \n0 \n8. X\n1\nX\n2 \n9. 3x\n2 \n-3xy -  y\n2 \n10. x\n� \n-  x\n� \n+ 8X\n1\nX\n2 \n-6X\nz\nX\n3 \n11. 5x\nl \n-\nx\ni \n+ 2x\n� \n+ 2x\n1\nx\n2 \n-4x\n1\nx\n3 \n+ 4x\n2\nx\n3 \n12. 2x\n2 \n-3y\n2 \n+ z\n2 \n-4xz \nDiagonalize th e quadratic forms in Exercises 13-18 by \nfinding an orth ogonal matrix Q such that th e change of \nvariable x = Qy transforms the given form into one with no","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":67011,"to":67148}}}}],[1158,{"pageContent":"2\nx\n3 \n12. 2x\n2 \n-3y\n2 \n+ z\n2 \n-4xz \nDiagonalize th e quadratic forms in Exercises 13-18 by \nfinding an orth ogonal matrix Q such that th e change of \nvariable x = Qy transforms the given form into one with no \ncross-product terms. Give Q and th e new quadratic form. \n13. 2x\nl \n+ 5x\n� \n-4x\n1\nx\n2 \n14. x\n2 \n+ 8xy + y\n2 \n15. 7x\nl \n+ x\ni \n+ xj \n+ 8X\n1\nX\n2 \n+ 8X\n1\nX\n3 \n-16X\nz\nX\n3 \n16. x\nl \n+ x\ni \n+ 3xj -4x\n1\nx\n2 \n17. x\n2 \n+ z\n2 \n-2xy + 2yz \n18.2xy + 2xz + 2yz","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":67148,"to":67204}}}}],[1159,{"pageContent":"424 \nChapter 5  Orthogonality \nClassify each of th e quadratic forms in Exercises 19-26 as \npositive definite, positive semidefinite, negative definite, \nnegative semidefinite, or indefinite. \n19. x\nf \n+ 2x\n} \n21. -2x\n2 \n-2y\n2 \n+ 2xy \n20. x\nf \n+ \nx\ni \n-\n2x\n,\nx\n2 \n22. x\n2 \n+ y\n2 \n+ 4xy \n23. 2x\nf \n+ 2x\ni \n+ 2x\n� \n+ 2x\n1\nx\n2 \n+ 2x\n1\nx\n3 \n+ 2x\n2\nx\n3 \n24. x\nf \n+ x\ni \n+ x\n� \n+ 2x\n1\nx\n3 \n25. x\ni \n+ x\n� \n-  x\n� \n+ 4x\n1\nx\n2 \n26.  - x\n2 \n-  y\n2 \n-  z\n2 \n-2xy  -2xz -2yz \n27. Prove Theorem 5.22. \n28. Let A = \n[ \n� �\n] \nbe a symmetric 2 X 2 matrix. Prove \nthat A  is positive definite if and only if a > 0 and \n<let A > 0. [Hint: ax\n2 \n+ 2bxy + dy\n2 \n= \na( x + �y y + ( d  -\n�\n2\n)y\n2\n.] \n29. Let B be an invertible matrix. Show that A = B\nT\nB is \npositive definite. \n30. Let A be a positive definite symmetric matr  ix. Show \nthat there exists an invertible matrix B  such that A = \nB\nT\nB.  [Hint: Use the Spectral Theorem to write A = \nQDQ\nT\n. Then show that D can be factored as c\nT \nc for \nsome invertible matrix C.] \n31. Let A and B be positive definite symmetric n X n","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":67206,"to":67313}}}}],[1160,{"pageContent":"B\nT\nB.  [Hint: Use the Spectral Theorem to write A = \nQDQ\nT\n. Then show that D can be factored as c\nT \nc for \nsome invertible matrix C.] \n31. Let A and B be positive definite symmetric n X n \nmatrices and let c be a   positive scalar. Show that the \nfollowing matrices are positive definite. \n(a) cA \n(b) A\n2 \n(c) A+ B \n(d) A\n_\n, \n(First show that A  is necessarily invertible.) \n32. Let A be a positive definite symmetric matrix. Show \nthat there is a positive definite symmetric matrix B \nsuch that A = B\n2\n• (Such a matr  ix Bis called a square \nroot of A.) \nIn Exercises 33-36, find th e maximum and minimum val­\nues of th e quadratic form f\n(\nx\n) \nin the given exercise, subject \nto th e constraint \nll\nx\nll \n= 1, and determine the values of x for \nwhich th ese occur. \n33. Exercise 20 \n35. Exercise 23 \n34. Exercise 22 \n36. Exercise 24 \n37. Finish the proof of Theorem 5.23\n(a)\n. \n38. Prove Theorem 5.23\n(\nc\n)\n. \nGraphing Quadratic Equations \nIn Exercises 39-44, identify the graph of th e given equation. \n39. x\n2 \n+ 5y\n2 \n= 25 \n40. x\n2 \n-  y\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":67313,"to":67373}}}}],[1161,{"pageContent":"37. Finish the proof of Theorem 5.23\n(a)\n. \n38. Prove Theorem 5.23\n(\nc\n)\n. \nGraphing Quadratic Equations \nIn Exercises 39-44, identify the graph of th e given equation. \n39. x\n2 \n+ 5y\n2 \n= 25 \n40. x\n2 \n-  y\n2 \n-  4  =  0 \n41. x\n2 \n-  y  -   1  =  0 \n42. 2x\n2 \n+ y\n2 \n-  8  =  0 \n43. 3x\n2 \n= \ny\n2 \n-   1 \n44. x  = -2y\n2 \nIn Exercises 45-50, use a translation of axes to put th e conic \nin standard position. Identify th e graph, give its equation in \nthe translated coordinate system, and sketch th e curve. \n45. x\n2 \n+ y\n2 \n-4x -4y + 4  =  0 \n46. 4x\n2 \n+ 2y\n2 \n-Sx + l2y + 6  =  0 \n47. 9x\n2 \n-4y\n2 \n-4y =  37  48.  x\n2 \n+ lOx  -3y = -13 \n49. 2y\n2 \n+ 4x + Sy =  0 \n50.2y\n2 \n-3x\n2 \n-18x  -20y + 11=0 \nIn Exercises 51-54, use a ro tation of axes to put the conic in \nstandard position. Identify the graph, give its equation in th e \nrotated coordinate system, and sketch the curve. \n51. x\n2 \n+ xy + y\n2 \n=  6 52. 4x\n2 \n+ lOxy + 4y\n2 \n=  9 \n53. 4x\n2 \n+ 6xy -4y\n2 \n=  5  54. 3x\n2 \n-2xy + 3y\n2 \n=  8 \nIn Exercises 55-58, identify the conic with th e given equa­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":67373,"to":67458}}}}],[1162,{"pageContent":"51. x\n2 \n+ xy + y\n2 \n=  6 52. 4x\n2 \n+ lOxy + 4y\n2 \n=  9 \n53. 4x\n2 \n+ 6xy -4y\n2 \n=  5  54. 3x\n2 \n-2xy + 3y\n2 \n=  8 \nIn Exercises 55-58, identify the conic with th e given equa­\ntion and give its equation in standard form. \n55. 3x\n2 \n-4xy + 3y\n2 \n-28\nv'2\nx + 22\nVl\ny + 84 =  0 \n56. 6x\n2 \n-  4xy + 9y\n2 \n-20x  -lOy  -  5  =  0 \n57.2xy + 2\n\\/2\nx  -   1  =  0 \n58. x\n2 \n-2xy + y\n2 \n+ 4 \nV2\nx  -  4  =  0 \nSometimes the graph of a quadratic equation is a straight \nline, a pair of straight lines, or a single point.  We refer to \nsuch a graph as a degenerate conic. It is also possible that \nth e equation is not satisfied for any values of th e variables, \nin which case there is no graph at all and we refer to the \nconic as an imaginary conic. In Exercises 59-64, identify \nthe conic with the given equation as either degenerate or \nimaginary and, where possible, sketch the graph. \n59. x\n2 \n-  y\n2 \n=  0 \n60. x\n2 \n+ 2y\n2 \n+ 2  =  0 \n61. 3x\n2 \n+ y\n2 \n=  0 \n62. x\n2 \n+ 2xy + y\n2 \n=  0 \n63. x\n2 \n-2xy + y\n2 \n+ 2\nVl\nx  -  2\nVl\ny \n=  0 \n64. 2x\n2 \n+ 2xy + 2y\n2 \n+  2\n\\/2\nx  -  2\nVl\ny + 6  =  0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":67458,"to":67548}}}}],[1163,{"pageContent":"59. x\n2 \n-  y\n2 \n=  0 \n60. x\n2 \n+ 2y\n2 \n+ 2  =  0 \n61. 3x\n2 \n+ y\n2 \n=  0 \n62. x\n2 \n+ 2xy + y\n2 \n=  0 \n63. x\n2 \n-2xy + y\n2 \n+ 2\nVl\nx  -  2\nVl\ny \n=  0 \n64. 2x\n2 \n+ 2xy + 2y\n2 \n+  2\n\\/2\nx  -  2\nVl\ny + 6  =  0 \n65. Let A be a symmetric 2 X 2 matrix and let k be a \nscalar. Prove that the  graph of the quadratic equation \nx\nT\nAx = k is \n(a) a hyperbola if k * 0 and <let A < 0 \n(b) an ellipse, circle, or imaginary conic if k * 0 and \ndet A > 0 \n(c) a pair of straight lines or an imaginary conic if \nk * 0 and <let A = 0 \n(d) a pair of straight lines or a   single point if k = 0 \nand det A * 0 \n(e) a straight line if k = 0 and <let A = 0 \n[Hint: Use the Principal Axes Theorem.]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":67548,"to":67600}}}}],[1164,{"pageContent":"In Exercises 66-73, identify the quadric with th e given \nequation and give its equation in standard form. \n66. 4x\n2 \n+ 4y\n2 \n+ 4z\n2 \n+ 4xy + 4xz + 4yz =  8 \n67. x\n2 \n+ y\n2 \n+ z\n2 \n-4yz = 1 \n68. -x\n2 \n-  y\n2 \n-  z\n2 \n+ 4xy + 4xz + 4yz = 12 \n69.2xy + z = 0 \n70. 16x\n2 \n+ 100y\n2 \n+ 9z\n2 \n-24xz -   60x  -80z = 0 \n71. x\n2 \n+ y\n2 \n-2z\n2 \n+ 4xy -2xz + 2yz -  x + y + z = 0 \n72. 10x\n2 \n+ 25y\n2 \n+ 10z\n2 \n-40xz + 20\n\\/2\nx + s oy + \n20\n\\/2\nz = 15 \n73. llx\n2 \n+ l ly\n2 \n+ 14z\n2 \n+ 2xy + 8xz -8yz -12x + \n12y + 12z = 6 \nChapter Review \nKev Definitions and Concepts \nChapter Review \n425 \n74. Let A be a real 2 X 2 matrix with complex eigenvalues \nA = a :±: bi such that b =F 0 and I A I = 1. Prove that \nevery trajectory of the dynamical system x\nk\n+\ni \n= Ax\nk \nlies on an ellipse. [Hint: Theorem 4.43 shows that if v \nis an eigenvector corresponding to A = a  -bi, then \nthe matrix P =  [  Rev  Im v] is invertible and \nA= P\n[\n: \n-\n�\nJ\nP\n-\n1\n. Set B = \n(\nPP\nT\n)\n-\n1\n. Show that the \nquadratic x\nT\nBx = k defines an ellipse for all k > 0, \nand prove that if x lies on this ellipse, so does Ax.]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":67602,"to":67695}}}}],[1165,{"pageContent":"A= P\n[\n: \n-\n�\nJ\nP\n-\n1\n. Set B = \n(\nPP\nT\n)\n-\n1\n. Show that the \nquadratic x\nT\nBx = k defines an ellipse for all k > 0, \nand prove that if x lies on this ellipse, so does Ax.] \nfundamental subspaces \nof a matrix, 380 \nGram-Schmidt Process, 389 \northogonal basis, 370 \northogonal complement \northogonal projection, 382 \northogonal set of vectors, 369 \nOrthogonal Decomposition \northonormal set of vectors, \n372 \nproperties of orthogonal \nmatrices, 374-376 \nQR factorization, 393 \nRank Theorem, 386 \nof a subspace, 378 \northogonal matrix,  374 \nTheorem, 384 \northogonally diagonalizable \nmatrix, 400 \northonormal basis,  372 \nspectral decomposition, 405 \nSpectral Theorem, 403 \nReview Questions \n1. Mark each of the following statements true or false: \n(a) Every orthonormal set of vectors is linearly \nindependent. \n(b) Every nonzero subspace of \nu;g\nn \nhas an orthogonal \nbasis. \n(c) If A is a square matrix with orthonormal rows, \nthen A is an orthogonal matrix. \n(d) Every orthogonal matrix is invertible.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":67695,"to":67749}}}}],[1166,{"pageContent":"independent. \n(b) Every nonzero subspace of \nu;g\nn \nhas an orthogonal \nbasis. \n(c) If A is a square matrix with orthonormal rows, \nthen A is an orthogonal matrix. \n(d) Every orthogonal matrix is invertible. \n(e) If A is a matrix with det A = 1, then A is an \northogonal matrix. \n(f) If A is an m X n matrix such that (row(A))_j_ = \nu;g\nn\n, \nthen A must be the zero matrix. \n(g) If Wis a  subspace of \nu;g\nn \nand v is a vector in \nu;g\nn \nsuch \nthat projw(v) = 0, then v must be the zero vector. \n(h) If A is a symmetric, orthogonal matrix, then A \n2 \n= I. \n(i) Every orthogonally diagonalizable matrix is invertible. \n(j)  Given any n real numbers A\n1\n, .•. ,  A\nn\n, there exists \na symmetric n X n matrix with A\n1\n, .•. , A\nn \nas its \neigenvalues. \n2. Find all values of a and b such that \n\\ [H [ J [fl ) i\n< \nan mthogonal \n<\net of vedm. \n3. Find the coordinate vector [ v] 8 of v = [\n-\n�\n] \nwith \nrespect to the orthogonal basis \n2 \nB � ml [ J [\n-\n� l) orn\n'","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":67749,"to":67804}}}}],[1167,{"pageContent":"426 \nChapter 5  Orthogonality \n4. The coordinate vector of a vector v with respect to an \northonormal basis B  = {v\n1\n, v\n2\n} oflR\n2 \nis [v]8 = \n[\n-3\n] \n1/2 \n. \n[\n3/5\n] \nIf v\n1 \n= \n4/ 5 \n, find  all possible vectors v. \n5. Show that \n[\n-\n�j� \n2\n�\n7 \n2��\n] \nis an \n4/7\nVs \n-15 /7\nVs \n2/7\nVs \northogonal matrix. \n6. If \n[ \n1\n�\n2 \n: \n] \nis an orthogonal matrix, find all possible \nvalues of a, b, and c. \n7. If Q is an orthogonal n X n matr  ix and {v\n1\n, ••• , v\nk\n} is \nan orthonormal set in !R\nn\n, prove that {Qv\n1\n, .•. , Qv\nk\n} \nis an orthonormal set. \n8. If Q is an n X n matrix such that the angles \nL\n(\nQx, Qy\n) \nand L\n(\nx, y\n) \nare equal for all vectors x and \nyin IR\n\"\n, prove that Q is an orthogonal matrix. \nIn Questions 9-12,find a basis for W \n_j_\n. \n9. Wis the  line in IR\n2 \nwith general equation \n2x -Sy\n= 0 \n10. W is the line in IR\n3 \nwith parametric equations \nx  =  t \ny \n= 2t \nz  = -t \n13. Find bases for each of the  four fundamental subspaces of \n[\n-\n� \n-\n� \n-\n� \n� \n-\n�] \nA= \n2 \n1 4 8 9 \n3 -5 6 -1 7 \n14. Find the orthogonal decomposition of \nv -\n[\n-\n�\n] \nwith respect to","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":67806,"to":67917}}}}],[1168,{"pageContent":"x  =  t \ny \n= 2t \nz  = -t \n13. Find bases for each of the  four fundamental subspaces of \n[\n-\n� \n-\n� \n-\n� \n� \n-\n�] \nA= \n2 \n1 4 8 9 \n3 -5 6 -1 7 \n14. Find the orthogonal decomposition of \nv -\n[\n-\n�\n] \nwith respect to \n15. (a) Apply the Gram-Schmidt Process to \n� \n-\n[\n!\nJ·� \n-\n[\niJ·� \n-\n[\nrJ \nto find an orthogonal basis for W = span{x\n1\n, x\n2\n, xJ. \n(b) Use the result of part (a) to find a QR factorization \nof A\n-\n[\n! \n� ] \n16. Find an orthogonal basis for IR\n4 \nthat contains the \nvecto\" m '\n\"\nd \n[ \nJ \n17. Find an orthogonal basis for the subspace \nw - ml· + x\n, \n+ x\n, \n+ x. -0\n} \nom' \n18. Let A = \n[ \n� \n2 \n-\n� \nl · \n-1 1 \n2 \n(a) Orthogonally diagonalize A. \n(b) Give the sp   ectral decomposition of A. \n19. Find a symmetric matrix with eigenvalues ,\\\n1 \n= ,\\\n2 \n= 1, \nA\n3 \n= - 2 and eigenspaces \n20. If {v\n1\n, v\n2\n, •.• , v\n\"\n} is an  orthonormal basis for !R\nn \nand \nprove that A is a symmetric matrix with eigenval­\nues c\n1\n, c\n2\n, •.• , e\nn \nand corresponding eigenvectors","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":67917,"to":68017}}}}],[1169,{"pageContent":"Algebra is generous; she oft en gives \nmore than is asked of  her. \n-Jean le Rond d'Alembert \n(1717-1783) \nIn Carl B. Boyer \nA History of Mathematics \nWiley, 1968, p. 481 \nVector S\npaces \n6.0 Introduction: Fibonacci in (Vector>  Space \nThe Fibonacci sequence was introduced in Section 4.6. It is the sequence \n0, 1, 1, 2, 3, 5, 8, 13, ... \nof nonnegative integers with the property that after the first two terms, each term \nis \nthe sum of the two terms preceding it.  Thus 0 +  1  = 1, 1  +  1  = 2, 1  +  2  = 3, \n2  +  3  = 5, and so on. \nIf we denote the terms of the Fibonacci sequence by f0,f\n1\n,f\n2\n, .•• , then the entire \nsequence is completely determined by specifying that \nJo \n= O,f1 \n=  1 and f\nn \n= f\nn\n-\nl \n+ f\nn\n-\nl \nfor n 2 2 \nBy analogy with vector notation, let's write a sequence x0, x\n1 , x\n2\n, x\n3\n, ... as \nx = [x\no\n, X1, X\n2\n, X\n3\n, ... \n) \nThe Fibonacci sequence then becomes \nf = [f0,\nJ\n1 ,\nJ\n2\n,f\n3\n, •.. \n) \n= \n[ O, 1, 1, 2, ... \n) \nWe now generalize this notion. \nDefinition \nA Fibonacci-type sequence is any sequence x = [x0, x\n1 , x\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":68019,"to":68085}}}}],[1170,{"pageContent":"2\n, X\n3\n, ... \n) \nThe Fibonacci sequence then becomes \nf = [f0,\nJ\n1 ,\nJ\n2\n,f\n3\n, •.. \n) \n= \n[ O, 1, 1, 2, ... \n) \nWe now generalize this notion. \nDefinition \nA Fibonacci-type sequence is any sequence x = [x0, x\n1 , x\n2\n, x\n3\n, •.• \n) \nsuch that x0 and x\n1 \nare real numbers and X\nn \n= x\nn\n-\nl \n+ X\nn\n-\nz \nfor n 2 2. \nFor example, [1, \n\\/2\n, 1  + \n\\/2\n, 1+2\n\\/2\n,2 +  3\n\\/2\n, ... \n) \nis a Fibonacci-type sequence. \nProblem 1 Write down the first five terms of three more Fibonacci-type sequences. \nBy analogy with vectors again, let's define the sum of two sequences x = [x0, x\n1 , \nx\n2\n, •.. \n) \nand y = [y0, y\n1\n, y\n2\n, •.• \n) \nto be the sequence \nIf c is a scalar, we can likewise define the scalar multiple of a sequence by \nex \n=  [\ncx\n0\n,  cx\n1\n,  cx\n2\n, \n••• \n) \n421","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":68085,"to":68162}}}}],[1171,{"pageContent":"428 Chapter 6 Vector Spaces \nThe Lucas sequence is named after \nEdouard Lucas (see page 336). \nProblem 2 (a) Using your examples from Problem 1 or other examples, compute \nthe sums of various pairs of Fibonacci-type sequences. Do the re­\nsulting sequences appear to be Fibonacci-type? \n(b)  Compute various scalar multiples of your  Fibonacci-type  se­\nquences from Problem 1. Do the resulting sequences appear to be \nFibonacci-type? \nProblem 3 (a) Prove that if x and y are Fibonacci-type sequences, then so is x + y. \n(b) Prove that if x is a Fibonacci-type sequence and c is a scalar, then \nex is also a Fibonacci-type sequence. \nLet's denote the set of   all Fibonacci-type sequences by Fib. Problem 3 shows that, \nlike 11;r, Fib is clo  sed under addition and scalar multiplication. The next exercises \nshow that Fib has much more in common with 11;r. \nProblem 4 Review the algebraic properties of vectors in Theorem 1.1. Does Fib","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":68164,"to":68179}}}}],[1172,{"pageContent":"show that Fib has much more in common with 11;r. \nProblem 4 Review the algebraic properties of vectors in Theorem 1.1. Does Fib \nsatisfy all of these properties? What Fibonacci-type sequence plays the role ofO? For a \nFibonacci-type sequence x, what is -x? Is -xalso a Fibonacci-type sequence? \nProblem 5 In !R\nn\n, we  have the standard basis vectors e1, e\n2\n, •.• , e\nn\n-The Fibonacci \nsequence f = [O, 1, 1, 2, ... ) can be thought of as the analogue of e\n2 \nbecause its first \ntwo terms are 0 and 1. What sequence e in Fib plays the role of e1? \nWhat about e\n3\n,  e4, .•• ? Do these vectors have analogues in Fib? \nProblem 6 Let x = \n[x\n0\n, x1, x\n2\n, .•. ) be a Fibonacci-type sequence. Show that xis a \nlinear combination of e and f. \nProblem 1 Show that e and  f are  linearly independent.  (That is, show that if \nce + df = 0, then c = d = 0.) \nProblem 8 Given your answers to Problems 6 and 7, what would be a sensible \nvalue to assign to the \"dimension\" of Fib? Why?","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":68179,"to":68207}}}}],[1173,{"pageContent":"ce + df = 0, then c = d = 0.) \nProblem 8 Given your answers to Problems 6 and 7, what would be a sensible \nvalue to assign to the \"dimension\" of Fib? Why? \nProblem 9 Are there any geometric sequences in Fib? That is, if \n[1, r, r\n2\n,  r\n3\n, ... ) \nis a Fibonacci-type sequence, what are the possible values of r? \nProblem 10 Find a \"basis\" for Fib consisting of geometric Fibonacci-type sequences. \nProblem 11 Using your answer to Problem 10, give an alternative derivation of \nBinet's formula [formula \n(\n5\n) \nin Section 4.6]: \nj, \n= \n_\n1\n_\n(\n1 \n+ \nVs\n)\nn \n_\n_ \n1_\n(\n1 \n-\nVs\n)\nn \nn \nVs 2 Vs 2 \nfor the terms of the Fibonacci sequence f =  [f\n0\n,f1,f\n2\n, .•. ). [Hint: Express fin terms of \nthe basis from Problem 10.] \nThe Lucas sequence is the Fibonacci-type sequence \nl = \n[\n1\n0\n, \n11\n, \n1\n2\n, \n1\n3\n, .•. ) \n= \n[2, 1, 3, 4, ... ) \nProblem 12 Use the basis from Problem 10 to find an analogue ofBinet's formula \nfor the nth term Z\nn \nof the Lucas sequence. \nProblem 13 Prove that the Fibonacci and Lucas sequences are related by the identity \nf\nn\n-\n1 \n+ f\nn\n+I \n=\nZ\nn","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":68207,"to":68282}}}}],[1174,{"pageContent":"for the nth term Z\nn \nof the Lucas sequence. \nProblem 13 Prove that the Fibonacci and Lucas sequences are related by the identity \nf\nn\n-\n1 \n+ f\nn\n+I \n=\nZ\nn \nfor n 2:  1 \n_... \n[\nHint: The Fibonacci-type sequences C = [1, 1, 2, 3,   ... ) and f\n+ \n= [1, 0, 1, 1, ... ) \nform a basis for Fib. (Why?)] \nIn this Introduction, we have seen that the collection Fib of all Fibonacci-type \nsequences behaves in many respects like IR\n2\n, even though the \"vectors\" are actually \ninfinite sequences. This useful analogy leads to the general notion of a vector space \nthat is the  subject of this chapter.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":68282,"to":68308}}}}],[1175,{"pageContent":"The German mathematician \nHermann Grassmann (1809-\n1877) is generally credited with \nfirst introducing the idea of a \nvector space (although he did \nnot call it that) in 1844. Unfor­\ntunately, his work was very diffi­\ncult to read and did not receive \nthe attention it deserved. One \nperson who did study it was the \nItalian mathematician Giuseppe \nPeano (1858-1932). In his \n1888 book Calcolo Geometrico, \nPeano clarified Grassmann's \nearlier work and laid down the \naxioms for a vector space as \nwe know them today. Peano's \nbook is also remarkable for \nintroducing operations on sets. \nHis notations U,  n, and E (for \n\"union;' \"intersection;' and \"is \nan element of\") are the ones we \nstill use, although they were not \nimmediately accepted by other \nmathematicians. Peano's axio­\nmatic definition of a vector space \nalso had very little influence for \nmany years. Acceptance came \nin 1918, after Hermann Wey! \n(1885-1955) repeated it in \nhis book Sp ace, Time, Matter, an \nintroduction to Einstein's general","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":68310,"to":68341}}}}],[1176,{"pageContent":"also had very little influence for \nmany years. Acceptance came \nin 1918, after Hermann Wey! \n(1885-1955) repeated it in \nhis book Sp ace, Time, Matter, an \nintroduction to Einstein's general \ntheory of relativity. \nSection 6.1 \nVector Spaces and Subspaces \n429 \nVector  Spaces and Subspaces \nIn Chapters 1 and 3, we saw that the algebra of vectors and the algebra of matrices \nare similar in many respects. In particular, we can add both vectors and matrices, \nand we can multiply both by scalars. The properties that result from these two opera­\ntions (Theorem 1.1 and Theorem 3.2\n) \nare identical in both settings. In this section, \nwe use these properties to define generalized \"vectors\" that arise in a wide variety \nof examples. By proving general theorems about these \"vectors:' we will therefore \nsimultaneously be proving results about all of these examples. This is the real power \nof algebra: its ability to take properties from a concrete setting, like !R\nn\n, and abstract \nthem into a general setting.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":68341,"to":68364}}}}],[1177,{"pageContent":"of algebra: its ability to take properties from a concrete setting, like !R\nn\n, and abstract \nthem into a general setting. \nDefinition \nLet Vbe a  set on which two opera  tions, called addition and scalar \nmultiplication, have been defined. If u and v are in V, the sum of u and vis denoted \nby u + v, and if c is a scalar, the scalar multiple of u by c is denoted by cu. If the \nfollowing axioms hold for all u, v, and w in V and for all scalars c and d, then Vis \ncalled a vector space and its elements are called vectors. \n1. u + v is in V. \nClosure under addition \n2. u + v = v + u \nCommutativity \n3. \n( u + v) + w \n= u + (v + \nw) \nAssociativity \n4. There exists an element 0 in V, called a zero vector, such that u + 0 = u. \n5. \nFor each u in V, there is an element -u in V such that u +  (-u) \n= 0. \n6. cu is in  V. \n7. \nC \n( \nU + V) \n= \nCU + CV \n8. \n(\nc + d)u = cu+ du \n9.  c\n( du) \n= \n(\ncd)u \n10.lu=  u \nRemarks \nClosure under scalar multiplication \nDistributivity \nDistributivity \n•","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":68364,"to":68407}}}}],[1178,{"pageContent":"= 0. \n6. cu is in  V. \n7. \nC \n( \nU + V) \n= \nCU + CV \n8. \n(\nc + d)u = cu+ du \n9.  c\n( du) \n= \n(\ncd)u \n10.lu=  u \nRemarks \nClosure under scalar multiplication \nDistributivity \nDistributivity \n• \nBy \"scalars\" we will usually mean the real numbers. Accordingly, we should \nrefer to Vas a real vector space (or a vector space over the real numbers\n)\n. It is also pos­\nsible for scalars to be complex numbers or to belong to \"ll.\nP\n, where p is prime. In these \ncases, Vis called a complex vector space or a vector space over \"ll.\nP\n, respectively. Most of \nour examples will be real vector spaces, so we will usually omit the adjective \"real:' If \nsomething is referred to as   a \"vector space:' assume that we are working over the real \nnumber system. \nIn fact, the scalars can be chosen from any number system in which, roughly \nspeaking, we can add, subtract, multiply, and divide according to the usual laws of \narithmetic. In abstract algebra, such a number system is called a field. \n•","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":68407,"to":68445}}}}],[1179,{"pageContent":"speaking, we can add, subtract, multiply, and divide according to the usual laws of \narithmetic. In abstract algebra, such a number system is called a field. \n• \nThe definition of a vector space  does not specify what the set  V  consists \nof. Neither does it specify what the operations called \"addition\" and \"scalar multi­\nplication\n'\n' look like. Often, they will be familiar, but they need not be. See Example 6.6 \nand Exercises 5-7. \nWe will now look at several examples of vector spaces. In each case, we need to \nspecify the set V and the operations of addition and scalar multiplication and to verify \naxioms 1 through 10. We need to pay particular attention to axioms 1 and 6 (closure),","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":68445,"to":68456}}}}],[1180,{"pageContent":"430 \nChapter 6 Vector Spaces \nExample 6.1 \nExample 6.2 \nExample 6.3 \naxiom 4 (the existence of a zero vector in V\n)\n, and axiom 5 (each vector in V must have \na negative in V\n)\n. \nFor any n 2: 1, !R\nn \nis a vector space with the usual operations of addition and scalar \nmultiplication. Axioms 1 and 6 follow from the definitions of these operations, and \nthe remaining axioms follow from Theorem 1.1. \nThe set of all 2 X 3 matrices is a vector space with the usual operations of matrix \naddition and matrix scalar multiplication. Here the \"vectors\" are actually matrices. \nWe know that the sum of two 2 X 3 matrices is also a 2 X 3 matrix and that multiply­\ning a 2 X 3 matr  ix by a scalar gives another 2 X 3 matrix; hence, we have closure. \nThe remaining axioms follow from Theorem 3.2. In particular, the zero vector 0 is the \n2 \nX 3 zero matrix, and the negative of a 2 X 3 matrix A  is just the 2 X 3 matrix -A. \nThere is nothing special about 2 X 3 matrices. For any positive integers m and n,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":68458,"to":68481}}}}],[1181,{"pageContent":"2 \nX 3 zero matrix, and the negative of a 2 X 3 matrix A  is just the 2 X 3 matrix -A. \nThere is nothing special about 2 X 3 matrices. For any positive integers m and n, \nthe set of all m X n matrices forms a vector space with the usual operations of matrix \naddition and matrix scalar multiplication. This vector space is denoted Mm\nn\n· \nLet <;IP \n2 \ndenote the  set of all polynomials of degree 2 or   less with real coefficients. \nDefine addition and scalar multiplication in the usual way. (See Appendix D.) If \np\n(\nx\n) \n= a\n0 \n+  a\n1\nx  +  a\n2\nx\n2 \nand q (\nx\n) \n= b\n0  + \nb\n1\nx  +  b\n2\nx\n2 \nare in <!P \n2\n, then \nhas degree at most 2 and so is in <!P\n2\n• If c is a scalar, then \ncp\n(\nx\n) \n= \nca\n0  + ca\n1\nx  + ca\n2\nx\n2 \nis also in <!P\n2\n. This verifies axioms 1 and  6. \nThe zero vector 0 is the zero polynomial-that is, the polynomial all of whose \ncoefficients are zero. The negative of a polynomial p\n(\nx\n) \n= a0 + a\n1\nx + a\n2\nx\n2 \nis the poly­\nnomial -p\n(\nx\n) \n= -a\n0 \n-  a 1x -  a\n2\nx\n2\n• It is now easy to verify the remaining axioms. We","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":68481,"to":68558}}}}],[1182,{"pageContent":"coefficients are zero. The negative of a polynomial p\n(\nx\n) \n= a0 + a\n1\nx + a\n2\nx\n2 \nis the poly­\nnomial -p\n(\nx\n) \n= -a\n0 \n-  a 1x -  a\n2\nx\n2\n• It is now easy to verify the remaining axioms. We \nwill check axiom 2 and leave the others for Exercise 12. With p\n(\nx\n) \nand q\n(\nx\n) \nas above, \nwe have \np\n(\nx)  +  q\n(\nx\n) \n= \n(\na\n0 \n+  a\n1\nx +  a\n2\nx\n2\n)  +  (\nb\n0 \n+  b\n1\nx +  b\n2\nx\n2\n) \n= \n(\na\n0 \n+  b\n0) + \n(\na\n1 \n+  b\n1\n)\nx + \n(\na\n2 \n+  b\n2\n)\nx\n2 \n= \n(\nb\n0 \n+  a\n0) + \n(\nb\n1 \n+  a\n1\n)\nx +  (\nb\n2 \n+  a\n2\n)\nx\n2 \n= \n(\nb\n0  +  b\n1\nx  +  b\n2\nx\n2\n)  + \n(\na\n0  +  a\n1\nx +  a\n2\nx\n2\n) \n= \nq\n(\nx\n)  + p\n(\nx\n) \nwhere the  third equality follows from  the  fact  that addition of real  numbers is \ncommutative.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":68558,"to":68685}}}}],[1183,{"pageContent":"Example 6.4 \nFigure 6.1 \nSection 6.1 \nVector Spaces and Subspaces \n431 \nIn general, for any fixed n 2: 0, the  set <!J' n of all polynomials of degree less than or \nequal to n is a vector space, as is the set <!J' of all polynomials. \nLet ?Jf denote the set of all real-valued functions defined on the real line. If f and g are \ntwo such functions and c    is a scalar, then f + g and cf are defined by \n(f +   g)(x) \n= f (x)  + g(x) \nand \n(cj)(x) = cf (x) \nIn other words, the value off + g  at x is obtained by adding together the values off \nand g at x [  Figure 6.1 (a) J. Similarly, the value of cf at xis just the value off at x  mul­\ntiplied by the scalar c [ Figure 6.1 (b) J. The zero vector in ?Jf is the constant function \nf\n0 \nthat is identically zero; that is,f\n0\n(x) = 0 for  all x. The negative of a functionf is the \nfunction -f defined by (-j) (x) = -f(x) [Figure 6.l(c)]. \nAxioms 1 and 6 are obviously true. Verification of the remaining axioms is left as \nExercise 13. Thus, ?Jf is a vector space. \ny \ny \n(","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":68687,"to":68713}}}}],[1184,{"pageContent":"function -f defined by (-j) (x) = -f(x) [Figure 6.l(c)]. \nAxioms 1 and 6 are obviously true. Verification of the remaining axioms is left as \nExercise 13. Thus, ?Jf is a vector space. \ny \ny \n(\nx\n,f(\nx\n) \n+ \ng(\nx\n)) \n(\nx\n, \n2\n/(\nx\n)) \n\\ \n2f \nI \nI \n(\nx\n, - 3/(\nx\n)) \n(a) \n(b) \ny \n/ \n(\nx\n, \n-f\n(\nx\n)) \n-f \n(c) \nThe graphs of (a) f, g, and f + g, (b) f, 2f, and -3f, and (c) f and -f","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":68713,"to":68756}}}}],[1185,{"pageContent":"432 \nChapter 6 Vector Spaces \nExample 6.5 \nExample 6.6 \nExample 6.1 \nExample 6.8 \nIn Example 6.4, we could also have considered only those functions defined on \nsome closed interval [a, b] of the real line. This approach also produces a vector space, \ndenoted by ?f [a, b]. \nThe  set \"1l_ of integers with the usual operations is not a vector space. To  demon­\nstrate this, it is enough to find that one of the ten axioms fails and to give a specific \ninstance in which it fails (a counterexample\n)\n. In this case, we find that we do not have \nclosure under scalar multiplication. For example, the multiple of the integer 2 by the \nscalar \nt \nis (t)( 2\n)  = \nt\n, \nwhich is not an integer. Thus, it is not true that \nex is in \"1l_ for \nevery x in \"1l_ and every scalar c (i.e., axiom 6 fails). \nLet V =  IR\n2 \nwith the usual definition of addition but the following definition of scalar \nmultiplication: \nThen, for example, \nso axiom 10 fails. [In fact, the other nine axioms are all true (check this), but we do","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":68758,"to":68787}}}}],[1186,{"pageContent":"with the usual definition of addition but the following definition of scalar \nmultiplication: \nThen, for example, \nso axiom 10 fails. [In fact, the other nine axioms are all true (check this), but we do \nnot need to look into them, because V has already failed to be a vector space. This \nexample shows the value of looking ahead, rather than working through the list of \naxioms in the order in which they have been given.] \nLet C\n2 \ndenote the set of all ordered pairs of complex numbers. Define addition and \nscalar multiplication as in IR\n2\n, except here the scalars are complex numbers. For \nexample, \nand \n[ \n1  +  i\n.\n] \n+ \n[-3 + 2i]    [-2 + \n�\ni] \n2  -31 \n4 6 -31 \n(1-i)\n[l   +   i]\n=\n[( l-i)(\nl   +\ni)]\n=\n[ 2] \n2-3i \n(l-i)(2-3i) \n-1-Si \nUsing properties of the complex numbers, it is straightforward to check that all ten \naxioms hold. Therefore, C\n2 \nis a complex vector space. \n4 \nIn general, e\nn \nis a complex vector space for all n 2 1. \nIf pis prime, the set z; (with the usual definitions of addition and multiplication by","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":68787,"to":68831}}}}],[1187,{"pageContent":"axioms hold. Therefore, C\n2 \nis a complex vector space. \n4 \nIn general, e\nn \nis a complex vector space for all n 2 1. \nIf pis prime, the set z; (with the usual definitions of addition and multiplication by \nscalars from \"ll_\np\n) \nis a vector space over Z\nP \nfor all n 2: 1. \n4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":68831,"to":68845}}}}],[1188,{"pageContent":"Theorem 6.1 \nSection 6.1 \nVector Spaces and Subspaces \n433 \nBefore we consider further examples, we state a theorem that contains some use­\nful properties of vector spaces. It is important to note that, by proving this theorem \nfor vector spaces in general, we are actually proving it for every specific vector space. \nLet V be a vector space, u a vector in V, and c a scalar. \na. Ou= 0 \nb. co= 0 \nc. \n(\n-l)u \n= -u \nd. If cu = 0, then c = 0 or u = 0. \nProof We prove properties (b) and ( d) and leave the proofs of the remaining proper­\nties as exercises. \n(b) We have \ncO =  c ( O + 0\n)  = cO + cO \nby vector space axioms 4 and 7. Adding the negative of cO to both sides produces \nco + \n(\n-co\n)  =  (\nco + co\n) \n+ \n(\n-co\n) \nwhich implies \no =co+ \n(\nco+ \n(\n-co\n)) \n=co+ 0 \n=co \nby axioms 5 and 3 \nby axiom 5 \nby axiom 4 \n(d)  Suppose cu  =  0. To show that either c = 0 or u = 0, let's assume that c *  0.   (If \nc = 0, there is nothing to prove.) Then, since c * 0, its reciprocal l/c is defined, and \nu  = lu \n1 \n-\n(\ncu\n) \nc \n1 \n-\n0 \nc \nby axiom 10","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":68847,"to":68903}}}}],[1189,{"pageContent":"c = 0, there is nothing to prove.) Then, since c * 0, its reciprocal l/c is defined, and \nu  = lu \n1 \n-\n(\ncu\n) \nc \n1 \n-\n0 \nc \nby axiom 10 \nby axiom 9 \n0 \nby property (b) \nWe will write u -  v for u + ( -v\n)\n, thereby defining subtraction of vectors. We will \nalso exploit the associativity property of addition to unambiguously write u + v + w \nfor the sum of three vectors and, more generally, \nfor a linear combination of vectors. \nSubspaces \nWe have seen that, in !R\nn\n, it is possible for one vector space to sit inside another one, \ngiving rise to the notion of a subspace. For example, a plane through the origin is a \nsubspace of IR\n3\n. We now extend this concept to general vector spaces.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":68903,"to":68932}}}}],[1190,{"pageContent":"434 \nChapter 6 Vector Spaces \nTheorem 6.2 \nExample 6.9 \nExample 6.10 \nDefinition \nA subset W of a vector space Vis called a subspace of V if Wis \nitself a vector space with the same scalars, addition, and scalar multiplication as V. \nAs in   !R\nn\n, checking to see whether a subset W of a vector space Vis a  subspace of \nV involves testing only two of the ten vector space axioms. We prove this observation \nas a theorem. \nLet V be a vector space and let W be a nonempty subset of V. Then W is a sub­\nspace of V if and only if the following conditions hold: \na.  If u and v are in W, then u + v is in W. \nb.  If u is in  Wand c is a scalar, then cu is in W. \nProof Assume that Wis a  subspace of V. Then W satisfies vector space axioms 1 to \n10. In particular, axiom 1 is condition (a) and axiom 6 is condition (b). \nConversely,  assume that  W is a subset of a vector space V,  satisfying condi­\ntions (a) and (b). By hypothesis, axioms 1and6 hold. Axioms 2, 3, 7, 8, 9,  and 10 hold","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":68934,"to":68954}}}}],[1191,{"pageContent":"Conversely,  assume that  W is a subset of a vector space V,  satisfying condi­\ntions (a) and (b). By hypothesis, axioms 1and6 hold. Axioms 2, 3, 7, 8, 9,  and 10 hold \nin W because they are true for all vectors in V and thus are true in particular for those \nvectors in W. (We say that W inherits these properties from V.) This leaves axioms 4 \nand 5 to be checked. \nSince W is nonempty,  it contains at least one vector u. Then condition (b) and \nTheorem 6.l(a) imply that Ou = 0 is also in W. This is axiom 4. \nIf u is in  V, then, by taking c = -1 in condition (b ), we have that -u = ( -   l)u is \nalso in W, using Theorem 6.l(c). \nRemark Since Theorem 6.2 generalizes the notion of a subspace from the con­\ntext of !R\nn \nto general vector spaces, all of the subspaces of !R\nn \nthat we encountered in \nChapter 3 are subspaces of !R\nn \nin the current context. In particular, lines and planes \nthrough the origin are subspaces of IR\n3\n. \nWe have already shown that the set <!/' \nn","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":68954,"to":68976}}}}],[1192,{"pageContent":"n \nthat we encountered in \nChapter 3 are subspaces of !R\nn \nin the current context. In particular, lines and planes \nthrough the origin are subspaces of IR\n3\n. \nWe have already shown that the set <!/' \nn \nof all polynomials with degree at most n is a \nvector space. Hence, <!/' \nn \nis  a subspace of the vector space <!/' of all polynomials. \n4 \nLet W be the set of symmetric n X n matrices. Show that Wis a  subspace of M\nnn\n-\nSolulion Clearly, Wis nonempty, so we need only check conditions (a) and (b) in \nTheorem 6.2. Let A and B be in W and let c be a scalar. Then A\nT \n= A and B\nT \n= B, \nfrom which it follows that \nTherefore, A + B   is symmetric and, hence, is in W. Similarly, \n(cAf = cA\nT \n= cA \nso cA is symmetric and, thus, is in W. We have shown that Wis closed under addition \nand scalar multiplication. Therefore, it is a subspace of M\nnn\n' \nby Theorem 6.2. \n4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":68976,"to":69010}}}}],[1193,{"pageContent":"Example 6.11 \nExample 6.12 \nSection 6.1 \nVector Spaces and Subspaces \n435 \nLet C(;S be the set of all continuous real-valued functions defined on IR and let 0J be \nthe set of all differentiable real-valued functions defined on IR. Show that C(;S and 0J are \nsubspaces of�, the vector space of all real-valued functions defined on IR. \nSolution From calculus, we know that if f and g are continuous functions and c is a \nscalar, then f + g and cf are also continuous. Hence, C(;S is closed under addition and \nscalar multiplication and so is a subspace of�. If f andg are differentiable, then so are \nf + g and cf Indeed, \n(j + g\n)\n' = \nf' + g'  and \n(\ncf\n)\n' = \nc(j'\n) \nSo 0J is also closed under addition and scalar multiplication, making it a subspace \nof�. \n...+ \nIt is a theorem of calculus that every differentiable function is continuous. Conse­\nquently, 0J is contained in C(;S (denoted by 0J C C(;S), making 0J a subspace ofC(;S. It is also","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":69012,"to":69038}}}}],[1194,{"pageContent":"of�. \n...+ \nIt is a theorem of calculus that every differentiable function is continuous. Conse­\nquently, 0J is contained in C(;S (denoted by 0J C C(;S), making 0J a subspace ofC(;S. It is also \nthe case that every polynomial function is differentiable, so rt/' C 0J, and thus rt/' is a \nsubspace of 0J. We therefore have a hierarchy of subspaces of�, one inside the other: \nThis hierarchy is depicted in Figure 6.2. \nFigure 6.2 \nThe hierarchy of subspaces of� \nThere are other subspaces of �  that can be placed into this hierarchy. Some of \nthese are explored in the exercises. \nIn the preceding discussion, we could have restricted our attention to functions \ndefined on a closed interval [a, b]. Then the corresponding subspaces of � [a,  b] \nwould be C(;S [a, b], 0J [a, b], and rt/' [a, b]. \nLet S be the set of all functions that satisfy the  differential equation \nf\n\" \n+ f \n= 0 \n(\n1\n) \nThat is,  Sis the  solution set of Equation \n(\n1\n)\n. Show that Sis a  subspace of�\n-","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":69038,"to":69065}}}}],[1195,{"pageContent":"436 Chapter 6 Vector Spaces \nIn the words of Yogi Berra, \"It's \ndeja vu  all over again:' \nExample 6.13 \nSolulion S is nonempty, since the zero function clearly satisfies Equation \n(\n1\n)\n. Let \nf and g be in S and let c be a scalar. Then \n(\nf + g\n)\n\" \n+ \n(\nf + g\n) \n= \n(j\" \n+ g\n\"\n)  +  (\nf + g\n) \n= \n(\nf\n\" \n+ f)  +  (\ng\n\" \n+ g\n) \n=O\n+\nO \n=O \nwhich shows that f + g is in S. Similarly, \nso cf is also in S. \n(\ncf\n)\n\" \n+ cf= cf\n\" \n+ cf \n=  c\n(\nf\n\" \n+ f) \n=c\no \n=O \nTherefore, S is closed under addition and scalar multiplication and is a subspace \nof?F. \nThe differential Equation \n( \n1\n) \nis an  example of a homogeneous linear diff erential \nequation. The solution sets of such equations are always subspaces of ?F. Note that in \nExample 6.12 we did not actually solve Equation \n(\n1\n) \n(i.e., we did not find any specific \nsolutions, other than the zero function). We will discuss techniques for finding solu­\ntions to this type of equation in Section 6.7. \nAs you gain experience working with vector spaces and subspaces, you will notice","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":69067,"to":69137}}}}],[1196,{"pageContent":"tions to this type of equation in Section 6.7. \nAs you gain experience working with vector spaces and subspaces, you will notice \nthat certain examples tend to resemble one another. For example, consider the vector \nspaces IR\n4\n, <!/'\n3\n, and M\n22\n• Typical elements of these vector spaces are, respectively, \nu \n� \nm p\n(\nx\n)  �\na+ bx+ ex'  + dx', and A \n� \n[:  !] \nAny calculations involving the vector space operations of addition and scalar multi­\nplication are essentially the same in all three settings. To highlight the similarities, in \nthe next example we will perform the necessary steps in the three vector spaces side \nby side. \n(a) Show that the set W  of all vectors of the form \nis a subspace of IR\n4\n• \n(b)  Show that the set  W of all polynomials of the form a  + bx -   bx\n2 \n+ ax\n3 \nis a \nsubspace of<!!' \n3\n. \n( c)  Show that the set W of all matrices of the form [ \n_\n: �] is a subspace of M\n22\n.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":69137,"to":69176}}}}],[1197,{"pageContent":"Solution \n(a)  W is nonempty because it con­\ntains the zero vector 0. (Take a \n= b = \n0.\n) \nLet u and v be in W-say, \nThen \nu+v= \n[ a\n+ \nc \nl \nb+d \n-\nb \n- d \na + c \n[ a\n+ \nc \nl \nb+d \n-\n(\nb \n+ d\n) \na\n+ \nc \nso u + v is also in W (because it has \nthe right form\n)\n. \nSimilarly, if k is a scalar, then \nso ku is in W. \nSection 6.1 \nVector Spaces and Subspaces \n431 \n(b)  W is nonempty because it con­\ntains the zero polynomial. (Take a = \nb = O.)Letp(x) andq(x) bein W-say, \np\n(\nx\n) \n= a + \nb\nx - bx2 + ax\n3 \nand \nq\n(\nx\n) \n= c + dx - dx\n2 \n+ cx\n3 \nThen \np\n(\nx\n) \n+ q\n(\nx\n) \n= \n(\na + c\n) \n+ \n(\nb + d\n)\nx \n-\n(\nb + d\n)\nx\n2 \n+ \n(\na + c\n)\nx\n3 \nso p(x) + q(x) is also in W (because it \nhas the right form\n)\n. \nSimilarly, if k is a scalar, then \nkp\n(\nx\n) \n= ka + kbx \n-\nkb\nx\n2 \n+ kax\n3 \nso kp (x) is in W. \n(c)  W is nonempty because it  con­\ntains the zero matr  ix 0. (Take a \nb = 0.\n) \nLet A and B be in W-say, \nand \nThen \nA = \n[ \n_\n� �] \nB = \n[ \n_\n� �] \nA+B= \n[ \na\n+ \nc \n-\n(\nb \n+ d\n) \nb + d\n] \na + c \nso A + B is also in W (because it has \nthe right form\n)\n. \nSimilarly, if k is a scalar, then \n[ \nk\na \nkb\n] \nkA = \n-\nkb \nka \nso kA is in W.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":69178,"to":69327}}}}],[1198,{"pageContent":"and \nThen \nA = \n[ \n_\n� �] \nB = \n[ \n_\n� �] \nA+B= \n[ \na\n+ \nc \n-\n(\nb \n+ d\n) \nb + d\n] \na + c \nso A + B is also in W (because it has \nthe right form\n)\n. \nSimilarly, if k is a scalar, then \n[ \nk\na \nkb\n] \nkA = \n-\nkb \nka \nso kA is in W. \nThus, W is a nonempty subset of \nIR\n4 \nthat is clo  sed under addition and \nscalar multiplication. Therefore, W is \na subspace of iR\n4\n, by Theorem 6.2. \nThus, W is a nonempty subset of \n<;IP \n3 \nthat is closed under addition and \nscalar multiplication. Therefore, Wis \na \nsubspace of<;if \n3 \nby Theorem 6.2. \nThus, W is a nonempty subset of \nM\n22 \nthat is closed under addition and \nscalar multiplication. Therefore, W is \na subspace of M\n22\n, by Theorem \n6.\n4 \nExample 6.14 \nExample 6.13 shows that it  is often possible to relate examples that, on the surface, \nappear to have nothing in common. Consequently, we can apply our knowledge of \n!R\nn \nto polynomials, matrices, and other examples. We will encounter this idea several \ntimes in this chapter and will make it precise in Section 6.5.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":69327,"to":69398}}}}],[1199,{"pageContent":"!R\nn \nto polynomials, matrices, and other examples. We will encounter this idea several \ntimes in this chapter and will make it precise in Section 6.5. \nIf Vis a  vector space, then Vis clearly a subspace of itself. The set {O}, consisting of \nonly the zero vector, is also a subspace of V, called the zero subspace. To show this, we \nsimply note that the two closure conditions of Theorem 6.2 are satisfied: \n0 + 0 = 0 \nand c 0 = 0 for any scalar c \nThe subspaces {O} and V are called the trivial subspaces of V.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":69398,"to":69407}}}}],[1200,{"pageContent":"438 \nChapter 6 Vector Spaces \nExample 6.15 \nExample 6.16 \nAn examination of the proof of Theorem 6.2 reveals the following useful fact: \nIf W is a subspace of a vector space V, then W contains the zero vector 0 of V. \nThis fact is consistent with, and analogous to, the fact that lines and planes are sub­\nspaces of!R\n3 \nif and only if they contain the origin. The requirement that every subspace \nmust contain 0 is sometimes useful in showing that a  set is not a subspace. \nLet W be the set of all 2  X  2 matrices of the form \nIs W a   subspace of M\n22\n? \nSolulion Each matrix in W has the property that its \n( \n1, 2\n) \nentry is one more than its \n( \n1, 1\n) \nentry. Since the zero matr  ix \n0 = [�  �] \ndoes not have this property, it is not in W. Hence, Wis not a subspace of M\n22\n• \nLet W be the set of all 2  X  2 matrices with determinant equal to 0. Is W a   subspace \nof M\n22\n? (Since <let 0 = 0, the zero matrix is in W, so the method of Example 6.15 is \nof no use to us.) \nSolulion Let \nA = [ \n� \n�] and  B = [ � \n�\n]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":69409,"to":69447}}}}],[1201,{"pageContent":"of M\n22\n? (Since <let 0 = 0, the zero matrix is in W, so the method of Example 6.15 is \nof no use to us.) \nSolulion Let \nA = [ \n� \n�] and  B = [ � \n�\n] \nThen <let A = <let B = 0, so A and B are in W. But \nA + B = [\n� \n�\n] \nso det(A + B) = 1 =fa 0, and therefore A +Bis not  in W. Thus, Wis not closed under \naddition and so is not a subspace of M\n22\n. \nSpanning sets \nThe notion of a spanning set of vectors carries over easily from !R\nn \nto  general vector \nspaces. \nDefinition \nIf \nS = \n{v\n1\n, v\n2\n, ..• , vd is a set of vectors in a vector space V, then \nthe set of all linear combinations of v\n1\n, v\n2\n, ... , vk is   called the span of v\n1\n, v\n2\n, ... , vk \nand is denoted by span(v\n1\n, v\n2\n, ... , vk) or span(S). If V = span(S), then Sis called \na sp anning set for V and Vis said to be spanned by S.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":69447,"to":69493}}}}],[1202,{"pageContent":"Example 6.11 \nExample 6.18 \nExample 6.19 \nSection 6.1 \nVector Spaces and Subspaces \n439 \nShow that the polynomials 1, x, and x\n2 \nspan l!J' \n2\n• \nSolution By its very definition, a polynomial p (x) = a + bx + cx\n2 \nis a linear combi­\nnation of 1, x, and x\n2\n• Therefore, l!J'\n2 \n= span(l, x, x\n2\n). \nExample 6.17 can clearly be generalized to show that l!J' \nn \n=  span(l, x, x\n2\n, ... , \nx\nn\n). However, no finite set of polynomials can possibly span l!J', the vector space of \nall polynomials.  (See Exercise 44 in Section 6.2.\n) \nBut,  if we allow a spanning set \nto be infinite, then clearly the set of all nonnegative powers of x will do. That is, \nl!J' =   span(l, x, x\n2\n, ••• ). \nShow that M\n2\n3 \n= span(E\n11\n, E\n1\n2\n, E\n1\n3\n, E\n2\n1\n, E\n22\n, E\n23\n), where \n[\n� \n0 \n�\n] \n[\n� \n�\n] \n[\n� \n0 \n�\n] \nE\nll\n= \n0 \nE\n1\n2 \n= \n0 \nE\n1\n3 \n= \n0 \nE\n2\n1 \n= \n[ \n� \n0 \n�\n] \nE\n22 \n= \n[\n� \n0 \n�\n] \nE\n2\n3 \n= \n[\n� \n0 \n�\n] \n0 0 \n(That is,  E;\nj \nis the matrix with a 1  in row i, columnj and  zeros elsewhere.) \nSolution \nWe need only observe that \nExtending this example, we see that, in general, M\nm\nn \nis spanned by the mn matri­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":69495,"to":69613}}}}],[1203,{"pageContent":"0 0 \n(That is,  E;\nj \nis the matrix with a 1  in row i, columnj and  zeros elsewhere.) \nSolution \nWe need only observe that \nExtending this example, we see that, in general, M\nm\nn \nis spanned by the mn matri­\nces E\nij\n, where i = 1, ... , m andj =   1, ... , n. \nIn l!J'\n2\n, determine whether r(x) = 1 -4x + 6x\n2 \nis in  span(p(x), q(x)), where \np\n(\nx\n) \n=  1 -x + x\n2 \nand q\n(\nx\n) \n= 2 + x -3x\n2 \nSolution We are looking for scalars c and d such that cp(x) +  dq(x) = r(x). This \nmeans that \nc\n(\nl  - x + x\n2\n) \n+ d\n(\n2 + x -3x\n2\n) \n=  1  -4x + 6x\n2 \nRegrouping according powers of x, we have \n(\nc + 2d\n) \n+ \n( \n-c + d\n)\nx + \n(\nc -  3d\n)\nx\n2 \n=  1  -4x + 6x\n2 \nEquating the coefficients of like powers of x gives \nc + 2d = \n1 \n-c +  d = -4 \nc -  3d = \n6","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":69613,"to":69678}}}}],[1204,{"pageContent":"440 \nChapter 6 Vector Spaces \nExample 6.20 \nExample 6.21 \nwhich is easily solved to give c = 3 and d = -1. Therefore, r(x) = 3p(x) - q(x), so \nr(x) is in span(p(x), q( x)). (Check this.) \nIn ?F, determine whether sin 2x is in span(sin x, cos x). \nSolution We set c sin x + d cos x = sin 2x and try to determine c and d so that this \nequation is true. Since these are functions, the equation must be true for all values of \nx. Setting x =  0, we have \ncsinO+ dcosO= sin O  or  c\n(\nO\n)\n+d\n(\nl\n)\n=O \nfrom which we see that d =   0. Setting x = 1T \n/\n2, we get \ncsin\n(\n7r/\n2\n) \n+ dcos\n(\n'TT\n/\n2\n) \n= sin\n(\n7T\n) \nor  c\n(\nl\n) \n+ d\n(\nO\n) \n= 0 \ngiving c =  0. But this implies that sin  2x = O(sin x) + O(cos x) = 0 for all x, which \nis absurd, since sin 2x is not the zero function. We conclude that sin 2x is not in \nspan(sin x, cos x). \nRemark It is true that sin  2x can be written in terms of sin x and cos x. For \nexample, we have the double angle formula sin 2x = 2   sin x cos x. However, this is not \na linear combination. \nIn M\n22","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":69680,"to":69733}}}}],[1205,{"pageContent":"Remark It is true that sin  2x can be written in terms of sin x and cos x. For \nexample, we have the double angle formula sin 2x = 2   sin x cos x. However, this is not \na linear combination. \nIn M\n22\n, describe the span of A = [ \n� \n� \nl B = [ \n� \n� \nl and C = [ \n� \n�\n]\n. \nSolution Every linear combination of A, B, and C is of the form \ncA + dB + eC =  c[\n�  �\n] + d[\n�  �\n] + e[\n� \n�\n] \n= [\nc + d \nc + e\n] \nc + e d \nThis matrix is symmetric, so span(A, B, C\n) \nis contained within the subspace of sym­\nmetric 2 X 2 matrices. In fact, we have equality; that is, every symmetric 2 X 2 matrix is \nin span(A, B, C\n)\n. To show this, we let [\n; \n�] be a   symmetric 2 X 2 matr  ix. Setting \n[\nx y\n] = [\nc + d  c + e\n] \ny z \nc+e \nd \nand solving for c and d, we find that c = x -z, d = z, and e = -x + y + z. Therefore, \n� \n(Check this.) It follows that span(A, B, C\n) \nis the subspace of symmetric 2 X 2 matrices . \n.+","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":69733,"to":69785}}}}],[1206,{"pageContent":"Section 6.1 \nVector Spaces and Subspaces \n441 \nAs was the case in !R\nn\n, the span of a set of vectors is always a subspace of the vector \nspace that contains them. The next theorem makes this result precise. It generalizes \nTheorem 3.19. \nTheorem 6.3 \nLet \nV\ni, v\n2\n, •.. , vk be vectors in a vector space V. \na.  span(vi, v\n2\n, •.• , vk) is   a subspace of V. \nb.  span (vi, v\n2\n, .•. , vk) is   the smallest subspace of V that contains \nV\ni, v\n2\n, •.. , vk. \nProof (a)  The proofofproperty (a) is identical to the proofofTheorem 3.19, with \n!R\nn \nreplaced by V. \n(b) To establish property (b ), we need to show that any subspace of V that contains \nV\ni, v\n2\n, .•• , vk also contains span( vi, v\n2\n, .•• , vk)\n· \nAccordingly, let W be a subspace of V \nthat contains \nV\ni, v\n2\n, ... , vk. Then, since Wis closed under addition and scalar multi­\nplication, it contains every linear combination Ci \nV\ni + c\n2\nv\n2 \n+ ·  ·  · + ckvk of \nV\ni, v\n2\n, •.. , \nvk. Therefore, span(vi, v\n2\n, .•. , vk) is contained in W. \n1 \nExercises 6.1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":69787,"to":69844}}}}],[1207,{"pageContent":"plication, it contains every linear combination Ci \nV\ni + c\n2\nv\n2 \n+ ·  ·  · + ckvk of \nV\ni, v\n2\n, •.. , \nvk. Therefore, span(vi, v\n2\n, .•. , vk) is contained in W. \n1 \nExercises 6.1 \nIn Exercises 1-11, determine whether th e given set, together \nwith the specified operations of addition and scalar multi­\nplication, is a vector space. If it is not, list all of the axioms \nthat fail to hold. \n1. The set of all vectors in IR\n2 \nof the form [: l with the \nusual vector addition and scalar multiplication \n2. The set of all vectors [\n;\n] in IR\n2 \nwith x 2: 0, y 2: 0   (i.e., \nthe first quadrant), with the usual vector addition and \nscalar multiplication \n3. The set of all vectors [\n�\n] in IR\n2 \nwith xy 2: 0   (i.e., the \nunion of the first and third quadrants), with the usual \nvector addition and scalar multiplication \n4\n. \nThe set of all vectors [\n;\n] in IR\n2 \nwith x 2: y, with the \nusual vector addition and scalar multiplication \n5. IR\n2\n, with the usual addition but scalar multiplication \ndefined by \n6. IR\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":69844,"to":69895}}}}],[1208,{"pageContent":"4\n. \nThe set of all vectors [\n;\n] in IR\n2 \nwith x 2: y, with the \nusual vector addition and scalar multiplication \n5. IR\n2\n, with the usual addition but scalar multiplication \ndefined by \n6. IR\n2\n, with the usual scalar multiplication but addition \ndefined by \n[\nx\n,\n] \n+ \n[\nx\n2\n] = \n[\nx\n, \n+ x\n2 \n+ l\n] \nY\n1 \nY\nz \nY\ni \n+ \nY\nz +  1 \n7. The set of all positive real numbers, with addition EB \ndefined by x EB y \n= \nxy and scalar multiplication 0 \ndefined by c 0 x \n= \nxc \n8. The set of all rational numbers, with the usual addition \nand multiplication \n9. The set of all upper triangular 2 X 2 matrices, with the \nusual matr  ix addition and scalar multiplication \n10. The set of all 2  X  2 matrices of the form [: � l \nwhere ad \n= \n0, with the usual matrix addition and \nscalar multiplication \n11. The set of all skew-symmetric n X n matrices, with the \nusual matrix addition and scalar multiplication \n(see page 162). \n12. Finish verifying that <!f \n2 \nis a vector space \n(see Example 6.3). \n13. Finish verifying that ge is a vector space \n(see Example 6.4).","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":69895,"to":69960}}}}],[1209,{"pageContent":"442 \nChapter 6 Vector Spaces \nE:v In Exercises 14-17, determine whether th e given set, together \nwith the specified operations of addition and scalar multi­\nplication, is a complex vector space. If it is not, list all of \nth e axioms that fail to hold. \n14. The set of all vectors in C\n2 \nof the form [�], with the \nusual vector addition and scalar multiplication \n15. The set Mm\nn\n(\nC\n) \nof all m X n complex matrices, with \nthe usual matr  ix addition and scalar multiplication \n16. The set C\n2\n, with the usual vector addition but scalar \nmultiplication defined by c[\nz\n1\n] = [\n�\nz\n1\n] \nZ\n2 \nC\nZ\n2 \n17. !R\nn\n, with the usual vector addition and scalar multiplication \n30. V = M\nnn\n' W = {A in M\nnn \n: <let A = 1} \n31. V \n= \nM\nnn\n' Wis the set of diagonal n X n matrices \n32. V = M\nnn\n> Wis the set of idempotent n X n matrices \n33. V = M\nnn\n' W = {A in M\nnn \n: AB = BA}, where Bis a \ngiven (fixed) matrix \n34. V = C!P\n2\n, W = {bx  +  cx\n2\n} \n35. V = C!P\n2\n, W = {a+ bx+  cx\n2\n: a  +  b  +  c  = O} \n36. V = C!P\n2\n, W = \n{a+ bx+  cx\n2\n: abc = O}","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":69962,"to":70032}}}}],[1210,{"pageContent":"33. V = M\nnn\n' W = {A in M\nnn \n: AB = BA}, where Bis a \ngiven (fixed) matrix \n34. V = C!P\n2\n, W = {bx  +  cx\n2\n} \n35. V = C!P\n2\n, W = {a+ bx+  cx\n2\n: a  +  b  +  c  = O} \n36. V = C!P\n2\n, W = \n{a+ bx+  cx\n2\n: abc = O} \n37. V = C!P, Wis the set of all polynomials of degree 3 \n38. \nV = ':ffe, W ={fin '2F :f\n(\n-x\n)  = f (x\n)\n} \n39. V = ':ffe, W = {f in '2F :f\n(\n-x\n) \n= -f\n(\nx)} \nIn Exercises 18-21, determine whether th e given set, together \n40. \nV = ':ffe, w = {f in '2F :f(O)  = l} \nwith th e specified operations of addition and scalar multipli-\n_ \na;-; \n_ \n{f \n. \na;-; \n·\nf\n( \n) \n_ \n} \n. . \nth \n. \nd\" t d \n77 \nIf \"t \n. \nt l\" t \n41. v \n-\n'!:Y, w \n-\nm ';'Y. 0 \n-\n0 \ncatzon, is a \n�\nector space �ver e in zca e \nlL\np\n· 1 is no , is ·tt;. \n_ \nu.:: \n. \n. . \nall of the axzoms that fail to hold. \nJlh 42. V \n-\n'!:Y, W is the set of all mtegrable funct10ns \n18. The set of all vectors in \"11._� with an even number of \nJlh \n43. V = \nCZJJ , \nW = {fin CZl! :f'(x\n) \n2: 0 for all x} \nls, over \"ll\n2 \nwith the usual vector addition and scalar \n� \n44. V = ':ffe, W =  ct;\n(2\nl","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":70032,"to":70127}}}}],[1211,{"pageContent":"18. The set of all vectors in \"11._� with an even number of \nJlh \n43. V = \nCZJJ , \nW = {fin CZl! :f'(x\n) \n2: 0 for all x} \nls, over \"ll\n2 \nwith the usual vector addition and scalar \n� \n44. V = ':ffe, W =  ct;\n(2\nl\n, the set of all functions with \nmultiplication \ncontinuous second derivatives \n19. The set of all vectors in \"11._� with an odd number of \nJlh \n45. \nV = ':ffe, W = {f in '2F: \n= lim f \n(\nx\n)  = \n00\n} \nX->0 \nls, over \"ll\n2 \nwith the usual vector addition and scalar \n46. Let Vbe a  vector space with subspaces U and W. Prove \nmultiplication \nthat U n Wis a  subspace of V. \n20. The set Mm\nn\n(\n\"ll\np\n) \nof all m X n matrices with entries \n47. Let Vbe a  vector space with subspaces U and W. Give \nfrom \"11._\nP\n' over \"11._\nP \nwith the usual matrix addition and \nan example with v =  [R\n2 \nto show that U U W need not \nscalar multiplication \nbe a subspace of V. \n21. \"ll 6, over \"ll\n3 \nwith the usual addition and multiplication \n48. Let Vbe a  vector space with subspaces U and W. \n(Think this one through carefully!)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":70127,"to":70183}}}}],[1212,{"pageContent":"scalar multiplication \nbe a subspace of V. \n21. \"ll 6, over \"ll\n3 \nwith the usual addition and multiplication \n48. Let Vbe a  vector space with subspaces U and W. \n(Think this one through carefully!) \nDefine the sum of  U and W to be \n22. Prove Theorem 6.l(a). \n23. Prove Theorem 6.l(c). \nIn Exercises 24-45, use Theorem 6.2 to determine whether \nW is a subspace of V \n24. v \n� \n�'\n. \nw \n� mii \n25. v \n� \n�'\n. \nw \n� trn \n26. \nv \n= \nIR\n3\n, \nw \n= \nf \n[ \n� \nl) \nl \na+b+l \n27. v\n� \n�'\n. \nw\n� \nlUJ\nl \nU + W \n= \n{ u + w : u is in U, w is in W} \n(a) If V = IR\n3\n, U is the x-axis, and Wis the y-axis, \nwhat is U + W? \n(b) If U and Ware subspaces of a vector space V, \nprove that U + Wis a  subspace of V. \n49. If U and V are vector spaces, define the Cartesian \nproduct of U and V to be \nU X V \n= \n{(u, v) : u is in U and vis in V} \nProve that U X Vis a  vector space. \n50. Let W be a subspace of a vector space V. Prove that \nLi  = {(w, w): w is in W} is a subspace of V X   V. \nIn Exercises 51 and 52, let A  = [ \n1 1\n] and \n[\nl -1\n] \n-1 1 \nB = \n1 0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":70183,"to":70254}}}}],[1213,{"pageContent":"50. Let W be a subspace of a vector space V. Prove that \nLi  = {(w, w): w is in W} is a subspace of V X   V. \nIn Exercises 51 and 52, let A  = [ \n1 1\n] and \n[\nl -1\n] \n-1 1 \nB = \n1 0 \n. Determine whether C is in span(A, B). \n[\nl 2\n] \n[\n3 -5\n] \n51. c \n= \n52. c \n= \n3     4 \n5 -1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":70254,"to":70277}}}}],[1214,{"pageContent":"Section 6.2 \nLinear Independence, Basis, and Dimension \n443 \nIn Exercises 53 and 54, let p\n(\nx\n) \n= 1 -2x, q (\nx\n) \n= x -  x\n2\n, \nand r\n(\nx\n) \n=  -2 + 3x + x\n2\n• Determine whether s\n(\nx\n) \nis in \nspan\n(\np\n(\nx\n)\n, q (\nx\n)\n, r\n(\nx\n) )\n. \n61. Is <!!'\n2 \nspanned by 1 + x, x + x\n2\n, 1 + x\n2\n? \ni\n] [\no \n-1\n]\n? \n1 \n' \n1 \n0 \n53. s\n(\nx\n) \n= 3 -5x -  x\n2 \n54.  s\n(\nx\n) \n= 1 + x + x\n2 \nIn Exercises 55-58, let f\n(\nx\n) \n= sin\n2\nx and g\n(\nx\n) \n= cos\n2\nx. \nDetermine whether h\n(\nx\n) \nis in span(j\n(\nx\n)\n, g\n(\nx\n) )\n. \n62. Is <!!'\n2 \nspanned by 1 + x + 2x\n2\n, 2 + x + 2x\n2\n, \n-1 + x + 2x\n2\n? \n63. Prove that every vector space has a   unique zero \nvector. \n55. h\n(\nx) = 1 \n56. h\n(\nx\n) \n= cos 2x \n57. h \n(\nx\n) = sin 2x \n58. h \n(\nx\n) \n= sin x \n64. Prove that for every vector v in a vector space V, \nthere is a unique v\n' \nin V such that v + v\n' \n= 0. \n59. Is M\n22 \nspanned by \n[ \n� \n� \nl \n[ \n� \n� \nl \n[ \n� \n� \nl \n[ \n� \n-\n�\n]\n? \nWriting Project \nThe Rise of Vector Spaces \nAs noted in the sidebar on page 429, in the late 19th century, the mathematicians \nHermann Grassmann and Giuseppe Peano were instrumental in introducing the","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":70279,"to":70431}}}}],[1215,{"pageContent":"? \nWriting Project \nThe Rise of Vector Spaces \nAs noted in the sidebar on page 429, in the late 19th century, the mathematicians \nHermann Grassmann and Giuseppe Peano were instrumental in introducing the \nidea of a vector space and the vector space axioms that we use today. Grassmann's \nwork had its origins in barycentric coordinates, a technique invented in 1827 by \nAugust Ferdinand Mobius (ofMobius strip fame). However, widespread acceptance \nof the vector space concept did not come until the early 20th century. \nWrite a report on the history of vector spaces. Discuss the origins of the notion of \na vector space and the contributions of Grassmann and Peano. Why was the math­\nematical community slow to adopt these ideas, and how did acceptance come about? \n1. Carl B. Boyer and Uta C. Merzbach, A History of Mathematics (Third Edition) \n(Hoboken, NJ: Wiley, 2011). \n2. Jean-Luc, Dorier (1995), A General Outline of the Genesis of Vector Space \nTheory, Historia Mathematica 22 (1995), pp. 227-261.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":70431,"to":70446}}}}],[1216,{"pageContent":"(Hoboken, NJ: Wiley, 2011). \n2. Jean-Luc, Dorier (1995), A General Outline of the Genesis of Vector Space \nTheory, Historia Mathematica 22 (1995), pp. 227-261. \n3. Victor J. Katz, A History of Mathematics: An Introduction (Third Edition) (Read­\ning, MA: Addison Wesley Longman, 2008). \nf \nlinear Independence. Basis,  and Dimension \nIn this section, we extend the notions of linear independence, basis, and dimension \nto general vector spaces, generalizing the results of Sections 2.3 and 3.5. In most cases, \nthe proofs of the theorems carry over; we simply replace !R\nn \nby the vector space V. \nlinear Independence \nDefinition \nA set of vectors {v\n1 , v\n2\n, ... , vd in a vector space Vis linearly  de­\npendent if there are scalars c\n1 , c\n2\n, ... , c\nk\n, at least one of which is not zero, such that \nA set of   vectors that is not linearly dependent is said to be linearly independent.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":70446,"to":70470}}}}],[1217,{"pageContent":"444 \nChapter 6 Vector Spaces \nTheorem 6.4 \nExample 6.22 \nExample 6.23 \nExample 6.24 \nExample 6.25 \nAs in IJ�r, {v\n1\n, v\n2\n, .•• , vd is line arly independent in a vector space V if and only if \nc\n1\nv\n1 \n+ \nc\n2\nv\n2 \n+ \n· · · \n+ \nc\nk\nv\nk \n=  0 \nimplies \nc\n1 \n= \n0, c\n2 \n= \n0, ... , c\nk \n= \n0 \nWe also have the following useful alternative formulation oflinear dependence. \nA set of vectors {v\n1\n, v\n2\n, .•. , v\nk\n} in a vector space Vis linearly dependent if and only \nif at least one of the vectors can be expressed as a linear combination of the others. \nProof \nThe proof is identical to that of Theorem 2.5. \nAs a special case of Theorem 6.4, note that a set of two vectors is linearly depen­\ndent if and only if one is a scalar multiple of the other. \nIn l!f \n2\n, the set {l + x + x\n2\n, 1 - x + 3x\n2\n, 1  + 3x - x\n2\n} is linearly dependent, since \nIn M\n22\n, let \n2\n(\n1 \n+ x + x\n2\n) \n-(1 - x \n+ 3x\n2\n) \n=  1 \n+ 3x - x\n2 \nA = [\n�  �\nl \nB \n= [\n� \n-1] \nC= \n[2 \nO\nJ \n0 \n, \n1 1 \nThen A + B = C, so the set {A, B, C} is linearly dependent. \nIn ':IF, the set {sin\n2\nx, cos\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":70472,"to":70568}}}}],[1218,{"pageContent":"In M\n22\n, let \n2\n(\n1 \n+ x + x\n2\n) \n-(1 - x \n+ 3x\n2\n) \n=  1 \n+ 3x - x\n2 \nA = [\n�  �\nl \nB \n= [\n� \n-1] \nC= \n[2 \nO\nJ \n0 \n, \n1 1 \nThen A + B = C, so the set {A, B, C} is linearly dependent. \nIn ':IF, the set {sin\n2\nx, cos\n2\nx, cos 2x} is linearly dependent, since \ncos 2x = cos\n2\nx -  sin\n2\nx \nShow that the set {l, x, x\n2\n, ... , x\nn\n} is linearly independent in <!P w \nSolution 1 \nSuppose that c\n0\n, c\n1\n, .•. , e\nn \nare scalars such that \nC\no\n· 1 \n+ C\n1\nX + C\nz\nX\n2 \n+ \n· · · \n+ C\nn\nX\nn \n= 0 \nThen the polynomial p (x) = c\n0 \n+ c\n1\nx + c\n2\nx\n2 \n+ ·  ·  · + c\nn\nx\nn \nis zero for all values of x. But \na polynomial of degree at most n cannot have more than n zeros (see Appendix D). \nSo p (x) must be the zero polynomial, meaning that c\n0 \n= c\n1 \n= c\n2 \n= ·  ·  · = e\nn \n= 0. \nTherefore, {l, x,  x\n2\n, ... , x\nn\n} is linearly independent. \nl� \nSolution 2 We begin, as in the first solution, by assuming that \np\n(\nx\n) \n= \nC\no \n+ C\n1\nX + C\nz\nX\n2 \n+ \n· · · \n+ C\nn\nX\nn \n= Q \nSince this is true for all x, we can substitute x = 0 to obtain c\n0 \n= 0. This leaves \nC\n1\nX + C\nz\nX\n2 \n+ \n· · · \n+ C\nn\nX\nn \n= 0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":70568,"to":70703}}}}],[1219,{"pageContent":"Example 6.26 \nSection 6.2 \nLinear Independence, Basis, and Dimension \n445 \nTaking derivatives, we obtain \nC\n1 \n+ 2C\nz\nX + 3C\n3\nX\n2 \n+ \n·   ·   · \n+ nc\nn\nx\nn\n-\nI \n= 0 \nand setting x = 0, we see that c\n1 \n= 0. Differentiating 2c\n2\nx + 3c\n3\nx\n2 \n+ ·  ·  · + nc\nn\nx\nn\n-\nI \n= 0 \nand setting x = 0, we find that 2c\n2 \n= 0, so c\n2 \n= 0. Continuing in this fashion, we \nfind that k!ck = 0 for k = 0, ... , n. Therefore, c\n0 \n= c\n1 \n= c\n2 \n= ·  ·  · = e\nn \n= 0, and {1, x, \nx\n2\n, ... \n'\nx\nn\n} is linearly independent. \n4 \nIn <;!/'\n2\n, determine whether the set {1 + x, x + x\n2\n, 1  + x\n2\n} is linearly independent. \nSolution \nLet c\n1\n, c\n2\n, and c\n3 \nbe scalars such that \nThen \nThis implies that \nC\n1 \n+ \nc\n3 \n= 0 \nC\n1 \n+ C\nz \n= 0 \nC\n2 \n+ C\n3 \n= 0 \nthe solution to which is c\n1 \n= \nc\n2 \n= \nc\n3 \n= 0. It follows that {1 + x, x + x\n2\n, 1  + x\n2\n} is \nlinearly independent. \nRemark Compare Example 6.26 with Example 2.23(b). The system of equations \nthat arises is exactly the same. This is because of the correspondence between <;!/' \n2 \nand \nIR\n3 \nthat relates \nI  + rn [iJ x + x\n' \n-\n[\n: \nJ \n1  + x\n' \n-m","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":70705,"to":70826}}}}],[1220,{"pageContent":"that arises is exactly the same. This is because of the correspondence between <;!/' \n2 \nand \nIR\n3 \nthat relates \nI  + rn [iJ x + x\n' \n-\n[\n: \nJ \n1  + x\n' \n-m \nand produces the columns of the coefficient matrix of the linear system that we have \nto solve. Thus, showing that {1 \n+ x, x + x\n2\n, 1 \n+ x\n2\n} is linearly independent is equiva­\nlent to showing that \nis line arly independent. This can be done simply by establishing that the  matrix \n[\ni \n: \n�\n] \nhas rank 3, by the Fundamental Theorem of Invertible Matrices.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":70826,"to":70856}}}}],[1221,{"pageContent":"446 \nChapter 6 Vector Spaces \nExample 6.21 \nExample 6.28 \nIn ?JP, determine whether the set {sin x, cos x} is linearly independent. \nSolulion The functions f(x) = sin x and g(x) = cos x are linearly dependent if and \nonly if one of them is a scalar multiple of the other. But it is clear from their graphs \nthat this is not the case, since, for example, any nonzero multiple ofj(x) = sin x has \nthe same zeros, none of which are zeros ofg(x) = cos x. \nThis approach may not always be appropriate to   use, so we offer the following \ndirect, more computational method. Suppose c and dare scalars such that \nc sin x + d cos x = 0 \nSetting x = 0, we obtain d = 0, and setting x =   n/2, we obtain c = 0. Therefore, the \nset {sin x, cos x} is linearly independent. \nAlthough the definitions of linear dependence and independence are phrased \nin terms of finite sets of vectors, we  can extend the concepts to infinite sets as \nfollows:","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":70858,"to":70874}}}}],[1222,{"pageContent":"Although the definitions of linear dependence and independence are phrased \nin terms of finite sets of vectors, we  can extend the concepts to infinite sets as \nfollows: \nA set S of vectors in a vector space V is linearly  dependent if it contains finitely \nmany linearly dependent vectors. A  set of   vectors that is not linearly dependent is \nsaid to be linearly independent. \nNote that for finite sets of vectors, this is just the original definition. Following is an \nexample of an infinite set of linearly independent vectors. \nIn <lP, show that S = {l, x, x\n2\n, ... } is linearly independent. \nSolulion Suppose there is a finite subset T of S that is linearly dependent.  Let x\nm \nbe \nthe highest power of x in T and let x\nn \nbe the lowest power of x in T. Then there are \nscalars en, Cn+l' ... , c\nm\n, not all zero, such that \nBut, by an argument similar to that used in Example 6.25, this implies that en \n= \nCn+i = ·  ·  · = cm\n= \n0, which is a contradiction. Hence, S cannot contain finitely many","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":70874,"to":70898}}}}],[1223,{"pageContent":"m\n, not all zero, such that \nBut, by an argument similar to that used in Example 6.25, this implies that en \n= \nCn+i = ·  ·  · = cm\n= \n0, which is a contradiction. Hence, S cannot contain finitely many \nlinearly dependent ve ctors, so it is linearly independent. \nBases \nThe important concept of a basis now can be extended easily to arbitrary vector \nspaces. \nDefinition \nA subset l3 of a vector space Vis a basis for V if \n1. l3 spans V and \n2. l3 is linearly independent.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":70898,"to":70912}}}}],[1224,{"pageContent":"Example 6.29 \nExample 6.30 \nExample 6.31 \nExample 6.32 \nSection 6.2 \nLinear Independence, Basis, and Dimension \n441 \nIf e; is the ith column of the n X n identity matrix, then { e1, e\n2\n, ... , e\nn\n} is a basis for !R\nn\n, \ncalled the standard basis for !R\nn\n. \n{l, x, x\n2\n, •.• , x\nn\n} is a basis for '!P \nn\n' called the standard basis for '!P \nn\n-\nThe set [ \n= \n{E\n11\n, .•. , E\n1n\n, E\n2\n1\n, ••• , E\n2\nn\n, E\nm\n1\n, ••• , E\nm\nn\n} is a basis for \nMm\nn\n' where the \nmatrices E\nij \nare as defined in Example 6.18. [is called the standard basis for \nMm\nn\n­\nWe have already seen that [  spans \nMm\nn\n-It is easy to show that [is linearly inde­\npendent.  (Verify this!) Hence, [is a  basis for \nMm\nn\n-\nShow that B \n= \n{l + x, x +  x\n2\n, 1 +  x\n2\n} is a basis for '!1'\n2\n• \nSolution We have already shown that Bis linearly independent, in Example 6.26. To \nshow that B  spans '!1'\n2\n, let a  +  bx +  cx\n2 \nbe an arbitrary polynomial in '!1'\n2\n• We must \nshow that there are scalars c\n1 , c\n2\n, and c\n3 \nsuch that \nor, equivalently, \n(\nc\n1 \n+  c\n3\n) \n+ \n(\nc\n1 \n+  c\n2\n)\nx \n+ \n(c\n2 \n+  c\n3\n)\nx\n2 \n= \na  + bx +  cx\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":70914,"to":71024}}}}],[1225,{"pageContent":"2\n, let a  +  bx +  cx\n2 \nbe an arbitrary polynomial in '!1'\n2\n• We must \nshow that there are scalars c\n1 , c\n2\n, and c\n3 \nsuch that \nor, equivalently, \n(\nc\n1 \n+  c\n3\n) \n+ \n(\nc\n1 \n+  c\n2\n)\nx \n+ \n(c\n2 \n+  c\n3\n)\nx\n2 \n= \na  + bx +  cx\n2 \nEquating coefficients of like powers of x, we obtain the linear system \nwhi<h has a solution, since the weffideot matcix \n[ \ni : \n�\n] \nhas rnok 3 aod, hence, \nis invertible. (We do not need to know what the solution is; we only need to know that \nit exists.) Therefore, Bis a  basis for '!P \n2\n• \nRo11ark Obmve that the matrix \n[ \ni \nO \n�\n] \nis the  key to Example 6.32. We rnn \nimmediately obtain it using the correspondence between '!1'\n2 \nand IR\n3\n, as indicated in \nthe Remark following Example 6.26.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":71024,"to":71085}}}}],[1226,{"pageContent":"448 \nChapter 6 Vector Spaces \nExample 6.33 \nExample 6.34 \n(a) Since \nwe have W\n1 \n= span(u, \nv) , where \nSince {u, v} is clearly linearly in­\ndependent, it is also a  basis for W\n1\n. \nTheorem 6.5 \nShow that B = {l, x, x\n2\n, ••• } is a basis for <!P. \nSolulion In Example 6.28, we saw that B is linearly independent. It also spans <!P, \nsince clearly every polynomial is a linear combination of (finitely many) powers of x . \n.+ \nFind bases for the three vector spaces in Example 6.13: \nSolulion Once again, we will work the three examples side by side to highlight the \nsimilarities among them. In a strong sense, they are all the same example, but it will \ntake us until Section 6.5 to make this idea perfectly precise. \n(b)  Since \na  + bx -  bx\n2 \n+  ax\n3 \n=  a\n(\nl + x\n3\n) \n+ b\n(\nx - x\n2\n) \nwe have W\n2 \n= span(u(x), v(x)), \nwhere \nu\n(\nx\n) \n= 1 + x\n3 \nand \nv\n(\nx\n) \n= x \n-\nx\n2 \nSince {u(x),  v(x)}  is clearly lin­\nearly independent, it  is also  a   basis \nfor W\n2\n. \nCoordina1es \n(c)  Since \n[ \na b\nJ \n[ \n1 O\nJ \nb\n[ \n0 1 \nJ \n-b a \n=  a \n0 1 \n+ \n-1 0 \nwe have W\n3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":71087,"to":71168}}}}],[1227,{"pageContent":"u\n(\nx\n) \n= 1 + x\n3 \nand \nv\n(\nx\n) \n= x \n-\nx\n2 \nSince {u(x),  v(x)}  is clearly lin­\nearly independent, it  is also  a   basis \nfor W\n2\n. \nCoordina1es \n(c)  Since \n[ \na b\nJ \n[ \n1 O\nJ \nb\n[ \n0 1 \nJ \n-b a \n=  a \n0 1 \n+ \n-1 0 \nwe have W\n3 \n= span(U, V), where \nU = \n[ \n1 0\nJ \nand V = \n[ \nO 1 \nJ \n0 1 \n-1 0 \nSince {U, V} is clearly linearly in­\ndependent, it is also a basis for W\n3\n• \n.+ \nSection 3.5 introduced the idea of the coordinates of a vector with respect to a basis \nfor subspaces of !R\nn\n. We now extend this concept to arbitrary vector spaces. \nLet V be a vector space and let B be a basis for V. For every vector v in V, there is \nexactly one way to write v as a linear combination of the basis vectors in B. \nProof The proofis the same as the proof ofTheorem 3.29. It works even ifthe basis Bis \ninfinite, since linear combinations are, by definition, finite.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":71168,"to":71230}}}}],[1228,{"pageContent":"Example 6.35 \nSection 6.2 \nLinear Independence, Basis, and Dimension \n449 \nThe converse of Theorem 6.5 is also true. That is, if Bis a  set of vectors in a vector \nspace V with the property that every vector in V can be written uniquely as a linear \ncombination of the vectors in B, then Bis a  basis for V (see Exercise 30). In this sense, \nthe unique representation property characterizes a basis. \nSince representation of a vector with respect to a basis is unique, the next definition \nmakes sense. \nDefinition \nLet B \n= \n{ v\n1\n, v\n2\n, .•. , v\nn\n} be a basis for a vector space V. Let v be a \nvector in V, and write v \n= \nc\n1\nv\n1 \n+ c\n2\nv\n2 \n+ ·  ·  · + c\nn\nv\nn\n- Then c\n1\n, c\n2\n, •.• , e\nn \nare called the \ncoordinates of v with respect to B, and the column vector \nis called the coordinate vector of v with respect to B. \nObserve that if the basis B of Vhas n vectors, then [ v lB is a (column) vector in !R\nn\n. \nFind the coordinate vector \n[\np\n(\nx\n)\n] B of p (x) \n= \n2 -  3x + 5x\n2 \nwith respect to the stan­\ndard basis B \n= \n{l, x, x\n2\n} of(!J>\n2\n.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":71232,"to":71295}}}}],[1229,{"pageContent":"n\n. \nFind the coordinate vector \n[\np\n(\nx\n)\n] B of p (x) \n= \n2 -  3x + 5x\n2 \nwith respect to the stan­\ndard basis B \n= \n{l, x, x\n2\n} of(!J>\n2\n. \nSolution \nThe polynomial p(x) is already a linear combination of 1, x, and x\n2\n, so \nThis is the correspondence between (!J>\n2 \nand IR\n3 \nthat we remarked on after \nExample 6.26, and it can easily be generalized to show that the coordinate vector of a \npolynomial \nwith respect to the standard basis B \n= \n{l, x, x\n2\n, ... , x\n\"\n} is just the vector \nRemark The order in which the basis vectors appear in B affects the order of \nthe entries in a coordinate vector. For example, in Example 6.35, assume that the","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":71295,"to":71334}}}}],[1230,{"pageContent":"450 \nChapter 6 Vector Spaces \nExample 6.36 \nExample 6.31 \nstandard basis vectors are ordered as !3' \n= \n{x\n2\n, x, l}. Then the coordinate vector of \np (x) \n= \n2 -3x + 5x\n2 \nwith respect to !3' is \n[p(x)]\na \n� \nH\nl \nFind the coordinate vector [A] B of A \n= \n[\n2\n4 \n-1] \n3 \nwith respect to the standard basis \nl3 \n= \n{E\n11\n, E\n1\n2\n, E\n2\n1\n> \nE\n22\n} of M\n22\n• \nSolulion Since \nwe have \nThis is the correspondence between M\n22 \nand IR\n4 \nthat we noted before the intro­\nduction to Example 6.13. It too can easily be generalized to give a correspondence \nbetween M\nm\nn \nand \n!R\nm\nn\n. \nFind the coordinate vector \n[p(\nx\n) ]8 of p \n(\nx\n) \n= \n1 + 2x - x\n2 \nwith respect to the basis \nC \n= \n{l + x, x + x\n2\n, 1 + \nx\n2\n} of\\)}>\n2\n. \nSolulion \nWe need to find c1, c\n2\n, and c3 such that \nc\n1\n(\n1 + x) + c\n2\n(\nx  + x\n2\n) + c\n3\n(\n1 + x\n2\n) = 1 + 2x - x\n2 \nor, equivalently, \n(\nc\n1 \n+ c\n3\n) + \n(\nc\n1 \n+ c\n2\n)x +  (\nc\n2 \n+ \nc\n3\n)x\n2 \n= \n1 + 2x - x\n2 \nAs in   Example 6.32, this means we need to solve the system \nc\n1 \n+ c\n3 \n= \n1 \nC\n1 \n+ \nc\n2 \n2 \nc\n2 \n+ \nc\n3 \n= \n-1 \nwhose so   lution is found to be c1 \n= \n2, c\n2 \n= 0, c3 = -1. Therefore,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":71336,"to":71482}}}}],[1231,{"pageContent":"Theorem 6.6 \nSection 6.2 \nLinear Independence, Basis, and Dimension \n451 \n[\nSince this result says that p \n(\nx\n) \n= \n2 \n(\n1  +  x\n) \n-\n(\n1 \ncorrect.] \n+  x\n2\n)\n, it is  easy to check that it is \n4 \nThe next theorem shows that the process of forming coordinate vectors is com­\npatible with the vector space operations of addition and scalar multiplication. \nLet B \n= \n{v\n1\n, \nV\nz\n, ... , v\nn\n} be a basis for a vector space V. Let u and v be vectors in \nV and let e be a scalar. Then \na. \n[\nu \n+ \nv\nls \n= \n[\nu\nls \n+ \n[\nv\nls \nb. \n[\ncu]\ns \n= \ne\n[\nu]\ns \nProof \nWe begin by writing u and v in terms of the basis vectors-say, as \nThen, using vector space properties, we have \nand \nso \nand \nl\ne\ni\n+  d\n1\n]  [e\n1\nl \nl\nd\n1\n] \ne\nz \n+  d\nz \ne\nz \nd\nz \n. \n+ \n. \n. \n. \n. \n. \n. \ne\nn \n+  d\nn \ne\nn \nd\nn \n[ee\nl\n]  [e\nl\nl \nee\nz \ne\nz \n[\ncu]8 \n= \n: \n= \ne \n: \n= \ne\n[\nu]8 \nee\nn \ne\nn \nAn easy corollary to Theorem 6.6 states that coordinate vectors preserve linear \ncombinations: \n(\n1\n) \nYou are asked to prove this corollary in Exercise 31. \nThe most useful aspect of coordinate vectors is that they allow us to transfer","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":71484,"to":71615}}}}],[1232,{"pageContent":"combinations: \n(\n1\n) \nYou are asked to prove this corollary in Exercise 31. \nThe most useful aspect of coordinate vectors is that they allow us to transfer \ninformation from a general vector space to !R\nn\n, where we have the tools of Chapters 1 \nto 3 at our disposal. We will explore this idea in some detail in Sections 6.3 and 6.6. \nFor now, we have the following useful theorem.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":71615,"to":71625}}}}],[1233,{"pageContent":"452 \nChapter 6 Vector Spaces \nTheorem 6.1 \nTheorem 6.8 \nLet B = {\nv\n1\n, \nv2\n, ••• , \nv\n\"} be a basis for a vector space V and let u1, ... ,  uk \nbe vectors in V. Then {u1, ... , \nud is linearly independent in V if and only if \n{ \n[ u\n1\n] 13, ... , \n[ \nuk \nJ \n13} is line arly independent in IR\n\"\n. \nProof \nAssume that {u1, ... , ud is line arly independent in V and let \ncdu\n1\n]13 + ·  ·  · + cdud13 = 0 \nin IR\n\"\n. But then we have \n[\nc\n1\nu\n1 \n+ ·  ·  · + ckuk]13 = 0 \nusing Equation \n(\n1\n)\n, so the coordinates of the vector c1u1 + ··· + ckuk with respect to \nB are all zero. That is, \nc\n1\nu\n1 \n+ ·  ·  · + ckuk = \nO\nv\n1 \n+ O\nv2 \n+ ·  ·  · + \nO\nv\n\" = 0 \nThe linear independence of {u1,  ... ,  uk} now forces c1 = c\n2 \n= ·  ·  · = ck = 0, so \n{ \n[ u\n1\n] 13, ... , \n[ u\nk \nJ \n13} is linearly independent. \nThe converse implication, which uses similar ideas, is left as Exercise 32. \nObserve that, in the special case where \nU\n; = V;, we have \nV\n; \n= 0 · V\nI \n+ ·  ·  · +  1 · V\n; + ·  ·  · +  0 • V \nn \nDimension","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":71627,"to":71707}}}}],[1234,{"pageContent":"The converse implication, which uses similar ideas, is left as Exercise 32. \nObserve that, in the special case where \nU\n; = V;, we have \nV\n; \n= 0 · V\nI \n+ ·  ·  · +  1 · V\n; + ·  ·  · +  0 • V \nn \nDimension \nThe definition of dimension is the same for a vector space as for a subspace of !R\nn \n-the \nnumber of vectors in a basis for the space. Since a vector space can have more than one \nbasis, we need to show that this definition makes sense; that is, we need to establish \nthat different bases for the same vector space contain the same number of vectors. \nPart (a) of the next theorem generalizes Theorem 2.8. \nLet B = {\nv\n1\n, \nv2\n, ... , \nv\nn\n} be a basis for a vector space V. \na.  Any set of more than n vectors in V must be linearly dependent. \nb.  Any set of   fewer than n vectors in V cannot span V. \nProof (a)  Let {u1, ... , \num} be a set of vectors in V, with m > n. Then { \n[ u\n1 \n]13,  ... , \n[u\nm \nJ \n13} is a  set   of more  than  n vectors in !R\nn \nand,  hence, is linearly dependent,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":71707,"to":71747}}}}],[1235,{"pageContent":"Proof (a)  Let {u1, ... , \num} be a set of vectors in V, with m > n. Then { \n[ u\n1 \n]13,  ... , \n[u\nm \nJ \n13} is a  set   of more  than  n vectors in !R\nn \nand,  hence, is linearly dependent, \nby Theorem 2.8. This means that {u1, ... ,  um} is linearly dependent as well, by \nTheorem 6.7. \n(b) Let {u1, ... , um} be a set of vectors in V, with m < n. Then S = { \n[ u\n1\nJ\n13, ... , \n[um J \n13} \nis a set of fewer than n vectors in !R\nn\n. Now span(u\n1\n, ... ,  u\nm\n) \n=  V   if and only if \nspan(S) =  !R\nn \n(see Exercise 33\n)\n. But span(S) is just the column space of the n  X m \nmatrix \nA= \n[[u\n1\n] 13··· \n[u\nm\nJ\n13 ] \nso dim(span(S)) = dim(col(A)) :s  m < n. Hence, S cannot span !R\nn\n, so {u1, ... , um} \ndoes not span V. \nNow we extend Theorem 3.23.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":71747,"to":71793}}}}],[1236,{"pageContent":"Theorem 6.9 \nExample 6.38 \nExample 6.39 \nSection 6.2 \nLinear Independence, Basis, and Dimension \n453 \nThe Basis Theorem \nIf a vector space V has a basis with n vectors, then every basis for V has exactly n \nvectors. \nThe proof of Theorem 3.23 also works here, virtually word for word. However, it \nis easier to make use of Theorem 6.8. \nProof Let B be a basis for V with n vectors and let B' be another basis for V with m \nvectors. By Theorem 6.8, m ::::: n; otherwise, B' would be linearly dependent. \nNow use Theorem 6.8 with the roles of B and  B' interchanged. Since B'  is a \nbasis of V with m vectors, Theorem 6.8 implies that any set of more than m vectors \nin Vis linearly dependent.  Hence, n ::::: m, since B is a basis and is, therefore, linearly \nindependent. \nSince n ::::: m and m ::::: n, we must have n \n= \nm, as required. \nThe following definition now makes sense, since the number of vectors in a \n(finite) basis does not depend on the choice of basis. \nDefinition","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":71795,"to":71817}}}}],[1237,{"pageContent":"= \nm, as required. \nThe following definition now makes sense, since the number of vectors in a \n(finite) basis does not depend on the choice of basis. \nDefinition \nA vector space Vis called finite-dimensional if it has a basis con­\nsisting of finitely many vectors. The dimension of V, denoted by dim V, is the num -\nber of vectors in a basis for V. The dimension of the zero vector space { 0} is defined \nto be zero. A vector space that has no finite basis is called infinite-dimensional. \nSince the standard basis for !R\nn \nhas n vectors, dim !R\nn \n= \nn. In the case of IR\n3\n, a  one­\ndimensional subspace is just the span of a single nonzero vector and thus is a line \nthrough the origin. A two-dimensional subspace is spanned by its basis of two \nlinearly independent (i.e., nonparallel) vectors and therefore is a plane through the \norigin. Any three linearly independent vectors must span IR\n3\n, by the Fundamental \nTheorem. The subspaces of IR\n3 \nare now completely classified according to dimension,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":71817,"to":71842}}}}],[1238,{"pageContent":"origin. Any three linearly independent vectors must span IR\n3\n, by the Fundamental \nTheorem. The subspaces of IR\n3 \nare now completely classified according to dimension, \nas shown in Table 6.1. \nTable 6.1 \ndimV \n3 \n2 \n0 \nv \nIR\n3 \nPlane through the origin \nLine through the origin \n{\nO\n} \nThe standard basis for <!J> \nn \ncontains n +  1   vectors (see Example 6.30), so dim <!J> \nn \n= \nn + 1.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":71842,"to":71867}}}}],[1239,{"pageContent":"454 \nChapter 6 Vector Spaces \nExample 6.40 \nExample 6.41 \nExample 6.42 \nTheorem 6.10 \nThe  standard basis  for  M\nm\nn \ncontains mn vectors (see  Example 6.31 ), so \ndim M\nm\nn \n= mn. \nBoth <!P and 9F are infinite-dimensional, since they each contain the infinite linearly \nindependent set {l, x, x\n2\n, ••• } (see Exercise 44\n)\n. \nFind the  dimension of the  vector space W of symmetric 2  X  2 matrices (see \nExample 6.10\n)\n. \nSolulion \nA symmetric 2 X 2 matrix is of the form \n[\n� \n�\n] \n=  a\n[\n� \n�\n] \n+  b\n[\n� \n�\n] \n+  c\n[\n� \n�\n] \nso W   is spanned by the set \ns \n= \n{ \n[\n� \n�\nl \n[\n� �l \n[\n� \n�\n]\n} \nIf S is line arly independent, then it will be a   basis for W. Setting \nwe obtain \nfrom which it immediately follows that a  =  b =  c = 0. Hence, Sis linearly indepen­\ndent and is, therefore, a basis for W. We conclude that dim W = 3. \nThe dimension of a vector space is its \"magic number:' Knowing the dimension \nof a vector space V provides us with much information about V and can greatly sim­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":71869,"to":71934}}}}],[1240,{"pageContent":"The dimension of a vector space is its \"magic number:' Knowing the dimension \nof a vector space V provides us with much information about V and can greatly sim­\nplify the work needed in certain types of calculations, as the next few theorems and \nexamples illustrate. \nLet V be a vector space with dim V = n. Then: \na. Any linearly independent set in V contains at most n vectors. \nb.  Any spanning set for V contains at least n vectors. \nc. Any linearly independent set of   exactly n vectors in Vis a  basis for V. \nd.  Any spanning set for V consisting of exactly n vectors is a basis for V. \ne.  Any linearly independent set in V can be extended to a basis for V. \nf.  Any spanning set for V can be reduced to a basis for V.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":71934,"to":71944}}}}],[1241,{"pageContent":"Example 6.43 \nExample 6.44 \nSection 6.2 \nLinear Independence, Basis, and Dimension \n455 \nProof The proofs of properties (a) and (b) follow from parts (a) and (b) of Theo­\nrem 6.8, respectively. \n(c)  Let S be a linearly independent set of   exactly n vectors in V. If S does not span V, \nthen there is some vector v in V that is not a linear combination of the vectors in S. \nInserting v into S produces a set S' with n + 1 vectors that is still linearly independent \n(see Exercise 54). But this is impossible, by Theorem 6.S(a). We conclude that S  must \nspan V and therefore be a basis for V. \n(d)  Let S be  a spanning set for  V consisting of exactly n vectors. If S is linearly \ndependent, then some vector v in S is a linear combination of the others. Throwing v \naway leaves a set S' with n -1 vectors that still  spans V (see Exercise 55\n)\n. But this is \nimpossible, by Theorem 6.S(b ). We conclude that S must be linearly independent and \ntherefore be a basis for V.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":71946,"to":71964}}}}],[1242,{"pageContent":")\n. But this is \nimpossible, by Theorem 6.S(b ). We conclude that S must be linearly independent and \ntherefore be a basis for V. \n(e) Let S be a linearly independent set of vectors in V. If   S spans V, it is a basis for \nV and so consists of exactly n vectors, by the Basis Theorem. If S does not span V, \nthen, as in   the proof of property (c), there is some vector v in  V that is not a linear \ncombination of the vectors in S. Inserting v into S produces a set S' that is still linearly \nindependent. If S' still does not span V, we can repeat the process and expand it into \na larger, linearly independent set. Eventually, this process must stop, since no linearly \nindependent set in   V can contain more than n vectors, by Theorem 6.S(a). When the \nprocess stops, we have a linearly independent set S* that contains S and also spans V. \nTherefore, S* is   a basis for V that extends S. \n(f)  You are asked to prove this property in Exercise 56.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":71964,"to":71977}}}}],[1243,{"pageContent":"process stops, we have a linearly independent set S* that contains S and also spans V. \nTherefore, S* is   a basis for V that extends S. \n(f)  You are asked to prove this property in Exercise 56. \nYou  should view Theorem 6.10 as, in part, a  labor-saving  device. In many \ninstances, it can dramatically decrease the amount of work needed to check that a set \nof vectors is line arly independent, a spanning set, or a basis. \nIn each case, determine whether S is a basis for V. \n(a) V = rzf\n2\n, S = {l + x, 2  - x + x2,   3x  - 2x2, -1 + 3x + x2} \n(b) V = M\n22\n, S = { [ �    �], [ � \n-\n�], [ � _ �]} \n(c) V = rzf\n2\n, S = {l + x, x + x2, 1 + x2} \nSolution (a) Since dim\n(\n\\]f 2) \n= \n3 and S contains four vectors, S is linearly depen­\ndent, by Theorem 6.lO(a). Hence, Sis not a basis for \\)f \n2\n. \n(b) Since dim(M\n22\n) \n= \n4 and S contains three vectors, S cannot span M\n22\n, by  Theo­\nrem 6.lO(b). Hence, Sis not a basis for M\n22\n. \n( c)  Since dim \n( \n\\]f 2) \n= \n3 and S contains three vectors, S will be a   basis for rzf \n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":71977,"to":72018}}}}],[1244,{"pageContent":"22\n) \n= \n4 and S contains three vectors, S cannot span M\n22\n, by  Theo­\nrem 6.lO(b). Hence, Sis not a basis for M\n22\n. \n( c)  Since dim \n( \n\\]f 2) \n= \n3 and S contains three vectors, S will be a   basis for rzf \n2 \nif it is lin -\nearly independent or if it spans \\]f \n2\n, by  Theorem 6.10( c) or (d). It is easier to show that \nS is line arly independent; we did this in Example 6.26. Therefore, S is a basis for \\]f \n2\n• \n(This is   the same problem as in   Example 6.32-but see how much easier it becomes \nusing Theorem 6.10!) \nExtend {l + x, 1 -x} to  a basis for \\)f \n2\n. \nSolution First note that {l + x, 1 -x} is   linearly independent. (Why?) Since dim \n((lf 2) \n= \n3, we  need a third vector-one that is not linearly dependent on the first two.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":72018,"to":72048}}}}],[1245,{"pageContent":"456 \nChapter 6 Vector Spaces \nTheorem 6.11 \nI Exercises 6.2 \nWe could proceed, as in the proof of Theorem 6.IO(e), to find such a vector using trial \nand error. However, it is easier in practice to proceed in a different way. \nWe enlarge the given set of vectors by throwing in the entire standard basis for !!f \n2\n. \nThis gives \nS = {l  + x, 1  -x, 1, x, x\n2\n} \nNow S is linearly dependent, by Theorem 6.IO(a), so we need to th  row away some \nvectors-in this case, two. Which ones? We use Theorem 6.lO(f), starting with the \nfirst vector that was added, 1. Since 1  = i\n(\nl + x\n) \n+ i\n(\nl -x\n)\n, the set {l + x, 1 -x, l} \nis linearly dependent, so we throw away 1. Similarly, x = i\n(\nl  + x\n) \n-i\n(\nl  -x\n)\n, so \n{l + x, 1 -x, x} is line arly dependent also. Finally, we check that {l + x, 1 -x, x\n2\n} \nis linearly independent. (Can you see a quick way to tell this?) Therefore, {l  + x, \n1 -x, x\n2\n} is a basis for l!f\n2 \nthat extends {l + x, 1 -x}. \nIn Example 6.42, the vector space W of symmetric 2 X 2 matrices is a subspace of","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":72050,"to":72092}}}}],[1246,{"pageContent":"1 -x, x\n2\n} is a basis for l!f\n2 \nthat extends {l + x, 1 -x}. \nIn Example 6.42, the vector space W of symmetric 2 X 2 matrices is a subspace of \nthe vector space M\n22 \nof all 2 X 2 matrices. As we showed, dim W = 3 ::=::: 4 = dim M\n22\n. \nThis is an example of a general result, as the final theorem of this section shows. \nLet W be a subspace of a finite-dimensional vector space V. Then: \na.  Wis finite-dimensional and dim W ::=::: dim V. \nb.  dim W =   dim V if and only if W =   V. \nProof (a)  Let dim V = n. If W = {O}, then dim(W) =  0 ::=:::  n =  dim V.  If   Wis \nnonzero, then any basis B for V (containing n vectors) certainly spans W, since Wis \ncontained in V. But B can be  reduced to a basis B' for W (containing at most n vec­\ntors), by Theorem 6.IO(f). Hence, Wis finite-dimensional and dim(W) ::=::: n = dim V. \n(b) If W =   V, then certainly dim W =   dim V. On the other hand, if dim W =   dim \nV = n, then any basis B for W consists of exactly n vectors. But these are then n lin­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":72092,"to":72112}}}}],[1247,{"pageContent":"(b) If W =   V, then certainly dim W =   dim V. On the other hand, if dim W =   dim \nV = n, then any basis B for W consists of exactly n vectors. But these are then n lin­\nearly independent vectors in V and, hence, a basis for V, by Theorem 6.IO(c). There­\nfore, V =   span(B) = W. \nIn Exercises 1-4, test th e sets of matrices for linear indepen­\ndence in M\n22\n. For those that are linearly dependent, express \none of the matrices as a linear combination of th e others. \n4\n· { \n[ \n� \n� \nl \n[ \n� \n� \nl \n[ \n� \n� \nl \n[ � \n�\n] \n} \nIn Exercises 5-9, test th e sets of polynomials for linear inde­\npendence. For those that are linearly dependent, express one \nof the polynomials as a linear combination of th e others. \n5. {x, 1 + x} in l!f 1 \n6.  {l + x, 1 + x\n2\n, 1 -x + x\n2\n} in l!f \n2 \n7.  {x, 2x -  x\n2\n, 3x + 2x\n2\n} in l!f \n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":72112,"to":72154}}}}],[1248,{"pageContent":"8. {2x, x  -  x\n2\n, 1 + x\n3\n, 2  -  x\n2 \n+ x\n3\n} in <;5}\n3 \n9. {l -  2x, 3x + x\n2 \n-  x\n3\n, 1 + x\n2 \n+ 2x\n3\n, 3 + 2x + 3x\n3\n} in <;5}\n3 \nIn Exercises 10-14, test the sets of functions for linear in­\ndependence in <;IF, For those that are linearly dependent, \nexpress one of the functions as a linear combination of the \nothers. \n10. {l, sin x, cos x} \n11. {l, sin\n2\nx, cos\n2\nx} \n12. {eX, e\n-\nx} \n13. {l, ln(2x), ln(x\n2\n)} \n14. {sin x, sin 2x, sin 3x} \n� \n15. If f and g are in C(b \n(J\nJ\n, the vector space of all functions \nwith continuous derivatives, then the determinant \nI \nf\n(\nx\n) \ng\n(\nx\n) \nI \nW(x\n) \n= \nj'\n(\nx\n) g'(x\n) \nis called the Wronskian off and g [named after the \nPolish-French mathematician J 6sef Maria Hoene­\nWronski (1776-1853), who worked on the theory of \ndeterminants and the philosophy of mathematics]. \nShow that f and g are linearly independent if their \nWronskian is not identically zero (that is, if there is \nsome x such that W\n(\nx\n) \n*  O\n)\n. \n� 16. In general, the Wronskian of f\n1\n, \n••• J\nn \nin C(b \n(\nn\n-\ni\nl is the \ndeterminant \nW(x\n) \n= \nfz\n(\nx\n) \nj{(x\n) \nand \nf\n1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":72156,"to":72255}}}}],[1249,{"pageContent":"Wronskian is not identically zero (that is, if there is \nsome x such that W\n(\nx\n) \n*  O\n)\n. \n� 16. In general, the Wronskian of f\n1\n, \n••• J\nn \nin C(b \n(\nn\n-\ni\nl is the \ndeterminant \nW(x\n) \n= \nfz\n(\nx\n) \nj{(x\n) \nand \nf\n1\n, ... ,f\nn \nare linearly independent, provided W\n(\nx\n) \nis not identically zero. Repeat Exercises 10-14 using \nthe Wronskian test. \n17. Let {u, v, w} be a linearly independent set of vectors in \na vector space V. \n(a) Is {u  + v, v + w, u +   w} linearly independent? \nEither prove that it is or give a counterexample \nto show that it is not. \n(b) Is {u -v, v -w, u -  w} linearly independent? \nEither prove that it is or give a counterexample \nto show that it is not. \nIn Exercises 18-25, determine whether th e set Bis a basis \nfor the vector space V \n18. V = M\n22\n, B = \n{ \n[ \n�  � \nl \n[ \n� \n� \nl \n[ \n_ \n� \n-\n�\n]\n} \nSection 6.2 \nLinear Independence, Basis, and Dimension \n451 \n20. V = M\nzz\n, \n21. V = M\n22\n, \nB = \n{ \n[\n� �\nl \n[\n� �\nl \n[ \n_\n� \n�\nl \n[\n� \n�\nl \n[� \n�\n]\n} \n22. V = <;IP\n2\n, B = {x, 1  + x, x -x\n2\n} \n23. V = <;IP\n2\n, B = {l -x, 1  -x\n2\n, x -x\n2\n} \n24. V = <;IP\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":72255,"to":72366}}}}],[1250,{"pageContent":"451 \n20. V = M\nzz\n, \n21. V = M\n22\n, \nB = \n{ \n[\n� �\nl \n[\n� �\nl \n[ \n_\n� \n�\nl \n[\n� \n�\nl \n[� \n�\n]\n} \n22. V = <;IP\n2\n, B = {x, 1  + x, x -x\n2\n} \n23. V = <;IP\n2\n, B = {l -x, 1  -x\n2\n, x -x\n2\n} \n24. V = <;IP\n2\n,B =  {l, 1  + 2x + 3x\n2\n} \n25. V = <;IP\n2\n, B = {l, 2  -x, 3 -x\n2\n, x + 2x\n2\n} \n26. Find the coordinate vector of A = \n[ � \n! ] \nwith \nrespect \nto the basis \nB \n= {E\n22\n, \nE\n2\n1\n, E w \nE\n11\n} of M\n22\n• \n27. Find the coordinate vector of A = \n[� \n!] \nwith respect \nto the basis B = \n{ \n[ \n� \n� \nl \n[ \n� � l \n[ \n� \n� l \n[ \n�  �\n]\n} \nof M\n22\n. \n28. Find the coordinate vector of p \n(\nx\n) \n= 1 + 2x + 3x\n2 \nwith respect to the basis B =  {l + x, 1 -x, x\n2\n} of<;JP\n2\n. \n29. Find the coordinate vector of p \n(\nx\n) \n= 2  -  x + 3x\n2 \nwith \nrespect to the basis B =  {l, 1  + x, -1 + x\n2\n} of <;JP \n2\n. \n30. Let B be a set of vectors in a vector space V with \nthe property that every vector in V can be written \nuniquely as a linear combination of the vectors in B. \nProve that B is a basis for V. \n31. Let B be a basis for a vector space V, let u\n1\n, ... , uk \nbe vectors in V, and let c\n1\n, ... , c\nk \nbe scalars. Show that \n[c","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":72366,"to":72494}}}}],[1251,{"pageContent":"Prove that B is a basis for V. \n31. Let B be a basis for a vector space V, let u\n1\n, ... , uk \nbe vectors in V, and let c\n1\n, ... , c\nk \nbe scalars. Show that \n[c\n1U1 \n+ ... + c\nk\nu\nds \n= C\n1 \n[u\n1\nls \n+ ... + c\nd\nu\nds· \n32. Finish the proof of Theorem 6.7 by showing that if \n{ [ u\n1\n] 8,  ... , [ u\nk\n] 8} is   linearly independent in IR\n\" \nthen \n{ u\n1\n, ... , u\nk\n} is linearly independent in V. \n33. Let { u\n1\n, ... , u\nm\n} be a set of vectors in an \nn-dimensional vector space V and let B be a basis for V. \nLet S = { [ u\n1\n] 8, ... ,  [u\nm\n] 8} be the set of coordinate \nvectors of {u\n1\n, ... , u\nm\n} with respect to B. Prove that \nspan\n(\nu\n1\n, •.• , u\nm\n) \n= V if and only if span(S) = !R\nn\n. \nIn Exercises 34-39, find th e dimension of th e vector space V \nand give a basis for V \n34. \nV = {p(x) in <;JP\n2\n: p (O) = O} \n{\n[     ] [ \n] \n[     ] [ \n]\n} \n35. \nV= {\np\n(\nx\n)\nin <;JP\n2\n:\np\n(\nl\n) \n= O} \nl9.\nV=\nM\n22\n,B = \n�  �\n' \n� \n-\n�\n' \n� \n�\n' \n� \n-\n� \n�\n36.\nV={\np\n(\nx\n) in <;IP\n2\n:\nxp\n'\n(\nx\n)\n=\np\n(\nx\n)\n}","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":72494,"to":72619}}}}],[1252,{"pageContent":"458 \nChapter 6 Vector Spaces \n37. V =  {A in M\n22\n: A  is upper triangular} \n38. V =   {A in M\n22\n: A  is skew-symmetric} \n39. V =   {A in M\n22\n: AB = BA}, where B = \n[\n0\n1 1\n1\n] \n40. Find a formula for the dimension of the vector space \nof symmetric n X n matrices. \n41. Find a formula for the dimension of the vector space \nof skew-symmetric n X n matrices. \n42. Let U and W be subspaces of a finite-dimensional \nvector space V. Prove Grassmann's Identity: \ndim(U +  W) = dimU +  dim W -   dim(U n W) \n[Hint: The subspace U +   Wis defined in Exercise 48 \nof Section 6.1. Let B =   {v\n1\n, ... , vk} be a basis for \nU n W. Extend B to a basis C of U and a basis D of W. \nProve that CUD is a basis for U + W.] \n43. Let U and V be finite-dimensional vector spaces. \n(a) Find a formula for dim(U X  V) in terms of dim U \nand dim V. (See Exercise 49 in Section 6.1.) \n(b) If Wis a  subspace of V, show that dim Li = \ndim W, where Li= {(w, w)  : w is in W}. \n44. Prove that the vector space i!f is infinite-dimensional.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":72621,"to":72655}}}}],[1253,{"pageContent":"and dim V. (See Exercise 49 in Section 6.1.) \n(b) If Wis a  subspace of V, show that dim Li = \ndim W, where Li= {(w, w)  : w is in W}. \n44. Prove that the vector space i!f is infinite-dimensional. \n[Hint: Suppose it has a finite basis. Show that there is \nsome polynomial that is not a linear combination of \nthis basis.] \n45. Extend {l + x, 1 + x + x\n2\n} to a basis for llf \n2\n. \n46. Extend \n{ \n[ \n� \n� \nl \n[ \n� \n�\n]\n} \nto a basis for M\n22\n• \n47. Extend\n{[\n� \n�\nl \n[\n� \n�\nl \n[\n� \n-\n�\n]\n}\ntoabasisforM\n22\n• \n48. Extend \n{ \n[ \n� \n�\n]\n, \n[ \n� \n�\n] \n} \nto a basis for the vector \nspace of symmetric 2 X 2 matrices. \n49. Find a basis for span(l, 1 + x, 2x) in llf \n1\n. \n50. Find a basis for span(l - 2x,   2x - x\n2\n, 1 - x\n2\n, 1 + x\n2\n) \nin i!f \n2\n. \n51. Find a basis for span(l - x, x - x\n2\n, 1 - x\n2\n, 1 - 2x + \nx\n2\n) in i!f \n2\n• \n52. Find a basis for  span( \n[ \n� \nO\nJ \n[\nO l\n] \n[\n-\n1 \n1 \n, \n1 0 \n, \n1 \n[ \n_ \n� \n-\n�\n]\n) \nin M\n22\n. \n53. Find a basis for span(sin\n2\nx, cos\n2\nx, cos 2x) in '!fa. \n54. Let S =  {v\n1\n, ... , v\nn\n} be a linearly independent set in","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":72655,"to":72771}}}}],[1254,{"pageContent":"2\n• \n52. Find a basis for  span( \n[ \n� \nO\nJ \n[\nO l\n] \n[\n-\n1 \n1 \n, \n1 0 \n, \n1 \n[ \n_ \n� \n-\n�\n]\n) \nin M\n22\n. \n53. Find a basis for span(sin\n2\nx, cos\n2\nx, cos 2x) in '!fa. \n54. Let S =  {v\n1\n, ... , v\nn\n} be a linearly independent set in \na vector space V. Show that if v is a vector in V that is \nnot in span(S ), then S' = { v\n1\n, ••• , v\nn\n, v} is still linearly \nindependent. \n55. Let S =  {v\n1\n, ••• , v\nn\n} be a spanning set for a vector \nspace V. Show that ifv\nn \nis in sp  an(v\n1\n, ... , v\nn\n_ \n1\n), then \nS' = {v\n1\n, •.• , v\nn\n_\n1\n} is still a spanning set for V. \n56. Prove Theorem 6.lO(f). \n57. Let {v\n1\n, ••• , vJ be a basis for a vector space V \nand let c\n1\n, •.. , c\nn \nbe nonzero scalars. Prove that \n{c\n1\nV\n1\n, ... , \nC\nn\nv\nn\n} is also a basis for V. \n58. Let {v\n1\n, ••• , vJ be a basis for a vector space V. Prove \nthat \n{v\ni\n, \nV\n1 \n+ \nV\n2\n, \nV\n1 \n+ \nV\nz \n+ \nV\n3\n, ... , \nV\n1 \n+  ... + vJ \nis also a basis for V. \nLet a0, a\n1\n, ••• , a\nn \nbe n + 1 distinct real numbers. Define \npolynomials p0\n(\nx\n)\n, p\n1 \n(\nx\n)\n, ... , \nP\nn\n(\nx\n) \nby \n() \n(\nx -\na0\n)\n· \n• ·\n(\nx -\na\n;\n_\n1\n)(\nx \n- a;\n+\n1\n)\n· • ·\n(\nx -\na\nn\n) \nP\n; \nx  = \n(\na\n; \n-a0\n) \n• • ·\n(\na\n; \n-  a\n;\n_","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":72771,"to":72942}}}}],[1255,{"pageContent":"Let a0, a\n1\n, ••• , a\nn \nbe n + 1 distinct real numbers. Define \npolynomials p0\n(\nx\n)\n, p\n1 \n(\nx\n)\n, ... , \nP\nn\n(\nx\n) \nby \n() \n(\nx -\na0\n)\n· \n• ·\n(\nx -\na\n;\n_\n1\n)(\nx \n- a;\n+\n1\n)\n· • ·\n(\nx -\na\nn\n) \nP\n; \nx  = \n(\na\n; \n-a0\n) \n• • ·\n(\na\n; \n-  a\n;\n_ \n1\n)(\na\n; \n-  a\n;\n+\n1\n) \n• • \n·\n(\na\n; \n-  a\nn\n) \nThese are called the Lagrange polynomials associated \nwith a0, a\n1\n, ... , aw [Joseph-Louis \nLagrange (1736-1813) \nwas born in Italy but spent most of his life in Germany and \nFrance. He made important contributions to such fields as \nnumber theory, algebra, astronomy, mechanics, and th e \ncalculus of variations. In 1773, Lagrange was th e first to give \nth e volume interpretation of a determinant (see Chapter 4).] \n59. (a) Compute the Lagrange polynomials associated \nwith a0 = 1, a\n1 \n= 2, a\n2 \n= 3. \n(b) Show, in general, that \n{\no \nif i * j \np\n;\n(\na) = \n1 if i = j \n6\n0. (a) Prove that the set B = {p0\n(\nx\n)\n, p\n1 \n(\nx\n)\n, ... , \nP\nn\n(\nx\n)\n} \nof Lagrange polynomials is linearly independent \nin i!f w [Hint: Set c0p0\n(x\n) \n+ \n·   ·   · \n+ c\nn\np\nn\n(x) = 0   and \nuse Exercise 59(b).] \n(b) \nDeduce that Bis a  basis for i!f w","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":72942,"to":73075}}}}],[1256,{"pageContent":"(\nx\n)\n, p\n1 \n(\nx\n)\n, ... , \nP\nn\n(\nx\n)\n} \nof Lagrange polynomials is linearly independent \nin i!f w [Hint: Set c0p0\n(x\n) \n+ \n·   ·   · \n+ c\nn\np\nn\n(x) = 0   and \nuse Exercise 59(b).] \n(b) \nDeduce that Bis a  basis for i!f w \n61. If q(x) is an arbitrary polynomial in i!f \nn\n' \nit follows from \nExercise 60(b) that \nq\n(\nx\n) \n= C\no\nP\no\n(\nx\n) \n+ \n... + c\nn\np\nn\n(x) \n(\n1\n) \nfor some scalars c0, ... , cw \n(a) Show that \nC\n; \n= q ( a\n;\n) for i =  0, ... , n, and deduce \nthat q\n(\nx\n) \n= q (a0)p0(x) + \n·   ·   · \n+ q\n( a\nn\n)\nP\nn\n(x) is the \nunique representation of q(x) with respect to the \nbasis B.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":73075,"to":73150}}}}],[1257,{"pageContent":"(b) Show that for any n + 1 points (a0, c0), (a 1 , c\n1\n), ..• , \n(a\nn\n, e\nn\n) \nwith distinct first components, the func­\ntion q\n(\nx\n) \ndefined by Equation (1) is the unique \npolynomial of degree at most n that passes \nthrough all of the points. This formula is known \nas the Lagrange interpolation fo rmula. (Com­\npare this formula with Problem 19 in Explora­\ntion: Geometric Applications of Determinants in \nChapter 4.\n) \n(c) Use the Lagrange interpolation formula to find \nthe polynomial of degree at most 2 that passes \nthrough the points \nSection 6.2 \nLinear Independence, Basis, and Dimension \n459 \n(i) (1, 6), \n(\n2, -1), and (3, -2\n) \n(ii) (-1, 10\n)\n, \n(\n0, 5\n) , and (3, 2\n) \n62. Use the Lagr  ange interpolation formula to show that \nif a polynomial in i!f \nn \nhas n + 1 zeros, then it must be \nthe zero polynomial. \n63. Find a formula for the number of invertible matrices \nin M\nnn\n(\n!f_\np\n)\n· \n[Hint: This is the  same as determining the \nnumber of different bases for z;. (Why?) Count the","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":73152,"to":73204}}}}],[1258,{"pageContent":"the zero polynomial. \n63. Find a formula for the number of invertible matrices \nin M\nnn\n(\n!f_\np\n)\n· \n[Hint: This is the  same as determining the \nnumber of different bases for z;. (Why?) Count the \nnumber of ways to construct a basis for z;, one vector \nat a time.]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":73204,"to":73216}}}}],[1259,{"pageContent":"460 \nExploration \nMagic Squares \nThe engraving shown on page 461 is Albrecht Durer's Melancholia I (1514). Among \nthe many mathematical artifacts in this engraving is the chart of numbers that hangs \non the wall in the upper right-hand corner. (It is enlarged in the detail shown.) Such \nan array of numbers is   known as a magic square. We can think of it as a 4 X 4 matrix \n3 \n10 \n6 \n15 \n2 \n11 \n7 \n14 \nObserve that the numbers in each row, in each column, and in both diagonals have \nthe same sum: 34. Observe further that the entries are the integers 1, 2, ... , 16. (Note \nthat Durer cleverly placed the 15 and 14 ad  jacent to each other in the last row, giving \nthe date of the engraving.) These observations lead to the following definition. \nDefinition \nAn n x n matrix Mis called a magic square if the sum of the \nentries is the same in each row, each column, and both diagonals. This common \nsum is called the weight of M, denoted wt(M). If Mis an n X n magic square that","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":73218,"to":73240}}}}],[1260,{"pageContent":"entries is the same in each row, each column, and both diagonals. This common \nsum is called the weight of M, denoted wt(M). If Mis an n X n magic square that \ncontains each of the entries 1, 2, ... , n\n2 \nexactly once, then Mis called a classical \nmagic square. \n1. \nIf M is a classical n X n magic square, show that \nwt\n(\nM\n) \n= \n_\nn(_n  _\n2 \n_+   _\nl\n_) \n2 \n[Hint: Use Exercise 51 in Section 2.4.] \n2.  Find a classical 3 X  3 magic square. Find a different one. Are your two ex­\namples related in any way?","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":73240,"to":73262}}}}],[1261,{"pageContent":"3.  Clearly, the 3  X  3 matrix with all entries equal to \nt \nis a magic square with \nweight 1. Using your answer to Problem 2, find a 3 X 3 magic square with weight 1, \nall of whose entries are different. Describe a method for constructing a 3 X  3 magic \nsquare with distinct entries and weight w for any real number w. \nLet Mag\nn \ndenote the set of all n X n magic squares, and let Mag� denote the set of all \nn X n magic squares of weight 0. \n4. \n(a)  Prove that Mag\n3 \nis a subspace of M\n33\n. \n(b)  Prove that Mag� is a subspace of Mag\n3\n. \n5. Use Problems 3 and 4 to show that if M is a 3 X 3 magic square with weight \nw, then we can write Mas \nM = M\n0 \n+ kJ \nwhere M\n0 \nis a 3 X 3 magic square of weight 0, J  is the 3 X 3 matrix consisting entirely \nof ones, and k is a scalar. What must k be? [Hint: Show that M -kJ is in Maij for an \nappropriate value of k.] \nLet's try to find a way of describing all 3 X 3 magic squares. Let \nbe a magic square with weight 0. The conditions on the rows, columns, and diag­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":73264,"to":73294}}}}],[1262,{"pageContent":"appropriate value of k.] \nLet's try to find a way of describing all 3 X 3 magic squares. Let \nbe a magic square with weight 0. The conditions on the rows, columns, and diag­\nonals give rise to a system of eight homogeneous linear equations in the variables a, \nb, ... , i. \n6. \nWrite out this system of equations and solve it. [Note: Using a CAS will \nfacilitate the calculations.] \n461","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":73294,"to":73302}}}}],[1263,{"pageContent":"462 \n7.  Find the dimension of Mag�. Hint: By doing a substitution, if necessary, use \nyour solution to Problem 6 to show that M can be written in the form \n[ s -s -t t l \nM \n= \n-s +  t \n0 \ns -t \n-t s +  t -s \n8. \nFind the dimension of Mag\n3\n• [Hint: Combine the results of Problems 5 and 7.] \n9. \nCan you find a direct way of showing that the \n(\n2, 2\n) \nentry of a 3 X 3 magic \nsquare with weight w must be w/3? [Hint: Add and subtract certain rows, columns, \nand diagonals to leave a multiple of the central entry.] \n10.  Let M be a 3 X 3 magic square of weight 0, obtained from a classical 3 X 3 \nmagic square as in Problem 5. If M has the form given in Problem 7, write out an \nequation for the sum of the squares of the entries of M. Show that this is the equation \nof a circle in the variables s and t, and carefully plot it. Show that there are exactly \neight points (s, t\n) \non this circle with both s and t integers. Using Problem 8, show that","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":73304,"to":73332}}}}],[1264,{"pageContent":"of a circle in the variables s and t, and carefully plot it. Show that there are exactly \neight points (s, t\n) \non this circle with both s and t integers. Using Problem 8, show that \nthese eight points give rise to eight classical 3 X 3    magic squares. How are these magic \nsquares related to one another?","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":73332,"to":73337}}}}],[1265,{"pageContent":"Change of Basis \nSection 6.3 \nChange of Basis \n463 \nIn many applications, a problem described using one coordinate system may be \nsolved more easily by switching to a new coordinate system. This switch is usually \naccomplished by performing a change of variables, a process that you have prob­\nably encountered in other mathematics courses. In linear algebra, a basis provides \nus with a coordinate system for a vector space, via the notion of coordinate vectors. \nChoosing the right basis will often greatly simplify a  particular problem. For example, \nconsider the molecular structure of zinc, shown in Figure 6.3(a). A scientist studying \nzinc might wish to measure the lengths of the bonds between the atoms, the angles \nbetween these bonds, and so on. Such an analysis will be greatly facilitated by intro­\nducing coordinates and making use of the tools of linear algebra. The standard basis \nand the associated standard xyz coordinate axes are not always the best choice. As","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":73339,"to":73353}}}}],[1266,{"pageContent":"ducing coordinates and making use of the tools of linear algebra. The standard basis \nand the associated standard xyz coordinate axes are not always the best choice. As \nFigure 6.3(b) shows, in this case {u, v, w} is probably a better choice of basis for IR\n3 \nthan the standard basis, since these vectors align nicely with the bonds between the \natoms of zinc. \n(a) \nv \n(b) \nfigure 6.3 \nChange-of-Basis Matrices \nFigure 6.4 shows two different coordinate systems for IR\n2\n, each arising from a different \nbasis. Figure 6.4(a) shows the coordinate system related to the basis !3 = {u\n,\n,  u\n2\n}, \nwhile Figure 6.4(b) arises from the basis C = {v\n1\n,   v\nz\n}, where \nThe same vector xis shown relative to each coordinate system. It is clear from the \ndiagrams that the coordinate vectors of x with respect to !3 and C are \n[x]8 \n= \n[\n�\n] \nand [x]c \n= \n[ \n_\n�\n] \nrespectively. It turns out that there is a direct connection between the two coordinate \nve\nctors. One way to find the relationship is to  use [ x] 8 \nto calculate","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":73353,"to":73393}}}}],[1267,{"pageContent":"464 \nChapter 6  Vector Spaces \n-4 \nFigure 6.4 \nExample 6.45 \ny y \n2 \n-2 \nx \n-2 \n-4 \n(a) \n(b) \nThen we can find [x]c by writing x as a linear combination ofv\n1 \nand v\n2\n• However, \nthere is a better way to proceed-one that will provide us with a general mechanism \nfor such problems. We illustrate this approach in the next example. \n1 \nUsing the bases Band C above, find [x]c, given that [x]8 = . \n3 \nSolulion Since x = u\n1 \n+ 3u\n2\n, writing u\n1 \nand u\n2 \nin terms ofv\n1 \nand v\n2 \nwill give us the \nrequired coordinates of x with respect to C. We find that \nu\n1 \n= \n[\n-\n�\n] \n-\n3\n[\n�\n] \n+ 2\n[\n�\n] = -3v\n1 \n+ 2v\n2 \nand \nu\n2 \n= \n[ \n_ \n�\n] = 3 \n[ \n�\n] \n[ \n�\n] = 3v\n1 \n-\nV\n2 \nso \nThis gives \n[x]c \n= \n[ \n_\n�\n] \nin agreement with Figure 6.4(b). \nThis method may not look any easier than the one suggested prior to Example 6.45, \nbut it has one big advantage: We can now find [ \ny\n] c from [ \ny\n] 8 for any vector \ny \nin IR\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":73395,"to":73486}}}}],[1268,{"pageContent":"Theorem 6.12 \nSection 6.3 \nChange of Basis \n465 \nwith very little additional work. Let's look at the calculations in Example 6.45 from a \ndifferent point of view. From x \n= \nu1 + 3u\n2\n, we have \nby Theorem 6.6. Thus, \n[\nx]\nc \n= \n[[u\n1\nJ\nd\nu\n2\nl\nc\nl[\n�\n] \n= \n[\n-\n� \n-\n�\n]\n[\n�\n] \nwhere Pis the  matrix whose columns are \n[ u\n1\n] \nc \nand \n[ u2] \nc\n· This  procedure generalizes \nvery nicely. \nDefinition \nLet B \n= \n{ u\n1\n, ... , u\"} and C \n= \n{ \nv\n1\n, ... , \nv\n\"} be bases for a vector \nspace V.  The n X  n matrix whose columns are the coordinate vectors \n[u1] \nC• \n... , \n[u\nn\nl\nc of the vectors in B with respect to C is denoted by P\nc<\n-\nB \nand is called the \nchange-of-basis matrix from B to C. That is, \nP\nc<\n-\nB \n= \n[  [u\n1\nJ\nd\nu2\nl\nc \n· · · \n[u\nnl\nc\nl \nThink of Bas the  \"old\" basis and C as the \"new\" basis. Then the columns of P\nc<\n-\nB \nare just the coordinate vectors obtained by writing the old basis vectors in terms of \nthe new ones. Theorem 6.12 shows that Example 6.45 is a special case of a general \nresult. \nLet B \n= \n{u\n1\n, ... , uJ and C \n= \n{\nv\n1, ... , \nv\nn","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":73488,"to":73594}}}}],[1269,{"pageContent":"the new ones. Theorem 6.12 shows that Example 6.45 is a special case of a general \nresult. \nLet B \n= \n{u\n1\n, ... , uJ and C \n= \n{\nv\n1, ... , \nv\nn\n} be bases for a vector space V and let \nP\nc<\n-\nB \nbe the change-of-basis matrix from B to C. Then \na. P\nc\n,_\ns\n[\nx] 8 \n= \n[\nx] c for all x in V. \nb. PC\n<\n-\n8 is the unique matrix P with the property that P \n[ \nx] 8 \n= \n[ \nx] \nc \nfor all x in V. \nc. \nP\nc<\n-\nB \nis invertibl\ne \nand \n(P\nc\n,_\n8)\n-\n1 \n= \nP\ns\n<\n-\nc\n· \nProof \n(a) Let x be in V and let \n[\nx\n]c = \n[\nC\n1\nU\n1 \n+ \n..\n. \n+ \nC\nn\nu\nn\nJc \n= \nC\n1 \n[ u\n1\nJc \n+ \n.\n.. \n+ \nC\nn \n[u\nn\nJc \n� \nI [u\n.\nJc \n[u\"\n]\n't\nl \n= \nP\nc<\n-s\n[\nx]\ns","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":73594,"to":73702}}}}],[1270,{"pageContent":"466 Chapter 6  Vector Spaces \nExample 6.46 \n(b) Suppose that Pis an n X n matr  ix with the property that P \n[\nx] 8 \n= \n[\nx] \nc \nfor all x \nin V. Taking x \n= \nU\n;, the ith basis vector in B, we see that \n[\nx] 8 \n= \n[ u ;] \n8 \n= \ne;, so the ith \ncolumn of Pis \nP\n; \n= \nPe\n; \n= \nP \n[ u\n;\n] \nB \n= \n[ U\n;\n] \nc \nwhich is the ith column of P\nc\n,_\n8, by definition. It follows that P \n= \nP\nc\n.-\nB\n· \n( c)  Since {u1, ... , \nu\nn\n} is line arly independent in V, the set { \n[u\n1\nJ\nc, ... , \n[ u\nn\nl\nd is line arly \nindependent in ll�r, by Theorem 6.7. Hence, P\nc\n.- 8 \n= \n[[u\n1\nJc \n·   ·   · \n[u\n]\nc \nis invert­\nible, by the Fundamental Theorem. \nFor all x in V, we have P C<-\nB [ \nx] 8 \n= \n[ \nx] \nc\n· Solving for \n[ \nx \nl \nt» we find that \n[\nx]8 \n= \n(P\nc\n<-\nB\n)-\n1 \n[\nx]\nc \nfor all x in V. Therefore, (P \nc\n.-\n8\n)-\n1 \nis a matrix that changes bases from C to B. Thus, by \nthe uniqueness property (b), we must have (P\nc\n.- 8)-\n1 \n= \nP8.-\nc\n. \nRemarks \n• You may find it helpful to think of change of basis as a transformation (indeed, \nit is a linear transformation) from !R\nn","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":73704,"to":73819}}}}],[1271,{"pageContent":"the uniqueness property (b), we must have (P\nc\n.- 8)-\n1 \n= \nP8.-\nc\n. \nRemarks \n• You may find it helpful to think of change of basis as a transformation (indeed, \nit is a linear transformation) from !R\nn \nto itself that simply switches from one coordi­\nnate system to another. The transformation corresponding to \nP\nc\n<-\nB \naccepts \n[\nx]8 \nas \ninput and returns \n[\nx] c as output; \n(P\nc\n,_\n8)-\n1 \n= \np\n8\n,_\nc \ndoes just the opposite. Figure 6.5 \ngives a schematic representation of the process. \nFigure 6.5 \nChange of basis \nx \n• \nv \n[J\nc\n/ �\n[]\nB \n/Multipli\nca\ntion� \nb\ny \nP\nc\n�\nB \n[x]\nc \n• \nMultipli\nca\ntion \nb\ny \nP\nB\n�\nc \n= \n(P\nc\n�\nB\n)-\n1 \n[x]\nB \n• \n• \nThe columns of P\nc\n.- 8 are the coordinate vectors of one basis with respect to \nthe other basis. To remember which basis is which, think of the notation C +---B as \nsaying \"B in terms of C:' It is also helpful to remember that P \nc\n<-t3 \n[ \nx] 8 is a linear com­\nbination of the columns of P\nc\n<-\nB· \nBut since the result of this combination is \n[\nx\nJ\nc, the \ncolumns of P\nc\n.-\n8 \nmust themselves be coordinate vectors with respect to C.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":73819,"to":73920}}}}],[1272,{"pageContent":"c\n<-t3 \n[ \nx] 8 is a linear com­\nbination of the columns of P\nc\n<-\nB· \nBut since the result of this combination is \n[\nx\nJ\nc, the \ncolumns of P\nc\n.-\n8 \nmust themselves be coordinate vectors with respect to C. \nFind the change-of-basis matrices \nP\nc\n.- 8 and P8.-\nc \nfor the bases B \n= \n{1, x, x\n2\n} and C \n= \n{\nI \n+ x, x + x\n2\n, 1 + x\n2\n} of\\]]>\n2\n. Then find the coordinate vector of p(x) = 1 + 2x - x\n2 \nwith respect to C. \nSolution \nChanging to a standard basis is easy, so we find \nP \nB\n<-\nc \nfirst. Observe that the \ncoordinate vectors for C in terms of B are","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":73920,"to":73967}}}}],[1273,{"pageContent":"Example 6.41 \nSection 6.3 \nChange of Basis \n461 \n(Look back at the Remark following Example 6.26.) It follows that \nTo find P C<-l3• we could express each vector in l3 as a linear combination of the vec­\ntors in C (do this), but it is much easier to use the fact that Pc,_13 \n= ( P13,_c)-\n1\n, by \nTheorem 6.12(c). We find that \nPc<-- 13 \n= (\nP13,_c\n)\n-\n1 \n= \nIt now follows that \n[\n-\n�\n-\n2\n::1 \n-\n�\n-\n2\n::\n1 \n-\n�\n-\n2\n:\n:\n1 \nl \n[p\n(x\n)\n] c \n= \nPc,_13[p\n(\nx\n)\n]\n13 \nwhich agrees with Example 6.37. \nRemark Ifwe do not need Pc,_13 explicitly, we can find [p\n(\nx\n) \nl\ne from [p\n(\nx\n) \n]\n13 and \nP13,_c using Gaussian elimination. Row reduction prod uces \n[I\nI \n[\np (\nx\n) \nl\nc\nl \n(See the next section on using Gauss-Jordan elimination.) \nIt is worth repeating the observation in Example 6.46: Changing to a standard \nbasis is easy. If£ is the standard basis for a vector space V and l3 is any other basis, \nthen the columns of P E<-l3 are the coordinate vectors of l3 with respect to £, and these","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":73969,"to":74044}}}}],[1274,{"pageContent":"basis is easy. If£ is the standard basis for a vector space V and l3 is any other basis, \nthen the columns of P E<-l3 are the coordinate vectors of l3 with respect to £, and these \nare usually \"visible:' We make use of this observation again in the next example. \nIn M\n22\n, let l3 be the basis {E\n11\n, E\n2\n1\n, E\n1\n2\n, E\n22\n} and let C be the basis {A, B, C, D}, where \nA\n-\nB\n-\nC\n-\n[ \n1 O\nJ \n[ \n1   1] \n[ \n1 \n-\nO     O\n' \n-\noo\n' \n-\n1 \n�\n] \nFind the change-of-basis matrix Pc,_13 and verify that [XJc \n= \nPc,_13[ X\n]\n13 for X \n= \n[\n� \n!\n]\n.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":74044,"to":74093}}}}],[1275,{"pageContent":"468 Chapter 6  Vector Spaces \nSolulion 1 To solve this problem directly, we must find the coordinate vectors of l3 \nwith respect to C. This involves solving four linear combination problems of the form \nX \n= \naA + bB + cC + dD, where Xis in l3 and we must find a, b, c, and d. However, \nhere we are lucky, since we can find the required coefficients by inspection. \nClearly, E\n11 \n= \nA, E\n2\n1 \n= \n-B + C, E\n1\n2 \n= \n-A + B, and E\n22 \n= \n-C + D. Thus, \n[E\nu\nl\nc \n� m \n[\n£\n, ,\n]\n,\n�\n[\n-\nI} \n[E\n,\n,]\n, \n� \n[\n-\n�\nl [E\n,,J\nc \n� \n[\n-\n!\n] \nrn \nP\nc�,;\n� \n[[E\nu\nl\nc \n[E\n, .\n]\n, \n[E\n, ,\nJ\nc \n[E\n,,J\nc\n] \n[\n� \n-\n: \n-\n� \n-\n!\n] \nand \nIf X \n= \n[\n� \n!\nl \nthen \nPc\n+-\n13\n[\nX]\n13 \n= \n[\ni \n[ X ]\ns \n� \nrn \n0  -1 \n-\n!\n][\n;\n] \n-1 \n1 \n1 \n0 \n0    0 \nThis is the coordinate vector with respect to C of the matrix \n[\n=] \n-\nA\n-\nB\n-\nC\n+\n4\nD\n=\n-\n[\n� \n�\n]\n-[\n� \n�\n]\n-[\n� \n�\n]+\n4\n[\n� \n�\n] \n[\n� \n!\n] \n= \nx \nas it should be. \nSolulion 2 We  can compute \nPc..-\n13 in a different way,  as follows. As you will be \nasked to prove in Exercise 21, if£ is another basis for M\n22\n> then P\nc\n..-13 \n= \nP\nc\n..-EPE..-13 \n= \n(PE\n..-c\n)-\n1\np\nE+-\nl3\n· \nIf£ is the standard basis, then P\nE+-\nl3 \nand PE+-\nc","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":74095,"to":74271}}}}],[1276,{"pageContent":"asked to prove in Exercise 21, if£ is another basis for M\n22\n> then P\nc\n..-13 \n= \nP\nc\n..-EPE..-13 \n= \n(PE\n..-c\n)-\n1\np\nE+-\nl3\n· \nIf£ is the standard basis, then P\nE+-\nl3 \nand PE+-\nc \ncan be found by inspec­\ntion. We have","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":74271,"to":74295}}}}],[1277,{"pageContent":"� \n(Do you see why?) Therefore, \nP\nc\n+-\nB \n= \n(P\nt:+-\nc\n)-\n1\nP\nt:\n+-\nB \nSection 6.3 \nChange of Basis \n[\nj \nJ\n[\nj \n0  0 \n�\n] \n1 \n0 \n1 \n0 \n1 1 \n0 \n0  0 0  0 \n[\nj \n-1 \n0 \n-\n�\nm \n0  0 \n�\n] \n1 \n-1 \n0 \n1 \n0 \n1 1 \n0 \n0    0 0  0 \n� \n[\nj \n0 \n-1 \n-\n�\n] \n-1 \n1 \n1 \n0 \n0    0 \nwhich agrees with the first solution. \n469 \n.+ \nRemark The second method has the advantage of not requiring the computa­\ntion of any linear combinations. It has the disadvantage of requiring that we find a \nmatrix inverse. However, using a CAS will facilitate finding a matrix inverse, so in \ngeneral the  second method is preferable to the first. For certain problems, though, \nthe first method may be just as easy to use. In any event, we are about to describe yet \na third approach, which you may find best of all. \nThe Gauss-Jordan Melhod for Compuling a Change-of-Basis Malrix \nFinding the change-of-basis matrix to a standard basis is easy and can be done \nby inspection. Finding the change-of-basis matrix from a standard basis is almost","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":74297,"to":74372}}}}],[1278,{"pageContent":"Finding the change-of-basis matrix to a standard basis is easy and can be done \nby inspection. Finding the change-of-basis matrix from a standard basis is almost \nas easy, but requires the calculation of a matrix inverse, as in Example 6.46. If we do \nit by  hand, then (except for the 2 X 2 case) we will usually find the necessary inverse \nby Gauss-Jordan elimination. We  now look at a modification of the Gauss-Jordan \nmethod that can be used to find the change-of-basis matrix between two nonstandard \nbases, as in Example 6.47. \nSu\nppose B = {u\n1\n, ... , \nu\n\"} and C = {\nv\n1\n, .•. , \nv\nn\n} are bases for a vector space V \nand P\nc\n+-\nB \nis the change-of-basis matrix from B to C. The ith column of Pis \nso \nU; \n= \np\n1 ;\nv\n1 \n+ · · · \n+ \nP\nn\ni\nv\nn\n-\nIf£ is any basis for V, then \n[u\n;\n]t; \n= \n[\nP1\n;\nV\n1 \n+ \n· · · \n+ \nPn\ni\nv\nnl\nt: \n= \nP1\nd\nv\n1\nJ\nt: \n+ \n· · · \n+ \nPn\nd\nv\nnl\nt: \nThis can be rewritten in matrix form as","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":74372,"to":74444}}}}],[1279,{"pageContent":"410 \nChapter 6  Vector Spaces \nTheorem 6.13 \nExample 6.48 \n� \nwhich we can solve by applying Gauss-Jordan elimination to the augmented matrix \n[[v\n1L�· \n· · · \n[v\nnl\nt:\nI \n[u\n;\n]EJ \nThere are n such systems of equations to be solved, one for each column of \nPc\n,_\n8\n, \nbut the coefficient matrix [ [ v\n1\n] E \n• • • \n[ v\nn\n] E] is the same in each case. Hence, we can \nsolve all the systems simultaneously by row reducing the n X 2n augmented matrix \n[[v\n1\nJ\nt: \n· · · \n[v\nnl\nt:\nl[u\n1\nJ\nt: \n· ·· \n[u\nnl\nt:\nl \n=  [C\nI\nB\nJ \nSince { \nV\n1\n' \n... \n' \nv \nn\n} is linearly independent, so is { [ v I \nl \nE> ••• \n' \n[ v \nn l \nE}\n' \nby Theorem 6. 7. \nTherefore, the matrix C whose columns are [ v\n1\n] E> ••• , [ v\nn\n] E  has the n X n identity \nmatrix I for its reduced row echelon form, by the Fundamental Theorem. It follows \nthat Gauss-Jordan elimination will necessarily produce \n[C\nI\nB\nJ\n--\n-+ \n[I\nI\nP\nJ \nwhere \nP \n= \nPc<--\n8. \nWe have proved the following theorem. \nLet l3 =  {u\n1\n, .•. , u\nn\n} and C =  {v\n1\n, ... , v\nn\n} be  bases for a vector space V.  Let \nB= [[u\n1\nl\nt: \n... \n[u\nnl\nt:\nl \nand C= [[v\n1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":74446,"to":74555}}}}],[1280,{"pageContent":"[C\nI\nB\nJ\n--\n-+ \n[I\nI\nP\nJ \nwhere \nP \n= \nPc<--\n8. \nWe have proved the following theorem. \nLet l3 =  {u\n1\n, .•. , u\nn\n} and C =  {v\n1\n, ... , v\nn\n} be  bases for a vector space V.  Let \nB= [[u\n1\nl\nt: \n... \n[u\nnl\nt:\nl \nand C= [[v\n1\n],<: ... [v\nnl\nt:\nLwhere £is any  basis for V. \nThen row reduction applied to the n X 2n augmented matrix [ C \nI \nB] produces \n[ C \nI\nB\nJ \n--\n-+ \n[I \nIPc.-al \nIf£ is a standard basis, this method is particularly easy to use, since in that case \nB = PE<--\nB \nand C = PE<--C\n· \nWe  illustrate this method by reworking the problem in \nExample 6.47. \nRework Example 6.47 using the Gauss-Jordan method. \nSolution \nTaking £to be the standard basis for M\n22\n, we see that \n[l \n0 \n0 \nB = PE<--\nB \n= \n1 \n0 \nRow reduction produces \nI\nC\nI\nBJ \n� \n[\nl \n1 \n1    1 \n1 1 \n0 \n0 \n1 \n0 \n0     0 1 \n0 \n0 \nf\nl \n0 \n0 \n0 0 \nf\n: \n0 1 \n1 \n0 \n0     0 \nand \nC = \nP\nE.-\nc \n= \n[\nl \n0     0 \n0 \n� \n0 \n1 \n0     0 \n(Verify this row reduction.) It follows that \nP\n,�, \n� \n[\nl \n0 \n-1 \n-\n�] \n-1 \n1 \n1 0 \n0 0 \nas we found before. \n[l \n0 \n0 \n0 \n:\n: \n0 \n1 \n0     0 \n1 \n0 \n-1 \n-\n�] \n0 \n-1 \n1 \n0 1 \n0 \n0 \n0 \n0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":74555,"to":74705}}}}],[1281,{"pageContent":"I \nExercises 6.3 \nIn Exercises 1-4: \n(a) Find the coordinate vectors [\nx\n]8 and [\nx\n]c of \nx \nwith \nrespect to th e bases B and C, respectively. \n(b) Find the change-of-basis matrix P\nc\n,_ 8fro m B to C. \n( c) Use your answer to part (b) to compute [ \nx\n] \nc\n, and \ncompare your answer with th e one found in part (a). \n(d) Find th e change-of-basis matrix P8,_\nc\nfrom C to B. \n(e) Use your answers to parts (c) and (d) to compute [\nx\n]8, \nand compare your answer with th e one found in part (a). \n1. \nx \n= \n[\n�\n]\n, B = \n{ \n[\n�\n]\n, \n[\n�\n]\n}\n, \nC = \n{ \n[ \n� \nl \n[ \n_ \n�\n]\n} \nin \n�\n2 \n2. \nx \n= \n[ \n_ \n� \nl \nB = \n{ \n[ \n� \nl \n[ \n�\n] \n} \n, \nC = \n{ \n[ \n� \nl \n[ \n�\n] \n} \nin \n�\n2 \n3. \nx \n� \n[ \n_ \nH \nB \n� \nu \nn \nm\n. \n[ \n� \nJ \nl\n. \nc \n� \nm\nH\n:\nW\nJ\nl \nin �\n' \n4\n.\nx\n� \n[;\nJ\ns\n� \nm\nJ\nm\n.\n[\nm \nc\n� \nm\nrnrn\nJ\nl\nin �\n' \nIn Exercises 5-8, follow the instructions for Exercises 1-4 \nusing p\n(\nx\n) \ninstead of \nx\n. \n5. p\n(\nx\n) \n= 2 -  x, B = {l,x}, C = {x, 1 + x} in 0\\ \n6. p (x) = 1 + 3x, B = {l + x, 1 \n-\nx}, \nC = {2x, 4} in \n0\\ \n7. p (x) = 1 + \nx\n2\n, B = {l + \nx \n+ \nx\n2\n, x + \nx\n2\n, x\n2\n}, \nC = {l, x, x\n2\n} in rzf \n2 \n8. p (x) = 4 \n-\n2x - x\n2\n, B = {x, 1 + x\n2\n, x + x\n2\n}, \nC = {l, 1 + x, x","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":74707,"to":74887}}}}],[1282,{"pageContent":"6. p (x) = 1 + 3x, B = {l + x, 1 \n-\nx}, \nC = {2x, 4} in \n0\\ \n7. p (x) = 1 + \nx\n2\n, B = {l + \nx \n+ \nx\n2\n, x + \nx\n2\n, x\n2\n}, \nC = {l, x, x\n2\n} in rzf \n2 \n8. p (x) = 4 \n-\n2x - x\n2\n, B = {x, 1 + x\n2\n, x + x\n2\n}, \nC = {l, 1 + x, x\n2\n} in rzf \n2 \nSection 6.3 \nChange of Basis \n411 \nIn Exercises 9 and 10, follow the instructions for \nExercises 1-4 using A instead of \nx\n. \n9. A = \n[ \n� \n_ \n�\n]\n, B = the standard basis, \nc\n=\n{\n[\n� \n-\n�\nl\n[\n� \n�\n]\n,\n[\n� \n�\n]\n,\n[\n� \n�\n]}\nin M\n22 \n10. A \n= \n[\nl \n1\n] \n1  1 \n, \nB = \n{ \n[\n� \n�\nl \n[\n� \n�\nl \n[\n� �\nl \n[\n� \n�\n] \n}\n, \nC = \n{\n[\n� \n�\nl \n[\n� \n�\nl \n[\n� \n�\nl \n[\n� \n�\n]\n} \nin M\n22 \nIn Exercises 11 and 12, follow th e instructions for \nExercises 1-4 using f \n(\nx\n) \ninstead of \nx\n. \n11. f\n(\nx\n) \n= 2 sin x \n-\n3 cos x, B = {sin x + \ncos x, cos x}, \nC = {sinx \n+ \ncosx, sinx -  cosx} in span(sin x,   cos x) \n12. f\n(\nx\n) \n=  sin x, B =  {sin x + \ncos x, cos x}, \nC = {cos x - sin x, sin x + cos x} in span(sin x,   cos x) \n13. Rotate the xy-axes in the plane counterclockwise \nthrough an angle () = 60° to obtain new x' y'  -axes. \nUse the methods of this section to find (a) the","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":74887,"to":75037}}}}],[1283,{"pageContent":"13. Rotate the xy-axes in the plane counterclockwise \nthrough an angle () = 60° to obtain new x' y'  -axes. \nUse the methods of this section to find (a) the \nx' y'-coordinates of the point whose xy-coordinates \nare (3, 2) and (b)  the xy-coordinates of the point \nwhose x'y' -coordinates are (4, \n-\n4). \n14. Repeat Exercise 13 with () = 135°. \n15. LetBand Cbe bases for\n�\n2\n.IfC = \n{\n[\n�\n],[�] }and \nthe change-of-basis matrix from B to C is \nfind B. \nP\nc\n<-B \n= \n[ _\n� \n-\n�\n] \n16. Let B   and C be  bases for rzf \n2\n. If B = {x, 1 + x, \n1 \n-\nx \n+ \nx\n2\n} and the change-of-basis matr  ix \nfrom B to C is \nfind C.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":75037,"to":75076}}}}],[1284,{"pageContent":"412 \nChapter 6  Vector Spaces \nIn calculus, you learn that a Taylor polynomial of degree n \nabout a is a polynomial of th e form \n18. Express p\n(\nx\n) \n= 1 + 2x -sx\n2 \nas a Taylor polynomial \nabout a= -2. \np (x\n)  = a0 + a\n1\n(x -  a\n) \n+ a\n2\n(\nx -  a\n)\n2 \n+ \n· · · \n+ a\n\"\n(x  -  a\n)\n\" \nwhere a\nn \n* \n0. In other words, it is a polynomial that has \nbeen expanded in terms of powers of x -  a instead of pow­\ners of x. Taylor polynomials are very useful for approximat­\ning functions that are \"well behaved\" near x = a. \n19. Express p \n(\nx\n) \n= x\n3 \nas a Taylor polynomial about a = -1. \n20. Express p \n(\nx\n) \n= x\n3 \nas a Taylor polynomial about a  = \nt\n. \n21. Let B, C, and V be bases for a   finite-dimensional ve c­\ntor space V. Prove that \nP\nv\n+-\nc\nP\nc\n+-\nB \n=  P\nv\n+-\nB \nThe set B = {l, x -   a, \n(\nx  -  a\n)\n2\n, ••• , (x  -  a\n)\n\"\n} is a basis \nfor rtl' \nn \nfor any real number a. (Do you see a quick way to \nshow this? Try using Theorem 6.7.) This fact allows us to use \nth e techniques of this section to rewrite a polynomial as a \nTaylor polynomial about a given a.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":75078,"to":75159}}}}],[1285,{"pageContent":"show this? Try using Theorem 6.7.) This fact allows us to use \nth e techniques of this section to rewrite a polynomial as a \nTaylor polynomial about a given a. \n22. Let V be an n-dimensional vector space with basis \nB = \n{v\n1\n, .•. , v\nn\n}. Let P be an invertible n X n matr  ix \nand set \n17. Express p \n(\nx\n) \n= 1 + 2x -5x\n2 \nas a Taylor polynomial \nabout a= 1. \nfor i = 1, ... , n. Prove that C = \n{u\n1\n, •.• , uJ is a basis \nfor V and show that P  =  P\ns+-\nc· \nExample 6.49 \nlinear Transtormalions \nWe encountered linear transformations in Section 3.6 in the context of matr  ix trans­\nformations from IR\n\" \nto !R\nm\n. In this section, we extend this concept to linear transfor­\nmations between arbitrary vector spaces. \nDefinition \nA linear transformation from a vector space V to a vector space \nWis a  mapping T : V---+ W such that, for all u and v in V and for all scalars c, \n1. T\n(\nu + v) \n=  T\n( u) + T\n(\nv) \n2. T\n( cu) \n= cT\n( u) \nIt is straightforward to show that this definition is equivalent to the requirement","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":75159,"to":75208}}}}],[1286,{"pageContent":"1. T\n(\nu + v) \n=  T\n( u) + T\n(\nv) \n2. T\n( cu) \n= cT\n( u) \nIt is straightforward to show that this definition is equivalent to the requirement \nthat T preserve all linear combinations. That is, \nT: V---+  Wis a  linear transformation if and only if \nT (c\n1\nv\n1 \n+ c\n2\nv\n2 \n+ \n· · · \n+ c\nk\nv\nk\n)  =  c\n1 \nT (\nv\n1\n) \n+ c\n2\nT (\nv\n2\n) \n+ \n· · · \n+  c\nk\nT (v\nk\n) \nfor all v\n1\n, ... , v\nk \nin V and scalars c\n1\n, ... , c\nk\n. \nEvery matrix transformation is a linear transformation. That is,  if A is an m X n \nmatrix, then the transformation T\nA \n: IR\n\" \n---+ \n!R\nm \ndefined by \nT\nA \n(\nx\n) \n= Ax for x in IR\n\" \nis a linear transformation. This is a restatement of Theorem 3.30.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":75208,"to":75280}}}}],[1287,{"pageContent":"Example 6.50 \nExample 6.51 \nExample 6.52 \nSection 6.4 Linear Transformations \n413 \nDefine T: M\nnn\n---+ M\nnn \nby T\n(\nA\n) \n=\nA\nT\n. Show that Tis a  linear transformation. \nSolution \nWe check that, for A and B in M\nnn \nand scalars c, \nT (A +  B\n) = \n(A \n+  B f \n= \nA\nT \n+  B\nT \n= \nT\n(A\n) \n+  T ( B\n) \nand \nT (cA\n) = \n(cAf \n= \ncA \nT \n= \ncT(A\n) \nTherefore, T is a linear transformation. \nLet D be the differential operator D : 0J \n---+ \n'2F defined by D\n(\nj\n) \n= \nf'. Show that D is a \nlinear transformation. \nSolution Let f and g be differentiable functions and let c be a scalar. Then, from \ncalculus, we  know that \nand \nD(\nf + g\n) = (j \n+ g\n) ' \n= \nf'  +  g' \n= D(j\n) \n+ \nD(g\n) \nD(cj\n) = (\ncf\n) ' \n= \ncf' \n= \ncD(j\n) \nHence, D is a linear transformation. \nIn calculus, you learn that every continuous function on [a, b] is integrable. The \nnext example shows that integration is a linear transformation. \nDefine S: \n� \n[a, b] \n---+ \nIR: by S(j) \n= \nJ: f \n(\nx\n) \ndx. Show that Sis a  linear transformation. \nSolution \nLet f and g be in\n� \n[a, b]. Then \nand \nIt follows that S  is linear. \nS(\nf + g\n) =","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":75282,"to":75384}}}}],[1288,{"pageContent":"Define S: \n� \n[a, b] \n---+ \nIR: by S(j) \n= \nJ: f \n(\nx\n) \ndx. Show that Sis a  linear transformation. \nSolution \nLet f and g be in\n� \n[a, b]. Then \nand \nIt follows that S  is linear. \nS(\nf + g\n) = \nr(j \n+ g\n)(\nx\n)\ndx \na \n= \nr (j (x\n) \n+ g\n(x\n)) \ndx \na \n= \nrf\n(x\n) \ndx + rg\n(x)\ndx \na \na \n= \nS(f\n) \n+ \nS(g) \nS\n(\ncf\n) = \nr (cj)(x) dx \na \n= \nf cf(x)dx \na \n= \ncff(x)dx \na \n= \ncS(f\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":75384,"to":75447}}}}],[1289,{"pageContent":"414 \nChapter 6  Vector Spaces \nExample 6.53 \nShow that none of the following transformations is linear: \n(a)  T: M\n22\n---+ IR   defined by T(A) =  <let A \n(b)  T: IR ---+ IR defined by T(x) = 2x \n(c) T: IR ---+ IR defined by T(x) = x +  1 \nSolution In each case, we give a specific counterexample to show that one of the \nproperties of a linear transformation fails to hold. \n(a)  Let A= \n[\n� \n�\n] \nand B = \n[\n� \n�\n]\n. Then A + B = \n[\n� \n�\nl \nso \nBut \nT\n(\nA \n+ B\n) \n=  <let \n(\nA \n+ B\n) \n= I\n� \n�\nI = 1 \nT\n(\nA\n) \n+ T\n(\nB\n) \n=  detA + detB \n= I\n� \n�\nI + I\n� \n�\nI \n= 0  +  0 = 0 \nso T(A + B) * T(A) + T(B) and Tis not  linear. \n(b) Let x = 1 andy = 2. Then \nT\n(\nx + y\n) \n=  T\n(\n3\n) \n= 2\n3 \n= 8  * 6 = 2\n1 \n+  2\n2 \n=  T\n(\nx\n) \n+ \nT\n(\ny\n) \nso T   is not linear. \n(c) Let x = 1 andy = 2. Then \nT\n(\nx + y\n) \n=  T\n(\n3\n) \n= 3 +  1 = 4  * 5 = (1 \n+ \n1 ) \n+ \n( 2 \n+ \n1 ) \n=  T\n(\nx\n) \n+ \nT\n(\ny\n) \nTherefore, T is not linear. \n4 \nRemark Example 6.53(c) shows that you need to be careful when you encounter \nthe \nword \"linear:' As a function, T(x) = x +  1 is linear, since its graph is a straight","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":75449,"to":75563}}}}],[1290,{"pageContent":"T\n(\ny\n) \nTherefore, T is not linear. \n4 \nRemark Example 6.53(c) shows that you need to be careful when you encounter \nthe \nword \"linear:' As a function, T(x) = x +  1 is linear, since its graph is a straight \nline. However, it is not a linear transformation from the vector space IR to itself, since \n� \nit fails to satisfy the  definition. (Which linear functions from IR to IR will also be linear \ntransformations?) \nExample 6.54 \nThere are two special linear transformations that deserve to be singled out. \n(a)  For any vector spaces V and W, the transformation T\n0 \n: V---+ W that maps every \nvector in V to the zero vector in W is called the zero transformation. That is, \nT\n0\n(\nv\n) \n= 0 for all v in V \n(b)  For any vector space V, the transformation I : V\n---+ \nV that maps every vector in V \nto itself is called the identity transformation. That is, \nJ\n(\nv\n) \n= v    forall vin V \n(If it  is important to identify the   vector space V, we may write Iv for clarity.) The","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":75563,"to":75597}}}}],[1291,{"pageContent":"to itself is called the identity transformation. That is, \nJ\n(\nv\n) \n= v    forall vin V \n(If it  is important to identify the   vector space V, we may write Iv for clarity.) The \nproofs that the zero and identity transformations are linear are left as easy exercises. \n4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":75597,"to":75605}}}}],[1292,{"pageContent":"Section 6.4 Linear Transformations \n415 \nProperties of linear Transformations \nIn Chapter 3, all linear transformations were matrix transformations, and their \nproperties were directly related to properties of the matrices involved. The fol­\n_., \nlowing theorem is easy to prove  for  matrix transformations. (Do it!) The  full \nproof for linear transformations in general takes a bit more care, but it  is still \nstraightforward. \nTheorem 6 \n.14 \nLet T: V ---+ W be a linear transformation. Then: \na. T(O) \n= \n0 \nb. T(-v) \n= \n-T(v) for all v in V. \nc. T(u -v) \n= \nT(u) -T(v) for all \nu \nand v in V. \nProof We  prove properties (a) and (c) and leave the proof of property (b) for \nExercise 21. \n_., \n(a)  Let v be any vector in V. Then T(O) \n= \nT(Ov) \n= \nOT(v) \n= \n0, as required. (Can you \ngive a reason for each step?) \nExample 6.55 \n(c) T(u -v) \n= \nT(u + (-l)v) \n= \nT(u) + (-l)T(v) \n= \nT(u) -T(v) \nRemark Property (a) can be useful in showing that certain transformations are","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":75607,"to":75649}}}}],[1293,{"pageContent":"give a reason for each step?) \nExample 6.55 \n(c) T(u -v) \n= \nT(u + (-l)v) \n= \nT(u) + (-l)T(v) \n= \nT(u) -T(v) \nRemark Property (a) can be useful in showing that certain transformations are \nnot linear. As an ill  ustration, consider Example 6.53(b ). If T(x) \n= \n2x, then T(O) \n= \n2\n° \n= \n1 *  0, so Tis not  linear, by  Theorem 6.14(a). Be  warned, however, that there are lots \nof transformations that do map the zero vector to the zero vector but that are still not \nlinear. Example 6.53(a) is a case in point: The zero vector is the 2 X 2 zero matrix 0, \nso T( 0) \n= \ndet 0 \n= \n0, but we have seen that T(A) \n= \ndet A is not linear. \nThe most important property of a linear transformation T : V ---+ W is that T is \ncompletely determined by its effect on a basis for V. The next example shows what \nthis means. \nSuppose T is a linear transformation from IR\n2 \nto <!J' 2 such that \nr\n[\n�\n] = \n2 -3x + x\n2 \nand r\n[\n�\n] = \n1  - x\n2 \nFind r\n[\n-\n�\nJ and r\n[\n�J. \n_., \nSolution Since B \n= \n{ [\n�\n]\n,\n[\n�\n]\n}\nis a basis for IR\n2 \n(why?), every vector in IR\n2 \nis in","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":75649,"to":75716}}}}],[1294,{"pageContent":"2 \nto <!J' 2 such that \nr\n[\n�\n] = \n2 -3x + x\n2 \nand r\n[\n�\n] = \n1  - x\n2 \nFind r\n[\n-\n�\nJ and r\n[\n�J. \n_., \nSolution Since B \n= \n{ [\n�\n]\n,\n[\n�\n]\n}\nis a basis for IR\n2 \n(why?), every vector in IR\n2 \nis in \nspan(B). Solving","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":75716,"to":75753}}}}],[1295,{"pageContent":"416 \nChapter 6  Vector Spaces \nwe find that c\n1 \n= -7  and c\n2 \n= 3. Therefore, \nso \n= -7r\n[\n�\n] \n+ 3T\n[\n�\n] \n= -7\n(\n2 -  3x + x\n2\n) \n+ 3\n( \n1  - x\n2\n) \n= -11 + 21x -10x\n2 \nSimilarly, we discover that \n[\n:\n] = \n(\n3a -2b\n)\n[\n�\n] \n+ \n(\nb -  a\n)\n[\n�\n] \nr\n[\n:J = r(\n(\n3a - 2b\n)\n[\n�\nJ \n+ \n(\nb -  a\n)\n[\n�\n]\n) \n= \n(\n3a - 2b\n)\nr\n[\n�\nJ \n+ \n(\nb -a\n)\nr\n[\n�\nJ \n= \n(\n3a - 2b\n)(\n2 -  3x + x\n2\n) \n+ \n(\nb -a\n)(\nl  -  x\n2\n) \n= (Sa - 3b\n) \n+ (-9a  + 6b\n)\nx + (4a - 3b\n)\nx\n2 \nII-\n(Note tha\\by setting a  = -1 and b =  2, we recover the solution r\n[ \n-\n�\n] = -11 \n+ \n2lx-lOx .) \n4 \nThe proof of the general theorem is quite straightforward. \nTheorem 6.15 \nLet T: V-+ Wbe a  linear transformation and let l3  = {v\n1\n, ••• , v\nn\n} be a spanning \nset for V. Then T\n(\nB\n) \n= {T\n(\nv\n1\n)\n, ••• , T\n(\nv\nn\n)\n} spans the range of T. \nProof The range of Tis the set of all vectors in W that are of the form T(v),  where \nvis in V. Let T(v) be in the range of T. Since l3 spans V, there are scalars c\n1\n, ..• , e\nn \nsuch that \nApplying T and using the fact that it  is a linear transformation, we see that \nT\n(\nv\n) \n= T\n(\nc\n1\nv\n1 \n+ \n· · · \n+ c\nn\nv\nn\n) = \nc\n1\nT\n(\nv\n1\n) \n+ \n· · · \n+ c\nn\nT\n(\nv\nn\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":75755,"to":75925}}}}],[1296,{"pageContent":"1\n, ..• , e\nn \nsuch that \nApplying T and using the fact that it  is a linear transformation, we see that \nT\n(\nv\n) \n= T\n(\nc\n1\nv\n1 \n+ \n· · · \n+ c\nn\nv\nn\n) = \nc\n1\nT\n(\nv\n1\n) \n+ \n· · · \n+ c\nn\nT\n(\nv\nn\n) \nIn other words, T(v) is in span(T(B)), as required. \nTheorem 6.15 applies, in particular, when l3 is a basis for V. You might guess that, \nin this case, T(B) would then be a basis for the range of T. Unfortunately, this is not \nalways the case. We will address this issue in Section 6.5. \nComposition of linear Transformations \nIn Section 3.6, we defined the composition of matrix transformations. The definition \nextends to general linear transformations in an obvious way.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":75925,"to":75969}}}}],[1297,{"pageContent":"S \n0 \nT is read \"S of T:\n' \nExample 6.56 \nTheorem 6.16 \nSection 6.4 Linear Transformations \n411 \nDefinition \nIf T: U ---+ V and S : V ---+ Ware linear transformations, then the \ncomposition of S with Tis the mapping S 0 T, defined by \n(S 0 T\n)(\nu\n) = S(T (\nu\n)) \nwhere u is in U. \nObserve that S 0 Tis a  mapping from U to W (see Figure 6.6). Notice also that for \nthe definition to make sense, the range of T must be contained in the domain of S. \nu \n• \nT \n-\nT(u) \nU\n• \nFigure 6.6 \nComposition of linear transformations \nv w \ns \n--+ S(T(u)) = (S \n0 \nT)(u) \n• \nLet T : IR\n2 \n---+ \n<!/' 1 and S : <!/' 1 \n---+ \n<!/' \n2 \nbe the linear transformations defined by \nr\n[\n:\n] = \na  + \n(\na  +  b\n)\nx and S\n(\np\n(\nx\n)) = \nxp\n(\nx\n) \nFind (S 0 T\n) \n[ \n_ \n�\n] \nand (S 0 T\n) \n[\n:\n]\n. \nSolution We compute \nand \n(\nS0T\n)\n[ \n_�\n] = \ns(\nr\n[ \n_�\n]\n) \n= \nS\n(\n3 \n+  (\n3  -  2\n)\nx\n) \n= \nS\n(\n3  +  x\n) \n= \nx\n(\n3  +  x\n) \n= \n3x +  x\n2 \n(\nS 0 T)\n[\n:\nJ = \ns\n( r\n[\n:\n]\n) \n= \nS\n(\na +(a+ \nb\n)\nx\n) \n= \nx\n(\na +(a+ \nb\n)\nx\n) \n= \nax + \n(\na  +  b\n)\nx\n2 \nChapter 3 showed that  the composition of two  matrix transformations was","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":75971,"to":76114}}}}],[1298,{"pageContent":"+  (\n3  -  2\n)\nx\n) \n= \nS\n(\n3  +  x\n) \n= \nx\n(\n3  +  x\n) \n= \n3x +  x\n2 \n(\nS 0 T)\n[\n:\nJ = \ns\n( r\n[\n:\n]\n) \n= \nS\n(\na +(a+ \nb\n)\nx\n) \n= \nx\n(\na +(a+ \nb\n)\nx\n) \n= \nax + \n(\na  +  b\n)\nx\n2 \nChapter 3 showed that  the composition of two  matrix transformations was \nanother matrix transformation. In general, we have the following theorem. \nIf T : U ---+ V and S : V ---+ W are linear transformations, then S 0 T : U \n---+ \nW is a \nlinear transformation.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":76114,"to":76171}}}}],[1299,{"pageContent":"418 \nChapter 6  Vector Spaces \nExample 6.51 \nProof \nLet u and v be in U and let c be a scalar. Then \n(S 0 T)(\nu  +  v\n) = \nS(T(\nu  +  v\n)) \n= \nS(T(\nu\n) \n+  T\n(\nv\n)) \n= \nS(T(\nu\n)) \n+ \nS(T(v\n)) \n= \n(S 0 T)(\nu\n) \n+ \n(S 0 T\n)(\nv\n) \nand \nsince Tis linear \nsince S is linear \n(S 0 T)(\ncu\n) = S(T (\ncu\n)) \n= \nS(cT(\nu\n)) \n= \ncS( T (\nu\n)) \n= \nc (S 0 T\n)(\nu\n) \nsince Tis linear \nsince S is linear \nTherefore, S 0 T is a linear transformation. \nThe algebraic properties of linear transformations mirror those of matrix trans­\nformations, which, in turn, are related to the algebraic properties of matrices. For \nexample, composition of linear transformations is associative. That is, if R, S, and T \nare linear transformations, then \nprovided these compositions make sense. The proof of this property is identical to \nthat given in Section 3.6. \nThe next example gives another useful (but not surprising) property of linear \ntransformations. \nLet S : U � V and T : V � W be linear transformations and let I : V � V be the iden­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":76173,"to":76240}}}}],[1300,{"pageContent":"The next example gives another useful (but not surprising) property of linear \ntransformations. \nLet S : U � V and T : V � W be linear transformations and let I : V � V be the iden­\ntity transformation. Then for every v in V, we have \n(\nT 0 J\n)(\nv\n) = \nT( I(\nv\n)) = \nT(\nv\n) \nSince T 0 I and T have the same value at  every v in their domain, it follows that \nT \no \nI \n= \nT. Similarly, I o S \n= \nS. \nRemark The method of Example 6.57 is worth noting. Suppose we want to show \nthat two linear transformations T\ni \nand T\n2 \n(both from V to W) are equal. It suffices to \nshow that T\ni (\nv\n) \n= T\n2\n(\nv\n) \nfor every v in V. \nFurther properties oflinear transformations are explored in the exercises. \nInverses or Linear Transrormalions \nDefiniliOD \nA linear transformation T: V �Wis invertible if there is a linear \ntransformation T' : W � V such that \nT' o \nT \n= \nIv and \nT 0 T' \n= \nIw \nIn this case, T' is called an inverse for T.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":76240,"to":76291}}}}],[1301,{"pageContent":"Example 6.58 \nTheorem 6.11 \nSection 6.4 Linear Transformations \n419 \nRemarks \n• \nThe domain V and codomain W of T do not have to be the same, as they do in \nthe case of invertible matrix transformations. However, we will see in the next section \nthat V and W must be very closely related. \n• \nThe requirement that T' be linear could have been omitted from this definition. \nFor, as we will see in Theorem 6.24, if T' is any mapping from W to V such that \nT' 0 T \n= \nIv and T 0 T' \n= \nIw, then T' is forced to be linear as well. \n• \nIf T' is an inverse for T, then the definition implies that Tis an inverse for T'. \nHence, T' is invertible too. \nVerify that the mappings T : IR\n2 \n---+ r!J> \n1 \nand T' : r!J> \n1 \n---+ IR\n2 \ndefined by \nr\n[\n:\n] = \na + \n(\na + b\n)\nx  and \nT'\n(\nc + dx\n) = \n[\nd \n� J \nare inverses. \nSolution We compute \nand \n(\nT' \no \nr\n)\n[\n:\nJ \n= \nr'( \nr\n[\n:\n]\n) \n= \nT'\n(\na +\n(\na+ \nb\n)\nx\n) \n= \n[\n(\na +\n:\n)\n_ \na\n] \n[\n:\n] \n(\nT0T'\n)(\nc + dx\n) = \nT\n(\nT'\n(\nc + dx\n)) = \nr\n[ \nc \n] = \nc + \n(\nc + \n(\nd-c\n))\nx \n= \nc + dx \nd - c \nHence, T' o T \n= \nother. \nIIR2 and T 0   T' \n= \nI21'\n1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":76293,"to":76409}}}}],[1302,{"pageContent":"and \n(\nT' \no \nr\n)\n[\n:\nJ \n= \nr'( \nr\n[\n:\n]\n) \n= \nT'\n(\na +\n(\na+ \nb\n)\nx\n) \n= \n[\n(\na +\n:\n)\n_ \na\n] \n[\n:\n] \n(\nT0T'\n)(\nc + dx\n) = \nT\n(\nT'\n(\nc + dx\n)) = \nr\n[ \nc \n] = \nc + \n(\nc + \n(\nd-c\n))\nx \n= \nc + dx \nd - c \nHence, T' o T \n= \nother. \nIIR2 and T 0   T' \n= \nI21'\n1\n• Therefore, T and  T' are inverses of each \n4 \nAs was the case for invertible matrices, inverses of linear transformations are \nunique if they exist. The following theorem is the analogue of Theorem 3.6. \nIf Tis an invertible linear transformation, then its inverse is unique. \nProof The proof is the same as that of Theorem 3.6, with prod ucts of matrices re­\nplaced by compositions of linear transformations. (You are asked to complete this \nproof in Exercise 31.) \nThanks to Theorem 6.17, if Tis invertible, we can refer to the inverse of T. It   will \nbe denoted by r\n-\n1 \n(pronounced \"T inverse\"). In the next two sections, we will ad­\ndress the issue of determining when a given linear transformation is invertible and \nfinding its inverse when it exists.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":76409,"to":76493}}}}],[1303,{"pageContent":"480 Chapter 6  Vector Spaces \n.. \nI Exercises 6.4 \nIn Exercises 1-12, determine whether Tis a linear \ntransformation. \n1. T: M\n22 \n--,lo \nM\n22 \ndefined by \nr\n[\na b\n] = \n[\na  +  b \nO \nJ \nc d 0 \nc+d \n2. T: M\n22 \n--,lo \nM\n22 \ndefined by \nr\n[\nw x\n] \n[ \n1 w  -  z\n] \ny z \nx-y \n1 \n3. T: M\nnn \n--,lo \nM\nnn \ndefined by T\n(\nA\n) = \nAB, where B is a \nfixed n X n matrix \n4. T: M\nnn \n--,lo \nM\nnn \ndefined by T\n(\nA\n) = \nAB -BA, where B \nis a fixed n X n matrix \n5. T: M\nnn \n--,lo \nIR defined by T\n(\nA\n) = \ntr\n(\nA\n) \n6. \nT: M\nnn \n--,lo \nIR \ndefined \nby T(A) \n= \na\nll\na\n22 \n•   •   • \na\nnn \n7. T: M\nnn \n--,lo \nIR defined by T\n(\nA\n) = \nrank\n(\nA\n) \n8. T: <JP2 --,lo <!P2 defined by T\n(\na  + bx + cx\n2\n) \n= \n(\na + 1) + \n(\nb +  l\n)\nx +  ( c +  l\n)\nx\n2 \n9. T: <!P2 --,lo <!P2 defined by T\n(\na  + bx +  cx\n2\n) \n= \na + \nb\n(\nx + 1) +  b\n(\nx +  1\n)\n2 \n10. T: gjf ---,l> gji defined by T\n(\nj\n) = \nf \n(\nx\n2\n) \n11. \nT: gji --,lo gji defined \nby T\n(\nj\n) = \n(j\n(x\n) )\n2 \n12. T: gji --,lo IR defined by T\n(\nj\n) = \nj( c), where c is a fixed \nscalar \n13. Show that the transformations S and Tin Exam­\nple 6.56 are both linear. \n14. Let T: IR\n2 \n--,lo IR\n3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":76495,"to":76650}}}}],[1304,{"pageContent":"by T\n(\nj\n) = \n(j\n(x\n) )\n2 \n12. T: gji --,lo IR defined by T\n(\nj\n) = \nj( c), where c is a fixed \nscalar \n13. Show that the transformations S and Tin Exam­\nple 6.56 are both linear. \n14. Let T: IR\n2 \n--,lo IR\n3 \nbe a linear transformation for which \nFind r\n[\n�\nJ and r\n[\n�J. \n15. Let T: IR\n2 \n--,lo <JP2 be a   linear transformation for which \nT \n[ \n�\n] = \n1 -2x and T \n[ \n_ �\n] = \nx  + 2x\n2 \nFind r\n[ \n-\n�\n] and r\n[\n� J. \n16. Let T: <JP2 --,lo <JP2 be a linear transformation for which \nT\n(\nl\n) \n= \n3  -2x, T\n(\nx\n) \n= \n4x -  x\n2\n, and T\n(\nx\n2\n) \n= \n2  + 2x\n2 \nFind T(6 +  x -4x\n2\n) \nand T (\na  + bx+  cx\n2\n)\n. \n17. Let T: <!P2 --,lo <!P2 be a   linear transformation for which \nT\n(\nl  +  x\n) = 1 \n+  x\n2\n, \nT\n(\nx + \nx\n2\n) = \nx  -  x\n2\n, \nT\n( \n1 +  x\n2\n) = \n1 +  x +  x\n2 \nFind T\n(\n4 -  x  + 3x\n2\n) and T (\na  + bx +  cx\n2\n)\n. \n18. Let T: M\n22 \n--,lo \nIR be a   linear transformation for which \nFind r\n[\n� \nr\n[\n� \nr\n[\n� \nO\nJ = \n1 \n0 \n' \n1\n] = \n3 \n0 \n, \nr\n[\n� \nr\n[\n� \n�\n]and r\n[\n: �\n]. \n1\n] = \n2 \n0 \n' \n�\n] = \n4 \n19. Let T: M\n22 \n--,lo \nIR be a   linear transformation. Show that \nthere are scalars a, b, c, and d such that \nr\n[\n; \n:J = \naw+ bx+  cy + dz \nfor all","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":76650,"to":76811}}}}],[1305,{"pageContent":"1 \n0 \n' \n1\n] = \n3 \n0 \n, \nr\n[\n� \nr\n[\n� \n�\n]and r\n[\n: �\n]. \n1\n] = \n2 \n0 \n' \n�\n] = \n4 \n19. Let T: M\n22 \n--,lo \nIR be a   linear transformation. Show that \nthere are scalars a, b, c, and d such that \nr\n[\n; \n:J = \naw+ bx+  cy + dz \nfor all\n[\n; \n:J in M\n22\n. \n20. Show that there is no linear transformation T : IR\n3 \n--,lo <JP 2 \nsuch that \n{\n] \n� 1 + x, \nr\n[\n�\n] \n� 2  -  x  + x'\n. \nt!\nJ \n� -2 + 2x' \n21. Prove Theorem 6.14(b). \n22. Let {v\n1\n, •.• , v\nn\n} be a basis for a   vector space Vand \nlet T : V \n--,lo \nV be a linear transformation. Prove that if \nT(v1 ) \n= \nV1\n, T(v2 ) = \nV\n2 \n... , T(v\nn\n) = \nv\nn\n, \nthen Tis the \nidentity transformation on V. \n� 23. Let T: <JP n  --,lo <JP n be a linear transformation such that \nT(x\nk\n) = \nkx\nk\n-\nI \nfor k \n= \n0, 1, ... , n. Show that T must \nbe the differential operator D.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":76811,"to":76904}}}}],[1306,{"pageContent":"Section 6.5 \nThe Kernel and Range of a Linear Transformation \n481 \n24. Let v\n1\n, ••• , v\n\" \nbe vectors in a vector space V and let \nT : V \n---+ \nW be a linear transformation. \n(a) If {T(v\n1\n)\n, ... , T(v\nn\n)\n} is linearly independent in W, \nshow that { v\n1\n, ••• , vJ is linearly independent in V. \n(b) Show that the converse of part (a) is false. \nThat is, it is not necessarily true that if \n{v\n1\n, .•. , v\nn\n} is linearly independent in V, then \n{T\n(\nv\n1\n)\n, ••• , T (v)} is line arly independent in W. \nIllustrate this with an example T: IR\n2\n---+ \nIR\n2\n• \n25. Define linear transformations S : IR\n2 \n---+ M\n22 \nand \nT : IR\n2 \n---+ IR\n2 \nby \ns[\n�\n] = \n[\na\n: \nb \na\n� \nb\n] \nand T\n[\n�\n] \n[\n2c\n_\n: \nd\n] \nCompute (S 0 T\n) \n[ \n�\n] \nand (S 0 T\n) \n[;\n]\n. Can you \ncompute ( T 0 S\n) \n[;\n]\n? If so, compute it. \n26. Define linear transformations S : <!P \n1 \n---+ <!P \n2 \nand \nT: <!P\n2\n---+ <!P\n1 \nby \nS(\na  + bx\n) = \na  + \n(\na  +  b \n)\nx  + 2bx\n2 \nand \nT ( a  + bx +  cx\n2\n) = \nb  + 2cx \nCompute (S 0 T\n)(\n3  + 2x -  x\n2\n) and \n(S 0 T)(\na  + bx + cx\n2\n)\n. Can you compute \n( T 0 S) (\na  + bx\n) ? If so, compute it. \n�","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":76906,"to":77029}}}}],[1307,{"pageContent":"1 \nby \nS(\na  + bx\n) = \na  + \n(\na  +  b \n)\nx  + 2bx\n2 \nand \nT ( a  + bx +  cx\n2\n) = \nb  + 2cx \nCompute (S 0 T\n)(\n3  + 2x -  x\n2\n) and \n(S 0 T)(\na  + bx + cx\n2\n)\n. Can you compute \n( T 0 S) (\na  + bx\n) ? If so, compute it. \n� \n27. Define linear transformations S : <;if\n\" \n---+ <;if\n\" \nand \nT: <!P\nn\n-+\n<!P\nn \nby \nS(p (\nx\n)) = \np (\nx  + 1\n) \nand \nT (p (x\n)) = \np'(x\n) \nFind (S 0 T)(p(x)) and ( T 0 S)(p (x\n))\n.  [Hint: Remember \nthe Chain Rule.] \n� \n28. Define linear transformations S: <;if \nn\n---+ <;if \nn \nand \nT: <;if \nn\n-+ \n<;if \nn \nby \nS(p (x\n)) = \np (\nx  + 1\n) \nand T (p (\nx\n)) = \nxp'(x\n) \nFind (S 0 T\n)(\np (x)) and ( T 0 S )(p (x)) . \nIn Exercises 29 and 30, verify that S and Tare inverses. \n29. S: IR\n2\n-+ \nIR\n2 \ndefined by \ns\n[\nx\n] = \n[\n4x + y\n] \nand T: IR\n2\n-+ \nIR\n2 \ny \n3x + y \ndefined by T\n[\nx\n] \n= \n[ \nx -  y \n] \ny \n-3x  +  4y \n30. S: <!P\n1\n---+ <!P\n1 \ndefined by S(a  +  bx\n) = \n(\n-4a  +  b\n) \n+  2ax and T: <!P\n1\n---+ <!P\n1 \ndefined by \nT ( a  + bx\n) = \nb/2 + \n(\na  + 2b\n)\nx \n31. Prove Theorem 6.17. \n32. Let T: V\n---+ \nV be a linear transformation such that \nToT=I. \n(a) Show that {v, T (\nv\n)\n} is linearly dependent if and","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":77029,"to":77170}}}}],[1308,{"pageContent":"1\n---+ <!P\n1 \ndefined by \nT ( a  + bx\n) = \nb/2 + \n(\na  + 2b\n)\nx \n31. Prove Theorem 6.17. \n32. Let T: V\n---+ \nV be a linear transformation such that \nToT=I. \n(a) Show that {v, T (\nv\n)\n} is linearly dependent if and \nonly if T\n(\nv\n) = \n± v. \n(b) Give an example of such a linear transformation \nwith V \n= \nIR\n2\n• \n33. Let T: V\n---+ \nV be a   linear transformation such that \nT 0 T \n= \nT. \n(a) Show that {v, T (\nv\n)\n} is linearly dependent if and \nonly if T (\nv\n) = \nv or T (\nv\n) = \n0. \n(b) Give an example of such a linear transformation \nwith V \n= \nIR\n2\n. \nThe set of all linear transformations from a vector space V \nto a vector space W is denoted by ;£', \n( \nV, W). If S and T are \nin ;£', \n( \nV, W \n)\n, we can define th e sum S  +  T of S and T by \n(S \n+ \nT\n)(\nv\n) = \nS(v\n) \n+ \nT (\nv\n) \nfor all v in V If c is a scalar, we define th e scalar multiple \ncT of T by c to be \n(cT)(\nv\n) = \ncT\n(\nv\n) \nfor all v in V Then S  +  T and cT are both transformations \nfrom Vto W \n34. \nProve that S  +  T and cT are linear transformations. \n35. Prove that H',\n(","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":77170,"to":77259}}}}],[1309,{"pageContent":"cT of T by c to be \n(cT)(\nv\n) = \ncT\n(\nv\n) \nfor all v in V Then S  +  T and cT are both transformations \nfrom Vto W \n34. \nProve that S  +  T and cT are linear transformations. \n35. Prove that H',\n(\nV, W) is a vector space with this addi­\ntion and scalar multiplication. \n36. Let R, S, and T be linear transformations such that the \nfollowing operations make sense. Prove that: \n(a) R o (S \n+  T\n) = \nR o S  +  R o T \n(b) c(R 0 S\n) = (cR) 0 S \n= \nR 0 (cS) for any scalar c \nII \nThe Kernel and Range of a  linear Transformation \nThe null space and column space are two of the fundamental subspaces associated \nwith a matrix. In this section, we extend these notions to the kernel and range of a \nlinear transformation.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":77259,"to":77289}}}}],[1310,{"pageContent":"482 \nChapter 6  Vector Spaces \nThe word kernel is derived from \nthe Old English word cyrnel, a \nform of the word corn, meaning \n\"seed\" or \"grain:' Like a kernel of \ncorn, the kernel of a linear trans­\nformation is its \"core\" or \"seed\" in \nthe sense that it carries informa­\ntion about many of the important \nproperties of the transformation. \nExample 6.59 \nExample 6.60 \nDefinition \nLet T: V ---+ Wbe a  linear transformation. The kernel of T, denoted \nker(T), is the set of   all vectors in V that are mapped by T   to 0 in W. That is, \nker\n(\nT\n) \n= {v in V: T\n(\nv\n) \n= O} \nThe range of T, denoted range(T), is the set of all vectors in W that are images of \nvectors in V under T. That is, \nrange\n(\nT\n) \n= {T\n(\nv\n)\n: v in V} \n= {w in W:w =  T\n(\nv\n)\nforsomevin V} \nLet A be an m X n matrix and let T =   TA be the corresponding matrix transformation \nfrom !R\nn \nto !R\nm \ndefined by T(v) = Av. Then, as we saw in Chapter 3, the range of Tis \nthe column space of A. \nThe kernel of T is \nker\n(\nT\n) \n=  {v in !R\nn\n: T\n(\nv\n) \n= O} \n=  {v in !R\nn\n:Av = O} \n= null\n(\nA\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":77291,"to":77357}}}}],[1311,{"pageContent":"from !R\nn \nto !R\nm \ndefined by T(v) = Av. Then, as we saw in Chapter 3, the range of Tis \nthe column space of A. \nThe kernel of T is \nker\n(\nT\n) \n=  {v in !R\nn\n: T\n(\nv\n) \n= O} \n=  {v in !R\nn\n:Av = O} \n= null\n(\nA\n) \nIn words, the kernel of a matrix transformation is just the null space of the corre­\nsponding matrix. \nFind the kernel and range of the differential operator D :  <;if \n3 \n---+ <!P\n2 \ndefined by \nD\n(\np\n(\nx\n)) = p'\n(\nx\n)\n. \nSolulion \nSince D\n(\na + bx+ cx\n2 \n+ dx\n3\n) \n= \nb + 2cx + 3dx\n2\n, we have \nker\n(\nD\n) \n= {a+ bx+  cx\n2 \n+ dx\n3\n: D(a  + bx+  cx\n2 \n+ dx\n3\n) \n= O } \n= {a  + bx+  cx\n2 \n+ dx\n3\n: b  + 2cx  + 3dx\n2 \n= O } \nBut b + 2cx + 3dx\n2 \n= 0 if and only if b = 2c = 3d = 0, which implies that b = c = \nd = 0. Therefore, \nker\n(\nD\n) \n= {a + bx+ cx\n2 \n+ dx\n3\n: b = c = d = O } \n= {a: a in IR} \nIn other words, the kernel of D is the set of constant polynomials. \nThe range of Dis all of\n<!P\n2\n, since every polynomial in <!P\n2 \nis the image under D (i.e., \nthe derivative) of some polynomial in <!P\n3\n. To be specific, if a+ bx+ cx\n2 \nis in <!P\n2\n, then","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":77357,"to":77459}}}}],[1312,{"pageContent":"b \n2 \nb \n2 \ny \nExample 6.61 \n1 \n2 \nFigure 6.1 \nb \nIfy = --+bx \n2 \n' \nthen ry dx = 0 \n0 \nExample 6.62 \nSection 6.5 \nThe Kernel and Range of a Linear Transformation \n483 \nLet S : <!/' \n1 \n� \nIR be the linear transformation defined by \nS\n(\np\n(\nx\n)) = \nrp\n(\nx\n)d\nx \n0 \nFind the kernel and range of S. \nSolution \nIn detail, we have \nTherefore, \nS\n(\na +bx\n) = \nr\n(\na + bx\n)d\nx \n0 \n=\n[\nax + !x\n2\nI \n=\n(\na+%\n)\n-o\n=a+% \nker\n(\nS\n) = \n{a + bx : S\n(\na + bx\n) = \nO} \n{\na + bx : a + % \n= \n0\n} \n{\na + bx : a \n= \n-\n! \n} \n{\n-\n!+bx\n} \nGeometrically, ker(S) consists of all those linear polynomials whose graphs have the \nproperty that the area between the line and the x-  axis is equally distributed above and \nbelow the axis on the interval [O, l] (see Figure 6.7). \nThe range of S is IR, since every real number can be obtained as the image under \nS of some polynomial in <!/'\n1\n. For example, if a is an arbitrary real number, then \n{a \nd\nx \n= \n[axn \n=\na \n- o \n=\na \n0 \nso a \n= \nS(a). \nLet  T :  M\n22 \n� M\n22 \nbe the linear transformation defined by taking transposes: \nT(A) = A\nT\n. Find the kernel and range of T.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":77461,"to":77573}}}}],[1313,{"pageContent":"{a \nd\nx \n= \n[axn \n=\na \n- o \n=\na \n0 \nso a \n= \nS(a). \nLet  T :  M\n22 \n� M\n22 \nbe the linear transformation defined by taking transposes: \nT(A) = A\nT\n. Find the kernel and range of T. \nSolution We see that \nker\n(\nT\n) = \n{A in M\n22\n: T\n(\nA\n) = \nO} \n= \n{A in M\n22\n:A\nT = \nO} \nBut if A\nT\n= 0, then A \n= \n(A \nT\n)\nT \n= \no\nT \n= \n0. It follows that ker(T) \n= \n{ O}. \nSince, for any matrix A  in M\n22\n,  we have A \n= \n(A\nT\n)\nT \n= \nT(A\nT\n) (and A\nT \nis in M\n22\n), \nwe deduce that range(T) \n= \nM\n22\n. \nIn all of these examples, the kernel and range of a linear transformation are sub­\nspaces of the domain and co domain, respectively, of the transformation. Since we are \ngeneralizing the null space and column space of a matrix, this is perhaps not surpris­\ning. Nevertheless, we should not take anything for granted, so we need to prove that \nit is not a    coincidence.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":77573,"to":77653}}}}],[1314,{"pageContent":"484 \nChapter 6  Vector Spaces \nTheorem 6.18 \nExample 6.63 \nLet T: V---+ W be a linear transformation. Then: \na.  The kernel of T is a subspace of V. \nb.  The range of T is a subspace of W. \nProof (a)  Since T(O) = 0, the zero vector of Vis in ker(T), so ke  r(T) is nonempty. \nLet u and v be in ker(T) and let c be a scalar. Then T(u) = T(v) = 0, so \nand \nT(u \n+ v)  =   T(u) \n+ T(v)  = 0  +  0 = 0 \nT(cu)  =  cT(u) =  cO = 0 \nTherefore, u + v and cu are in ker(T), and ker(T) is a subspace of V. \n(b)  Since 0 =  T(O), the zero vector of Wis in range(T), so range(T) is nonempty. \nLet T(u) and T(v) be in  the range of T and let c be a scalar. Then T(u) + T(v) = \nT(u + v) is the image of the vector u + v. Since u and v are in V, so is u + v, and \nhence T(u) + T(v) is in range (T). Similarly, cT(u) = T(cu).  Since u is in  V, so is cu, \nand hence cT(u) is in range(T). Therefore, range(T) is a nonempty subset of W that is \nclosed under addition and scalar multiplication, and thus it is a subspace of W.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":77655,"to":77675}}}}],[1315,{"pageContent":"and hence cT(u) is in range(T). Therefore, range(T) is a nonempty subset of W that is \nclosed under addition and scalar multiplication, and thus it is a subspace of W. \nFigure 6.8 gives a schematic representation of the kernel and range of a linear \ntransformation. \n� \nran\nge(\nT) \nv \n0 \n•\no \nT \nW \nFigure 6.8 \nThe kernel and range of T : V--+ W \nIn Chapter 3, we   defined the rank of a matrix to be the dimension of its column \nspace and the nullity of a matrix to be the dimension of its null space. We now extend \nthese definitions to linear transformations. \nDefinition \nLet T: V ---+ W be a linear transformation. The rank of Tis the \ndimension of the range of T and is denoted by rank(T). The nullity of T is \nthe dimension of the kernel of T and is denoted by nullity(T). \nIf A is a matrix and T =   TA is the matr  ix transformation defined by T(v) =  Av, then \nthe range and kernel of Tare the column space and the null space of A, respectively, \nby Example 6.59. Hence, from Section 3.5, we have \nrank\n(\nT\n) \n= rank\n(\nA","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":77675,"to":77707}}}}],[1316,{"pageContent":"the range and kernel of Tare the column space and the null space of A, respectively, \nby Example 6.59. Hence, from Section 3.5, we have \nrank\n(\nT\n) \n= rank\n(\nA\n) \nand nullity\n(\nT\n) \n= nullity\n(\nA\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":77707,"to":77724}}}}],[1317,{"pageContent":"Example 6.64 \nExample 6.65 \nExample 6.66 \nSection 6.5 \nThe Kernel and Range of a Linear Transformation \n485 \nFind the rank and the nullity of the linear transformation D :  l!P\n3 \n� <!J\n2 \ndefined by \nD(p(x)) =  p'(x). \nSolution \nIn Example 6.60, we computed range \n(\nD\n) \n= <!J\n2\n, so \nrank\n(\nD\n) \n=  dim <!f \n2 \n= 3 \nThe kernel of Dis the set of all constant polynomials: ker(D) = {a : a  in IR} = {a · 1 : a \nin IR}. Hence, {1} is a basis for ke  r(D), so \nnullity\n(\nD\n) \n=  dim\n(\nker\n(\nD\n)) \n=  1 \nFind the rank and the nullity of the linear transformation S : <!J 1 \n� \nIR defined by \nS(p(x)) = \nr\np(x) dx \n0 \nSolution \nFrom Example 6.61, range(S) = IR and rank(S) =  dim IR =   1. Also, \nker\n(\nS\n) \n= \n{ \n-� +  bx : b in IR\n} \n=  {b\n(\n- t \n+ x\n)\n: bin IR} \n= span\n(\n-\nt \n+ x\n) \nso { -\nt \n+ x} is a basis for ke  r(S). Therefore, nullity(S) =  dim(ker(S)) = 1. \nFind the rank and the nullity of the linear transformation T: M\n22 \n� M\n22 \ndefined by \nT(A) =A\nT\n. \nSolution \nIn Example 6.62, we found that range(T) = M\n22 \nand ker(T) =  {O}. Hence, \nrank\n(\nT\n) = \ndim M\n22 \n= \n4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":77726,"to":77817}}}}],[1318,{"pageContent":"Find the rank and the nullity of the linear transformation T: M\n22 \n� M\n22 \ndefined by \nT(A) =A\nT\n. \nSolution \nIn Example 6.62, we found that range(T) = M\n22 \nand ker(T) =  {O}. Hence, \nrank\n(\nT\n) = \ndim M\n22 \n= \n4 \nand  nullity\n(\nT\n) \n= dim{O} = 0 \nIn Chapter 3, we   saw that the rank and nullity of an m X n matrix A  are related \nby the formula rank(A) + nullity(A) = n. This is the Rank Theorem (Theorem 3.26). \nSince the matrix transformation T = TA has !R\nn \nas its domain, we could rewrite the \nrelationship as \nrank\n(\nA\n) \n+ nullity\n(\nA\n) = \ndim !R\nn \nThis version of the Rank Theorem extends very nicely to general linear transforma­\ntions, as you can see from the last three examples: \nrank\n(\nD\n) \n+ nullity\n(\nD\n) \n=  3 +  1  = 4 = dim l!P\n3 \nrank\n(\nS\n) \n+ nullity\n(\nS\n) \n=  1  +  1  = 2 = dim <!J 1 \nrank\n( \nT\n) \n+ nullity\n( \nT\n) \n= 4 + 0 = 4 = dim M\n22 \nExample 6.64 \nExample 6.65 \nExample 6.66","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":77817,"to":77891}}}}],[1319,{"pageContent":"486 Chapter 6  Vector Spaces \nTheorem 6.19 \nThe Rank Theorem \nLet T : V ---+ W be a linear transformation from a finite-dimensional vector space \nV into a vector space W. Then \nrank\n( \nT\n) \n+ \nnullity\n( \nT\n) = \ndim V \nIn the next section, you will see how to adapt the proof of Theorem 3.26 to prove \nthis version of the result. For now, we give an al  ternative proof that does not use \nmatrices. \nProof Let dim V \n= \nn and let {v\n1 , ... , v\nk\n} be a   basis for ker(T)[so that nullity(T) \n= \ndim(ker(T)) \n= \nk]. Since {v1 , ... , vd is a linearly independent set, it can be extended \nto a basis for V, by Theorem 6.28. Let B \n= \n{v\n1\n, ... , v\nk\n> \nv\nk\n+\n1\n, ... , v\nn\n} be such a basis. \nIfwe can show that the set C \n= \n{T\n(\nv\nk\n+\n1)\n, ... , T(v\nn\n)} is a basis for range(T), then we \nwill have rank(T) \n= \ndim(range(T)) \n= \nn -k and thus \nrank\n(\nT\n) \n+ \nnullity\n(\nT\n) = \nk \n+ \n(n -k\n) = \nn \n= \ndim V \nas required. \nCertainly C is contained in the range of T. To show that C spans the range of T, let","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":77893,"to":77968}}}}],[1320,{"pageContent":"will have rank(T) \n= \ndim(range(T)) \n= \nn -k and thus \nrank\n(\nT\n) \n+ \nnullity\n(\nT\n) = \nk \n+ \n(n -k\n) = \nn \n= \ndim V \nas required. \nCertainly C is contained in the range of T. To show that C spans the range of T, let \nT(v) be a vector in the range of T. Then vis in V, and since Bis a  basis for V, we can \nfind scalars c\n1 , ... , e\nn \nsuch that \nSince v1, .•. , v\nk \nare in the kernel of T, we have T(v1) \n= \n· · · \n= \nT(v\nk\n) \n= \n0, so \nT\n(\nv\n) = \nT ( c\n1\nV\n1 \n+ \n... \n+ \nc\nk\nv\nk \n+ \nC\nk\n+\n1\nV\nk\n+\n1 \n+ \n... \n+  c\nn\nv\nn\n) \n= \nc\n1\nT (\nv\n1\n) \n+ \n· · · \n+  c\nk\nT (\nv\nk\n) \n+  c\nk +\n1\nT (\nv\nk +\n1\n) \n+ \n· · · \n+  c\nn\nT (\nv\nn\n) \n= \nC\nk\n+\n1\nT (\nv\nk\n+\n1\n) \n+ \n... \n+  c\nn\nT (\nv\nn\n) \nThis shows that the range of Tis spanned by C. \nTo show that C is linearly independent, suppose that there are scalars c\nk\n+ \n1\n, ... , e\nn \nsuch that \nThen T\n(\nc\nk\n+\nI\nV\nk\n+\nI \n+ ··· +  c\nn\nv\nn\n) \n= \n0, which means that \nCk\n+\n!\nV\nk\n+\nl \n+ ··· +  c\nn\nv\nn \nis in the \nkernel of T and is, hence, expressible as a linear combination of the basis vectors \nv1 , ... , v\nk \nofker(T)-say, \nBut now \nand the linear independence of B forces c\n1 \n= \n·  ·  · \n= \ne\nn \n= \n0. In particular, c\nk\n+\n1 \n=","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":77968,"to":78142}}}}],[1321,{"pageContent":"v1 , ... , v\nk \nofker(T)-say, \nBut now \nand the linear independence of B forces c\n1 \n= \n·  ·  · \n= \ne\nn \n= \n0. In particular, c\nk\n+\n1 \n= \n·  ·  · \n= \ne\nn \n= \n0, which means C is linearly independent. \nWe have shown that C is a basis for the range of T, so, by our comments above, the \nproof is complete. \nWe have verified the Rank Theorem for Examples 6.64, 6.65, and 6.66. In practice, \nthis theorem allows us to find the rank and nullity of a linear transformation with \nonly half the work. The following examples illustrate the process.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":78142,"to":78169}}}}],[1322,{"pageContent":"Example 6.61 \n� \nExample 6.68 \nSection 6.5 \nThe Kernel and Range of a Linear Transformation 481 \nFind the rank and nullity of the linear transformation T :  <!/' \n2 \n� <!/' \n3 \ndefined by \nT(p(x)) =  xp (x). (Check that T  really is linear.) \nSolution \nIn detail, we have \nIt follows that \nT\n(\na +   bx + cx\n2\n) \n= ax +  bx\n2 \n+ cx\n3 \nker\n(\nT\n) \n=  {a  + bx + cx\n2\n: T\n(\na +   bx + cx\n2\n) \n= O} \n= {a + bx + cx\n2\n: ax +  bx\n2 \n+ cx\n3 \n= O} \n={a+ bx +  cx\n2\n:a =  b = c = o} \n= {o} \nso we have nullity(T) = dim(ker(T)) = 0. The Rank Theorem implies that \nrank\n(\nT\n) \n=  dim <!/'\n2 \n- nullity\n(\nT\n) \n= 3 -0 = 3 \nRemarll In Example 6.67, it would be just as easy to find the rank of T first, since \n{x, x\n2\n, x\n3\n} is easily seen to be a basis for the range of T. Usually, though, one of the two \n(the rank or the nullity of a linear transformation) will be easier to compute; the Rank \nTheorem can then be used to find the other. With practice, you will become better at \nknowing which way to proceed.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":78171,"to":78237}}}}],[1323,{"pageContent":"Theorem can then be used to find the other. With practice, you will become better at \nknowing which way to proceed. \nLet W be the vector space of all symmetric 2 X 2 matrices. Define a linear transfor­\nmation T : W \n� \n<!/' \n2 \nby \nr\n[\n:  �\n] \n= \n(\na - b\n) \n+ \n(\nb - c\n)\nx + \n(\nc - a\n)\nx\n2 \niJ)-'\"--\n(Check that Tis linear.) Find the rank and nullity of T. \nSolution The nullity of Tis easier to compute directly than the rank, so we proceed \nas follows: \nker\n( \nT\n) \n= \n{ \n[\n: \n�] : T \n[\n:  �] = 0\n} \n{ \n[\n: \n�\n] \n: \n(\na -\nb\n) \n+ \n(\nb \n- c\n)\nx + \n(\nc - a\n)\nx\n2 \n= \nO\n} \n{ \n[\n: \n�] :\n(\na - b\n) \n= \n(\nb - c\n) \n= \n(\nc - a\n) \n= 0\n} \n{\n[\n:  �\n]\n:a= b   =c\n} \n{ \n[\n� �\n]\n} \n= span(\n[\n�  �\n]\n) \nTherefore, \n{ \n[ \n�  �\n] \n} \nis a basis for the kernel of T, so nullity( T) =   dim (ker (   T)) = 1. \nThe Rank Theorem and Example 6.42 tell us that rank(T) = dim W - nullity(T) = \n3 - 1 = 2.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":78237,"to":78344}}}}],[1324,{"pageContent":"488 Chapter 6  Vector Spaces \none-to-one and Onto Linear Transformations \nWe now investigate criteria for a linear transformation to be invertible. The keys to \nthe discussion are the very important properties one-to-one and onto. \nDefinition \nA linear transformation T: V ---+ Wis called one-to-one if T maps \ndistinct vectors in V to distinct vectors in W. If range( T) \n= \nW, then Tis called onto. \nRemarks \n• \nThe definition of one-to-one may be written more formally as follows: \nT: V ---+ Wis one-to-one if, for all u and v in V, \nu * v implies that T\n(\nu\n) \n* T\n(\nv\n) \nThe above statement is equivalent to the following: \nT: V ---+ Wis one-to-one if, for all u and v in V, \nT\n(\nu\n) = \nT\n(\nv\n) \nimplies that u \n= \nv \nFigure 6.9 illustrates these two statements. \nv \nw \nv w \n(a) \nTis one-to-one \nFioure 6.9 \n(b) Tis not one-to-one \n• \nAnother way to write the definition of onto is as  follows: \nT : V ---+ W is onto if, for all w in W, there is at least one v    in V such that \nw \n= \nT\n(\nv\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":78346,"to":78396}}}}],[1325,{"pageContent":"Fioure 6.9 \n(b) Tis not one-to-one \n• \nAnother way to write the definition of onto is as  follows: \nT : V ---+ W is onto if, for all w in W, there is at least one v    in V such that \nw \n= \nT\n(\nv\n) \nIn other words, given win W, does there exist some v in V such that w \n= \nT(v)? If, \nfor an arbitrary w, we can solve this equation for v,   then Tis onto (see Figure 6.10).","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":78396,"to":78410}}}}],[1326,{"pageContent":"Example 6.69 \nSection 6.5 \nThe Kernel and Range of a Linear Transformation \n489 \nv \n(a) \nTis onto \nFigure 6.10 \nw v \n(b) Tis not onto \nWhich of the following linear transformations are one-to-one? onto? \n(')  T \n!I\n'\n-> !I\n' \ndefined by r\n[;\n] \n� \n[ \nx � y\n] \n(b) D: <!/'\n3\n--\n-+ \n<!1'\n2 \ndefined by D(p(x)) = p'(x) \n(c)  T: M\n22\n--\n-+ M\n22 \ndefined by T(A) =A\nT \nSolution (a)  Let r\n[;\n:\nJ \n= r\n[;\n:J\n. Then \nw \nso 2x\n1 \n= 2x\n2 \nand x\n1 \n- y\n1 \n= x\n2 \n- y\n2\n• Solving these equations, we see that x\n1 \n= x\n2 \nand \ny\n1 \n= Y\nz\n. Hence, \n[;\n:\n] \n= \n[;\n:\n]\n, so Tis one-to-one. \nTis not onto, since its range is not all of IR\n3\n. To be specific, there is no   vector \nIt-\"-\n[; \nl \nin !I\n' \n'uch th\" r\n[\n; \nl \n� m (Why not?) \nTheorem 6.20 \n(b) In Example 6.60, we showed that range(D) \n= \n<!/'\n2\n, so Dis onto. Dis not one­\nto-one, since distinct polynomials in <!/'\n3 \ncan have the same derivative. For example, \nx\n3 \n-=F x\n3 \n+ 1, but D(x\n3\n) =  3x\n2 \n= D(x\n3 \n+ 1). \n(c) Let A and B be in M\n22\n, with T(A) \n= \nT(B). Then A\nT \n= \nB\nT\n, so A \n= \n(A\nT\n)\nT \n= \n(B\nT","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":78412,"to":78538}}}}],[1327,{"pageContent":"3 \ncan have the same derivative. For example, \nx\n3 \n-=F x\n3 \n+ 1, but D(x\n3\n) =  3x\n2 \n= D(x\n3 \n+ 1). \n(c) Let A and B be in M\n22\n, with T(A) \n= \nT(B). Then A\nT \n= \nB\nT\n, so A \n= \n(A\nT\n)\nT \n= \n(B\nT\nl = B. Hence, Tis one-to-one. In Example 6.62, we showed that range(T) = M\n22\n• \nHence, Tis onto. \n4 \nIt turns out that there is a very simple criterion for determining whether a linear \ntransformation is one-to-one. \nA linear transformation T: V--\n-+ \nWis one-to-one if and only ifker(T) = {O}.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":78538,"to":78578}}}}],[1328,{"pageContent":"490 \nChapter 6  Vector Spaces \nExample 6.10 \nProof Assume that Tis one-to-one. If vis in the kernel of T, then T(v)  = 0. But \nwe also know that T(O) = 0, so T(v) =   T(O). Since Tis one-to-one, this implies that \nv = 0, so the only vector in the kernel of T is the zero vector. \nConversely, assume that ker(T) = {O}. To show that Tis one-to-one, let u and v be \nin Vwith T(u) = T(v).  Then T(u -  v) = T(u) - T(v) = 0, which implies that u - v \nis in the kernel of T. But ker(T) = {O}, so we must have u - v = 0 or, equivalently, \nu =   v. This proves that Tis one-to-one. \nShow that the linear transformation T : IR\n2 \n---+ rtP 1 defined by \nis one-to-one and onto. \nr\n[\n�\n] \n= a + \n(\na + b\n)\nx \nSolulion If \n[ \n�\n] is in the kernel of T, then \n0 = r\n[\n�\n] \n=a+ \n(\na+ b\n)\nx \nIt follows that a = 0 and a + b = 0.   Hence, b = 0,   and therefore \n[ \n�\n] \nquently, ker\n( \nT\n) \n= \n{ \n[ \n�\n] \n} \n, and  Tis one-to-one, by Theorem 6.20. \nBy the Rank Theorem, \nrank\n(\nT\n) \n= dim IR\n2 \n- nullity\n(\nT\n) \n= 2 - 0 = 2 \n[\n�\n]\n. Conse-","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":78580,"to":78646}}}}],[1329,{"pageContent":"[ \n�\n] \nquently, ker\n( \nT\n) \n= \n{ \n[ \n�\n] \n} \n, and  Tis one-to-one, by Theorem 6.20. \nBy the Rank Theorem, \nrank\n(\nT\n) \n= dim IR\n2 \n- nullity\n(\nT\n) \n= 2 - 0 = 2 \n[\n�\n]\n. Conse-\nTherefore, the  range of  T  is  a  two-dimensional subspace of IR\n2\n, and  hence \nrange(T) = IR\n2\n. It follows that Tis onto. \nFor linear transformations between two n-dimensional vector spaces, the proper­\nties of one-to-one and onto are closely related. Observe first that for a linear trans­\nformation T: V---+ W, ker(T) = {O} if and only if nullity(T) = 0, and Tis onto if and \n� \nonly if rank(T) = dim W. (Why?) The proof of the next theorem essentially uses the \nmethod of Example 6.70. \nTheorem 6.21 \nLet dim V =   dim W =   n. Then a linear transformation T: V---+  Wis one-to-one \nif and only if it is onto. \nProof Assume that Tis one-to-one. Then nullity(T) = 0   by Theorem 6.20 and the \nremark preceding Theorem 6.21. The Rank Theorem implies that \nrank\n( \nT\n) \n= dim V -  nullity\n( \nT\n) \n=  n - 0 =  n \nTherefore, T is onto.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":78646,"to":78702}}}}],[1330,{"pageContent":"remark preceding Theorem 6.21. The Rank Theorem implies that \nrank\n( \nT\n) \n= dim V -  nullity\n( \nT\n) \n=  n - 0 =  n \nTherefore, T is onto. \nConversely, assume that Tis onto. Then rank(T) =  dim W = n. By the Rank \nTheorem, \nnullity\n( \nT\n) \n= dim V -  rank\n( \nT\n) \n=  n - n = 0 \nHence, ker(T) = {O}, and Tis one-to-one.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":78702,"to":78724}}}}],[1331,{"pageContent":"Theorem 6.22 \ncoronarv  6.23 \nExample 6.11 \nTheorem 6.24 \nSection 6.5 \nThe Kernel and Range of a Linear Transformation \n491 \nIn Section 6.4, we pointed out that if T :   V---+ Wis a  linear transformation, then \nthe image of a basis for V under T need not be a basis for the range of T. We can now \ngive a condition that ensures that a basis for V will be mapped by T to a basis for W. \nLet T: V---+ W be a one-to-one linear transformation. If S \n= \n{v\n1\n, ... , vk} is a lin­\nearly independent set in V, then T(S) \n= \n{T(v\n1\n), ... , T(vk\n)} is a linearly indepen­\ndent set in W. \nProof \nLet c\n1\n, ... , ck be scalars such that \nThen T(c\n1\nv\n1 \n+ \n· · · \n+ ckvk) \n= \n0, which implies that c\n1\nv\n1 \n+ \n· · · \n+ ckvk \nis in  the kernel of \nT. But, since Tis one-to-one, ker(T) \n= \n{O}, by Theorem 6.20. Hence, \nc\n1\nv\n1 \n+ \n· · · \n+ c\nk\nv\nk \n= \n0 \nBut, since {v\n1\n, ••• , vd is line arly independent, all of the scalars \nC\n; must be 0. Therefore, \n{T(v\n1\n), ... , T(vk)} is linearly independent. \nLet dim V \n= \ndim W \n=","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":78726,"to":78794}}}}],[1332,{"pageContent":"c\n1\nv\n1 \n+ \n· · · \n+ c\nk\nv\nk \n= \n0 \nBut, since {v\n1\n, ••• , vd is line arly independent, all of the scalars \nC\n; must be 0. Therefore, \n{T(v\n1\n), ... , T(vk)} is linearly independent. \nLet dim V \n= \ndim W \n= \nn. Then a one-to-one linear transformation T: V---+ W \nmaps   a basis for V   to a basis for W. \nProof Let B \n= \n{v\n1\n, .•. , v\nn\n} be a basis for V. By Theorem 6.22, T\n(\nB\n) = \n{T(v\n1\n), ••• , \nT\n(\nv\nn\n)\n} is a linearly independent set in W, so we need only show that T\n(\nB\n) \nspans \nW. But, by Theorem 6.15, T\n(\nB\n) \nspans the range of T. Moreover, Tis onto, by Theo­\nrem 6.21, so range(T) \n= \nW. Therefore, T\n(\nB\n) \nspans W, which completes the proof. \nLet T: IR\n2\n---+ \n\\fP\n1 \nbe the linear transformation from Example 6.70, defined by \nr[:\nJ =\na+ \n(\na+ b\n)\nx \nThen, by Corollary 6.23, the standard basis [ \n= \n{ e\n1\n, e\n2\n} for IR\n2 \nis mapped to a basis \nT\n(\n[\n) \n= \n{T\n(\ne\n1\n)\n, T\n(\ne\n2\n)\n} of\\fP\n1\n• We find that \nT\n(\ne\n1\n) \n=  r[\n�] \n= \n1 \n+ \nx  and  T\n(\ne\n2\n) \n=  r[\n�\n] \n= x \nIt follows that { 1  + x, x} is a basis for \\fP \n1\n.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":78794,"to":78916}}}}],[1333,{"pageContent":"= \n{ e\n1\n, e\n2\n} for IR\n2 \nis mapped to a basis \nT\n(\n[\n) \n= \n{T\n(\ne\n1\n)\n, T\n(\ne\n2\n)\n} of\\fP\n1\n• We find that \nT\n(\ne\n1\n) \n=  r[\n�] \n= \n1 \n+ \nx  and  T\n(\ne\n2\n) \n=  r[\n�\n] \n= x \nIt follows that { 1  + x, x} is a basis for \\fP \n1\n. \nWe can now determine which linear transformations T :  V---+ Ware invertible. \nA linear transformation T :  V   ---+ W is invertible if and only if it is one-to-one \nand onto.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":78916,"to":78966}}}}],[1334,{"pageContent":"492 \nChapter 6  Vector Spaces \nProof Assume that Tis invertible. Then there exists a linear transformation T\n-\n1 \n: \nW � V such that \nT\n-\n1 \n0 T = I\nv \nand  T 0 T\n-\n1 \n= I\nw \nTo show that Tis one-to-one, let v be in the kernel of T. Then T(v) = 0. Therefore, \nY\n-\n1\n(\nT\n(\nv\n)) \n= \nY\n-\n1\n(\n0\n) \n=:> \n(\nT\n-\n1 \n0 T\n)(\nv\n) \n= 0 \n=:> I(v)  = 0 \n=:>v=O \nwhich establishes that ker(T) = {  O}. Therefore, Tis one-to-one, by Theorem 6.20. \nTo show that Tis onto, let w be in Wand let v = T\n-\n1\n(w). Then \nT\n(\nv\n) \n=  T\n(\nT\n-\n1\n(\nw\n)) \n= \n(\nTo T\n-\n1\n)(\nw\n) \n= I\n(\nw\n) \n=w \nwhich shows that w is the  image of v under T. Since vis in V, this shows that Tis onto. \nConversely, assume that Tis one-to-one and onto. This means that nullity(T) = 0 \nand rank(T) =  dim W. We need to show that there exists a linear transformation \nT' : W � Vsuch that T' 0 T  =Iv and T0 T' = Iw. \nLet w be in W. Since Tis onto, there exists some vector v in V such that T(v) =   w. \nThere is only one such vector v, since, if v' is another vector in V such that T (   v') =  w,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":78968,"to":79048}}}}],[1335,{"pageContent":"Let w be in W. Since Tis onto, there exists some vector v in V such that T(v) =   w. \nThere is only one such vector v, since, if v' is another vector in V such that T (   v') =  w, \nthen T(v) =   T(v'); the fact that Tis one-to-one then implies that v =  v'. It therefore \nmakes sense to define a mapping T' : W � Vby setting T'(w) = v. \nIt follows that \nand \n(\nT' o T\n)(\nv\n) \n=  T'\n(\nT\n(\nv\n)) \n=  T'\n(\nw\n) \n= v \n(\nT 0   T'\n)(\nw\n) \n=  T\n(\nT'\n(\nw\n)) \n=  T\n(\nv\n) \n= w \nIt then follows that T' 0 T =Iv and T 0 T' = Iw. Now we must show that T' is a linear \ntransformation. \nTo  this end, let w\n1 \nand w\n2 \nbe in W and let c\n1 \nand c\n2 \nbe scalars. As above, let \nT(v\n1\n) = w\n1 \nand T(v\n2\n) = w\n2\n• Then v\n1 \n= T'(w\n1\n) and v\n2 \n= T'(w\n2\n) and \nT'\n(\nc\n1\nw\n1 \n+ c\n2\nw\n2\n) \n=  T'\n(\nc\n1\nT\n(\nv\n1\n) \n+ c\n2\nT\n(\nv\n2\n)) \n=  T'\n(\nT\n(\nc\n1\nv\n1 \n+ \nc\n2\nv\n2\n)) \n= I\n(\nc\n1\nV\n1 \n+ C\nz\nV\nz\n) \nConsequently, T' is linear, so, by Theorem 6.17, T' = T\n-\n1\n•","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":79048,"to":79169}}}}],[1336,{"pageContent":"The words isomorphism and \nisomorphic are derived from \nthe Greek words isos, meaning \n\"equal;' and morph, meaning \n\"shape:' Thus, figuratively speak­\ning, isomorphic vector spaces have \n\"equal shapes:' \nExample 6.12 \nExample 6.13 \nSection 6.5 \nThe Kernel and Range of a Linear Transformation \n493 \nIsomorphisms of Vector Spaces \nWe now are in a position to describe, in concrete terms, what it means for two vector \nspaces to be \"essentially the same:' \nDefinition \nA linear transformation T: V � Wis called an isomorphism if it \nis one -to-one and onto. If V and Ware two vector spaces such that there is an iso­\nmorphism from V to W, then we say that Vis isomorphic to Wand write V = W. \nShow that \n<!P \nn\n-I \nand !R\nn \nare isomorphic. \nSolution The process of forming the coordinate vector of a polynomial provides us \nwith one possible isomorphism (as we observed already in Section 6.2, although we did \nnot use the term isomorphism there). Specifically, define T: <;JP \nn\n-\ni \n� \n!R\nn \nby T\n(\np\n(\nx\n)) \n= \n[p\n(\nx\n) \n],0, where£ \n=","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":79171,"to":79218}}}}],[1337,{"pageContent":"with one possible isomorphism (as we observed already in Section 6.2, although we did \nnot use the term isomorphism there). Specifically, define T: <;JP \nn\n-\ni \n� \n!R\nn \nby T\n(\np\n(\nx\n)) \n= \n[p\n(\nx\n) \n],0, where£ \n= \n{ 1, x, ... ,  x\n\"\n-1\n} is the  standard basis for <;JP \nn\n-\ni \n· That is, \nTheorem 6.6 shows that T is a linear transformation. If p\n(\nx\n) \n= \na0 +  a\n1\nx  + \n· · · \n+ \na\nn\n-\n1X\nn\n-\nl \nis in the kernel of T, then \nHence, a0 \n= \na\n1 \n= \n·  ·  · \n= \na\nn\n-\ni \n= \n0, so p\n(\nx\n) \n= \n0. Therefore, ker(T) \n= \n{O}, and Tis one­\nto-one. Since dim <!P\nn\n-\ni \n= \ndim IR\n\" \n= \nn, Tis also onto, by Theorem 6.21. Thus, Tis \nan isomorphism, and <!P \nn\n-\ni \n= IR\n\"\n. \nShow that M\nm\nn \nand !R\nm\n\" \nare isomorphic. \nSolution Once again, the coordinate mapping from M\nm\nn \nto !R\nm\n\" \n(as in Example 6.36\n) \nis an isomorphism. The details of the proof are left as an exercise. \nIn fact, the easiest way to tell if two vector spaces are isomorphic is simply to \ncheck their dimensions, as the next theorem shows.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":79218,"to":79318}}}}],[1338,{"pageContent":"494 \nChapter 6  Vector Spaces \nTheorem 6.25 \nLet V and W be two finite-dimensional vector spaces (over the same field of scalars). \nThen Vis isomorphic to W if and only if dim V \n= \ndim W. \nProof Let  n \n= \ndim  V.  If V is isomorphic to  W, then there is an isomorphism \nT: V---+ W. Since Tis one-to-one, nullity(T) \n= \n0. The Rank Theorem then implies \nthat \nrank\n( \nT\n) = \ndim V -nullity\n( \nT\n) = \nn -0 \n= \nn \nTherefore, the range of T is an n-dimensional subspace of W. But, since T is onto, \nW \n= \nrange(T), so dim W \n= \nn, as we wished to show. \nConversely, assume that Vand Whavethesamedimension, n. LetB \n= \n{v\n1\n, .•. , v\nn\n} \nbe a   basis for V and let C \n= \n{w\n1 , ... , w\nn\n} be a   basis for W. We will define a linear \ntransformation T: V---+ Wand then show that Tis one-to-one and onto. An arbitrary \nvector v in V can be written uniquely as a linear combination of the vectors in the \nbasis B-say, \nWe define T by \n.-.. \nIt is straightforward to check that Tis linear. (Do so.) To see that Tis one-to-one, \nExample 6.14 \nExample 6.15","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":79320,"to":79371}}}}],[1339,{"pageContent":"basis B-say, \nWe define T by \n.-.. \nIt is straightforward to check that Tis linear. (Do so.) To see that Tis one-to-one, \nExample 6.14 \nExample 6.15 \nsuppose vis in the kernel of T.  Then \nand the linear independence of C forces c\n1 \n= \n· · · \n= \ne\nn \n= \n0. But then \nso ker(T) \n= \n{O}, meaning that Tis one-to-one. Since dim V \n= \ndim W, Tis also onto, \nby Theorem 6.21. Therefore, Tis an isomorphism, and V = W. \nShow that u;g\nn \nand <lP \nn \nare not isomorphic. \nSolution Since dim u;g\nn \n= \nn * n +  1 \n= \ndim <lP \nn ' \nu;g\nn \nand <lP \nn \nare not isomorphic, by \nTheorem 6.25. \nLet W be the vector space of all symmetric 2 X 2 matrices. Show that Wis isomorphic \nto IFR\n3\n. \nSolution In Example 6.42, we showed that dim W \n= \n3. Hence, dim W \n= \ndim IFR\n3\n, \nso  W = IFR\n3\n, by  Theorem 6.25. (There is an obvious candidate for an isomorphism \nT: W---+ IFR\n3\n. What is it?)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":79371,"to":79427}}}}],[1340,{"pageContent":".. \nI \nExercises 6.5 \nSection 6.5 \nThe Kernel and Range of a Linear Transformation \n495 \nRemark Our examples have all been real vector spaces, but the theorems we \nhave proved are true for vector spaces over the complex numbers C or Z\nP\n' \nwhere p is \nprime. For example, the vector space M\n22\n(Z\n2\n) of all 2 X 2 matrices with entries from \n2\n2 \nhas dimension 4 as a vector space over 2\n2\n, and hence M\n22\n(Z\n2\n) � Zi. \n1. Let T: M\n22\n---+ M\n22 \nbe the linear transformation \ndefined by \nj]h \n4. Let T: '!!'\n2\n---+ '!!'\n2 \nbe the linear transformation defined by \nT\n(\np\n(\nx\n)) = \nxp'(x\n)\n. \n(a) Which, if any, of the following matrices are in \nker(T)? \n(ii) \n[\n� \n�\n] \n(iii) \n[\n3 O\nJ \n0 -3 \n(b) Which, if any, of the matrices in part (a) are in \nrange(T)? \n(c) Describe ker(T) and range(T). \n2. Let T: M\n22 \n---+ \nIR be the linear transformation defined \nby T\n(\nA\n) = \ntr\n(\nA\n)\n. \n(a) Which, if any, of the following matrices are in \nker(T)? \n(i) \n[ \n1 2\n] \n-\n1 3 \n(ii) \n[\n� \n�\n] \n(iii) \n[\n1 3\n] \n0 \n-\n1 \n(b) Which, if any, of the following scalars are in \nrange(T)? \n(i) 0 (ii) 2 (iii) \nVl\n/2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":79429,"to":79527}}}}],[1341,{"pageContent":") = \ntr\n(\nA\n)\n. \n(a) Which, if any, of the following matrices are in \nker(T)? \n(i) \n[ \n1 2\n] \n-\n1 3 \n(ii) \n[\n� \n�\n] \n(iii) \n[\n1 3\n] \n0 \n-\n1 \n(b) Which, if any, of the following scalars are in \nrange(T)? \n(i) 0 (ii) 2 (iii) \nVl\n/2 \n(c) Describe ker(T) and range(T). \n3. Let T: '1P\n2\n---+ \nIR\n2 \nbe the linear transformation \ndefined by \nT\n(\na  + bx +  cx\n2\n) \n= [\na  -   b\n] \nb  +  c \n(a) Which, if any, of the following polynomials are in \nker(T)? \n(i) 1 +  x (ii) x - x\n2 \n(iii) 1 +  x  -  x\n2 \n(b) Which, if any, of the following vectors are in \nrange(T)? \n(i) \n[ \n�\n] \n(ii) \n[\n�\n] \n(iii) \n[\n�\n] \n(c) Describe ker(T) and range(T). \n(a) Which, if any, of the following polynomials are  in \nker(T)? \n(i) 1 (ii) x (iii)  x\n2 \n(b) Which, if any, of the polynomials in part (a) are in \nrange(T)? \n(c) Describe ker(T) and range(T). \nIn Exercises 5-8, find bases for th e kernel and range of th e \nlinear transformations T in th e indicated exercises. In each \ncase, state the nullity and rank of T and verify th e Rank \nTheorem. \n5. Exercise 1 \n7. Exercise 3 \n6. Exercise 2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":79527,"to":79609}}}}],[1342,{"pageContent":"linear transformations T in th e indicated exercises. In each \ncase, state the nullity and rank of T and verify th e Rank \nTheorem. \n5. Exercise 1 \n7. Exercise 3 \n6. Exercise 2 \n8. Exercise 4 \nIn Exercises 9-14, find either th e nullity or th e rank of T \nand then use th e Rank Theorem to find th e other. \n2 \n[\na b\n]    [\na  -   b\n] \n9. T: M\n22\n---+ \nIR  defined by \nT \nc d \n= \nc \n_ \nd \n10. T: r;;\n2\n---+ \n1R\n2 \ndefined by TCpCx\n)) = \n[\n�\n�\n�\nn \n11. T: M\n22 \n---+ \nM\n22 \ndefined by T\n(\nA\n) = \nAB, where \nB=\n[ \n1 \n-\n1\n] \n-\n1 \n1 \n12. T: M\n22 \n---+ \nM\n22 \ndefined by T\n(\nA\n) = \nAB - BA, where \nB = \n[\n� \n�\n] \nJ]h \n13. \nT: '1P\n2\n---+ \nIR defined \nby \nT\n(p\n(x\n)) \n= \np'\n(O\n) \n14. T: M\n33\n---+ \nM\n33 \ndefined by T\n(\nA\n) = \nA -  Ar \nIn Exercises 15-20, determine whether the linear transfor­\nmation Tis (a) one-to-one and (b) onto. \n15. T: IR\n2\n---+ \nIR\n2 \ndefined by \nT\n[\nx\n] \n= \n[\n2x \n-  y\n] \ny \nx \n+  2y","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":79609,"to":79724}}}}],[1343,{"pageContent":"496 \nChapter 6  Vector Spaces \n16. T: IR\n2\n--\n-+ \n<.5P\n2 \ndefined by \nr\n[\n:J =  (\na - 2b\n) \n+ \n(\n3a +  b\n)\nx +\n(\na+ b\n)\nx\n2 \n17. T: <.5P \n2 \n--\n-+ \nIR\n3 \ndefined by \nT\n(\na +bx+ cx\n2\n) \n=\n[\na� b\n-\n-\nb\n3c\n] \nc-a \n18. T: <.5P\n2\n--\n-+ \nIR\n2 \ndefined by T\n(\np\n(\nx\n)) \n= \n[\n�\n�\n�\n;\n] \n1\n9. \nT:\nIR\n3\n--\n-+ \nM\n22\nde\nfin\ned\nby\nr\n[\n:\nc\n] \n=\n[\na \n- b \nb -  c] \na+b b+c \n20. 1\"' nl' --+ w defined by r\n[ \n� l � \n[\na +  b + c b - 2c\n] \nb -2c a  -  c \n, where Wis the  vector space of \nall symmetric 2 X 2 matrices \nIn Exercises 21-26, determine whether V and Ware \nisomorphic. If th ey are, give an explicit isomorphism \nT: V\n--\n-+ \nW. \n21. V = D\n3 \n(diagonal 3 X 3 matrices), W = IR\n3 \n22. V = S\n3 \n(symmetric 3 X 3 matrices), W = U\n3 \n(upper \ntriangular 3 X 3 matrices) \n23. V = S\n3 \n(symmetric 3 x 3 matrices), W = S� (skew­\nsymmetric 3 X 3 matrices) \n24. V = <.5P\n2\n, W = {p\n(\nx\n) \nin <.5P\n3\n: p\n(\nO\n) \n= O} \nF+\"S7 \n25. V \n= \nC, \nW \n= \nIR\n2 \n26. V = {A in M\n22\n: tr\n(\nA\n) \n= O}, W = IR\n2 \n� \n27. Show that T: <.5P \nn\n--\n-+ \n<.5P \nn \ndefined by T(p(x) ) = p(x) + \np'(x) is   an isomorphism. \n28. Show that T: <.5P \nn\n--\n-+ \n<.5P \nn \ndefined by T(p(x) ) = p(x -  2)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":79726,"to":79887}}}}],[1344,{"pageContent":"26. V = {A in M\n22\n: tr\n(\nA\n) \n= O}, W = IR\n2 \n� \n27. Show that T: <.5P \nn\n--\n-+ \n<.5P \nn \ndefined by T(p(x) ) = p(x) + \np'(x) is   an isomorphism. \n28. Show that T: <.5P \nn\n--\n-+ \n<.5P \nn \ndefined by T(p(x) ) = p(x -  2) \nis an isomorphism. \n29. �how\n_ \nthat T: <.IP\n_\nn\n--\n-+ \n<.5P \nn \ndefined by T\n(\np\n(\nx\n)) \n= x\nn\np( �) \n1s an 1Somorph1sm. \n30. \n(a) Show that 'ii;  [O, l]   = 'ii; [   2, 3]. [Hint: Define T: \n� \n[O, l]--\n-+ \n� \n[2, 3] by letting T(j) be the function \nwhose value at xis \n(\nT(j\n))(\nx\n) \n= j\n(\nx - 2\n) \nfor x in \n[2, 3].] \n(b) Show that\n� \n[O, l] =<'.{;;[a, a + l] for all a. \n31. Showthat 'ii; [O, l] = \n�\n[0, 2]. \n32. Showthat 'ii; [a, b] =  'ii;   [c, d] for all a < band c < d. \n33. Let S: V---+  Wand T: U--\n-+ V  be linear transformations. \n(a) Prove that if S and Tare both one-to-one, so is \nS 0 T. \n(b) Prove that if Sand Tare both onto, so is S 0 T. \n34. Let S : V---+ W and T: U--\n-+ V be linear \ntransformations. \n(a) Prove that if S 0 Tis one-to-one, so is T. \n(b) Prove that if S 0 Tis onto, so is S.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":79887,"to":79966}}}}],[1345,{"pageContent":"34. Let S : V---+ W and T: U--\n-+ V be linear \ntransformations. \n(a) Prove that if S 0 Tis one-to-one, so is T. \n(b) Prove that if S 0 Tis onto, so is S. \n35. Let T: V---+ W  be a linear transformation between two \nfinite-dimensional vector spaces. \n(a) Prove that if dim V < dim W, then T cannot be \nonto. \n(b) Prove that if dim V > dim W, then T cannot be \none-to-one. \n36. Let a\n0\n, a\n1\n, ••• , a\nn \nbe n +  1   distinct real numbers. \nDefine T: <.5P \nn \n--\n-+ \n!R\nn\n+ I \nby \nProve that Tis an isomorphism. \n37. If Vis a  finite-dimensional vector space and T: V---+  Vis \na linear transformation such that rank(T) = rank(T\n2\n), \nprove that range(T)n ke  r(T) = {O}. [Hint: T\n2 \ndenotes \nT 0 T. Use the Rank Theorem to help show that the \nkernels of T and T\n2 \nare the same.] \n38. Let U and W be subspaces of a finite-dimensional \nvector space V. Define T: U X   W--\n-+ \nVby T(u, w) = \nu-w. \n(a) Prove that Tis a  linear transformation. \n(b) \nShow that range(T) = U +   W. \n(c) Show that ker(T) = Un W. [Hint: See Exercise 50","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":79966,"to":80012}}}}],[1346,{"pageContent":"vector space V. Define T: U X   W--\n-+ \nVby T(u, w) = \nu-w. \n(a) Prove that Tis a  linear transformation. \n(b) \nShow that range(T) = U +   W. \n(c) Show that ker(T) = Un W. [Hint: See Exercise 50 \nin Section 6.1.] \n(d) Prove Grassmann's Identity: \ndim(U +  W) = dimU +  dim W-dim(U n   W) \n[Hint: Apply the Rank Theorem, using results \n(a) and (b) and Exercise 43(b) in Section 6.2.]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":80012,"to":80024}}}}],[1347,{"pageContent":"Section 6.6 \nThe Matrix of a Linear Transformation \n491 \nThe Matrix 01  a  linear Transtormation \nTheorem 6.15 showed that a linear transformation T: V � Wis completely deter­\nmined by its effect on a spanning set for V. In particular, if we know how T acts on a \nbasis for V, then we can compute T(v) for any vector v in V. Example 6.55 illustrated \nthe process. We implicitly used this important property of linear transformations \nin Theorem 3.31 to help us compute the standard matrix of a linear transformation \nT: !Rn �  !Rm. In this section, we will show that every linear transformation between \nfinite-dimensional vector spaces can be represented as a matrix transformation. \nSuppose that Vis an n-dimensional vector space, Wis an m-dimensional vector \nspace, and T : V � Wis a  linear transformation. Let B and C be bases for V and W, \nrespectively. Then the coordinate vector mapping R \n( \nv\n) = \n[ v] 6 defines an isomor­\nphism R : V �  !Rn. At the same time, we have an isomorphism S : W � !Rm given by","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":80026,"to":80044}}}}],[1348,{"pageContent":"respectively. Then the coordinate vector mapping R \n( \nv\n) = \n[ v] 6 defines an isomor­\nphism R : V �  !Rn. At the same time, we have an isomorphism S : W � !Rm given by \nS(w) \n= \n[wJc,  which allows us to   associate the image T(v) with the vector [T(v)Jc in \n!Rm. Figure 6.11 illustrates the relationships. \nR\n-\n1 \nv \nv \nT \n• \nU\nR \n[Rn \n. \n-------� \n[v]s\nS\n0\nT\n0\nR\n-\nl \nfigure 6.11 \nT(v) \nw \n• \nl\ns \n[Rnl \n• \n[T(\nv)\nJ\nc \nSince R is an isomorphism, it is invertible, so we   may form the composite mapping \nSo \nTo \nR-\n1 \n: \n[R\nn\n� \n[R\nm \nwhich maps [v]6 to  [T(v)Jc .  Since this mapping goes from !Rn to !Rm, we know \nfrom Chapter 3 that it  is a  matrix transformation. What, then, is the standard \nmatrix of S 0 T 0 R\n-\n1\n? We would like to find the m X n matrix A  such that A [ v J 6 \n(\nS0T0R\n-\n1\n)(\n[v]6\n)\n.0r, since (S 0T0R\n-\n1\n)([v]6) \n= \n[ T(v) ]\nc\n,we require \nA[v]\ns \n= \n[T\n(\nv\n)\n]\nc \nIt turns out to be surprisingly easy to find. The basic idea is that of Theorem 3.31. The \ncolumns of A are the images of the standard basis vectors for !Rn under S 0 T 0 R\n-\n1\n• \nBut, if B","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":80044,"to":80132}}}}],[1349,{"pageContent":"s \n= \n[T\n(\nv\n)\n]\nc \nIt turns out to be surprisingly easy to find. The basic idea is that of Theorem 3.31. The \ncolumns of A are the images of the standard basis vectors for !Rn under S 0 T 0 R\n-\n1\n• \nBut, if B \n= \n{v\n1\n, ••• , v\nn\n} is a basis for V, then \nR\n(\nv;\n) \n= \n[\nv;\n]\n6 \n0 \n+--ith entry \n0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":80132,"to":80163}}}}],[1350,{"pageContent":"498 \nChapter 6  Vector Spaces \nTheorem 6.26 \nso R\n-\n1\n(e;) = v;. Therefore, the ith column of the matrix A we seek is given by \n(\nS 0 T 0 R\n-\n1\n)(\ne\n;\n) \n= S\n(\nT\n(\nR\n-\n1\n(\ne\n;\n))) \n= S\n(\nT\n(\nv\n;\n)) \n=  [ T\n(\nv;\n) \nle \nwhich is the  coordinate vector of T(v;) with respect to the basis C of W. \nWe summarize this discussion as a theorem. \nLet V and W be two finite-dimensional vector spaces with bases B and C, respec­\ntively, where B =   {v\n1\n, ••• , vJ. If T: V ---+ Wis a  linear transformation, then the \nm X n matrix A  defined by \nA= \n[[\nT(v\n1\n)\nl\nc \n! \n[ T(v\n2\n)\nl\nc \n! \n· · · \n! \n[ T(v\nn\n)\nl\nc\nl \nsatisfies \nA [v]8 =  [ T\n(\nv\n)\nJc \nfor every vector v in V. \nThe matrix A in Theorem 6.26 is called the matrix of  T with  respect to the bases B \nand C. The relationship is illustrated below. (Recall that TA denotes multiplication \nby A.) \nRemarks \nT \nv  -----+   T\n(\nv\n) \nJ, \nJ, \nT\nA \n[v]8 -----+ A [v]8 =  [ T\n(\nv\n)\nJc \n• \nThe matrix of a linear transformation T with respect to bases B and C is some­\ntimes denoted by \n[ T]c<--\nB\n·","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":80165,"to":80262}}}}],[1351,{"pageContent":"by A.) \nRemarks \nT \nv  -----+   T\n(\nv\n) \nJ, \nJ, \nT\nA \n[v]8 -----+ A [v]8 =  [ T\n(\nv\n)\nJc \n• \nThe matrix of a linear transformation T with respect to bases B and C is some­\ntimes denoted by \n[ T]c<--\nB\n· \nNote the direction of the arrow: right-to-left (not left-to­\nright, as for T: V ---+ W). With this notation, the final equation in Theorem 6.26 becomes \n[ T Jc\n<--\nB\n[v]\ns \n=  [T\n(\nv\n)\nJc \nObserve that the Bs in   the subscripts appear side by side and appear to \"cancel\" each \nother. In words, this equation says, \"The matrix for T times the coordinate vector for \nv gives the coordinate vector for T(v):' \nIn the special case where V \n= \nW and B = C, we write [ T] 8 (instead of \n[ T] \nB\n<-\n-B\n). \nTheorem 6.26 then states that \n[TJ\ns\n[vJ\ns \n=  [T\n(\nv\n)\nJ\ns \n• \nThe matrix of a linear transformation with respect to given bases is unique. \nThat is, for every vector v in V, there is only one matrix A with the property specified \nby Theorem 6.26-namely, \nA[v]8 =   [T\n(\nv\n)\nJc \n(You are asked to prove this in Exercise 39.\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":80262,"to":80328}}}}],[1352,{"pageContent":"Example 6.16 \nSection 6.6 \nThe Matrix of a Linear Transformation \n499 \n• \nThe diagram that follows Theorem 6.26 is sometimes called a commutative \ndiagram because we can start in the upper left-hand corner with the vector v and get \nto [T(v)Jc  in the lower right-hand corner in two different, but equivalent, ways. If, as \nbefore, \nwe denote the coordinate mappings that map vto [ v J 8 and wto [w]c by Rand \nS, respectively, then we can summarize this \"commutativity\" by \ns \n0 T \n= \nTA 0 R \nThe reason for the term commutative becomes clearer when V \n= \nWand B \n= \nC, for \nthen R \n= \nS too, and we have \nR 0 T \n= \nTA 0 R \nsuggesting that the  coordinate mapping R commutes with the linear transformation T \n(provided we use the matrix version of T-namely, TA \n= \nTl \nTl 8-where it is required). \n• The matr  ix \n[ T\nJ\nc\n<--\nB \ndepends on the order of the vectors in the bases B and \nC. Rearranging the vectors within either basis will affect the matrix  [ T\nJ\nc\n._\n8. \n[See \nExample 6.77(b).] \nLet T : IR\n3 \n---+ IR\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":80330,"to":80378}}}}],[1353,{"pageContent":"[ T\nJ\nc\n<--\nB \ndepends on the order of the vectors in the bases B and \nC. Rearranging the vectors within either basis will affect the matrix  [ T\nJ\nc\n._\n8. \n[See \nExample 6.77(b).] \nLet T : IR\n3 \n---+ IR\n2 \nbe the linear transformation defined by \nand let B \n= \n{ e\n1\n,  e\n2\n,  eJ and C \n= \n{ e\n2\n,  ei} be bases for IR\n3 \nand !R\n2\n, respectively. Find the \nmote ix of T whh '\"P\"' to B ond C and vecify Themem 6.26 foe v � \n[ \n_ \n� l · \nSolution First, we compute \nNext, we need their coordinate vectors with respect to C. Since \nwe have \nTherefore, the matrix of T with respect to B and C is \n= \n[\n� \n-2 \n-\n�\n]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":80378,"to":80425}}}}],[1354,{"pageContent":"500 \nChapter 6  Vector Spaces \nTo verify Theorem 6.26 for v, we   first compute \nThen \nand \nT\n(\nv\n) \n� \nT\n[ \nJ \n� \n[ \n��\n] \n[v\nis\n� \nu-\n[ T\n(\nv\n)\nle \n= \n[ ��\nL \n= \n[ \n�\n�\n] \n_.,.. (Check these.) \nExample 6.11 \nUsing all of these facts, we confirm that \nA [v]8 � \n[\n: \n-\n� -�1\n[ \nJ \n� \n[ \n�\n�\n] \n�  [ T\n(\nv\n)\nJ\n, \nLet D: l!P\n3 \n� <!P\n2 \nbe the differential operator D(p(x)) = p'\n(\nx\n)\n. Let B = {l, x, x\n2\n, x\n3\n} \nand C = {l, x, x\n2\n} be bases for l!P\n3 \nand l!P\n2\n, respectively. \n(a)  Find the matrix A of D with respect to B and C. \n(b)  Find the matrix A' of D with respect to B' and C, where B' = {x\n3\n, x\n2\n, x, l}. \n(c) Using part (a), compute D(S \n-\nx + 2x\n3\n) \nand D(a + bx+  ex\n2 \n+ dx\n3\n) \nto verify \nTheorem 6.26. \nSolulion FirstnotethatD(a +bx+ cx\n2 \n+ dx\n3\n) \n= b + 2cx + 3dx\n2\n• (SeeExample6.60.) \n(a)  Since the images of the basis B under D are D(l) = 0, D\n(\nx\n) \n= 1, D\n(\nx\n2\n) \n= 2x, and \nD\n(\nx\n3\n) \n= 3x\n2\n, their coordinate vectors with respect to Care \n[D(l)J\n, \n� \n[H \n[D(x)J\n, \n� \n[H \n[D(x\n'\n)J\n, \n� \n[H \n[D(x\n'\n)J\n, \n� m \nConsequently, \nA =  [D\nJ\nc.-\ns \n=  [ [   D(l) \nle \n! \n[D(x) \nle ! \n[D(x\n2\n) \nle ! \n[D(x\n3\n) \nlel \n[O 1 0 OJ","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":80427,"to":80583}}}}],[1355,{"pageContent":"x\n3\n) \n= 3x\n2\n, their coordinate vectors with respect to Care \n[D(l)J\n, \n� \n[H \n[D(x)J\n, \n� \n[H \n[D(x\n'\n)J\n, \n� \n[H \n[D(x\n'\n)J\n, \n� m \nConsequently, \nA =  [D\nJ\nc.-\ns \n=  [ [   D(l) \nle \n! \n[D(x) \nle ! \n[D(x\n2\n) \nle ! \n[D(x\n3\n) \nlel \n[O 1 0 OJ \n=  0  0  2  0 \n0  0  0 3 \n(b) Since the basis B' is just Bin the reverse order, we see that \nA' =  [  Db-\ns\n· =  [[D(x\n3\n)\nle \n! \n[D(x\n2\n)\nle ! \n[D(x)\nle ! \n[D(l)\nlel \n= \n[\n� \n� \n� \n�\ni \n3 0  0  0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":80583,"to":80652}}}}],[1356,{"pageContent":"Example 6.18 \nSection 6.6 \nThe Matrix of a Linear Transformation 501 \n(This sh  ows that the order of the vectors in the bases l3 and C affects the matrix of a \ntransformation with respect to these bases.) \n( c)  First we compute D\n(\nS \n-\nx + 2x\n3\n) \n= -1 + \n6x\n2 \ndirectly, getting the coordinate \nvector \n[\nD\n(\nS \n-\nx  + \n2x'\n)]\n, \n� [\n-\n1 \n+ 6x' ]\n, \n� [\n-\n�\n] \nOn the other hand, \nso \n[\nD(S \n-\nx  + 2x\n3\n) \nl\ne \nwhich agrees with Theorem 6.26. We leave proof of the general case as an exercise . \n.+ \nSince the linear transformation in Example 6. 77 is easy to use directly, there is re­\nally no advantage to using the matrix of this transformation to do calculations. How­\never, in other examples-especially large ones-the matrix approach may be si  mpler, \nas it is very well-suited to computer implementation. Example 6.78 illustrates the \nbasic idea behind this indirect approach. \nLet T : <!J' \n2 \n� <!J' \n2 \nbe the linear transformation defined by \nT\n(\np\n(\nx\n)) \n= p\n(\n2x -   1\n) \n(a)  Find the matrix of Twith respect to £= {l, x, x\n2\n}.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":80654,"to":80723}}}}],[1357,{"pageContent":"basic idea behind this indirect approach. \nLet T : <!J' \n2 \n� <!J' \n2 \nbe the linear transformation defined by \nT\n(\np\n(\nx\n)) \n= p\n(\n2x -   1\n) \n(a)  Find the matrix of Twith respect to £= {l, x, x\n2\n}. \n(b) Compute T(3 + 2x -  x\n2\n) \nindirectly, using part (a). \nSolution (a) We see that \nT(l) = 1, T\n(\nx\n) \n= 2x \n-\n1, \nT\n(\nx\n2\n) \n= \n(\n2x \n-\n1\n)\n2 \n= 1 \n-\n4x \n+ 4x\n2 \nso the coordinate vectors are \nTherefore,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":80723,"to":80772}}}}],[1358,{"pageContent":"502 Chapter 6  Vector Spaces \nExample 6.19 \n(b) We apply Theorem 6.26 as follows: The coordinate vector of   p(x) \n= \n3 + 2x - x\n2 \nwith respect to Eis \n[p(x)], \n� \n[ \nJ \nTherefore, by Theorem 6.26, \n[\nT(3 + 2x - x\n2\n)]£ \n= \n[ T(p(x))]£ \n[ T]£\nl\np(x) ]£ \n[\n� \n-\n� \n-:\nJ\n[\nJ \nU\nl \nIt follows that T(3 + 2x - x\n2\n) \n= \n0 · 1 + 8 · x - 4 · x\n2 \n= \n8x - 4x\n2\n. [Verify this by \ncomputing T(3 + 2x - x\n2\n) \n= \n3 + 2(2x -  1) - (2x - 1)\n2 \ndirectly.] \nThe matrix of a linear transformation can sometimes be used in surprising ways. \nExample 6.79 shows its application to a traditional calculus problem. \nLet <f/J be the vector space of all differentiable functions. Consider the subspace W of \n<f/J given by W \n= \nspan(e\n3\nx,  xe\n3\nX, x\n2\ne\n3\nx). Since the set B \n= \n{e\n3\nX\n, xe\n3\nX\n, x\n2\ne\n3\nx} is linearly \nindependent (why?), it is a basis for W. \n(a)  Show that the differential operator D maps W into itself. \n(b)  Find the matrix of D with respect to B. \n(c) Compute the derivative of 5e\n3\nx \n+ 2xe\n3\nx \n- x\n2\ne\n3\nx \nindirectly, using Theorem 6.26, \nand verify it using part (a).","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":80774,"to":80863}}}}],[1359,{"pageContent":"(b)  Find the matrix of D with respect to B. \n(c) Compute the derivative of 5e\n3\nx \n+ 2xe\n3\nx \n- x\n2\ne\n3\nx \nindirectly, using Theorem 6.26, \nand verify it using part (a). \nSolulion (a)  Applying D to a general element of W, we see that \nD\n(\nae\n3\nx + bxe\n3\nx + cx\n2\ne\n3\nx\n) = (\n3a + b \n)\ne\n3\nx + \n(\n3b + 2c\n)\nxe\n3\nx + 3cx\n2\ne\n3\nx \n� \n(check this), which is again in W. \n(b) Using the formula in part (a), we see that \nso \nD\n(\ne\n3\nx\n) \n= \n3e\n3\nX\n, \nD\n(\nxe\n3\nx\n) \n= \ne\n3\nx \n+ \n3xe\n3\nX\n, \nD\n(\nx\n2\ne\n3\nx\n) \n= \n2xe\n3\nx \n+ \n3x\n2\ne\n3\nx \nIt follows that","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":80863,"to":80953}}}}],[1360,{"pageContent":"Example 6.80 \nSection 6.6 \nThe Matrix of a Linear Transformation \n503 \n(c) Forf(x) = 5e\n3\nx + 2xe\n3\nx - x\n2\ne\n3\nX, we see by inspection that \nV\n(x) ]\na \n� \n[ \nJ \nHence, by Theorem 6.26, we have \n[D\n(j\n(x)) \nls\n=  [D\nls\n[f(x) \nls\n= \n[\n� \n� \n�\ni \n[ \n�\ni \n= \n[ \n1\n:\n] \n0     0 3   -1 -3 \nwhich, in turn, implies thatf'(x) = D(j(x)) =  17e\n3\nx + 4xe\n3\nx -  3x\n2\ne\n3\nx, in  agreement \nwith the formula in part (a). \n4 \nRemark The point of Example 6. 79  is not that this method is easier than direct \ndifferentiation. Indeed, once the formula in part (a) has been established, there is \nlittle to do. What is significant is that matrix methods can be used at all in what \nappears, on the surface, to be a calculus problem. We will explore this idea further in \nExample 6.83. \nLet V be an n-dimensional vector space and let I be the identity transformation on \nV. What is the matrix of I with respect to bases B and C of V if B = C (including the \norder of the basis vectors)? What if B * C? \nSolution Let B =   {v\n1\n, ... , vJ.    Then \nJ\n(v\n1\n) = v\n1\n, ... , \nJ\n(v\nn\n) = v\nn","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":80955,"to":81030}}}}],[1361,{"pageContent":"V. What is the matrix of I with respect to bases B and C of V if B = C (including the \norder of the basis vectors)? What if B * C? \nSolution Let B =   {v\n1\n, ... , vJ.    Then \nJ\n(v\n1\n) = v\n1\n, ... , \nJ\n(v\nn\n) = v\nn\n, so \nand, if B =   C, \n[I\nls \n=  [[\nJ\n(v\n1\n)\nls \n! \n[\nJ\n(v\nz\n)\nls \n! \n· · · \n! \n[\nJ\n(v\nn\n)\nlsl \n=  [ e\n1 ! \ne\nz \n! \n· · · \n! \ne\nn l \n=I\nn \nthe n X n identity matrix. (This is   what you expected, isn't it   ?) \nIn the case B * C, we have \n[\nJ\n(\nv\n1\n)\nle \n=  [v\ni\nle\n, \n·  ·\n., \n[\nJ\n(\nv\nn\n)\nle \n=  [v\nnle \nso \n[IJ\ne\n..-\ns \n= [[v\ni\nl\ne!\n···\n! \n[v\nnlel \n= P\ne\n..-\ns \nthe change-of-basis matrix from B to C. \nMalrices of  Composite and Inverse linear Transformations \nWe  now generalize Theorems 3.32 and 3.33 to get a theorem that will allow us to \neasily find the inverse of a linear transformation between finite-dimensional vector \nspaces (if it  exists).","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":81030,"to":81128}}}}],[1362,{"pageContent":"504 \nChapter 6  Vector Spaces \nTheorem 6.21 \nExample 6.81 \nLet U, V, and W be finite-dimensional vector spaces with bases B, C, and D, \nrespectively. Let T :  U � V and S : V � W be linear transformations. Then \nRemarks \n• \nIn words, this theorem says,   \"The matrix of the composite is the product of \nthe matrices:' \n• \nNotice how the \"inner subscripts\" C must match and appear to cancel each \nother out, leaving the \"outer subscripts\" in the form D +---B. \nProof We will show that corresponding columns of the matrices [ S 0 T] v<----6   and \n[SJ v<--\ncl \nT]\nc\n<----6  are the same. Let V; be  the ith basis vector in B. Then the ith column \nof [ S 0 T]v<----6  is \n[\n(\nS 0 T\n)(\nv\n;\n)\n]v \n= \n[S\n(\nT\n(\nv\n;\n)\nJv \n[ S \nl \nv<--\ncl \nT\n( \nv;\n) \n] c \n= \n[ \ns \nl \nv<--\ncl \nT\nl \nC<--6 [ v;] 6 \nby \ntwo applications of Theorem 6.26. But [ v;] 6 \n= \ne\n; \n(why?), so \nis the ith column of the matrix  [S\nl\nv<--\nc l\nT\nl\nc\n<----6. Therefore, the ith columns of \n[ S 0 T\nl \nv<----6   and [ S \nl \nv<--\nc [  T] C<--6   are the same, as we wished to prove. \nUse matrix methods to compute \n(","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":81130,"to":81206}}}}],[1363,{"pageContent":"l\nv<--\nc l\nT\nl\nc\n<----6. Therefore, the ith columns of \n[ S 0 T\nl \nv<----6   and [ S \nl \nv<--\nc [  T] C<--6   are the same, as we wished to prove. \nUse matrix methods to compute \n(\nS 0 T\n) \n[:] \nfor the linear transformations S and T of \nExample 6.56. \nSolution \nRecall that T: IR\n2 \n� \n<J/'\n1 \nand S: <JP\n,\n� <!P\n2 \nare defined by \nr\n[:] \n= \na \n+ \n(a + b \n)\nx  and \nS\n(\na + bx\n) = \nax \n+ bx\n2 \nChoosing the standard bases £, [', and £\" for IR\n2\n, <JP \n1\n, and <JP \n2\n, respectively, we see that \n� \n(Verify these.) By Theorem 6.27, the matrix of S 0 T with respect to [and £\" is","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":81206,"to":81260}}}}],[1364,{"pageContent":"Theorem 6.28 \nThus, by Theorem 6.26, \nSection 6.6 \nThe Matrix of a Linear Transformation \n505 \nConsequently, \n(\nS 0 T\n) \n[ �] \n= ax + \n(\na  +  b \n)\nx\n2\n, which agrees with  the  solution  to \nExample 6.56. \nIn Theorem 6.24, we proved that a  linear transformation is invertible if and only \nif it is one-to-one and onto (i.e., if it is an isomorphism). When the vector spaces in­\nvolved are finite-dimensional, we can use the matrix methods we have developed to \nfind the inverse of such a linear transformation. \nLet T: V ---+ W be a linear transformation between n-dimensional vector spaces V \nand W and let B and C be bases for V and W, respectively. Then T is invertible if \nand only if the matrix \n[ T] C\n+\n-\nB \nis invertible. In this case, \nProof Observe that the matrices of T and r\n-\n1 \n(if Tis invertible) are n X   n. If Tis \ninvertible, then r\n-\n1 \n0 T \n= \nIv. Applying Theorem 6.27, we have \nI\nn\n=  [I\nv \nls\n=  [\nY-\n1 \n0 T\nJ\ns \n[ \nY\n-\n1 \nJ \nB+\n-\nc\n[ T] C\n+\n-\nB \nThis shows that [ T\n] \nc\n.-\nB \nis invertible and that \n( \n[ T]\nc..-8\n)-\n1 \n= \n[ \nr\n-\n1\n] \nB+-\nC\n·","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":81262,"to":81344}}}}],[1365,{"pageContent":"invertible, then r\n-\n1 \n0 T \n= \nIv. Applying Theorem 6.27, we have \nI\nn\n=  [I\nv \nls\n=  [\nY-\n1 \n0 T\nJ\ns \n[ \nY\n-\n1 \nJ \nB+\n-\nc\n[ T] C\n+\n-\nB \nThis shows that [ T\n] \nc\n.-\nB \nis invertible and that \n( \n[ T]\nc..-8\n)-\n1 \n= \n[ \nr\n-\n1\n] \nB+-\nC\n· \nConversely, assume that A = [ T J \nc\n..-\nB \nis invertible. To show that Tis invertible, it \n� \nis enough to show that ker(T) = {O}. (Why?) To this end, let v be  in the kernel of T. \nExample 6.82 \nThen T(v) = 0, so \nA[v]\na \n= [T\nl\nc..-\na\n[v]\na \n= [T(v) J\nc \n= [O\nle\n= 0 \nwhich means that [ v] 8 is in the null space of the invertible matrix A.  By the Fundamen­\ntal Theorem, this implies that [ v J 8 = 0, which, in turn, implies that v \n= \n0, as required. \nIn Example 6.70, the linear transformation T: IR\n2\n---+ \n!Jl\n1 \ndefined by \nr\n[�] \n=a+ \n(\na+ b)x \nwas shown to be one-to-one and onto and hence invertible. Find r\n-\n1\n.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":81344,"to":81433}}}}],[1366,{"pageContent":"506 \nChapter 6  Vector Spaces \nExample 6.83 \nSolulion In Example 6.81, we found the matrix of T with respect to the standard \nbases £ and £' for IR\n2 \nand <!P\n1\n, respectively, to be \n[T] £'\n+-\n£ = \n[\n� \n�\n] \nBy Theorem 6.28, it follows that the  matrix of r\n-\n1 \nwith respect to £' and [is \n[\nY\n-\n1\n1\n£\n+-\n£'\n=\n(\n[T\n]£'\n+-\n£) -\n1\n=\n[\nl \n0\n]\n-\n1\n=\n[ \n1 \nO\nJ \n1  1 -1  1 \nBy Theorem 6.26, \nThis means that \n[ \n_� \n�\n][�] \n[\nb : a\n] \n(Note that  the choice of the  standard basis makes this last  calculation virtually \nirrelevant.) \n4 \nThe next example, a continuation of Example 6.79, shows that matrices can be \nused in certain integration problems in calculus. The specific integral we consider is \nusually evaluated in a calculus course by means of two applications of integration by \nparts. Contrast this approach with our method. \nShow that the differential operator, restricted to the subspace W  = span(e\n3\nx\n, xe\n3\nx, \nx\n2\ne\n3\nx\n) of'20, is invertible, and use this fact to find the integral \nI x\n2\ne\n3\nx dx","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":81435,"to":81515}}}}],[1367,{"pageContent":"Show that the differential operator, restricted to the subspace W  = span(e\n3\nx\n, xe\n3\nx, \nx\n2\ne\n3\nx\n) of'20, is invertible, and use this fact to find the integral \nI x\n2\ne\n3\nx dx \nSolulion In Example 6.79, we found the matrix of D with respect to  the basis \nB = { e\n3\nX\n, xe\n3\nX\n, x\n2\ne\n3\nx} of W to be \nBy Theorem 6.28, therefore, Dis invertible on W, and the matrix of D\n-\n1 \nis \no\nl\n-\n1 \n[\nt \n2 \n= 0 \n3 \n0 \n�:i","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":81515,"to":81558}}}}],[1368,{"pageContent":"Section 6.6 \nThe Matrix of a Linear Transformation 501 \nSince integration is antidifferentiation, this is the matrix corresponding to integration \non W. We want to integrate the function x\n2\ne\n3\nx whose coordinate vector is \nConsequently, by Theorem 6.26, \n[\n� \nIt follows that \n(To be  fully correct, we need to add a    constant of integration. It does not show up here \nbecause we are working with linear transformations, which must send zero vectors to \nzero vectors, forcing the constant of integration to be zero as well.) \nWarning \nIn general, differentiation is not an invertible transformation. (See \nExercise 22.) What the preceding example shows is that, suitably restricted, it some­\ntimes is. Exercises 27-30 explore this idea further. \nChange of  Basis and Similarilv \nSuppose T : V--+ Vis a  linear transformation and 13 and C are two different bases for \nV. It  is natural to wonder how, if at all, the matrices [T]8 and [T]\nc \nare related. It turns","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":81560,"to":81583}}}}],[1369,{"pageContent":"Suppose T : V--+ Vis a  linear transformation and 13 and C are two different bases for \nV. It  is natural to wonder how, if at all, the matrices [T]8 and [T]\nc \nare related. It turns \nout that the answer to this question is quite satisfying and relates to some questions \nwe first considered in Chapter 4. \nFigure 6.12 suggests one way to address this problem. Chasing the arrows around \nthe diagram from the upper left-hand corner to the lower right-hand corner in two \ndifferent, but equivalent, ways shows that I 0 T \n= \nT 0 I, something we already knew, \nsince both are equal to T. However, if the \"upper\" version of T is with respect to the \nv \nT \nv \n} \nb\nasis C \n-\n•V \n•\nT(\nv\n) \ni \nI \ni1 \nv \nT \nv \n} basis 13 \n-\n•V •\nT(\nv\n) \nFigure 6.12 \nJoT= ToJ","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":81583,"to":81620}}}}],[1370,{"pageContent":"508 \nChapter 6  Vector Spaces \nTheorem 6.29 \nExample 6.84 \nbasis C and the \"lower\" version is with respect to B,   then T \n= \nI 0 T \n= \nT 0 I is with \nrespect to C in its domain and with respect to B in its codomain. Thus, the matrix of \nTin this case is [ T] \nB<-\nC\n· \nBut \n[ T]\na.-- c \n= \n[I 0 T]\na.-- c \n= \n[IJ\na.-- d\nT\nl\nc.-- c \nand \n[T]\na.-c \n= \n[T 0 I]\na\n.- c \n= \n[T]\na.-- a\n[I]\na.-- c \nTherefo\nre, \n[I\nl\na.-- d \nT]\nc.-c \n= \n[ T]\na.-- a \n[I]\na\n.-c· \nFrom Example 6.80, we know that [I]8\n.-c \n= \nP8\n.- c\n, the (invertible) change-of­\nbasis matrix from C to B. If we denote this matr  ix by P,   then we also have \np\n-\n1 \n= (\nP5\n.-- c\n)-\n1 \n= \nP\nc.-- a \nWith this notation, \nso \nThus, the matrices [ T] 8 and [ T] \nc \nare similar, in the terminology of Section 4.4. \nWe summarize the foregoing discussion as a theorem. \nLet V be a   finite-dimensional vector space with bases B and C and let T : V � V \nbe a   linear transformation. Then \nwhere Pis the change-of-basis matrix from C to B. \nRemark As an aid in remembering that P  must be the change-of-basis matrix","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":81622,"to":81700}}}}],[1371,{"pageContent":"be a   linear transformation. Then \nwhere Pis the change-of-basis matrix from C to B. \nRemark As an aid in remembering that P  must be the change-of-basis matrix \nfrom C to B, and not B to C, it is instructive to look at what Theorem 6.29 says when \nwritten in full detail. As shown below, the \"inner subscripts\" must be the same (all Bs) \nand must appear to cancel, leaving the \"outer subscripts;' which are both Cs. \nSame \nTheorem 6.29 is often used when we are trying to find a basis with respect to \nwhich the matrix of a linear transformation is particularly simple. For example, we \ncan \nask whether there is a basis C of V such that the matrix [ T J \nc \nof T : V � V is a \ndiagonal matrix. Example 6.84 illustrates this application. \nLet T: IR:\n2 \n� IR:\n2 \nbe defined by \n[\nx\n]    [ \nx \n+  3y\n] \nT\ny \n= \n2x \n+ 2y \nIf possible, find a basis C for IR:\n2 \nsuch that the matrix of T with respect to C is diagonal.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":81700,"to":81732}}}}],[1372,{"pageContent":"Example 6.85 \nSection 6.6 \nThe Matrix of a Linear Transformation \n509 \nSolution The matrix of T with respect to the standard basis Eis \n[ T]E \n= \n[\n� \n�\n] \nThis matrix is diagonalizable, as we saw in Example 4.24. Indeed, if \np \n= \n[\n� \n-\n�\nJ \nand D = \n[\n4 \n0\n] \n0 -1 \nthen P\n-\n1 \n[ T] Ep \n= \nD. If we let C be the basis of   IR\n2 \nconsisting of the columns of P, then \nPis the change-of-basis matr  ix PE.-\nc \nfrom C to E. By Theorem 6.29, \n[ T]\nc \n= \nP\n-\n1 \n[ T]EP \n= \nD \nso the matr  ix of T with respect to the basis C \n= \n{ [ \n�\n]\n, \n[ \n_ \n�\n] \n} \nis diagonal. \nRemarks \n• \nIt is easy to check that the solution above is correct by computing [ T] \nc \ndirectly. \nWe find that \nr\n[\n�\n]  [\n!\n] \n= \n4\n[\n�\n] \n+ \no\n[ \n_\n�\nJ \nand \nr\n[ \n_\n�\nJ \n[\n-\n�\n] \n= \no\n[\n�\n]  [ \n_\n�\nJ \nThus, the coordinate vectors that form the columns of [T]c are \n[ \nr\n[\n�\nJ\nL \n[\n�\n] \nand \n[ \nr\n[ \n_\n�\nJ\nL \n[ \n_\n�\nJ \nin agreement with our solution above. \n• \nThe general procedure for a problem like Example 6.84 is to take the stan­\ndard matrix [  T] E and determine whether it is diagonalizable by finding bases for its","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":81734,"to":81857}}}}],[1373,{"pageContent":"• \nThe general procedure for a problem like Example 6.84 is to take the stan­\ndard matrix [  T] E and determine whether it is diagonalizable by finding bases for its \neigenspaces, as in Chapter 4. The solution then proceeds exactly as in the preceding \nexample. \nExample 6.84 motivates the following definition. \nDefinition \nLet Vbe a  finite-dimensional vector space and let T: V---+ Vbe a \nlinear transformation. Then T is called diagonalizable if there is a basis C for V \nsuch \nthat the matrix [ T] c is a diagonal matrix. \nIt is not hard to show that if B is any basis for V, then T is diagonalizable if and \nonly if the matrix [  T] 8 is diagonalizable. This is essentially what we did, for a special \ncase, in the last example. You are asked to prove this result in general in   Exercise 42. \nSometimes it is easiest to write down the matrix of a linear transformation with \nrespect to a \"nonstandard\" basis. We can then reverse the process of Example 6.84","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":81857,"to":81872}}}}],[1374,{"pageContent":"Sometimes it is easiest to write down the matrix of a linear transformation with \nrespect to a \"nonstandard\" basis. We can then reverse the process of Example 6.84 \nto find the standard matrix. We illustrate this idea by revisiting Example 3.59. \nLet e be the line through the origin in IR\n2 \nwith direction vector d \n= \n[ \n�\n:\n]\n. Find the \nstandard matrix of the projection onto e.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":81872,"to":81884}}}}],[1375,{"pageContent":"510 \nChapter 6  Vector Spaces \nSolulion Let T denote the projection. There is no harm in assuming that d is \na unit vector (i.e., d\nf \n+ \nd \n� \n= \n1\n)\n, since any nonzero multiple of d can serve as a \ndirection vector for -€. Let d' \n= \n[ \n-\nd\n�\n2\n] \nso  that d and d' are orthogonal. Since \nd' is  also a  unit vector, the  set D \n= \n{d, d'} is  an  orthonormal  basis for  IR\n2\n. \nso \nAs Figure 6.13 shows, \nT(\nd\n) \n= \nd and T\n(\nd'\n) \n= \n0. Therefore, \n[T\n( d\n)\nJ\nv \n= \n[\n�\n] \nand \n[T\n( d'\n)\nJ\nv \n= \n[\n�\n] \ny \nFigure 6.13 \nProjection onto-€ \n[T]\nv \n= \n[\n� \n�\n] \nThe change-of-basis matrix from D to the standard basis [is \nso the change-of-basis matrix from E to Dis \nBy Theorem 6.29, then, the standard matrix of Tis \nwhich agrees with part (b) ofExample 3.59.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":81886,"to":81956}}}}],[1376,{"pageContent":"Example 6.86 \nSection 6.6 \nThe Matrix of a Linear Transformation \n511 \nLet T : <!P \n2 \n---+ <!P \n2 \nbe the linear transformation defined by \nT(p (\nx\n)) = \np\n(\n2x \n-\n1\n) \n(a)  Find the matrix of T with respect to the basis B \n= \n{l + x, 1 -x, x\n2\n} of<!f \n2\n. \n(b) Show that T is diagonalizable and find a basis C for <!P \n2 \nsuch that [ T] c is a diago­\nnal matrix. \nSolution (a)  In Example 6.78, we found that the matr  ix of T with respect to the \nstandard basis £ \n= \n{l, x, x\n2\n} is \n[\nT\n]\n, \n� \n[\n� \n-\n� \n-\n:\nl \nThe change-of-basis matrix from B to £ is \np = PE\n<-B \n= \n[\n0\n1\n1 -� \no\n�\nl \nIt follows that the matrix of T with respect to B is \n[ T]\nB \n= p\n-\n1 \n[ T]EP \nu -! \n�\nm \n[\n-\n� \n� \n-:i \n0 0 \n4 \n-\n1 \n2 \n0 \n-\n:\nJ\n[\ni \n-\ni \n�\n] \n� (Check this.) \n� \n(b) The eigenvalues of [ T]E are 1\n, \n2, and 4 (why?), so we know from Theorem 4.25 \nthat [ T] Eis diagonalizable. Eigenvectors corresponding to these eigenvalues are \n[\n�\nJTJ H\nl \nrespectively. Therefore, setting \nP � \n[ \n� \n-\n� -: \nl \nand D � \n[ \n� \n� � l \nwe have p-\ni \n[ T] \nEp \n= \nD. Furthermore, Pis the  change-of-basis matrix from a basis","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":81958,"to":82072}}}}],[1377,{"pageContent":"[\n�\nJTJ H\nl \nrespectively. Therefore, setting \nP � \n[ \n� \n-\n� -: \nl \nand D � \n[ \n� \n� � l \nwe have p-\ni \n[ T] \nEp \n= \nD. Furthermore, Pis the  change-of-basis matrix from a basis \nC to £, and the columns of P are thus the coordinate vectors of C in terms of £. It \nfollows that \nC \n= \n{l, \n-\n1 + x, 1 -2x +  x\n2\n} \nand [Tl\nc \n= \nD.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":82072,"to":82105}}}}],[1378,{"pageContent":"512 \nChapter 6  Vector Spaces \nTheorem 6.30 \n.. \nI Exercises 6.6 \nThe preceding ideas can be generalized to relate the matrices [ T] C<--\nB \nand \n[ T]\nc\n' <--\nB\n' of a linear transformation T: V ---+ W, where 13 and 13' are bases for Vand C \nand C' are bases for W. (See Exercise 44.) \nWe  conclude this section by revisiting the Fundamental Theorem of Invertible \nMatrices and incorporating some results from this chapter. \nThe Fundamental Theorem of Invertible Matrices: Version 4 \nLet A be an n X n matrix and let T :  V ---+  W be a   linear transformation whose \nmatrix [ T] C<--\nB \nwith respect to bases 13 and C of V and W, respectively, is A. The \nfollowing statements are equivalent: \na.  A is invertible. \nb.  Ax = b has a   unique solution for every b in !R\nn\n. \nc.  Ax = 0 has only the trivial solution. \nd. The reduced row echelon form of A is Iw \ne.  A is a product of elementary matrices. \nf.  rank(A) = n \ng.  nullity(A) = 0 \nh.  The column vectors of A are linearly independent.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":82107,"to":82138}}}}],[1379,{"pageContent":"d. The reduced row echelon form of A is Iw \ne.  A is a product of elementary matrices. \nf.  rank(A) = n \ng.  nullity(A) = 0 \nh.  The column vectors of A are linearly independent. \ni.  The column vectors of A span !R\nn\n. \nj.  The column vectors of A form a basis for !R\nn\n. \nk.  The row vectors of A are linearly independent. \n1. The row vectors of A span !R\nn\n. \nm. The row vectors of A form a basis for !R\nn\n. \nn.  det A *  0 \no. 0 is not an eigenvalue of A. \np.  Tis invertible. \nq.  Tis one-to-one. \nr.  Tis onto. \ns.  ker(T) =  {O} \nt.  range(T) = W \nProof The equivalence (q) -¢:? (s) is Theorem 6.20, and (r) -¢:? (t) is the definition of \nonto. Since A is n X n, we must have dim V = dim W = n. From Theorems 6.21 and \n6.24, we get (p) -¢:? (   q) -¢:? (   r). Finally, we connect the last five statements to the others \nby Theorem 6.28, which implies that (a) -¢:? (p ). \n.. \nIn Exercises 1-12, find th e matrix \n[ \nT\n] C<--\nB \no\nf th\ne \nlin\ne\nar \ntransformation T : V ---+ W with respect to th e bases 13 and C","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":82138,"to":82179}}}}],[1380,{"pageContent":"by Theorem 6.28, which implies that (a) -¢:? (p ). \n.. \nIn Exercises 1-12, find th e matrix \n[ \nT\n] C<--\nB \no\nf th\ne \nlin\ne\nar \ntransformation T : V ---+ W with respect to th e bases 13 and C \nof V and W, respectively. Verify Theorem 6.26 for the vector v \nby computing T(v) directly and using th e theorem. \n2. T: <!P \n1\n---+ <!P \n1 \ndefined by T ( a + bx) = b  -ax, \n13 \n= {l + x, 1  -x}, C = {l, x}, v = p (x) = 4 \n+ 2x \n3. T: <!P\n2\n---+ <!P\n2 \ndefined by T\n(\np\n(\nx\n)) \n= p\n(\nx + 2\n)\n, \n13 = {l, x, x\n2\n}, C = {l, x + 2, (x + 2\n)\n2\n}, \n1. T : <!P \n1\n---+ <!P \n1 \ndefined by T\n(\na + bx\n) \n= b  -ax, \n13= C= {l,x},v = p\n(\nx\n) \n= 4 + 2x \nv = p\n(\nx\n) \n= a + bx + cx\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":82179,"to":82243}}}}],[1381,{"pageContent":"4. T : <J/l\n2\n---+ <J/l\n2 \ndefined by T\n(\np\n(\nx)) \n= p\n(\nx + 2\n)\n, \nB = {l,x + 2, \n(x + 2\n)\n2\n},C = {l,x,x\n2\n}, \nv = p\n(\nx\n) \n= a + bx + cx\n2 \n5. \nT: \n<J/l\n2\n---+ \nIR\n2 \ndefined \nby \nT\n(\np\n(\nx)) \n= \n[\n��\n�\n� \nl \nB \n= \n{l, \nx, \nx\n2\n}, \nC = \n{e\n1\n, e\nz\n}, \nv = p\n(\nx\n) \n= a + bx + cx\n2 \n6. \nT: \n<J/l\n2\n---+ \nIR\n2 \ndefined \nby \nT\n(\np\n(\nx)) \n= \n[\n��\n�\n� \nl \nB={x2,x,\nl\n}\n,C\n= \n{\n[\n�\n]\n,\n[\n�\n]\n}\n, \nv = p\n(\nx\n) \n=a + bx + cx\n2 \n7. T: IR\n2\n---+ \nIR\n3 \ndefined by \nr\n[\n:\nJ \n[\na \n�\na\n2b \nl \nB \n� \nWH\n-\n�\n]\n}\n. \nc\n� \nm\nH\nJ\n[:\nl\nl\n· \nv\n� \nn \n8. Repeat Exercise 7 with v = \n[ �]\n. \n9. T: M\n22\n---+ M\n22 \ndefined by T (A ) = A\nr\n, B = C = \n{E\n11\n, E\n1\n2\n, E\n2\n1\n, E\n22\n}, \nv \n=A \n= \n[\n: �\n] \n10. Repeat Exercise 9 with B = {E\nw \nE\n2\n1\n, E \n1\n2\n> \nE \n11\n} and \nC= \n{E\n1\n2\n,E\n2\n1\n>\nE\nw\nE\n11\n}. \n11. T: M\n22\n---+ M\n22 \ndefined by T\n(\nA\n) \n= AB -BA, where \nB\n= \n[ \nl   -\nl\n]\n,B=C= \n{E\n11\n,E\n1\n2\n>\nE\n2\n1\n,E\n22\n}, \n-\n1 1 \nv =A = \n[\n: \n�\n] \n12. T: M\n22\n---+ M\n22 \ndefined by T (A\n) \n= A  -  A\nr\n, B = \nC = {E\n11\n, E\n1\n2\n> \nE\n2\n1\n, E\n22\n}, v = A = \n[\n: \n�\n] \n� 13. Consider the subspace W of'.2ll, given by \nW = span(sin x, cos x\n)\n. \n(a) Show that the differential operator D maps W into \nitself. \n(b) \nFind the matrix of D with respect to \nB =   {sin x, cos x}.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":82245,"to":82508}}}}],[1382,{"pageContent":"�\n] \n� 13. Consider the subspace W of'.2ll, given by \nW = span(sin x, cos x\n)\n. \n(a) Show that the differential operator D maps W into \nitself. \n(b) \nFind the matrix of D with respect to \nB =   {sin x, cos x}. \n(c) Compute the derivative off\n(\nx\n) \n= 3 sin x - 5 cos x \nindirectly, using Theorem 6.26, and verify that it \nagrees withj'(x) as computed directly. \nSection 6.6 \nThe Matrix of a Linear Transformation \n513 \nilili_ 14. Consider the subspace W of'.2ll, given by \nW = span(e\n2\nx, e-\n2\nx)\n. \n(a) Show that the  differential operator D maps W into \nitself. \n(b) Find the matrix of D with respect to B \n= { e\n2\nX\n, e \n-\nz\nx\n}. \n(c) Compute the derivative of f\n(\nx\n) \n= e\n2\nx -  3e\n-\nz\nx \nindirectly, using Theorem 6.26, and verify that it \nagrees with j'(x) as computed directly. \nilili_ 15. Consider the subspace W of'.2ll, given by W = span(e\n2\nX\n, \ne\n2\nx cos \nx, \ne\n2\nx sin \nx)\n. \n(a) Find the matrix of D with respect to B = {e\n2\nx, \ne\n2\nx cos \nx, \ne\n2\nx sin \nx}\n. \n(b) Computethederivativeoff(x) = 3e\n2\nx - e\n2\nxcosx+ \n2e\n2\nx sin x indirectly, using Theorem 6.26, and","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":82508,"to":82591}}}}],[1383,{"pageContent":"2\nX\n, \ne\n2\nx cos \nx, \ne\n2\nx sin \nx)\n. \n(a) Find the matrix of D with respect to B = {e\n2\nx, \ne\n2\nx cos \nx, \ne\n2\nx sin \nx}\n. \n(b) Computethederivativeoff(x) = 3e\n2\nx - e\n2\nxcosx+ \n2e\n2\nx sin x indirectly, using Theorem 6.26, and \nverify that it agrees with f' \n(\nx\n) \nas computed directly. \nilili_ \n16. Consider the subspace W of'.2ll, given by \nW = span(cos x, sin x, x cos x, x sin x\n)\n. \n(a) Find the matrix of D with respect to B =   {cos x, \nsin x, x cos x, x sin x}. \n(b) Compute the derivative off(x) =  cos x + 2x cos x \nindirectly, using Theorem 6.26, and verify that it \nagrees with f' \n(\nx\n) \nas computed directly. \nIn Exercises 17 and 18, T : U---+ V and S : V---+ W are linear \ntransformations and B, C, and D are bases for U, V, and \nW, respectively. Compute [S 0 \nT\nlv\n+-\nB \nin two ways: (a) by \nfinding S 0 T directly and then computing its matrix and \n(b) by finding th e matrices of S and T separately and using \nTheorem 6.27. \n17. T: <J/l\n1\n---+ IR\n2 \ndefined by T\n(\np\n(\nx\n)) \n= \n[\np\n(\nO\n)\n]\n, S: IR\n2\n---+ IR\n2 \np\n(\nl\n) \ndefined by\ns\n[\n�\n] \n= \n[;\na\n-\n-\n2\n� \nl \nB = {l, x},","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":82591,"to":82691}}}}],[1384,{"pageContent":"(b) by finding th e matrices of S and T separately and using \nTheorem 6.27. \n17. T: <J/l\n1\n---+ IR\n2 \ndefined by T\n(\np\n(\nx\n)) \n= \n[\np\n(\nO\n)\n]\n, S: IR\n2\n---+ IR\n2 \np\n(\nl\n) \ndefined by\ns\n[\n�\n] \n= \n[;\na\n-\n-\n2\n� \nl \nB = {l, x}, \nC = D = {e\n1\n,e\nz\n} \n18. T: <J/l\n1\n---+ \n<J/l\n2 \ndefined by \nT(p(\nx)) \n= p\n(\nx + 1), \nS: <JP\n2\n---+ \n<JP\n2 \ndefined by S\n(p(\nx\n)) \n= p\n(\nx + 1), \nB =   {l,x}, C = D = {l,x,x\n2\n} \nIn Exercises 19-26, determine whether th e linear transfor­\nmation Tis invertible by considering its matrix with respect \nto th e standard bases. If Tis invertible, use Theorem 6.28 \nand th e method of Example 6.82 to find T\n-\n1\n• \n19. Tin Exercise 1 \n20.  T in Exercise 5 \n21. Tin Exercise 3 \n22. T:<Jfl\n2\n-+<Jfl\n2\ndefinedbyT(\np\n(x)) =  p'(x) \n� \n23. \nT: \n<J/l\n2\n---+ \n<JP\n2 \ndefined \nby \nT(\np(\nx\n)) \n= \np\n(\nx\n) \n+ \np' \n(\nx\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":82691,"to":82803}}}}],[1385,{"pageContent":"514 \nChapter 6  Vector Spaces \n24. T: M\n22 \n� M\n22 \ndefined by T\n(\nA\n) \n= AB, where \nB = \n[\n� \n�\n] \n25. Tin Exercise 11 \n26. Tin Exercise 12 \n�In Exercises 27-30  , use the method of Example 6.83 \nto evaluate the given integral. \n27. f (sin x -  3 cos x\n) \ndx. (See Exercise 13.) \n28. f 5e\n-\n2\nx dx. (See Exercise 14.) \n29. f \n(\ne\n2\nx cos x  -2e\n2\nx sin x\n)\ndx. (See Exercise 15.) \n30. f \n(\nx cos x + x sin x\n) \ndx. (See Exercise 16.) \nIn Exercises 31-36, a linear transformation T: V �Vis \ngiven. If possible, find a basis C for V such that the matrix \n[ T] \nc \nof T with respect to C is diagonal. \n31. T: IR\n2 \n� \nIR\n2 \ndefined by r\n[ \n�\n] \n[ \n-4b \n] \na  + Sb \n32. T: IR\n2 \n� \nIR\n2 \ndefined by r\n[\n�\n] \n= \n[\n: : �] \n33. T:<!/'\n1 \n�<!/'\n1 \ndefined by T\n(\na +bx)\n= \n(\n4a + 2b\n)  + \n(\na + 3b\n)\nx \n34. T: <!1'\n2 \n� <!1'\n2 \ndefined by \nT(p(\nx)) \n= p\n(\nx + \n1) \n� \n35. T: <!/'\n1 \n� <!/'\n1 \ndefined by \nT(p(\nx\n)) \n= p\n(\nx\n)  + \nxp'\n(\nx\n) \n36. T: <!1'\n2 \n� <!1'\n2 \ndefined by \nT(p(\nx\n)) \n= p\n(\n3x + \n2\n) \n37. Let € be the line through the origin in IR\n2 \nwith direction \nvector d = \n[\n�:\n]\n. Use the method of Example 6.85 to","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":82805,"to":82939}}}}],[1386,{"pageContent":"T(p(\nx\n)) \n= p\n(\nx\n)  + \nxp'\n(\nx\n) \n36. T: <!1'\n2 \n� <!1'\n2 \ndefined by \nT(p(\nx\n)) \n= p\n(\n3x + \n2\n) \n37. Let € be the line through the origin in IR\n2 \nwith direction \nvector d = \n[\n�:\n]\n. Use the method of Example 6.85 to \nfind the standard matrix of a reflection in e. \n38. Let W be the plane in IR\n3 \nwith equation x - y + \n2z = 0. Use the method of Example 6.85 to find \nthe standard matr  ix of an orthogonal projection \nonto W. Verify that your answer is correct by using \nit to compute the orthogonal projection of v onto W, \nwhere \nCompare your answer with Example 5.11. \n[Hint: Find an orthogonal decomposition of IR\n3 \nas \nIR\n3 \n= W + WJ_ using an orthogonal basis for W. See \nExample 5.3.] \n39. Let T: V � W be a linear transformation between \nfinite-dimensional vector spaces and let l3 and C be \nbases for V and W, respectively. Show that the matr  ix \nof T with respect to l3 and C is unique. That is, if A is a \nmatr  ix such that A [ v    ]8 =  [ T\n(\nv\n) \nle \nfor all v in V, then \nA \n=  [ T\nl\nc\n+-\nB\n· \n[Hint: Find values ofv that will show","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":82939,"to":83005}}}}],[1387,{"pageContent":"of T with respect to l3 and C is unique. That is, if A is a \nmatr  ix such that A [ v    ]8 =  [ T\n(\nv\n) \nle \nfor all v in V, then \nA \n=  [ T\nl\nc\n+-\nB\n· \n[Hint: Find values ofv that will show \nthis, one column at a   time.] \nIn Exercises 40-45, let T: V � W be a linear transforma­\ntion between finite-dimensional vector spaces V and W \nLet l3 and C be bases for V and W, respectively, and let \nA= [ T]\nc\n+-\nB\n· \n40. Show that nullity(T) = nullity(A). \n41. Show that rank(T) = rank(A). \n42. If V =   Wand l3 = C, show that Tis diagonalizable if \nand only if A is diagonalizable. \n43. Use the results of this section to give a matrix­\nbased proof of the Rank Theorem (Theorem 6.19). \n44. If !3' and C' are also bases for V and W, respectively, \nwhat is the relationship between [ T] \nc\n+-\nB \nand  [ T] \nc\n· +-\nB\n'? \nProve your assertion. \n45. If dim V = n and dim W = m, prove that �(V, W) = \nMmn-(See the exercises for Section 6.4.\n) \n[Hint: Let l3 \nand C be  bases for V and W, respectively. Show that the \nmapping cp\n(\nT\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":83005,"to":83054}}}}],[1388,{"pageContent":"45. If dim V = n and dim W = m, prove that �(V, W) = \nMmn-(See the exercises for Section 6.4.\n) \n[Hint: Let l3 \nand C be  bases for V and W, respectively. Show that the \nmapping cp\n(\nT\n) \n=  [   T]c._8,  for Tin �(V, W), defines \na linear transformation <p: �(V, W) � Mmn that is an \nisomorphism.] \n46. If Vis a  vector space, then the dual space of Vis \nthe vector space V* = �(V, IR\n)\n. Prove that if Vis \nfinite-dimensional, then V* = V.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":83054,"to":83070}}}}],[1389,{"pageContent":"Exploration \nTilings, Lattices, and the \nCrystallographic Restriction \nRepeating patterns are frequently found in nature and in art. The molecular struc­\nture of crystals often exhibits repetition, as do the tilings and mosaics found in the \nartwork of many cultures. Tiling (or tessellation\n) \nis covering of a plane by shapes that \ndo not overlap and leave no gaps. The Dutch artist M. C. Escher (1898-1972) pro­\nduced many works in which he explored the possibility of tiling a plane using fanciful \nshapes (Figure 6.14). \nFigure 6.14 \nM. C. Escher's \"Symmetry Drawing El03\" \n515","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":83072,"to":83085}}}}],[1390,{"pageContent":"516 \nfigure 6.15 \nInvariance under translation \nfigure 6.11 \nRotational symmetry \nM. C. Escher's \"Symmetry Drawing El03\" \nM. C. Escher's \"Symmetry Drawing El03\" \n• • • • \n• • \n\"\nl\" \n• • \nu \n• • • • • \nfigure 6.16 \nA lattice \n• • \n• • \n• • \n• • \nIn this exploration, we will be interested in patterns such as those in Figure 6.14, \nwhich we assume to be infinite and repeating in all directions of the plane. Such a \npattern has the property that it can be shifted (or translated\n) \nin at least two directions \n(corresponding to two linearly independent vectors) so that it appears not to have \nbeen moved at all. We say that the pattern is invariant under translations and has \ntranslational symmetry in these directions. For example, the pattern in Figure 6.14 \nhas translational symmetry in the directions shown in Figure 6.15 . \nIf a pattern has translational symmetry in two directions, it has  translational sym­\nmetry in infinitely many directions.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":83087,"to":83117}}}}],[1391,{"pageContent":"has translational symmetry in the directions shown in Figure 6.15 . \nIf a pattern has translational symmetry in two directions, it has  translational sym­\nmetry in infinitely many directions. \n1.  Let the two vectors shown in Figure 6.15 be denoted by u and v. Show that the \npattern in Figure 6.14 is invariant under translation by any integer linear combination \nof u and v-that is, by any vector of the form au + bv, where a and b are integers. \nFor any two linearly independent vectors u and v in IR\n2\n, the set of points deter­\nmined by all integer linear combinations of u and vis called a lattice. Figure 6.16 \nshows an example of a lattice. \n2.  Draw the lattice corresponding to the vectors u and v of Figure 6.15. \nFigure 6.14 also exhibits rotational symmetry. That is, it is possible to rotate \nthe entire pattern about some point and have it appear unchanged. We say that it  is \ninvariant under such a rotation. For example, the pattern of Figure 6.14 is invariant","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":83117,"to":83131}}}}],[1392,{"pageContent":"the entire pattern about some point and have it appear unchanged. We say that it  is \ninvariant under such a rotation. For example, the pattern of Figure 6.14 is invariant \nunder a rotation of 120° about the point 0, as shown in Figure 6.17. We call 0 a center \nof rotational symmetry (or a rotation center). \nNote that if a pattern is based on an underlying lattice, then any symmetries of the \npattern must also be possessed by the lattice.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":83131,"to":83136}}}}],[1393,{"pageContent":"3.  Explain why, if a point 0 is a rotation center through an angle e, then it is \na rotation center through every integer multiple of e. Deduce that if 0 < e :s 360°, \nthen 360 \n/ \ne must be an integer. (If 360 \n/ \ne \n= \nn, we say the pattern or lattice has n-fold \nrotati   onal symmetry.) \n4. What is the smallest positive angle of rotational symmetry for the lattice in \nProblem 2? Does the pattern in Figure 6.14 also have rotational symmetry through \nthis angle? \n5.  Take various values of e such that 0 < e :s 360° and 360\n/\ne is an integer. Try \nto draw a lattice that has  rotational symmetry through the angle e. In particular, can \nyou draw a lattice with eight-fold rotational symmetry? \nWe will show that values of e that are possible angles of rotational symmetry for a \nlattice are severely restricted. The technique we will use is to  consider rotation trans­\nformations in terms of different bases. Accordingly, let Re denote a rotation about the","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":83138,"to":83158}}}}],[1394,{"pageContent":"lattice are severely restricted. The technique we will use is to  consider rotation trans­\nformations in terms of different bases. Accordingly, let Re denote a rotation about the \norigin through an angle e and let £ be the standard basis for IR\n2\n. Then the standard \nmatrix of Re is \n[ \nl \n-\n[\ncos e \nR\ne E   -. \nsm e \n-sin e] \ncos e \n6. \nReferring to Problems 2 and 4, take the origin to be at the tails of u and v. \n(a)  What is the actual (i.e., numerical) value of [R\ne\n]E in this case? \n(b) Let B be the basis { u, v}. Compute the matrix [ R \n0\n] \na-\n7. \nIn general, let u and v be any two linearly independent vectors in IR\n2 \nand \nsuppose that the lattice determined by u and vis invariant under a rotation through \nan \nangle e. If B = \n{u, v}, show that the matrix of R11 with respect to B must have the \nform \nwhere a, b, c, and dare integers. \n8.  In the terminology and notation of Problem 7, show that 2  cos e must be an \ninteger. [Hint: Use Exercise 35 in Section 4.4 and Theorem 6.29.]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":83158,"to":83194}}}}],[1395,{"pageContent":"form \nwhere a, b, c, and dare integers. \n8.  In the terminology and notation of Problem 7, show that 2  cos e must be an \ninteger. [Hint: Use Exercise 35 in Section 4.4 and Theorem 6.29.] \n9.  Using Problem 8, make a   list of all possible values of e, with 0 < e :s 360°, \nthat can be angles of rotational symmetry of a lattice. Record the corresponding val­\nues of n, where n \n= \n360/e, to show that a lattice can have n-fold rotational symmetry \nif and only if n \n= \n1, 2, 3,  4, or  6. This result, known as the crystallographic restriction, \nwas first proved by W Barlow in 1894. \n10.  In the library or on the Internet, see whether you can find an Escher tiling \nfor each of the five possible types of rotational symmetry-that is, where the smallest \nangle of rotational symmetry of the pattern is one of those specified by the crystal­\nlographic restriction. \n511","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":83194,"to":83211}}}}],[1396,{"pageContent":"518 \nChapter 6  Vector Spaces \nApplications \nHomogeneous Linear Differential Equalions \nJlh \nIn Exercises 69-72 in Section 4.6, we showed that if y =  y(t) is a twice-differentiable \nfunction that satisfies the differential equation \ny\" + ay' +  by = 0 \n(1) \nthen y is of the form \nif ,\\ \n1 \nand ,\\\n2 \nare distinct roots of the associated characteristic equation ,\\ \n2 \n+ a,\\ + \nb =  0. (The case where ,\\\n1 \n= ,\\\n2 \nwas left unresolved.) Example 6.12 and Exercise 20 \nin this section show that the set of solutions to Equation ( 1) forms a subspace of 9F, \nthe vector space of functions. In this section, we pursue these ideas further, paying \nparticular attention to the role played by vector spaces, bases, and dimension. \nTo set the stage, we consider a simpler class of examples. A differential equation \nof the form \ny' + ay = 0 (2) \nis called a first-order,  homogeneous, linear differential equation. (\"First-order\" \nrefers to the fact that the highest derivative that is involved is a first derivative, and","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":83213,"to":83242}}}}],[1397,{"pageContent":"y' + ay = 0 (2) \nis called a first-order,  homogeneous, linear differential equation. (\"First-order\" \nrefers to the fact that the highest derivative that is involved is a first derivative, and \n\"homogeneous\" means that the right-hand side is zero. Do you see why the equa­\ntion is \"linear\"?) A solution to Equation (2) is a differentiable function y =  y(t) that \nsatisfies Equation (2) for all values oft. \n� \nIt is easy to check that one solution to Equation (2) is y = e -at. (Do it.) However, \nwe would like to describe all solutions-and this is where vector spaces come in. We \nhave the following theorem. \nTheorem 6.31 \nThe set S of all solutions toy' + ay = 0   is a subspace of?F. \nProof Since the zero function certainly satisfies Equation (2), S is nonempty. Let x \nand y be two differentiable functions oft that  are in S and  let c be a   scalar. Then \nx' + ax = 0 and  y' + ay = 0 \nso, using rules for differentiation, we have \n(\nx + y)'  + a\n(\nx + y) = x' + y' + ax + ay = \n(\nx' + ax\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":83242,"to":83264}}}}],[1398,{"pageContent":"x' + ax = 0 and  y' + ay = 0 \nso, using rules for differentiation, we have \n(\nx + y)'  + a\n(\nx + y) = x' + y' + ax + ay = \n(\nx' + ax\n) \n+  (y'  + ay) = 0 + 0 = 0 \nand \n(\nc\ny)' + a\n(\ncy\n) = cy'  + c\n( ay) = c\n(y\n' + ay) = c\n·\nO = 0 \nHence, x + y and cy are also in S, so Sis a  subspace of ?F. \nNow we will show that S  is a one-dimensional subspace of 9F and that { e \n-\nat } is a \nbasis. To this end, let x = x\n(\nt\n) \nbe in S. Then, for all t, \nx'\n(\nt\n) \n+ ax\n(\nt\n) \n= 0 \nor x'\n(\nt\n) \n= -ax\n(\nt\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":83264,"to":83311}}}}],[1399,{"pageContent":"Theorem 6.32 \nExample 6.81 \nE. coli is mentioned in Michael \nCrichton's novel The Andromeda \nStrain (New York: Dell, 1969), \nalthough the \"villain\" in that novel \nwas supposedly an alien virus. In \nreal life, E. coli contaminated the \ntown water supply of Walkerton, \nOntario, in 2000, resulting in seven \ndeaths and causing hundreds of \npeople to become seriously ill. \nSection 6.7 Applications \n519 \nDefine a new function z (t) =  x(t) ea\nt\n. Then, by the Chain Rule for differentiation, \nz'\n(\nt\n) \n= x\n(\nt\n)\nae\nat \n+ \nx'(\nt)\ne\nat \n= \na\nx \n( t)\ne\nat \n-\nax \n( t)\ne\nat \n=O \nSince z' is identically zero, z must be a constant function-say, z(t) = k. But this \nmeans that \nx ( t\n)\ne\nat \n= z \n(\nt\n) \n= k \nfor all t \nso x(t)  = ke\n-\na\n1\n• Therefore, all solutions to Equation (2) are scalar multiples of the \nsingle solution y = e \n-\na\nt\n. We have proved the following theorem. \nIf Sis the  solution space of y' + ay = 0, then dim S = 1 and {e\n-\na\n1\n} is a basis for S. \nOne model for population growth assumes that the growth rate of the popula­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":83313,"to":83384}}}}],[1400,{"pageContent":"If Sis the  solution space of y' + ay = 0, then dim S = 1 and {e\n-\na\n1\n} is a basis for S. \nOne model for population growth assumes that the growth rate of the popula­\ntion is proportional to the size of the population. This model works well if there are \nfew restrictions (such as limited space, food, or the like) on growth. If the size of the \npopulation at time tis p(t), then the growth rate, or rate of change of the population, \nis its  derivative p' (t). Our assumption that the growth rate of the population is pro­\nportional to its size can be written as \np'(t)  =  kp (t) \nwhere k is the proportionality constant. Thus, p  satisfies the differential equation \np' - kp  = 0, so, by Theorem 6.32, \np (t)  = ce\nk\nt \nfor some scalar c. The constants c and k are determined using experimental data. \nThe bacterium Escherichia coli (or E. coli, for short) is commonly found in the \nintestines of humans and other mammals. It poses severe health risks if it escapes","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":83384,"to":83403}}}}],[1401,{"pageContent":"The bacterium Escherichia coli (or E. coli, for short) is commonly found in the \nintestines of humans and other mammals. It poses severe health risks if it escapes \ninto the environment. Under laboratory conditions, each cell of the bacterium divides \ninto two every 20 minutes. If we start with a single E. coli cell, how many will there \nbe after 1 day? \nSolution We do not need to use differential equations to solve this problem, but we \nwill, in order to illustrate the basic method. \nTo  determine c and k, we use the data given in the statement of the problem. If \nwe take 1 unit of time to be 20 minutes, then we are given that p(O) = 1   and p(l) = 2. \nTherefore, \nc = c · 1  = ce\nk\n·\no \n= \n1 \nand \n2 = ee\nk\n·\n! \n= \ni \nIt follows that k = ln 2, so \np(t\n)  = \ne\nt\nln\n2 \n= \ne\nln\n2\n' \n= \n2\nt \nAfter 1 day, t =  72, so the number of bacteria cells will be p(72) = 2\n72 \n= \n4.72 X 10\n2\n1 \n(see Figure 6.18).","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":83403,"to":83447}}}}],[1402,{"pageContent":"520 \nChapter 6  Vector Spaces \nExample 6.88 \np\n(t) \n5 \nx \n1\n0\n21 \n4 \nx \n1\n0\n21 \n3 \nx \n1\n0\n21 \n2 \nx \n1\n0\n21 \n1 \nx \n1\n0\n21 \n4.72 \nx \n1\n0\n21 \nI \nI \nI \nI \nI \nI \nI \nI \nI \nI \n0 \n'--\n-+--\n-+--I\n--+--\n.+---+\n�\n-+-'--\nt \n0 \n1\n0 20 30 40 50 60 \n70\n\"'\n72 \nFigure 6.18 \nExponential growth \nRadioactive substances decay by  emitting radiation. If m\n(\nt\n) \ndenotes the mass of \nthe substance at time t, then the rate of decay is m'\n(\nt\n)\n. Physicists have found that the \nrate of decay of a substance is proportional to its mass; that is, \nm'\n(\nt\n) \n= km\n(\nt\n) \nor m' -km= 0 \nwhere k is a negative constant. Applying Theorem 6.32, we have \nm \n(\nt\n) \n= ce\nk\nt \nfor some constant c. The time required for half of a radioactive substance to decay is \ncalled its half-life. \nAfter 5.5 days, a 100 mg sample of radon-222 decayed to 37 mg. \n(a)  Find a formula for m\n(\nt\n)\n, the mass remaining after t days. \n(b)  What is the half-life ofradon-222? \n(c) When will only 10 mg remain? \nSolution \n(a)  From m\n(\nt\n) \n= c/1, we have \n100 =  m (O) = ce\nk\n·\no \n= c • 1 = c \nso m \n(\nt\n) \n= lOOe\nk\nt","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":83449,"to":83566}}}}],[1403,{"pageContent":"t\n)\n, the mass remaining after t days. \n(b)  What is the half-life ofradon-222? \n(c) When will only 10 mg remain? \nSolution \n(a)  From m\n(\nt\n) \n= c/1, we have \n100 =  m (O) = ce\nk\n·\no \n= c • 1 = c \nso m \n(\nt\n) \n= lOOe\nk\nt \nWith time measured in days, we are given that m\n(\n5.5\n) \n= 37. Therefore, \n100e5·5\nk \n= 37 \nso \nSolving for k, we find \nso \nTherefore, m\n(\nt\n) \n= lOOe-\n0\n·\n1\n81. \ne55\nk \n= 0.37 \n5.5k = ln\n(\n0.37\n) \nln\n(\n0.37\n) \nk  = \n= \n-0.18 \n5.5","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":83566,"to":83623}}}}],[1404,{"pageContent":"See Linear Algebra by S. H. \nFriedberg, A. J. Insel, and L. E. \nSpence (Englewood Cliffs, NJ: \nPrentice-Hall, 1979). \nm\n(t) \n100 \n80 \n60 \n\"so \n40 \n20 \nI \nI \nI \nI \nI \nI \nI \nI \n3.85 : \n\"\" \n2   4 \nFigure 6.19 \nRadioactive decay \n6 \nSection 6.7 Applications \n521 \n8 \n10 \n(b) To find the half-life of radon-222, we need the value of t for  which m(t) \n= \n50. \nSolving this equation, we find \nso \nHence, \nand \nl00e\n-\n0\n·\n1\n8\nt = \n50 \ne\n-\no\n.\n1\ns\nt = \n0.50 \n-0.1  8t \n= \nln\n(\nt\n) = \n-ln 2 \nln 2 \nt \n= \n-= 3.85 \n0.18 \nThus, radon-222 has a half-life of approximately 3.85 days. (See Figure 6.19.) \n(c) We need to determine the value oft such that m(t) \n= \n10. That is, we must solve \nthe equation \nIOOe\n-\no\n.\n1\ns\nt \n= 10  or e\n-\no\n.\n1\ns\nt \n= \n0.1 \nTaking the natural logarithm of both sides yields -0.18t = ln 0.1. Thus, \nln 0.1 \nt \n= \n--= 12.79 \n-0.18 \nso 10 mg of the sample will remain after approximately 12.79 days. \nThe solution set S of the second-order differential equation y\n\" \n+ ay' + by \n= \n0 \nis also a  subspace of ?F  (Exercise 20), and it turns out that the  dimension of S is 2.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":83625,"to":83723}}}}],[1405,{"pageContent":"The solution set S of the second-order differential equation y\n\" \n+ ay' + by \n= \n0 \nis also a  subspace of ?F  (Exercise 20), and it turns out that the  dimension of S is 2. \nPart (a) of   Theorem 6.33, which extends Theorem 6.32, is implied by Theorem 4.40. \nOur approach here is to use the power of vector spaces; doing so allows us to   obtain \npart (b) of Theorem 6.33 as well, a result that we  could not obtain with our previous \nmethods.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":83723,"to":83732}}}}],[1406,{"pageContent":"522 \nChapter 6  Vector Spaces \nTheorem 6.33 \ny \nY \n=\ne\nr \ne\nA2  ------------\nFigure 6.20 \nLet S be the solution space of \ny\n\" \n+ ay'  + by= 0 \nand let A \n1 \nand A\n2 \nbe the roots of the characteristic equation A \n2 \n+  a,\\  +  b = 0. \na. If A\n1 \n* A\n2\n, then {e\n;\\\n'\nt\n, e\n;\\\n21\n} is a basis for S. \nb. If A\n1 \n= A\n2\n, then {e\nA\n1\n1\n, \nte\n;\\\n1\n1\n} is a basis for S. \nRemarks \n• \nObserve that what the theorem says,   in other words, is that the solutions of \ny\n\" \n+ ay' + by= 0 are of the form \nin the first case and \nin the second case. \n• Compare Theorem  6.33 with Theorem  4.38. Linear differential equations \nand linear recurrence relations have much in common. Although the former belong to \ncontinuous mathematics and the latter to discrete mathematics, there are many parallels. \nProof (a) We first show that {e\nA1\n1\n, \ne\nA\n21\n} is contained in S. Let A be any root of the \ncharacteristic equation and let f(t) = e\nA\nt\n. Then \nJ'( t)  = Ae\nA\nt \nand J\"\n( t\n) \n=  A\n2\ne\nA\nt \nfrom which it follows that \nf\" \n+ \naf\n' \n+ \nbf\n= \nA\n2\ne\nA\nt \n+ \naAe\nA\nt \n+ \nbe\nA\nt \n= \n(\nA\n2 \n+  a,\\  +  b\n)\ne\nA\nt \n= 0 · e\nA\nt \n= 0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":83734,"to":83849}}}}],[1407,{"pageContent":"characteristic equation and let f(t) = e\nA\nt\n. Then \nJ'( t)  = Ae\nA\nt \nand J\"\n( t\n) \n=  A\n2\ne\nA\nt \nfrom which it follows that \nf\" \n+ \naf\n' \n+ \nbf\n= \nA\n2\ne\nA\nt \n+ \naAe\nA\nt \n+ \nbe\nA\nt \n= \n(\nA\n2 \n+  a,\\  +  b\n)\ne\nA\nt \n= 0 · e\nA\nt \n= 0 \nTherefore,f is in S. But, since A\n1 \nand A\n2 \nare roots of the characteristic equation, this \nmeans that e\nA\n,\nt \nand e\nA\n,\nt \nare in S. \nThe set {e\nA1\n1\n, e\nA\n21\n} is also linearly independent, since if \nthen, setting t = 0, we have \nNext, we set t = 1    to obtain \nc\n1 \n+  c\n2 \n= 0 \nor c\n2 \n=  - c\n1 \nBut e\nA\n, \n-  e\nA\n2 \n* \n0, since e\nA\n, \n-  e\nA\n2  = 0 implies that e\nA\n, \n= e\\ which is clearly im­\npossible if A\n1 \n* \nA\n2\n. (See Figure 6.20.) We deduce that c\n1 \n= 0 and, hence, c\n2 \n= 0, so \n{e\nA1\n1\n, e\nA\n2t\n} is line arly independent. \nSince dim S = 2, \n{e\nA\n,\nt\n, e\nA\n21\n} \nmust be a basis for S. \n(b) You are asked to prove this property in Exercise 21.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":83849,"to":83973}}}}],[1408,{"pageContent":"Example 6.89 \nExample 6.90 \nSection 6.7 Applications \n523 \nFind all solutions of y 11 -Sy' + 6y \n= 0. \nSolution \nThe characteristic equation is A\n2 \n-SA + 6 = \n(\nA - 2)(,\\ - 3) = 0. Thus, \nthe roots are 2 and 3, so {e\n2\n1, e\n3\n1} is a basis for the solution space. It follows that the \nsolutions to the given equation are of the form \nThe constants c\n1 \nand c\n2 \ncan be determined if additional equations, called bound­\nary conditions, are specified. \nFind the solution of y11 + 6y ' + 9y = 0 that satisfies \ny(\nO\n) \n= 1, y '\n(\nO\n) \n= 0. \nSolution The characteristic equation is A\n2 \n+  6A + 9  = \n(\nA +  3)\n2 \n= 0, so -3 is a \nrepeated root. Therefore, {e\n-\n3\n1, te\n-\n3\n1} is a basis for the solution space, and the general \nsolution is of the form \nThe first boundary condition gives \n1  = \ny (\nO\n) \n= c\n1\ne\n-\n3\n·\no \n+ 0 = c\n1 \nsoy = e \n-\n3\nt \n+ c\n2\nte \n-\n3\n1. Differentiating, we have \ny' = -3e\n-\n3\nt \n+ c\n2\n(\n-3te\n-\n3\nt \n+ e\n-\n3\nt\n) \nso the second boundary condition gives \n0 = y'\n(\nO\n) \n= -3e\n_\n3\n.\n0 \n+ c\n2\n(0 + e\n_\n3\n.\n0\n) = -3 + c\n2 \nor \nTherefore, the required solution is \ny \n= e\n-\n3\nt \n+ 3te\n-\n3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":83975,"to":84091}}}}],[1409,{"pageContent":"y' = -3e\n-\n3\nt \n+ c\n2\n(\n-3te\n-\n3\nt \n+ e\n-\n3\nt\n) \nso the second boundary condition gives \n0 = y'\n(\nO\n) \n= -3e\n_\n3\n.\n0 \n+ c\n2\n(0 + e\n_\n3\n.\n0\n) = -3 + c\n2 \nor \nTherefore, the required solution is \ny \n= e\n-\n3\nt \n+ 3te\n-\n3\n1 = (1 +  3t\n)\ne\n-\n3\n1 \nTheorem 6.33 includes the case in which the roots of the characteristic equation \nare complex. If A = p + qi is a complex root of the equation A \n2 \n+ a,\\ + b = 0, then so is \nits conjugate A = p - qi. (See Appendices C and D.) By Theorem 6.33(a), the solution \nspace S of the differential equation y11 + ay' + by= 0 has {e\nA\n1, e\nX\n1} as a basis. Now \nand \nso \ne\nA\nt \n= \ne\n(\np\n+\nq\ni\n)\nt \n= \ne\nP\n1\ne\ni(\nqt\n) \n= \ne\nP\n1(\ncos \nqt + \ni sin \nqt) \ne\nA\nt \n+ e\nA\nt \ne\nA\nt \n- e\nA\nt \ne\nP\n1 cos qt = \nand  e\nP\n1 sin qt = \n---\n2 2i \nIt follows that {eP1 cos qt, \neP1 sin qt} is contained in span(e\nA\n1, e\nx\n1) \n=  S. Since eP1 cos qt \nand eP1 sin qt are linearly independent (see Exercise 22) and dim S  = 2, {e  P1 cos qt, \neP1 sin qt} is also a basis for S. Thus, when its characteristic equation has a    complex","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":84091,"to":84210}}}}],[1410,{"pageContent":"and eP1 sin qt are linearly independent (see Exercise 22) and dim S  = 2, {e  P1 cos qt, \neP1 sin qt} is also a basis for S. Thus, when its characteristic equation has a    complex \nroot p + qi, the differential equation y 11 + ay' + by = 0 has solutions of the form","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":84210,"to":84212}}}}],[1411,{"pageContent":"524 \nChapter 6  Vector Spaces \nExample 6.91 \nExample 6.92 \n-?-\n> \n� \n> \n0 \n-< \n& \nx \nFigure 6.21 \nFind all solutions ofy\" -  2y' \n+ \n4 \n= \n0. \nSolution The characteristic equation is ,\\ \n2 \n- 2,\\ \n+ \n4 \n= \n0 with roots 1 ±  i\n\\13\n. The fore­\ngoing discussion tells us that the general solution to the given differential equation is \ny \n= \nc\n1\ne\n1 \ncos \nV3\nt \n+ \nc\n2\ne\n1 \nsin \nV3\nt \nA mass is attached to the end of a vertical spring (Figure 6.21). If the mass is pulled \ndownward and released, it will oscillate up and down. Two laws of physics govern this \nsituation. The first, Hooke's law, states that if the spring is stretched (or compressed) \nx units, the force F needed to restore it to its original position is proportional to x: \nF \n= \n-kx \nwhere k is a positive constant (called the spring constant). Newton's Second Law of \nMotion states that force equals mass times acceleration. Since x \n= \nx\n(\nt\n) \nrepresents \ndistance, or displacement, of the spring at time t, x' gives its velocity and x\n\" \nits ac­\nceleration. Thus, we have \nmx\n\" \n= \n- kx or x\n\" \n+","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":84214,"to":84283}}}}],[1412,{"pageContent":"= \nx\n(\nt\n) \nrepresents \ndistance, or displacement, of the spring at time t, x' gives its velocity and x\n\" \nits ac­\nceleration. Thus, we have \nmx\n\" \n= \n- kx or x\n\" \n+ \n( \n� \n)x \n= \n0 \nSince both k and m are positive, so is K \n= \nk/m, and our differential equation has the \nform x\n\" \n+ \nKx \n= \n0, where K is positive. \nThe characteristic equation is ,\\\n2 \n+ \nK \n= \n0 with roots ±ivK. Therefore, the gen­\neral solution to the differential equation of the oscillating spring is \nx \n= \nc\n1 \ncos \nvK\nt \n+ \nc\n2 \nsin \nvK\nt \nSuppose the spring is at rest \n(\nx \n= \nO\n) \nat time t \n= \n0 seconds and is stretched as far \nas possible, to a length of 20 cm, before it is released. Then \n0 \n= \nx (O) \n= \nc\n1 \ncos 0 \n+ \nc\n2 \nsin 0 \n= \nc\n1 \nx \nx  = 20 sin ,!Ki \n20 \n10 \n-10 \n-20 \nFigure 6.22","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":84283,"to":84363}}}}],[1413,{"pageContent":"Section 6.7 Applications \n525 \nso x =  c\n2 \nsin VKt. Since the maximum value of the sine function is 1, we must have \nc\n2 \n= 20 (occurring for the first time when t = 'TT \n/\n2 VK), giving us the solution \nx  = 20 sin VKt \n(See Figure 6.22.) \nOf course, this is an  idealized solution, since it neglects any form of resistance \nand predicts that the spring will oscillate forever. It is possible to take damping effects \n(such as friction) into account, but this simple model has served to introduce an im­\nportant application of differential equations and the techniques we have developed. \n.. \n1 \nExercises 6.1 \n� \nHomogeneous Linear Differential Equations \nIn Exercises 1-12, find the solution of th e differential equa­\ntion that satisfies th e given boundary condition(s) . \nI\n.y'  -3y \n= O,y(l) = 2 \n2. x' \n+ \nx  = 0, x(I) \n= 1 \n3. y\n\" \n-7y' \n+ \nI2y \n= O,y(O) \n= y(l) \n= 1 \n4. x\n\" \n+ \nx' - I2x = 0, x(O) \n= 0, x'(O) = 1 \n5. f\n\" \n-f' - f \n= O,f (O) = O,f(l) \n= I \n6. g\n\" \n- 2g = O,g(O) = 1,g(l) = 0 \n7. y\n\" \n- 2y' \n+ \ny \n= 0, y(O) = y(I) = 1 \n8. x\n\"","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":84365,"to":84423}}}}],[1414,{"pageContent":"= 1 \n3. y\n\" \n-7y' \n+ \nI2y \n= O,y(O) \n= y(l) \n= 1 \n4. x\n\" \n+ \nx' - I2x = 0, x(O) \n= 0, x'(O) = 1 \n5. f\n\" \n-f' - f \n= O,f (O) = O,f(l) \n= I \n6. g\n\" \n- 2g = O,g(O) = 1,g(l) = 0 \n7. y\n\" \n- 2y' \n+ \ny \n= 0, y(O) = y(I) = 1 \n8. x\n\" \n+ 4x' \n+ \n4x = 0, x(O) \n= 1, x'(O) = 1 \n9. y\n\" \n-  k\n2\ny \n= 0, k  -=F O,y(O) \n= y'(O) = 1 \n10. y\n\" \n- 2ky' \n+ \nk\n2\ny \n= 0, k  -=F O,y(O) \n= \n1,y(l) \n= \n0 \n11.f\n\" \n- 2f ' \n+\nSf\n= \nO,f(O) \n= \n1,f(7r/\n4) \n= \n0 \n12.h\n\" \n- 4h'  +Sh= O, h(O) \n= O, h'(O) \n= \n-1 \n13. A strain of bacteria has a growth rate that is propor­\ntional to the size of the population. Initially, there are \n100 bacteria; after 3 hours, there are 1600. \n(a) If p\n(\nt\n) \ndenotes the number of bacteria after \nt hours, find a formula for p\n(\nt\n)\n. \n(b) How long does it take for the population to double? \n(c) When will the population reach one million? \ncAs \n14. Table 6.2 gives the population of the United States at \nIO-year intervals for the years 1900-2000. \n(a) Assuming an exponential growth model, use the \ndata for 1900 and 1910 to find a formula for p\n(\nt\n)\n,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":84423,"to":84517}}}}],[1415,{"pageContent":"IO-year intervals for the years 1900-2000. \n(a) Assuming an exponential growth model, use the \ndata for 1900 and 1910 to find a formula for p\n(\nt\n)\n, \nthe population in year t. [Hint: Lett= 0 be  1900 \nand let t = 1 be 1910.] How accurately does your \nformula calculate the U.S. population in 2000? \n4 \n(b) Repeat part (a), but use the data for the years 1970 \nand 1980 to solve for p\n(\nt\n)\n. Does this approach give \na better approximation for the year 2000? \n(c) What can you conclude about U.S. population \ngrowth? \nTable 6.2 \nPopulation \nYear \n(in millions) \n1900 \n76 \n1910 \n92 \n1920 \n106 \n1930 123 \n1940 \n131 \n19SO \nISO \n1960 179 \n1970 \n203 \n1980 \n227 \n1990 \n2SO \n2000 \n281 \nSource: U.S. Bureau of the Census \n15. The half-life of radium-226 is 1S90 years. Suppose we \nstart with a sample of radium-226 whose mass is SO mg. \n(a) Find a formula for the mass m\n(\nt\n) \nremaining after \nt years and use this formula to predict the mass \nremaining after 1000 years. \n(b) When will only 10 mg remain?","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":84517,"to":84571}}}}],[1416,{"pageContent":"(a) Find a formula for the mass m\n(\nt\n) \nremaining after \nt years and use this formula to predict the mass \nremaining after 1000 years. \n(b) When will only 10 mg remain? \n16. Radiocarbon dating is a method used by scientists \nto estimate the age of ancient objects that were once \nliving matter, such as bone, leather, wo od, or paper.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":84571,"to":84581}}}}],[1417,{"pageContent":"526 \nChapter 6  Vector Spaces \nAll of these contain carbon, a proportion of which is \ncarbon-14, a  radioactive isotope that is continuously \nbeing formed in the upper atmosphere. Since liv­\ning organisms take up radioactive carbon along with \nother carbon atoms, the ratio between the two forms \nremains constant. However, when an organism dies, \nthe carbon -14 in its cells decays and is not replaced. \nCarbon-14 has a known half-life of 5730 years, so \nby measuring the concentration of carbon-14 in an \nobject, scientists can determine its approximate age. \nOne of the most successful applications of radio­\ncarbon dating has been to determine the age of the \nStonehenge monument in England (Figure 6.23). \nSamples taken from the remains of wooden posts \nwere found to have a concentration of carbon-14 that \nwas 45% of that found in living material. What is the \nestimated age of these posts? \nFigure 6.23 \nStonehenge \n17. A mass is attached to a spring, as in   Example 6.92. At","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":84583,"to":84604}}}}],[1418,{"pageContent":"was 45% of that found in living material. What is the \nestimated age of these posts? \nFigure 6.23 \nStonehenge \n17. A mass is attached to a spring, as in   Example 6.92. At \ntime t = 0 second, the spring is stretched to a length of \n10 cm below its position at rest. The spring is released, \nand its length 10 seconds later is observed to be 5 cm. \nFind a formula for the length of the spring at time \nt seconds. \n18. A 50 g mass is attached to a spring, as in Exam­\nple 6.92. If the period of oscillation is 10 seconds, \nfind the spring constant. \n19. A pendulum consists of a mass, called a bob, that \nis affixed to the end of a   string oflength L (see \nFigure 6.24). When the bob is moved from its rest \nposition and released, it swings back and forth. The \ntime it takes the pendulum to swing from its farthest \nright position to its farthest left position and back to \nits next farthest right position is called the period of \nthe pendulum. \n{) \nL \nFigure 6.24 \n/ \nI \nI \nI \nI \nLet e = 8\n(\nt\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":84604,"to":84636}}}}],[1419,{"pageContent":"right position to its farthest left position and back to \nits next farthest right position is called the period of \nthe pendulum. \n{) \nL \nFigure 6.24 \n/ \nI \nI \nI \nI \nLet e = 8\n(\nt\n) \nbe the angle of the pendulum from \nthe vertical. It can be shown that if there is no resis­\ntance, then when e is small it  satisfies the differential \nequation \ne\" + \nf\ne = \no \nL \nwhere g is the constant of acceleration due to gravity, \napproximately 9.7 m/s\n2\n. Suppose that L = 1 m  and \nthat the pendulum is at rest (i.e., e = O\n) \nat time \nt = 0 second. The bob is then drawn to the right at \nan angle ofe\n1 \nradians and released. \n(a) Find the period of the pendulum. \n(b) Does the period depend on the angle 8\n1 \nat which \nthe pendulum is released? This question was \nposed and answered by Galileo in 1638. [Galileo \nGalilei (1564-1642) studied medicine as a student \nat the University of Pisa, but his real interest was \nalways mathematics. In 1592, Galileo was ap­\npointed professor of mathematics at the U   niver­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":84636,"to":84680}}}}],[1420,{"pageContent":"Galilei (1564-1642) studied medicine as a student \nat the University of Pisa, but his real interest was \nalways mathematics. In 1592, Galileo was ap­\npointed professor of mathematics at the U   niver­\nsity of Padua in Venice, where he taught primarily \ngeometry and astronomy. He was the first to use \na telescope to look at the stars and planets, and in \nso doing, he produced experimental data in sup­\nport of the Copernican view that the planets re­\nvolve around the sun and not the earth. For this, \nGalileo was summoned before the Inquisition, \nplaced under house arrest, and forbidden to pub­\nlish his results. While under house arrest, he was \nable to write up his research on falling objects and \npendulums. His notes were smuggled out of Italy \nand published as Discourses on Two New Sciences \nin 1638.] \n20. Show that the solution set S of the second-order \ndifferential equation y\n\" \n+ ay' + by  = 0 is a subspace \nof2F. \n21. Prove Theorem 6.33(b). \n22. Show that e\nP\n1 \ncos qt and e\nP\n1 \nsin qt are linearly","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":84680,"to":84709}}}}],[1421,{"pageContent":"20. Show that the solution set S of the second-order \ndifferential equation y\n\" \n+ ay' + by  = 0 is a subspace \nof2F. \n21. Prove Theorem 6.33(b). \n22. Show that e\nP\n1 \ncos qt and e\nP\n1 \nsin qt are linearly \nindependent.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":84709,"to":84722}}}}],[1422,{"pageContent":"Chapter Review \nKev Definitions and concepts \nbasis, 446 \nBasis Theorem, 453 \nchange-of-basis matrix, 465 \ncomposition oflinear \ntransformations, 477 \ncoordinate vector,  449 \ndiagonalizable linear \ntransformation, 509 \ndimension, 453 \nFundamental Theorem of Invertible \nMatrices, 512 \nidentity transformation, 474 \ninvertible linear \ntransformation, 478 \nReview Questions \nisomorphism, 493 \nkernel of a linear \ntransformation, 482 \nlinear combination of vectors, \n433 \nlinear transformation, 472 \nlinearly dependent vectors, \n443, 446 \nlinearly independent vectors, \n443, 446 \nmatrix of a linear \ntransformation, 498 \nnullity of a linear \ntransformation, 484 \none-to-one, 488 \nonto, 488 \nrange of a linear \ntransformation, 482 \nrank of a linear \ntransformation, 484 \nRank Theorem, 486 \nspan of a set of   vectors, \n438 \nstandard basis, 447 \nsubspace, 434 \ntrivial subspace, 43 7 \nvector,  429 \nvector space,  429 \nzero subspace, 43 7 \nzero transformation, \n4 7 4 \n1. Mark each of the following statements true or false: \n(a) If V = span(v\n1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":84724,"to":84774}}}}],[1423,{"pageContent":"subspace, 434 \ntrivial subspace, 43 7 \nvector,  429 \nvector space,  429 \nzero subspace, 43 7 \nzero transformation, \n4 7 4 \n1. Mark each of the following statements true or false: \n(a) If V = span(v\n1\n, .•. , v\nn\n), then every spanning set \nfor V contains at least n vectors. \n3. V = M\n22\n, W = \n{ \n[\n: \n�]:a +  b  =  c  +  d \n(b) If {u, v,   w} is a linearly independent set of vectors, \nthen so is {u + v, v + w, u + w}. \n(c) M\n22 \nhas a   basis consisting of invertible matrices. \n(d) M\n22 \nhas a basis consisting of matrices whose trace \nis zero. \n=a+c=b+d\n} \n4. V =  IJ> \n3\n, W = {p\n(x) in IJ> \n3 \n: x\n3\np\n( \n1 / \nx\n) \n= p (x\n)\n} \n5. V = '!Jf, W = {fin '!Jf :f\n(\nx + n) = f\n(\nx\n) \nfor all x} \n6. Determine whether {1, cos 2x, 3 sin\n2\nx} is linearly \ndependent or independent. \n(e) The transformation T: !R\nn \n--\n-+ \nIR defined by \nT\n(\nx\n) \n= \nII \nx \nII is a linear transformation. \n(f) If T: V\n--\n-+ \nW is a linear transformation and dim \nV * dim W, then T cannot be both one-to-one \nand onto. \n7. Let A and B be nonzero n X n matrices such that A is","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":84774,"to":84851}}}}],[1424,{"pageContent":"II \nx \nII is a linear transformation. \n(f) If T: V\n--\n-+ \nW is a linear transformation and dim \nV * dim W, then T cannot be both one-to-one \nand onto. \n7. Let A and B be nonzero n X n matrices such that A is \nsymmetric and Bis skew-symmetric. Prove that {A, B} \nis linearly independent. \n(g) If T: V\n--\n-+ \nW is a linear transformation and \nker(T) = V, then W = {O}. \n(h) If T: M\n33 \n--\n-+ \nIJ> \n4 \nis a linear transformation and \nnullity(T) = 4, then Tis onto. \n(i) The vector space V =   {p(x) in IJ>\n4\n: p\n(\nl\n) = O} is \nisomorphic to rzf 3. \n(j)   If I: V\n--\n-+ \nV is the identity transformation, then \nthe matr  ix [ I  Jc+-\nB \nis the identity matrix for any \nbases B and C of V. \nIn Questions 2-5, determine whether Wis a subspace of V \n2. \nv \n= IR\n2\n, w = \n{ \n[;\n] \n: x\n2 \n+ 3y\n2 \n= \no\n} \nIn Questions 8 and 9, fi nd a basis for W and state the \ndimension of W \n8. W = \n{ \n[\n: �\n] \n: a  +  d =  b  + c\n} \n9. W = {p\n(\nx\n) \nin IJ>5 :p\n(\n-x\n) = p\n(\nx\n)\n} \n10. Find the change-of-basis matrices P\nc\n+-\nB \nand Ps+-C with \nrespect to the bases B =   {1, 1 + x, 1 + x + x\n2\n} and","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":84851,"to":84935}}}}],[1425,{"pageContent":"dimension of W \n8. W = \n{ \n[\n: �\n] \n: a  +  d =  b  + c\n} \n9. W = {p\n(\nx\n) \nin IJ>5 :p\n(\n-x\n) = p\n(\nx\n)\n} \n10. Find the change-of-basis matrices P\nc\n+-\nB \nand Ps+-C with \nrespect to the bases B =   {1, 1 + x, 1 + x + x\n2\n} and \nC = {1 + x, x + x\n2\n, 1 + x\n2\n} ofrzf>\n2\n. \nIn Questions 11-13, determine whether Tis a linear \ntransformation. \n11. T: IR\n2\n--\n-+ \nIR\n2 \ndefined by T\n(\nx\n) \n= yx\nT\ny, where y = \n[\n2\n1\n] \n521","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":84935,"to":84989}}}}],[1426,{"pageContent":"528 \nChapter 6  Vector Spaces \n12. T: M\nnn \n---+ \nM\nnn \ndefined by T(A) \n= \nA\nT \nA \n13. T: <JP \nn\n---+ \n<JP \nn \ndefined by T(p(x)) \n= \np(2x \n-\n1) \n14. If T: <ZP\n2\n---+ M\n22 \nis a linear transformation such that \nT\n(\nl\n) = \n[\n� \n�\nl \nT\n(\nl + \nx\n) = \n[\n� \n�\n]\nand \nT\n(\nl + x + x\n2\n) = \n[\n� \n-1\n] \n0 \n, find T(S \n-\n3x + 2x\n2\n). \n15. Find the nullity of the linear transformation \nT: M\nnn\n---+ IR   defined by T(A) \n= \ntr(A). \n16. Let W be the vector space of upper triangular 2 X 2 \nmatrices. \n(a) Find a linear transformation T: M\n22 \n---+ \nM\n22 \nsuch \nthat ker(T) \n= \nW. \n(b) Find a linear transformation T: M\n22\n---+ \nM\n22 \nsuch \nthat range(T) \n= \nW. \n17. Find the matrix [ T] C+--B of the linear transformation \nT in Question 14 with respect to the standard bases \nB \n= \n{l, x, x\n2\n} of<ZP\n2 \nand C \n= \n{E\n11\n, E\n1\n2\n, E\n2\n1\n, Ed of M\n22\n• \n18. Let S \n= \n{v\n1\n, ••• , v\nn\n} be a set of vectors in a vector \nspace V with the property that every vector in V can \nbe written as a linear combination ofv\n1\n, .•. , v\nn \nin ex­\nactly one way. Prove that S  is a basis for V. \n19. If T: U\n---+ \nV and S: V\n---+","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":84991,"to":85115}}}}],[1427,{"pageContent":"space V with the property that every vector in V can \nbe written as a linear combination ofv\n1\n, .•. , v\nn \nin ex­\nactly one way. Prove that S  is a basis for V. \n19. If T: U\n---+ \nV and S: V\n---+ \nWare linear transforma­\ntions such that range(T) � ker(S), what can be \ndeduced about S 0 T? \n20. Let T : V \n---+ \nV be a linear transformation, and let \n{v\n1\n, •.• , v\n\"\n} be a basis for V such that {T\n(\nv\n1\n)\n, •. ., \nT(\nv\n)\n} \nis also a basis for V. Prove that Tis invertible.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":85115,"to":85146}}}}],[1428,{"pageContent":"A straight line may be the shortest \ndistance between two points,  but it \nis by  no means the most interesting. \n-Doctor Who \nIn \"The Time Monster\" \nBy Robert Sloman \nBBC, 1972 \nAlthough this may seem a paradox, \nall exact science is dominated by the \nidea of  approximation. \n-Bertrand Russell \nIn W. H. Auden and \nL. Kronenberger, eds. \nThe Vi king Book of Ap horisms \nViking, 1962, p. 263 \nA \nFigure 1.1 \nTaxicab distance \nB \nDistan\nce \nan\nd \nApproximation \n1.0 Introduction: Taxicab Geometrv \nWe  live in a  three-dimensional Euclidean world, and, therefore, concepts from \nEuclidean geometry govern our way of looking at the world. In particular, imagine \nstopping people on the street and asking them to fill in the blank in the following \nsentence: \"The shortest distance between two points is a ___ :' They will almost \ncertainly respond with \"straight line:' There are, however, other equally sensible and \nintuitive notions of distance. By allowing ourselves to think of \"distance\" in a more","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":85148,"to":85178}}}}],[1429,{"pageContent":"certainly respond with \"straight line:' There are, however, other equally sensible and \nintuitive notions of distance. By allowing ourselves to think of \"distance\" in a more \nflexible way, we will open the door to the possibility of having a \"distance\" between \npolynomials, functions, matrices, and many other objects that arise in linear algebra. \nIn this section, you will discover a type of \"distance\" that is every bit as real as \nthe straight-line distance you are used to from Euclidean geometry (the one that is a \nconsequence of Pythagoras' Theorem). As you'll see, this new type of \"distance\" still \nbehaves in some familiar ways. \nSuppose you are standing at an intersection in a city, trying to get to a restaurant \nat another intersection. If you ask someone how far it is to the restaurant, that person is \nunlikely to measure distance \"as the crow flies\" (i.e., using the Euclidean version of \ndistance). Instead, the response will be something like \"It's five blocks awaY:' Since this","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":85178,"to":85189}}}}],[1430,{"pageContent":"unlikely to measure distance \"as the crow flies\" (i.e., using the Euclidean version of \ndistance). Instead, the response will be something like \"It's five blocks awaY:' Since this \nis the way taxicab drivers measure distance, we will refer to this notion of \"distance\" \nas taxicab distance. \nFigure 7.1 shows an example of taxicab distance. The shortest path from A to B \nrequires traversing the sides of five city blocks. Notice that although there is more \nthan one route from A to B, all shortest routes require three horiz  ontal moves and \ntwo vertical moves, where a \"move\" corresponds to the side of one city block. (How \nmany shortest routes are there from A to B?) Therefore, the taxicab distance from A \nto Bis 5. \nIdealizing this situation, we will assume that all blocks are unit squares, and we \nwill use the notation d\nt\n(A, B) for the taxicab distance from A to B. \nProblem 1 \nFind the taxicab distance between the following pairs of points: \n(\na\n)  (\n1, 2\n) \nand \n(\n5, 5\n) \n(\nc\n) \n(0, 0) and \n( \n-4, -3\n) \n(\ne\n)  (","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":85189,"to":85223}}}}],[1431,{"pageContent":"will use the notation d\nt\n(A, B) for the taxicab distance from A to B. \nProblem 1 \nFind the taxicab distance between the following pairs of points: \n(\na\n)  (\n1, 2\n) \nand \n(\n5, 5\n) \n(\nc\n) \n(0, 0) and \n( \n-4, -3\n) \n(\ne\n)  (\n1, !\n)\nand ( \nL \n�) \n(\nb\n) \n(2, 4) and (3, -2) \n(\nd\n) \n(-2, 3) and \n(\n1, 3\n) \n(f) \n(\n2.5, 4.6\n) \nand \n(\n3.1, 1.5) \n529","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":85223,"to":85270}}}}],[1432,{"pageContent":"530 \nChapter 7 \nDistance and Approximation \nProblem 2 Which of the following is the correct formula for the taxicab distance \nd\nt\n(A, B) between A = \n(\na\n1\n, a\n2\n) \nand B = \n(\nb\n1\n, b\ni\n)\n? \n(\na\n) \nd\nt\n(\nA, B\n) = ( a\n1 \n-  b\n1\n) \n+ \n( a\n2 \n-  b\n2\n) \n(b) d\n1\n(A, B) \n= (\nl\na\n1\nI \n-\nl\nb\n1\nI\n) \n+  (\nl\na\n2\nI \n-\nl\nb\n2\nI\n) \n(c) \nd\n1\n(A, B) \n= \nl\na\n1 \n-  b\n1\nI \n+ \nl\na\n2 \n-  b\nz\nl \nWe can define the taxicab norm of a vector v as \nProblem 3 Find \nllvllr for the following vectors: \n(a) v = \n[ \n_\n�\n] \n(b) v = \n[ \n_\n:\nJ \n(c) v = \n[\n-\n-\n3\n6\n] \n(d) v = \n[\n2\n1\n] \nProblem 4 Show that Theorem 1.3 is true for the taxicab norm. \nProblem 5 Verify the Triangle Inequality (Theorem 1.5), using the taxicab norm \nand the following pairs of vectors: \n(a) u = \n[\n�\n]\n, v = \n[\n�\n] \n(b) u = \n[ \n_\n�\n], v = \n[\n-\n�\n] \nProblem 6 Show that the  Triangle Inequality is true, in general, for the taxicab \nnorm. \nIn Euclidean geometry, we can define a circle of radius r, centered at the origin, as \nthe set of all x such that II x II = r. Analogously, we can define a taxicab circle of radius \nr, centered at the origin, as the set of all x such that \nII \nx","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":85272,"to":85405}}}}],[1433,{"pageContent":"the set of all x such that II x II = r. Analogously, we can define a taxicab circle of radius \nr, centered at the origin, as the set of all x such that \nII \nx \nII \n1 \n= r. \nProblem 1 Draw taxicab circles centered at the origin with the following radii: \n(a) \nr = 3 (b) r = 4 (c) r =  1 \nProblem 8 In Euclidean geometry, the value of n is half the circumference of a \nunit circle (a circle of radius 1). Let's define taxicab pi to be the number 'TT \nt \nthat is half \nthe circumference of a taxicab unit circle. What is the  value of 'TT \nt\n? \nIn Euclidean geometry, the perpendicular bisector of a line segment AB can be \ndefined as the set of all points that are equidistant from A and B. If we use taxicab \ndistance instead of Euclidean distance, it is reasonable to ask what the perpendicular \nbisector of a line segment now looks like. To be precise, the taxicab perpendicular \nbisector of AB is the set of   all points X such that \nd/X, A\n) \n= d\n1\n(\nX, B\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":85405,"to":85433}}}}],[1434,{"pageContent":"bisector of a line segment now looks like. To be precise, the taxicab perpendicular \nbisector of AB is the set of   all points X such that \nd/X, A\n) \n= d\n1\n(\nX, B\n) \nProblem 9 Draw the taxicab perpendicular bisector of AB for the following pairs \nof points: \n(a) A= (2, l\n) ,B = \n(\n4, 1) \n(c) A =   (1, 1), B =  (5, 3) \n(b) A = (-1, 3), B = (-1, \n-\n2) \n(d) A =   (1, 1), B =  (5, 5) \nAs these problems illustrate,  taxicab geometry shares some properties with \nEuclidean geometry, but it also differs in some striking ways. In this chapter, we will","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":85433,"to":85454}}}}],[1435,{"pageContent":"Ill \nExample 1.1 \nExample 1.2 \nSection 7.1 \nInner Product Spaces \n531 \nencounter several other types of distances and norms, each of which is useful in its \nown way. We will try to discover what they have in common and use these common \nproperties to our advantage. We will also explore a variety of approximation problems \nin which the notion of \"distance\" plays an important role. \nInner Product  Spaces \nIn Chapter 1, we defined the dot product u · v of vectors u and v in !R\nn\n, and we have \nmade repeated use of this operation throughout this book. In this section, we will use \nthe properties of the dot product as a means of defining the general notion of an inner \nproduct. In the next section, we will show that inner products can be used to define \nanalogues of \"length\" and \"distance\" in vector spaces other than !R\nn\n. \nThe following definition is our starting point; it is based on the properties of the \ndot product proved in Theorem 1.2. \nDefinition","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":85456,"to":85478}}}}],[1436,{"pageContent":"n\n. \nThe following definition is our starting point; it is based on the properties of the \ndot product proved in Theorem 1.2. \nDefinition \nAn inner product on a vector space Vis an operation that assigns \nto every pair of vectors u and v in Va real number (u, v) such that the following \nproperties hold for all vectors u, v, and w in V and all scalars c: \n1. \n(u, v) \n= \n(v, u) \n2. \n(u, v + w) \n= \n(u, v) + (u, w) \n3. (cu, v) \n= \nc(u, v) \n4. (u, u) 2:: 0 and (u, u) \n= \n0 if and only if u \n= \n0 \nA vector space with an inner prod uct is called an inner product space. \nRemark Technically, this definition defines a real inner product space, since it as­\nsumes that V is a real vector space and since the inner product of two vectors is a real \nnumber. There are complex inner product spaces too, but their definition is somewhat \ndifferent. (See Exploration: Vectors and Matrices with Complex Entries at the end of this \nsection.) \n!R\nn \nis an inner product space with (u, v) \n= \nu · v.  Properties (1) through \n(\n4\n) \nwere","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":85478,"to":85516}}}}],[1437,{"pageContent":"different. (See Exploration: Vectors and Matrices with Complex Entries at the end of this \nsection.) \n!R\nn \nis an inner product space with (u, v) \n= \nu · v.  Properties (1) through \n(\n4\n) \nwere \nverified as Theorem 1.2. \nThe dot product is not the only inner product that can be defined on !R\nn\n. \nLet u \n= \n[ \n::J and v \n= \n[ \n::J be two vectors in IR\n2\n. Show that \n(\nu, \nv) \n= \n2u\n1\nv\n1 \n+ \n3u\n2\nv\n2 \ndefines an inner product.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":85516,"to":85553}}}}],[1438,{"pageContent":"532 \nChapter 7 \nDistance and Approximation \nSolulion \nWe must verify properties (1) through (4). Property (1) holds because \n(\nu, v) \n= \n2\nU\n1\nV\n1 \n+  3\nU\n2\nV\n2 \n= \n2\nV\n1\nU\n1 \n+  3\nV\n2\nU\n2 \n= \n(v, u\n) \nNext, let w \n= \n[ :J\n. \nWe check that \nwhich proves property \n(\n2\n)\n. \nIf c is a scalar, then \nwhich verifies property \n(\n3\n)\n. \nFinally, \n= \n2\nU\n1\nV\n1 \n+  2\nU\n1\nW\n1 \n+ \n3\nU\n2\nV\n2 \n+ \n3\nU\n2\nW\n2 \n= (\n2\nU\n1\nV\n1 \n+  3\nU\nz\nV\nz\n) \n+ \n(\n2\nU\n1\nW\n1 \n+ \n} U\nzWz\n) \n= \n(\nu, v) + \n(\nu, w\n) \n(\ncu, v) \n= \n2\n( cu\n1\n)\nv\n1 \n+  3\n( cu\n2\n)\nv\n2 \n= \nc\n( 2u\n1\nv\n1 \n+ 3u\n2\nv\nz\n) \n= \nc (u,v) \nand it is clear that \n(\nu, u\n) \n= \n2uf + 3u� \n= \n0 if and only if u\n1 \n= \nu\n2 \n= \n0 (that is,  if and \nonly if u \n= \nO). This verifies property (  4), completing the proof that \n(\nu, v), as defined, \nis an inner product. \nExample 7.2 can be generalized to show that if w\n1\n, ••• , w\nn \nare positive scalars and \nare vectors in ll�r, then \n(1) \ndefines an inner prod uct on !R\nn\n, called a weighted dot product. If any of the weights \nW\n; is negative or zero, then Equation (1) does not define an inner product. (See Exer­\ncises 13 and 14.)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":85555,"to":85716}}}}],[1439,{"pageContent":"(1) \ndefines an inner prod uct on !R\nn\n, called a weighted dot product. If any of the weights \nW\n; is negative or zero, then Equation (1) does not define an inner product. (See Exer­\ncises 13 and 14.) \nRecall that the dot prod uct can be expressed as u · v \n= \nu\nT\nv. Observe that we can \nwrite the weighted dot prod uct in Equation ( 1) as","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":85716,"to":85728}}}}],[1440,{"pageContent":"Example 1.3 \nExample 1.4 \nSection 7.1 \nInner Product Spaces \n533 \nwhere Wis the n X n diagonal matrix \nThe next example further generalizes this type of inner product. \nLet A be a symmetric, positive definite n X n matr  ix (see Section 5.5\n) \nand let u and v \nbe vectors in !R\nn\n. Show that \ndefines an inner product. \nSolution We check that \n(u, v) \n=  u\nT\nAv = u·Av = Av·u \n=A\nr\nv· u \n= \n(\nv\nr\nA\n)\nr \n· u \n=  v\nr\nAu \n= \n(v, u) \nAlso, \nand \n( cu, v) \n=  (cuf Av=  c\n(\nu\nr\nAv\n)  = \nc (u, v) \nFinally, since A is positive definite, (u, u) =  u \nr \nAu >  0 for all u \n-=fa \n0, so (u, u) \n= \nu \nr \nAu = 0 if and only if u = 0. This establishes the last property. \nTo illustrate Example 7.3, let A  = \n[ \n4 \n-2 \n-2] \n7 \n. Then \nThe matrix A is positive definite, by Theorem 5.24, since its eigenvalues are 3 and 8. \nHence, \n(u, v) defines an inner product on IR\n2\n. \nWe now define some inner products on vector spaces other than !R\nn\n. \ndefines an inner product on <;5}\n2\n• (For example, if p\n(\nx\n) \n= 1 -5x + 3x\n2 \nand q\n(\nx\n) \n= \n6 + 2x -x\n2\n, then \n(\np\n(\nx\n)\n, q\n(\nx\n)) \n= 1 \n• \n6 \n+  (\n-5\n) • \n2 +","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":85730,"to":85830}}}}],[1441,{"pageContent":"n\n. \ndefines an inner product on <;5}\n2\n• (For example, if p\n(\nx\n) \n= 1 -5x + 3x\n2 \nand q\n(\nx\n) \n= \n6 + 2x -x\n2\n, then \n(\np\n(\nx\n)\n, q\n(\nx\n)) \n= 1 \n• \n6 \n+  (\n-5\n) • \n2 + \n3 \n• \n(-1) = -7.\n) \nSolution Since<;!} \n2 \nis isomorphic to IR\n3\n, we need only show that the  dot product in IR\n3 \nis an inner product, which we have already established.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":85830,"to":85874}}}}],[1442,{"pageContent":"534 \nChapter 7 \nDistance and Approximation \nExample 1.5 \nLet f and g be in C(6 [a, b] , the vector space of all continuous functions on the closed \ninterval [a, b] . Show that \n(f, \ng) = r f \n(\nx\n)g\n(\nx\n) \ndx \na \ndefines an inner product on <-fi: [a, b]. \nSolution We have \n( f,g) \n= r f\n(\nx\n)\ng\n(\nx\n) \ndx = r g\n(\nx\n)f\n(\nx\n) \ndx = \n(g\n,f) \na a \nAlso, if his in <-fi: [a, b], then \n(f,g + h) = rf\n(\nx\n)(\ng\n(\nx\n)  +  h\n(\nx\n)) \ndx \na \nIf c is a scalar, then \n= \nr (j\n(\nx\n)\ng\n(\nx\n)  + j\n(\nx\n)\nh\n(\nx\n)) \ndx \na \n= r f \n(\nx\n)\ng\n(\nx\n) \ndx + r f \n(\nx\n)\nh\n(\nx\n) \ndx \na a \n= \n(f, g) +  ( f, h) \n(\ncf, g) = r cf \n(\nx\n)g\n(\nx\n) \ndx \na \n= \nc r f \n(\nx\n)\ng\n(\nx\n) \ndx \na \n= \nc (f,g) \nFinally, \n( f, f) \n= r \n(\nj\n(\nx\n) \n)\n2 \ndx 2 0, and it follows from a theorem of calculus that, since f \na \nb \nis continuous, \n(\nf, f) \n= \nJ \n(\nj\n(\nx\n) \n)\n2 \ndx = 0 if and only if f is the zero function. Therefore, \na \n(\nf, g) is an inner product on <-fi: [a, b] . \nExample 7.5 also defines an inner product on any subspace of<-fi: [a, b]. For example, \nwe could restrict our attention to polynomials defined on the interval [a, b] . Suppose","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":85876,"to":86020}}}}],[1443,{"pageContent":"Example 7.5 also defines an inner product on any subspace of<-fi: [a, b]. For example, \nwe could restrict our attention to polynomials defined on the interval [a, b] . Suppose \nwe consider CZP [ O , l], the vector space of all po  lynomials on the interval [ O , l]. Then, \nusing the inner product of Example 7.5, we have \n(\nx\n2\n, 1  + x) \n= \nr \nx\n2\n(1 \n+ \nx) dx = \nr \n(x\n2 \n+ \nx\n3\n) dx \n0 0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":86020,"to":86042}}}}],[1444,{"pageContent":"Theorem 1.1 \nExample 1.6 \nSection 7.1 \nInner Product Spaces \n535 \nProperties of Inner Producls \nThe following theorem summarizes some additional properties that follow from the \ndefinition of inner product. \nLet u, v, and w be vectors in an inner product space V and let c be a scalar. \na. (u + v, w) = (u, w) + (v, w) \nb. (u, cv) =  c(u, v) \nc.  (u, 0) = (0, v) = 0 \nProof We  prove property  (a), leaving the  proofs  of properties (b) and  (c)  as \nExercises 23 and 24. Referring to the definition of inner product, we have \n(u + v, w\n) \n= \n(\nw, u + v) \nby (1) \n= \n(\nw, u) + \n(\nw, v) \nby (2) \n= (u, w\n) \n+ (v, w\n) \nby (1) \nLenglh,  Dislance, and Orlhogonalilv \nIn an inner product space, we can define the length of a vector, distance between vec­\ntors, and orthogonal vectors, just as we did in Section 1.2. We simply have to replace \nevery use of the dot product u · v by  the more general inner product (u, v). \nDefinilion \nLet u and v be vectors in an  inner prod uct space V. \n1. The length (or norm) ofv \nis \nll\nv\nll \n= \n\\l\\V,V1.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":86044,"to":86087}}}}],[1445,{"pageContent":"every use of the dot product u · v by  the more general inner product (u, v). \nDefinilion \nLet u and v be vectors in an  inner prod uct space V. \n1. The length (or norm) ofv \nis \nll\nv\nll \n= \n\\l\\V,V1. \n2.  The distancebetweenuandvis d(u, v) =\nll\nu - v\nii\n· \n3.  u and v are orthogonal if (u, v) = 0. \nNote that \nll\nv \nII \nis always defined, since (v, v) 2: 0 by the definition of inner prod uct, so \nwe can take the square root of this nonnegative quantity. As in !R\nn\n, a vector oflength 1 \nis called a unit vector. The  unit sphere in Vis the  set S of all unit vectors in V. \nConsider the inner prod uct on C{i; [ O , l] given in Example 7.5. Iff\n(\nx\n) \n= x and g\n(\nx\n) \n= \n3x - 2,   find \nCa) \nII\n! \n11 \n(b)  d(\nf, g) \n(c) (f, g) \nSolution \n(a) We find that \n(f,\nf) = \nf\n2\n(x) dx = x\n2 \ndx = � \nJ\nI \nJ\nI \n3\n]\n1 \n0 0 \n3 \n0 \nso \n11\n1\n11 \n=  vu:.n \n= \nl/\nv'3\n. \n1 \n3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":86087,"to":86158}}}}],[1446,{"pageContent":"536 \nChapter 7 \nDistance and Approximation \nExample 1.1 \n(b) \nSince d(f, \ng) \n= \nll\nJ -\ngll \n= \nv'\n(\nf-\ng,\nf \n- g) and \nwe have \nf \n(\nx\n)  -  g\n(\nx\n) \n= \nx  -\n(\n3x -  2\n) \n= \n2  -2x \n= \n2(1 -  x\n) \n( f -\ng,\nf -\ng) \n= \nr \n(\nj\n(\nx\n)  -  g\n(\nx\n))\n2 \ndx \n= \nr \n4(1 \n-\n2x \n+ \nx\n2\n) dx \n0 0 \n[ \nX\n3\n] \nI \n= \n4 x -  x\n2 \n+ \n3 o \n4 \n3 \nCombining these facts, we see that d(\nf, g) \n= \n\\/473 \n= \n2/ v'3. \n(c) We compute \n(f,g) \n= \nr\nf\n(\nx\n)g\n(\nx\n) dx \n= \nr \nx\n(\n3x \n-  2\n) dx \n= \nf \n(\n3x\n2 \n-\n2x\n) dx \n= \n[\nx\n3 \n-  x\n2\n]\n6 \n= \n0 \n0 0 0 \nThus, f and g are orthogonal. \nIt is important to remember tha  t the \"distance\" between f and gin Example 7.6 \ndoes not refer to any measurement related to the graphs of these functions. Neither \ndoes the fact that f and g are orthogonal mean that their graphs intersect at right \nangles. We are simply applying the definition of a particular inner product. However, \nin doing so, we should be guided by the corresponding notions in IR\n2 \nand IR\n3\n, where \nthe inner product is the  dot product. The geometry of Euclidean space can still guide","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":86160,"to":86287}}}}],[1447,{"pageContent":"in doing so, we should be guided by the corresponding notions in IR\n2 \nand IR\n3\n, where \nthe inner product is the  dot product. The geometry of Euclidean space can still guide \nus here, even though we cannot visualize things in the same way. \nUsing the inner product on IR\n2 \ndefined in Example 7.2, draw a sketch of the unit \nsphere (circle). \nSolulion Ifx \n= \n[; \nl \nthen (x\n, \nx) \n= \n2x\n2 \n+ \n3y\n2\n• Since the unit sphere (circle) consists \nof all x such that \nII \nx \nII \n= \n1, we have \n1 \n= \nll\nx\nll \n= \nv\\X,X} \n= \nv'\n2x\n2 \n+ \n3\n/ \nor \n2x\n2 \n+ \n3y\n2 \n= \n1 \nThis is the equation of an ellipse, and its graph is shown in Figure 7.2. \ny \n__,--+-+--+--+-1-+-+--+--+-+-+-+--+-+-f--+�•x \nFigure 1.2 \n1 \n\\ \n3 \nA unit circle that is an ellipse \n\\ 2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":86287,"to":86348}}}}],[1448,{"pageContent":"Theorem 1.2 \nExample 1.8 \nSection 7.1 \nInner Product Spaces \n531 \nWe will discuss properties of length, distance, and orthogonality in the next sec­\ntion and in the exercises. One result that we will need in this section is the generalized \nversion of Pythagoras' Theorem, which extends Theorem 1.6. \nPythagoras' Theorem \nLet u and v be vectors in an   inner prod uct space V. Then u and v are orthogonal \nif and only if \nll\nu  +  v\nll\n2 \n= \nll\nu\nll\n2 \n+ \nll\nv\nll\n2 \nProof \nAs you will be asked to prove in Exercise 32, we have \nll\nu  +  v\nll\n2 \n= (u +  v, u  + v) = \nll\nu\nll\n2 \n+  2(u, v) + \nll\nv\nll\n2 \nIt follows immediately that \nII \nu  +  v \n11\n2 \n= \nII \nu \n11\n2 \n+ \nII \nv \n11\n2 \nif and only if (u, v) = 0. \nOrthogonal Projections and the Gram-Schmidt Process \nIn Chapter 5, we discussed orthogonality in ll�r. Most of this material generalizes \nnicely to general inner product spaces. For example, an orthogonal set of vectors \nin an inner product space Vis a  set {v\n1\n, ... , v\nk\n} of vectors from V such that (v;, v\nj\n) = \n0 whenever \nV\n; \n-=fa \nv\nj","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":86350,"to":86422}}}}],[1449,{"pageContent":"nicely to general inner product spaces. For example, an orthogonal set of vectors \nin an inner product space Vis a  set {v\n1\n, ... , v\nk\n} of vectors from V such that (v;, v\nj\n) = \n0 whenever \nV\n; \n-=fa \nv\nj\n. An orthonormal set of vectors is then an orthogonal set  of unit \nvectors. An orthogonal basis for a subspace W of V is just a basis for W that is an \northogonal set; similarly, an orthonormal basis for a subspace W of Vis a  basis for W \nthat is an orthonormal set. \nIn IR:\nn\n, the Gram-Schmidt Process (Theorem 5.15) shows that every subspace has \nan orthogonal basis. We can mimic the construction of the Gram-Schmidt Process \nto show that every finite-dimensional subspace of an inner product space has an or­\nthogonal basis-all we need to do   is replace the dot prod uct by the more general inner \nproduct. We illustrate this approach with an example. (Compare the steps here with \nthose in Example 5.13.) \nConstruct an orthogonal basis for <lP \n2 \nwith respect to the inner product \n(f, g) = \nr \nf\n(\nx\n)\ng\n(\nx\n) \ndx \n-]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":86422,"to":86462}}}}],[1450,{"pageContent":"those in Example 5.13.) \nConstruct an orthogonal basis for <lP \n2 \nwith respect to the inner product \n(f, g) = \nr \nf\n(\nx\n)\ng\n(\nx\n) \ndx \n-] \nby applying the Gram-Schmidt Process to the basis {l, x, x\n2\n}. \nSolution \nLet x\n1 \n=  1, x\n2 \n= x, and x\n3 \n= x\n2\n. We begin by setting v\n1 \n= x\n1 \n=  1. Next we \ncompute","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":86462,"to":86495}}}}],[1451,{"pageContent":"538 \nChapter 7 \nDistance and Approximation \nAdrien Marie Legendre (1752-1833) \nwas a French mathematician who \nworked in astronomy, number \ntheory, and elliptic functions. He \nwas involved in several heated \ndisputes with Gauss. Legendre \ngave the first published statement \nof the law of quadratic reciprocity \nin number theory in 1765. Gauss, \nhowever, gave the first rigorous \nproofof this result in 1801 and \nclaimed credit for the result, \nprompting understandable outrage \nfrom Legendre. Then in 1806, \nLegendre gave the first published \napplication of the method of least \nsquares in a book on the orbits of \ncomets. Gauss published on the \nsame topic in 1809 but claimed \nhe had been using the method \nsince 1795, once again infuriating \nLegendre. \nperpw(v) \nw \nFigure 7.3 \nTherefore, \n(v\n1\n, x\n2) \n0 \nV\n2 \n= \nX\n2 \n-\n-\n( \n--\n, \nV1 \n= \nx  --\n(\n1\n) = \nx \nV1\n, \nV\nI/ \n2 \nTo find v\n3\n, we first compute \nf \nI \n3\n] \nI \n2 \n(v x ' \n= \nx\n2 \ndx \n= \n� \n= \n-\nI\n> \n3\n/ \n3 \n3\n' \n-I \n-I \n(v\n2\n, \nX\n3\n) \n= \nf \ni x\n3 \ndx \n= \nx\n4\n]\n1 \n= \n0, \n-I \n4 \n-I \nThen \nf \nI \n2 \n(v v \n' \n= \nx\n2 \ndx \n= \n-\n2\n> \n2\n/ \n3 \n-I","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":86497,"to":86618}}}}],[1452,{"pageContent":"X\n2 \n-\n-\n( \n--\n, \nV1 \n= \nx  --\n(\n1\n) = \nx \nV1\n, \nV\nI/ \n2 \nTo find v\n3\n, we first compute \nf \nI \n3\n] \nI \n2 \n(v x ' \n= \nx\n2 \ndx \n= \n� \n= \n-\nI\n> \n3\n/ \n3 \n3\n' \n-I \n-I \n(v\n2\n, \nX\n3\n) \n= \nf \ni x\n3 \ndx \n= \nx\n4\n]\n1 \n= \n0, \n-I \n4 \n-I \nThen \nf \nI \n2 \n(v v \n' \n= \nx\n2 \ndx \n= \n-\n2\n> \n2\n/ \n3 \n-I \nIt follows that {v1 , v\n2\n, v\n3\n} is an orthogonal basis for IJf \n2 \non the interval [ -1, 1]\n. \nThe \npolynomials \n1, x, x\n2 \n-\nt \nare the first three Legendre polynomials. If we divide each of these polynomials by \nits length relative to the same inner product, we obtain normalized Legendre polyno­\nmials (see Exercise 41). \nJust as we did in Section 5.2, we can define the orthogonal projection projw(v) \nof a vector v onto a subspace W of an inner product space. If {u1,  ... , \nu\nk} is an \northogonal basis for W, then \nThen the co mponent ofv orthogonal to Wis the vector \nperp\nw\n(\nv\n) = \nv  -proj\nw\n(\nv\n) \nAs  in  the  Orthogonal Decomposition  Theorem  (Theorem  5.11), projw(v)  and \nperpw(v) are orthogonal (see Exercise 43), and so, schematically, we have the situa­\ntion illustrated in Figure 7.3.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":86618,"to":86738}}}}],[1453,{"pageContent":"w\n(\nv\n) \nAs  in  the  Orthogonal Decomposition  Theorem  (Theorem  5.11), projw(v)  and \nperpw(v) are orthogonal (see Exercise 43), and so, schematically, we have the situa­\ntion illustrated in Figure 7.3. \nWe  will make use of these formulas in Sections 7.3 and 7.5 when we consider \napproximation problems-in particular, the problem of how best to approximate a","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":86738,"to":86746}}}}],[1454,{"pageContent":"Theorem 1.3 \nThis inequality was discovered by \nseveral different mathematicians, \nin several different contexts. It is \nno surprise that the name of the \nprolific Cauchy is attached to it. \nThe second name associated with \nthis result is that of Karl Herman \nAmandus Schwarz (1843-1921), \na German mathematician who \ntaught at the University of Berlin. \nHis version of the inequality that \nbears his name was published \nin 1885 in a paper that used \nintegral equations to study \nsurfaces of minimal area. A third \nname also associated with this \nimportant result is that of the \nRussian mathematician Viktor \nYakovlevitch Bunyakovsky (1804-\n1889). Bunyakovsky published the \ninequality in 1859, a full quarter­\ncentury before Schwarz's work on \nthe same subject. Hence, it is more \nproper to refer to the result as the \nCauchy-Bunyakovsky-Schwarz \nInequality. \nSection 7.1 \nInner Product Spaces \n539 \ngiven function by \"nice\" functions. Consequently, we will defer any examples until","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":86748,"to":86778}}}}],[1455,{"pageContent":"proper to refer to the result as the \nCauchy-Bunyakovsky-Schwarz \nInequality. \nSection 7.1 \nInner Product Spaces \n539 \ngiven function by \"nice\" functions. Consequently, we will defer any examples until \nthen, when they will make more sense. Our immediate use of orthogonal projection \nwill be to prove an inequality that we  first encountered in Chapter 1. \nThe Cauchv-Schwarz  and Triangle lnequalilies \nThe proofs of identities and inequalities involving the dot product in !R\nn \nare easily \nadapted to give corresponding results in general inner product spaces. Some of these \nare given in Exercises 31-36. In Section 1.2, we first encountered the Cauchy-Schwarz \nInequality, which is important in many branches of mathematics. We now give a \nproof of this result for  inner product spaces. \nThe Cauchy-Schwarz Inequality \nLet u and v be vectors in an inner prod uct space V. Then \nl\n(u, v)\nI \n::; \nll\nn\nll ll\nv\nll \nwith equality holding if and only if u and v are scalar multiples of each other. \nProof \nIf u \n=","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":86778,"to":86809}}}}],[1456,{"pageContent":"Let u and v be vectors in an inner prod uct space V. Then \nl\n(u, v)\nI \n::; \nll\nn\nll ll\nv\nll \nwith equality holding if and only if u and v are scalar multiples of each other. \nProof \nIf u \n= \n0, then the inequality is actually an equality, since \nl(o\n, v)\nI = 0 \n= \nll\nO\nll ll\nv\nll \n(u, v) \nIfu -=!=   O,then let Wbe the subspaceofVspannedby u. Sinceproj\nw\n(\nv\n) = \n-\n( \n-\n,\nuand \nU\n, \nll/ \nperpw v \n= \nv -  projw(v) are orthogonal, we can apply Pythagoras' Theorem to obtain \nll\nv\nll\n2 \n= \nll\nproj\nw\n(v) + (v - proj\nw\n(v)) \n11\n2 \n= \nll\nproj\nw\n(v) + perp\nw\n(v) \n11\n2 \n= \nll\nproj\nw\n(v) \n11\n2 \n+ \nll\nperp\nw\n(v) \n11\n2 \nIt follows that \nll\nproj\nw\n(v) \n11\n2 \n::; \nll\nv\nll\n2\n. \nNow \n. \n2 \n/ (u, v)   (u, v) )    ( (u, v) )\n2 \n(u, v)\n2 \n(u, v)\n2 \nll\npro\nJw\n(v)\nll \n= \n\\\n(u, u)\n\"\n' \n(u, u)\n\" \n= \n(u, u) \n(u, u) \n= \n(u, u) \n= \n� \nso we   have \n(u v)\n2 \nM>::; \nll\nv\nll\n2 \nor, \nequ\nivale\nntly, \n(u, v)\n2\n::; \nll\nn\nll\n2\nll\nv\nll\n2 \nTaking \nsquare \nroots, \nwe \nobtain \nl(\nu, v)\nI \n::; \nll\nu\nll ll\nv\nll \n· \n(\n2\n) \nClearly this last inequality is an equality if and only if \nll\nproj\nw\n(v) \n11\n2 \n= \nll\nv\nll\n2\n. By \nEquation \n(\n2\n) \nthis is true if and only if perpw(v) \n= \n0 or, equivalently,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":86809,"to":86987}}}}],[1457,{"pageContent":"we \nobtain \nl(\nu, v)\nI \n::; \nll\nu\nll ll\nv\nll \n· \n(\n2\n) \nClearly this last inequality is an equality if and only if \nll\nproj\nw\n(v) \n11\n2 \n= \nll\nv\nll\n2\n. By \nEquation \n(\n2\n) \nthis is true if and only if perpw(v) \n= \n0 or, equivalently, \n. \n(u, v) \nv \n= \npro\nJ\nw\n(v) \n= \n-\n( \n-\n, \nu \nU\n, \nll/","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":86987,"to":87038}}}}],[1458,{"pageContent":"540 \nChapter 7 \nDistance and Approximation \nTheorem 1.4 \n.. \nI \nExercises 1.1 \nIf this is so, then vis a  scalar multiple of u. Conversely, if v \n= \ncu, then \n. \n(u, cu) c(u, u) \nperp\nw\n(v) \n= \nv -  pro\nJ\nw\n(v) \n= \nc u \n-\n-\n(\n--\n' \nu \n= \nc u \n-\n-\n(\n-\n-\n' \nu \n= \n0 \nU\n,\nU\nf \nU\n,\nU\nf \nso equality holds in the Cauchy-Schwarz Inequality. \nFor an alternative proof of this inequality, see Exercise 44. We  will investigate \nsome  interesting consequences of the Cauchy-Schwarz Inequality and related in­\nequalities in Exploration: Geometric Inequalities and Optimization Problems, which \nfollows this section. For the moment, we use it to prove a generalized version of the \nTriangle Inequality (Theorem 1.5). \nThe Triangle Inequality \nLet u and v be vectors in an   inner prod uct space V. Then \nll\nu +v\nii \n:s \nll\nu\nll \n+ \nll\nv\nll \nProof \nStarting with the equality you will be asked to prove in Exercise 32, we have \nll\nu \n+  v\nll\n2 \n= \nll\nu\nll\n2 \n+  2\n(u, \nv) + \nll\nv\nll\n2 \n:s \nll\nu\nll\n2 \n+  2\nl\n(u, v)\nI \n+ \nll\nv\nll\n2 \n:s \nll\nu\nll\n2 \n+ 2 \nll\nu\nll ll\nv\nll \n+ \nll\nv\nll\n2 \n= (\nll\nu\nll \n+ \nll\nv\nll\n)\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":87040,"to":87164}}}}],[1459,{"pageContent":"ll\nu \n+  v\nll\n2 \n= \nll\nu\nll\n2 \n+  2\n(u, \nv) + \nll\nv\nll\n2 \n:s \nll\nu\nll\n2 \n+  2\nl\n(u, v)\nI \n+ \nll\nv\nll\n2 \n:s \nll\nu\nll\n2 \n+ 2 \nll\nu\nll ll\nv\nll \n+ \nll\nv\nll\n2 \n= (\nll\nu\nll \n+ \nll\nv\nll\n)\n2 \nTaking square roots yields the result. \nby Cauchy-Schwarz \nIn Exercises 1-4, let u \n= \n[ \n_ \n�\n] \nand v \n= \n[\n:]. \n� \n6. \n(\np\n(\nx\n)\n, q\n(\nx\n)) is the inner prod uct of Example 7.5 on the \nvector space <!/' 2 [ 0, l]\n. \nCompute \n1. (u, v) is the  inner prod uct of Example 7.2. Compute \n(a\n) (u, v) (b) \nll\nu\nll \n(c) d(u, v) \n2. (u, v) is the  inner product of Example 7.3 with \nA \n= \n[� �].Compute \n(\na\n) (\nu, v) (b) \nll\nu\nll \n(c) d(u, v) \n3. In Exercise l, find a nonzero vector orthogonal to u. \n4. In Exercise 2, find a nonzero vector orthogonal to u. \nIn Exercises 5-8, let p\n(\nx\n) \n= \n3 -  2x and q\n(\nx\n) \n= \n1 +  x  +  x\n2\n• \n5. \n(\np\n(\nx\n)\n, q\n(\nx\n)) \nis the inner product ofExample 7.4. Compute \n(\na\n) \n(p\n(\nx\n)\n, q\n(\nx\n)) (b) \nll\np\n(\nx\n) \nII \n(c) \nd(\np\n(\nx\n)\n, q\n(\nx\n)) \n(\na\n) (\np\n(\nx\n)\n, q\n(\nx\n)) \n(b) \nll\np (x\n) \nII \n(c) \nd(\np\n(\nx\n)\n, q\n(\nx\n)) \n7. In Exercise 5, find a nonzero vector orthogonal to p\n(\nx\n)\n. \n� \n8. In Exercise 6, find a nonzero vector orthogonal to p\n(\nx\n)\n.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":87164,"to":87355}}}}],[1460,{"pageContent":"ll\np\n(\nx\n) \nII \n(c) \nd(\np\n(\nx\n)\n, q\n(\nx\n)) \n(\na\n) (\np\n(\nx\n)\n, q\n(\nx\n)) \n(b) \nll\np (x\n) \nII \n(c) \nd(\np\n(\nx\n)\n, q\n(\nx\n)) \n7. In Exercise 5, find a nonzero vector orthogonal to p\n(\nx\n)\n. \n� \n8. In Exercise 6, find a nonzero vector orthogonal to p\n(\nx\n)\n. \n�In Exercises 9 and 10, let f\n(\nx\n) \n= sin x and g\n(\nx\n) \n= \nsin x + \ncos x in th e vector space C€, [O, 27T] with the inner product \ndefined by Example 7.5. \n9. Compute \n(\na\n) (\nf,g) \n(b) \nll\nJ\nll \n10. Find a nonzero vector orthogonal to f \n(c) d(f, g\n) \n11. Let a, b, and c be distinct real numbers. Show that \n(\np\n(\nx\n)\n, q\n(\nx\n)) \n= \np\n(\na\n)\nq\n(\na\n)  + p\n(\nb\n)\nq\n(\nb\n) \n+ p\n(\nc\n)\nq\n(\nc\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":87355,"to":87465}}}}],[1461,{"pageContent":"defines an inner product on rzf \n2\n. [Hint: You will need \nthe fact that a polynomial of degree n has at most n \nzeros. See Appendix D.] \n12. Repeat Exercise 5 using the inner product of Exer­\ncise 11 with a \n= \n0, b \n= \nl, c \n= \n2. \nIn Exercises 13-18, determine which of the four inner prod­\nuct axioms do not hold. Give a specific example in each case. \n13. Let u \n= \n[\n�:\n]\nand v \n= \n[\n:Jin IR\n2\n. Define (u, v) \n= \nu\n1\nv\n1\n• \n14. Let u \n= \n[\n�\n:\n] \nand v \n= \n[ \n:J in IR\n2\n. Define \n(u, v) \n= \nu\n1\nv\n1 \n-  u\n2\nv\n2\n• \n15. Let u \n= \n[\n�\n:\n] \nand v \n= \n[ \n:J in IR\n2\n. Define \n(u, v) \n= \nU\n1\nV\n2 \n+ \nU\nz\nV\n1\n. \n16. In rzf \n2\n, define \n(\np\n(\nx)\n, q(\nx)\n/ \n= \np (\nO\n) q(\nO\n)\n. \n17\n. In rzf\n2\n, define \n(\np\n(\nx)\n, q(\nx)\n/ \n= \np (\nl\n) q(\nl\n)\n. \n18. In Mw define (A, B\n/ = \ndet(AB). \nIn Exercises 19 and 20, (u, v) defines an inner product \n[\nU\n1\n] \n[\nV\n1\n] \n· \nd \n· \non IR\n2\n, where u \n= \nU\nz \nand v \n= \nv\n2 \n• Fm a symmetric \nmatrix A such that (u, v) \n= \nu\nr \nAv. \n19. (u, v) \n= \n4\nU\n1\nV\n1 \n+ \nU\n1\nV\n2 \n+ \nU\nz\nV\n1 \n+ \n4\nU\n2\nV\n2 \n20. (u, v) \n= \nU\n1\nV\n1 \n+  2\nU\n1\nV\n2 \n+ \n2\nU\n2\nV\n1 \n+ \n5\nU\n2\nV\n2 \nIn Exercises 21 and 22, sketch th e unit circle in IR\n2 \nfor the \ngiven inner product, where u \n=","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":87467,"to":87662}}}}],[1462,{"pageContent":"= \nu\nr \nAv. \n19. (u, v) \n= \n4\nU\n1\nV\n1 \n+ \nU\n1\nV\n2 \n+ \nU\nz\nV\n1 \n+ \n4\nU\n2\nV\n2 \n20. (u, v) \n= \nU\n1\nV\n1 \n+  2\nU\n1\nV\n2 \n+ \n2\nU\n2\nV\n1 \n+ \n5\nU\n2\nV\n2 \nIn Exercises 21 and 22, sketch th e unit circle in IR\n2 \nfor the \ngiven inner product, where u \n= \n[ \n�\n:\n] \nand v \n= \n[ \n:\n:\n]\n. \n21. \n(u, v) = \nu\n1\nv\n1 \n+ \ni\nu\n2\nV\n2 \n22. (u, v) \n= \n4\nU\n1\nV\n1 \n+ \nU\n1\nV\n2 \n+ \nU\nz\nV\n1 \n+ \n4\nU\n2\nV\n2 \n23. Prove Theorem 7.l(b). \n24. Prove Theorem 7.l(c). \nIn Exercises 25-29, suppose that u, v, and ware vectors in \nan inner product space such that \n(u, v) \n= \n1, \n(u, w) \n= \n5, (v, w) \n= \n0 \nll\nu\nll \n= \n1, \nll\nv\nll \n= \nV3\n, \nll\nw\nll \n= \n2 \nEvaluate the expressions in Exercises 25-28. \n25. \n(u + w, v -w) \nSection 7.1 \nInner Product Spaces \n541 \n26. (2v -w, 3u + 2w) \n27. \nll\nu +v\nii \n28. \nll\n2u - 3v +  w\nll \n29. Show that u + v \n= \nw. [Hint: How can you use \nthe properties of inner prod uct to verify that \nu + v -  w \n= \nO?] \n30. Show that, in an  inner product space, there cannot be \nunit vectors u and v with (u, v) < -1. \nIn Exercises 31-36, (u, v) is an inner product. In Exer­\ncises 31-34, prove that th e given statement is an identity. \n31. \n(u \n+ \nv, u \n- v) \n= \nll\nu\nll\n2 \n-","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":87662,"to":87827}}}}],[1463,{"pageContent":"unit vectors u and v with (u, v) < -1. \nIn Exercises 31-36, (u, v) is an inner product. In Exer­\ncises 31-34, prove that th e given statement is an identity. \n31. \n(u \n+ \nv, u \n- v) \n= \nll\nu\nll\n2 \n-\nll\nv\nll\n2 \n32. \nll\nu + v\nll\n2 \n= \nll\nu\nll\n2 \n+  2(u, v) + \nll\nv\nll\n2 \n33. \nll\nu\nll\n2 \n+ \nll\nv\nll\n2 \n= \nt \nll\nu \n+ v\nll\n2 \n+ \nt \nll\nu - v\nll\n2 \n34. \n(u, v) \n= \ni \nll\nu \n+ \nv\nll\n2 \n-\ni \nll\nu - v\nll\n2 \n35. Prove that \nll\nu \n+  v\nii \n= \nll\nu \n-  v\nii \nif and \nonly \nifu \nand \nv \nare orthogonal. \n36. Provethat d\n(\nu, v\n) = \nV\nll\nu\nll\n2 \n+ \nll\nv\nll\n2\nifand only ifu \nand v are orthogonal. \nIn Exercises 37-40, apply th e Gram-Schmidt Process to the \nbasis l3 to obtain an orthogonal basis for th e inner product \nspace V relative to the given inner product. \n37. V \n= \nIR\n2\n, l3 \n= \n{ \n[ \n� \nl \n[\n�\n]\n}\n,with the inner prod uct in \nExample 7.2 \n38. V \n= \nIR\n2\n, l3 \n= \n{ \n[ \n�\n], \n[ \n�\n] \n} \n, with the inner product \nimmediately following Example 7.3 \n39. V \n= \nrzf \n2\n, l3 \n= \n{1, 1 + x, 1 +  x  +  x\n2\n}, with the inner \nproduct in Example 7.4 \n�40. V \n= \nrzll\n2\n[0, 1], l3 \n= \n{1, 1 + x, 1 +  x  +  x\n2\n}, with the \ninner product in Example 7.5","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":87827,"to":87985}}}}],[1464,{"pageContent":"39. V \n= \nrzf \n2\n, l3 \n= \n{1, 1 + x, 1 +  x  +  x\n2\n}, with the inner \nproduct in Example 7.4 \n�40. V \n= \nrzll\n2\n[0, 1], l3 \n= \n{1, 1 + x, 1 +  x  +  x\n2\n}, with the \ninner product in Example 7.5 \n�41. (a) Compute the first three normalized Legendre \npolynomials. (See Example 7.8.) \n(b) Use the Gram-Schmidt Process to find the fourth \nnormalized Legendre polynomial. \n42. If we multiply the Legendre polynomial of degree n by \nan appropriate scalar we can obtain a polynomial L\n\"\n(x) \nsuch that L\nn\n(\nl\n) \n= \n1. \n(a) Find L\n0(\nx\n)\n, L\n1 \n(\nx\n)\n, L\n2\n(\nx\n)\n, and L\n3\n(x). \n(b) It can be shown that L\n\"\n(\nx\n) \nsatisfies the recurrence \nrelation \n2n - 1 \nn  -1 \nL\nn\n(\nx\n) \n= \nxL\nn\n_\n1\n(\nx\n) \n-\n-\nL\nn\n-\nz\n(X) \nn n","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":87985,"to":88066}}}}],[1465,{"pageContent":"542 \nChapter 7 \nDistance and Approximation \nfor all n 2  2. Verify this recurrence for L\n2\n(\nx\n) \nand \nL\n3\n(\nx\n)\n. Then use it to compute L\n4\n(\nx\n) \nand L\n5\n(\nx\n)\n. \n43. Verify that if Wis a  subspace of an inner product \nspace V and vis in V, then perpw(v) is orthogonal \nto all win W. \n44. Let u   and v be  vectors in an inner product space V. \nProve the Cauchy-Schwarz Inequality for u * 0 as \nfollows: \n(a) Let t be a    real scalar. Then (tu + v, tu + v) 2  0 \nfor all values oft. Ex pand this inequality to obtain \na quadratic inequality of the form \nat\n2 \n+  bt + c 2  0 \nWhat are a, b, and c in terms of u and v? \n(b) Use your knowledge of quadratic equations and \ntheir graphs to obtain a condition on a, b, and c \nfor which the inequality in part (a) is true. \n(c) Show that, in terms of u and v, your condition \nin part (b) is equivalent to the Cauchy-Schwarz \nInequality.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":88068,"to":88111}}}}],[1466,{"pageContent":"Vectors and Matrices with Complex Entries \nIn this book, we have developed the theory and applications of real vector spaces, \nthe most basic example of which is !R\nn\n. We have also explored the finite vector spaces \n2� and their applications. The set e\nn \nof n-tuples of complex numbers is   also a  vector \nspace, with the complex numbers IC as scalars. The vector space axioms (Section 6.1\n) \nall hold for e\nn\n, and concepts such as linear independence, basis, and dimension carry \nover from !R\nn \nwithout difficulty. \nThe first notable difference between !R\nn \nand e\nn \nis in the definition of dot product. \nIf we define the dot product in e\nn \nas in !R\nn\n, then for the nonzero vector v \n= \n[ \n�] we \nhave \nll\nv\nll \n= \nVV:V \n= \nVi2+l2 \n= \nv\n-1 \n+ \n1 \n= \nv'o \n= \no \nThis is clearly an undesirable situation (a nonzero vector whose length is zero) and \nviolates Theorems l.2(d) and 1.3. We now generalize the real dot product to e\nn \nin a \nway that avoids this type of difficulty. \nDefinition \nIfu � [ J:l and v � [ n ace vedo\" in C\n\"","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":88113,"to":88166}}}}],[1467,{"pageContent":"violates Theorems l.2(d) and 1.3. We now generalize the real dot product to e\nn \nin a \nway that avoids this type of difficulty. \nDefinition \nIfu � [ J:l and v � [ n ace vedo\" in C\n\"\n' then thecomplex \ndot product of u and v is defined by \nThe  norm  (or length) of  a    complex  vector v  is  defined  as in  the  real case: \nll\nv\nll \n= \nVV:V: \nLikewise, the distance between two complex vectors u and vis still \ndefined asd(u, v) \n= \nll\nu-v\nll\n· \n543","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":88166,"to":88188}}}}],[1468,{"pageContent":"544 \nI. \nShow th•t, fon \n� \n[ \nn \nin \nC \"\n, \n11\n•\n11 \n� \nV\nl\nv\ni\nl \n+ \nl\nv\ni\nl \n+  ... + \nl\nv\n;I\n. \n2. \nLet u = \n[\ni\n]\nandv = [\n2 -  3i\n]\n.Find: \n1 \n1  +   Si \n(a) u·v (b) \nll\nu\nll \n(c) \nll\nv\nll \n(d) d\n(\nu, v\n) \n(e) a nonzero vector orthogonal to u \n(f)  a nonzero vector orthogonal to v \nThe complex dot prod uct is an example of the more general notion of a complex \ninner product, which satisfies the same conditions as a real inner product with two \nexceptions. Problem 3 provides a summary. \n3.  Prove that the  complex dot prod uct satisfies the following properties for all \nvectors U, V, and Win e\nn \nand all complex scalars. \n(a)  u·v= v·u \n(b) u·(v + w)  = u·v + u·w \n(c)  (cu)·v=c(u·v)  and u·(cv) =  c(u·v) \n(d) u·u �  0 and  u·u = Oifand only ifu = 0. \nFor matrices with complex entries, addition, multiplication by complex scalars, \ntranspose, and matrix multiplication are all defined exactly as we did for real ma­\ntrices in Section 3.1, and the algebraic properties of these operations still hold. (See","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":88190,"to":88256}}}}],[1469,{"pageContent":"transpose, and matrix multiplication are all defined exactly as we did for real ma­\ntrices in Section 3.1, and the algebraic properties of these operations still hold. (See \nSection 3.2.) Likewise, we have the notion of the inverse and determinant of a square \ncomplex matrix just as in   the real case, and the techniques and properties all carry \nover to the complex case. (See Sections 3.3 and 4.2.) \nThe notion of transpose is, however, less useful in the complex case than in the \nreal case. The following definition provides an alternative. \nDefinition \nIf A is a complex matrix, then the conjugate transpose of A is the \nmatrix A* defined by \nIn the preceding definition, A refers to the matrix whose entries are the complex \nconjugates of the corresponding entries of A; that is,  if A =  [ aij], then A =  [ aiJ]. \n4.  Find the conjugate transpose A* of the given matrix: \n(a)  A= \n[\n2 - i \n(c) \nA = \n4 \n�\n] \n1  +   3i \n0 \n-2 \n] \n3 -  4i \n(b) A \n_ \n[ \n2    5 -2i\n] \n5 +   2i -1 \n[ \n3i \n. \n0 1  +\n. \ni\nl \n(d) \nA = 1 -1 4 1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":88256,"to":88296}}}}],[1470,{"pageContent":"4.  Find the conjugate transpose A* of the given matrix: \n(a)  A= \n[\n2 - i \n(c) \nA = \n4 \n�\n] \n1  +   3i \n0 \n-2 \n] \n3 -  4i \n(b) A \n_ \n[ \n2    5 -2i\n] \n5 +   2i -1 \n[ \n3i \n. \n0 1  +\n. \ni\nl \n(d) \nA = 1 -1 4 1 \n1  +  i 0 -i \nProperties of the complex conjugate (Appendix C) extend to matrices, as the next \nproblem shows. \n5.  Let A and B be complex matrices, and let c be a complex scalar. Prove the \nfollowing properties: \n(a) A  =A \n(c)  cA  = c A \n(e) \n(\nA \nf \n= \nC\n:\nF\n) \n(b) A+ B =A+ B \n---\n(d) AB = AB","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":88296,"to":88343}}}}],[1471,{"pageContent":"Hermitian matrices are named \nafter the French mathematician \nCharles Hermite (1822-1901). \nHermite is best known for his \nproof that the number e is tran­\nscendental, but he also was the \nfirst to use the term orthogonal \nmatrices, and he proved that sym­\nmetric (and Hermitian) matrices \nhave real eigenvalues. \nThe properties in Problem \n5 \ncan be used to establish the following properties of \nthe conjugate transpose, which are analogous to the properties of the transpose for \nreal matrices (Theorem 3.4). \n6.  Let A and B be complex matrices, and let c be a complex scalar. Prove the \nfollowing properties: \n(a) \n(\nA*\n)\n* =A \n(c) \n(\ncA\n)\n*  =   cA * \n(b) \n(\nA+B\n)\n*=A*+B* \n(d) \n(\nAB\n)\n*  =   B*A* \n7.  Show that for vectors u  and v in e\nn\n, the complex dot product satisfies \nu · v =   u*v. (This result is why we defined the complex dot prod uct as we did. It \ngives us the analogue of the formula u · v = u\nr \nv for vectors in 11;r.) \nFor real matrices, we have seen the importance of symmetric matr  ices, especially","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":88345,"to":88389}}}}],[1472,{"pageContent":"gives us the analogue of the formula u · v = u\nr \nv for vectors in 11;r.) \nFor real matrices, we have seen the importance of symmetric matr  ices, especially \nin our study of diagonalization. Recall that a  real matrix A  is symmetric if A\nr \n= A. For \ncomplex matrices, the following definition is the correct generalization. \nDefinition \nA square complex matrix A  is called Hermitian if A* = A-that \nis, if it is equal to its own conjugate transpose. \n8.  Prove that the diagonal entries of a Hermitian matrix must be real. \n9. \nWhich of the following matrices are Hermitian? \n(a) A = \n(c) A= \n(e)  A= \n[ 2    1 +\n1\n. \ni] \n1 \n-\ni \n[l -3 \n-Si \n[-� \n� \n-\n�\n] \n-\n2 \n1 \n0 \n[ \n-\n1    2 -\n5 \n3i] \n(b) A = \n. \n2 \n-\n31 \n[ \n1 + 4i \n(d) A = 1 \n-\n4i \n2 \n(f)  A= \n3 + i \n-i \n[ 0\n3  0\n2 \n-\n2\n1\n] \n-\n2 \n5 \n10. Prove that the eigenvalues of a Hermitian matrix are real numbers. [Hint: \nThe proof of Theorem 5.18 can be adapted by making use of the conjugate transpose \noperation.] \n11. \nProve that if A is a Hermitian matrix, then eigenvectors corresponding to","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":88389,"to":88457}}}}],[1473,{"pageContent":"The proof of Theorem 5.18 can be adapted by making use of the conjugate transpose \noperation.] \n11. \nProve that if A is a Hermitian matrix, then eigenvectors corresponding to \ndistinct eigenvalues of A are orthogonal. [Hint: Adapt the proof of Theorem 5.19 \nusing u \n· \nv = u *v instead of u \n· \nv = u \nr \nv.] \nRecall that a  square real matrix Q is orthogonal if Q\n-\n1 \n= Q\nr\n. The next definition \nprovides the complex analogue. \nDefinition \nA square complex matrix U  is called unitary if u\n-\n1 \n=  U*. \nJust as for orthogonal matrices, in practice it is not necessary to compute u\n-\n1 \ndirectly. \nYou need only show that U* U = I to verify that U is unitary. \n545","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":88457,"to":88486}}}}],[1474,{"pageContent":"546 \n12. Which of the following matrices are unitary? For those that are unitary, give \ntheir inverses. \n[\ni/\nVl \n(a) \ni/\nVl \n-i/\nVl\n] \ni/\nVl \n(b) \n[\nl \n+ i \n1 -  i \n1  + i] \n-1 + i \n[ \n3/5 \n-4/5] \n[ \n(\n1  + ;\n)\n/\nVii \n0 \n2/:6\n] \n(c) \n4i/5 \n(d) \n0 \n3i/5 \n(\n-1 \n-   i\n)\n/\nV3 \n0 \n1/\n0 \nUnitary matrices behave in most respects like orthogonal matrices. The following \nproblem gives some alternative characterizations of unitary matrices. \n13. Prove that the following statements are equivalent for a square complex \nmatrix U: \n(a)  U is unitary. \n(b)  The columns of U form an orthonormal set  in e\nn \nwith respect to the complex \ndot product. \n(c)  The rows of U form an orthonormal set in e\nn \nwith respect to the complex dot \nprod uct. \n(d) \nllUxll = \nllxll for\nevery xinc\nn\n. \n( e)  Ux · Uy = x · y for every x and y in e\nn\n. \n[Hint: Adapt the proofs of Theorems 5.4-5.7.] \n14. Repeat Problem 12, this time by applying the criterion in part (b) or part (c) \nof Problem 13. \nThe next definition is the natural generalization of orthogonal diagonalizability","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":88488,"to":88560}}}}],[1475,{"pageContent":"14. Repeat Problem 12, this time by applying the criterion in part (b) or part (c) \nof Problem 13. \nThe next definition is the natural generalization of orthogonal diagonalizability \nto complex matrices. \nDefinition \nA square complex matrix A  is called unitarily diagonalizable if \nthere exists a unitary matrix U  and a diagonal matrix D such that \nU*AU = D \nThe process for diagonalizing a unitarily diagonalizable n X n matr  ix A   mimics \nthe real case. The columns of U must form an orthonormal basis for e\nn \nconsisting of \neigenvectors of A. Therefore, we (1) compute the eigenvalues of A, \n(\n2\n) \nfind a basis for \neach eigenspace, \n(\n3\n) \nensure that each eigenspace basis consists of orthonormal vec­\ntors (using the Gram-Schmidt Process, with the complex dot product, if necessary), \n( \n4\n) \nform the matrix U whose columns are the orthonormal eigenvectors just found. \nThen U\n* \nAU will be a diagonal matrix D whose diagonal entries are the eigenvalues of","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":88560,"to":88589}}}}],[1476,{"pageContent":"( \n4\n) \nform the matrix U whose columns are the orthonormal eigenvectors just found. \nThen U\n* \nAU will be a diagonal matrix D whose diagonal entries are the eigenvalues of \nA, arranged in the same order as the corresponding eigenvectors in the columns of U. \n15. In each of the following, find a unitary matrix U and a diagonal matrix D \nsuch that U*  AU = D\n. \n(a) A= \n[ \n2 i] \n-i 2 \n(b) A= \n[\n� \n-\n�\n] \n1 ; i] \n[\ni \n0 \nI�;\n] \n(c) A= \n[ \n-1 \n(d) A= \n2 \n1 -  i \n1  + i","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":88589,"to":88622}}}}],[1477,{"pageContent":"See Linear Algebra with Applica­\ntions by S. J. Leon (Upper Saddle \nRiver, NJ: Prentice-Hall, 2002). \nThe matrices in (a), (c), and (d) of the preceding problem are all Hermitian. \nIt turns out that every Hermitian matrix is unitarily diagonalizable. (This is the \nComplex Spectral  Theorem, which can be proved by adapting the proof of Theo­\nrem 5.20.) At this point you probably suspect that the converse of this result must \nalso be true-namely, that every unitarily diagonalizable matr  ix must be Hermitian. \nBut unfortunately this is fals e! (Can you see where the complex analogue of the proof \nof Theorem 5.17 brea ks down?) \nFor a specific counterexample, take the matrix in part (b) of Problem 15. It is not \nHermitian, but it is unitarily diagonalizable. \nIt turns out that the correct characterization of unitary diagonalizability is the \nfollowing theorem, the proof of which can be found in more advanced textbooks. \nA square complex matrix A  is unitarily diagonalizable if and only if \nA*A =AA*","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":88624,"to":88639}}}}],[1478,{"pageContent":"following theorem, the proof of which can be found in more advanced textbooks. \nA square complex matrix A  is unitarily diagonalizable if and only if \nA*A =AA* \nA matrix A for which A* A =   AA* is called normal. \n16.  Show that every Hermitian matrix, every unitary matrix, and every skew­\nHermitian matr  ix (A* =   -A) is normal. (Note that in the real case, this result refers \nto symmetric, orthogonal, and skew-symmetric matrices, respectively.) \n17.  Prove that if a square complex matrix is unitarily diagonalizable, then it \nmust be normal. \nGeometric Inequalities and \nOptimization Problems \nThis exploration will introduce some powerful (and perhaps surprising) applications \nof various inequalities, such as the Cauchy-Schwarz Inequality. As you will see, certain \nmaximization/minimization problems \n(\noptimization proble ms\n) \nthat typically arise in a \ncalculus course can be solved without using calculus at all! \nRecall that the Cauchy-Schwarz Inequality in !R\nn \nstates that for all vectors u and v, \nl\nu","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":88639,"to":88662}}}}],[1479,{"pageContent":"(\noptimization proble ms\n) \nthat typically arise in a \ncalculus course can be solved without using calculus at all! \nRecall that the Cauchy-Schwarz Inequality in !R\nn \nstates that for all vectors u and v, \nl\nu\n·v\nl\n::; \nll\nu\nll ll\nv\nll \nwith  equality if and only if u and v are  scalar multiples of each other.  If u \n[x\n1 \n·  ·  · x\nn\nf and v = \n[y\n1 \n·  ·  · Y\nn\nf, the above inequality is equivalent to \nI\nx \ny \n+ \n·   ·   · \n+ \nx y \nI \n< \nV \nx\n2 \n+ \n·   ·   · \n+ \nx\n2 \nV\ny\n2 \n+ \n·   ·   · \n+ \ny\n2 \nII \nnn\n-I \nn \nI \nn \nSquaring both sides and using summation notation, we have \n541","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":88662,"to":88723}}}}],[1480,{"pageContent":"D ----� \nA \nC \n0 \nB \n'-----y-----' �--�---�I \nx \nFigure 7.4 \nx \nFigure 7.5 \n548 \ny \nExample 1.9 \ny \nx \ny \nEquality holds if and only if there is some scalar k such that y\n; \n= kx\n; \nfor i = 1, ... , n. \nLet's begin by using Cauchy-Schwarz to derive a special case of one of the most \nuseful of all inequalities. \n1.  Let x and y be nonnegative  real  numbers.  Apply the  Cauchy-Schwarz \nInequality to u = [ �] and v = [ �] to show that \nwith equality if and only if x = y. \nx+y \nvxy :s 2 \n(1) \n2. (a)  Prove inequality (1) directly. [Hint: Square both sides.] (b)  Figure 7.4 \nshows a circle with center 0 and diameter AB = AC + CB = x  + y. The segment \nCD is perpendicular to AB. Prove that CD  = \\/XY and use this result to deduce \ninequality (1). [ Hint: Use similar triangles.] \nThe right-hand side of inequality ( 1) is the familiar arithmetic mean (or average\n) \nof the numbers x and y. The left-hand side shows the less familiar geometric mean","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":88725,"to":88760}}}}],[1481,{"pageContent":"The right-hand side of inequality ( 1) is the familiar arithmetic mean (or average\n) \nof the numbers x and y. The left-hand side shows the less familiar geometric mean \nof x and y. Accordingly, inequality (1) is known as the Arithmetic Mean-Geometric \nMean Inequality  (AMGM). It  holds more generally; for n nonnegative variables \nXi, ... , X\nn\n, it states \nwith equality if and only if Xi = x\n2 \n= ·  ·  · = Xw \nIn words, the AMGM Inequality says that the geometric mean of a set of nonnega­\ntive numbers is always less than or equal to their arithmetic mean, and the two are \nthe same precisely when all of the numbers are the same. (For the general proof, see \nAppendix B.) \nWe now explore how such an inequality can be applied to optimization problems. \nHere is a typical calculus problem. \nProve that among all rectangles whose pe  rimeter is 100 units, the square has the \nlargest area. \nSolulion If we let x and y be the dimensions of the rectangle (see Figure 7.5), then","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":88760,"to":88779}}}}],[1482,{"pageContent":"Prove that among all rectangles whose pe  rimeter is 100 units, the square has the \nlargest area. \nSolulion If we let x and y be the dimensions of the rectangle (see Figure 7.5), then \nthe area we want to maximize is given by \nA= xy \nWe are given that the perimeter satisfies \n2x +  2y =  100","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":88779,"to":88785}}}}],[1483,{"pageContent":"Example 1.10 \nFigure 1.6 \nwhich is the same as x  +  y \n= \n50. We can relate xy and x  +  y using the AMGM \nInequality: \n-� \nx+y \nI \nv xy \n::::: --or, equivalently, \nxy ::::: 4\n(\nx + y)\n2 \n2 \nSince x + y \n= \n50 is a constant (and this is the key), we  see that the  maximum value \nof A \n= \nxy is 50\n2\n/4 \n= \n625 and it occurs when x \n= \ny \n= \n25. \nNot a derivative in sight!  Isn't that impressive?   Notice that in this maximiza­\ntion problem, the crucial step was showing that the right-hand side of the AMGM \nInequality was constant. In a similar fashion, we may be able to apply the inequality to \na minimization problem if we can arrange for the left-hand side to be constant. \nProve that among all rectangular prisms with volume 8 m\n3\n, the  cube has the mini­\nmum surface area. \nSolution As shown in Figure 7.6, if the dimensions of such a prism are x, y, and z, \nthen its volume is given by \nV \n= \nxyz \nThus, we are given that xyz \n= \n8. The surface area to be minimized is \nS \n= \n2xy + 2yz + 2zx","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":88787,"to":88835}}}}],[1484,{"pageContent":"then its volume is given by \nV \n= \nxyz \nThus, we are given that xyz \n= \n8. The surface area to be minimized is \nS \n= \n2xy + 2yz + 2zx \nSince this is a three-variable problem, the obvious thing to try is the version of the \nAMGM Inequality for n \n= \n3-namely, \n,3� \nx  + y  +  z \nv xyz  ::::: \n3 \nUnfortunately,  the  expression for S does not  appear here. However,  the AMGM \nInequality also implies that \nS \n2xy + 2yz + 2zx \n3 \n3 \n2: \n\\Y\n(\n2xy)\n(\n2yz\n)(\n2zx\n) \n= \n2V(xYz)2 \n= 2\nV64 \n= 8 \nwhich is equivalent to S 2:  24. Therefore, the minimum value of S is 24, and it \noccurs when \n2xy \n= \n2yz \n= \n2zx \n(Why?) This implies that x \n= \ny \n= \nz = 2 (i.e., the rectangular prism is a cube). \n3.  Prove that among all rectangles with area 100 square units, the square has the \nsmallest perimeter. \n1 \n4. \nWhat is the minimum value off \n(\nx\n) \n= \nx  + -for x > O? \nx \n549","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":88835,"to":88896}}}}],[1485,{"pageContent":"Example 1.11 \n550 \n5.  A cardboard box with a square base and an open top is to   be constructed \nfrom a square of cardboard 10 cm on a side by cutting out four squares at the corners \nand folding up the sides. What should the dimensions of the box be  in order to make \nthe enclosed volume as large as possible? \n6.  Find the minimum value of f(x\n, y, \nz) \n= \n(\nx \n+ \ny\n) (\ny \n+ \nz) (z \n+ \nx) if x\n, y, \nand z are \npositive real numbers such that x\ny\nz \n= \n1. \n8 \n7.  For x > \ny \n> 0, find the minimum value of x \n+ \n. [Hint: A substitu-\ntion might help.] \ny\n(x \n-\ny\n) \nThe Cauchy-Schwarz Inequality itself can be applied to similar problems, as the \nnext example illustrates. \nFind the maximum value of the function j(x\n, y, \nz) \n= \n3\nx \n+ \ny \n+ \n2z subject to the \nconstraint x\n2 \n+ \ny\n2 \n+ \nz\n2 \n= \n1. Where does the maximum value occur? \nSolulion This sort of problem is usually handled by techniques covered in a multi­\nvariable calculus course. Here's how to use the Cauchy-Schwarz Inequality. The func­\ntion \n3\nx \n+ \ny \n+","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":88898,"to":88966}}}}],[1486,{"pageContent":"Solulion This sort of problem is usually handled by techniques covered in a multi­\nvariable calculus course. Here's how to use the Cauchy-Schwarz Inequality. The func­\ntion \n3\nx \n+ \ny \n+ \n2z has the form of a dot product, so we let \nThen the componentwise form of the Cauchy-Schwarz Inequality gives \n(\n3\nx \n+ \ny \n+ \n2z)\n2 \n:s (\n3\n2 \n+ \n1\n2 \n+ \n2\n2\n)(x\n2 \n+ \ny\n2 \n+ \nz\n2\n) \n= \n14 \nThus, the maximum value of our function is v14, and it occurs when \nTherefore, x \n= \n3\nk\n, y \n= \nk\n, \nand z \n= \n2k\n, \nso \n3\n(\n3\nk) \n+ \nk \n+ \n2(2k) \n= \nv14. It follows that \nk \n= \n1 / vl4\n, \nand hence \n[\nx\nl \n[\n3\n/\nvl4\n] \ny \n= \nl\n/vl4 \nz \n2/\nvl4 \n8. \nFind the maximum value of j(x\n, y, \nz) = x \n+ \n2y \n+ \n4\nz subject to x\n2 \n+ \n2y\n2 \n+ \nz2 = 1. \nz\n2 \n9.  Find the minimum value of f\n(\nx\n, y, \nz\n) \n= x\n2 \n+ \ny\n2 \n+ \n-subject to x \n+ \ny \n+ \nz \n= \n10. \n2 \n10. \nFind \nthe maximum value of sin e + cos e. \n11. \nFind the point on the line x \n+ \n2y \n= \n5 that is closest to the origin. \nThere are many other inequalities that can be used to solve optimization prob-\nlems. The quadratic mean of the numbers x\n1\n, \n... \n, \nx\nn \nis defined as \n)x\n2 \n+ \n... \n+ \nx\n2 \nI \nn \nn","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":88966,"to":89112}}}}],[1487,{"pageContent":"y \n-r�--,�-� \nr \n2\nx \nFigure 1.1 \nIf x\n1\n, ... , X\nn \nare nonzero, their harmonic mean is given by \nn \nIt turns out that the  quadratic, arithmetic, geometric, and harmonic means are all \nrelated. \n12. \nLet x and y be positive real numbers. Show that \nR \nx+y \n.r---\n2 \n>--> \nv\nx\ny> \n-\n2 \n--\nl/x + l/y \nwith equality if and only if x \n= \ny. (The middle inequality is just AMGM, so you need \nonly establish the first and third inequalities.) \n13.  Find the area of the largest rectangle that can be inscribed in a semicircle of \nradius r (Figure 7.7). \n14.  Find the minimum value of the function \n(\nx  + y)\n2 \nf\n(x, y) \n= \nxy \nfor \nx, y >  0. [\nHint: \n(\nx + y)\n2 \n/ xy \n= \n(\nx + y) (\n1/ x + l/y).] \n15.  Let x and y be positive real numbers with x + y \n= \n1. Show that the mini­\nmum value of \nf\n(\nx,y) \n= \n(x + �y + (y + \nt Y \nis ¥-, and determine the values of x and y for which it occurs. \n551","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":89114,"to":89178}}}}],[1488,{"pageContent":"552 \nChapter 7 \nDistance and Approximation \nIll!!. \nExample 1.12 \nExample 1.13 \nNorms and Distance Functions \nIn the last section, you saw that it  is possible to define length and distance in an inner \nproduct space. As you will see shortly, there are also some versions of these two con­\ncepts that are not defined in terms of an inner product. \nTo begin, we need to specify the properties that we want a \"length function\" \nto have.  The following definition does this, using as its basis Theorem 1.3 and the \nTriangle Inequality. \nDefinition \nA norm on a vector space Vis a  mapping that associates with \neach vector v a real number \nll\nv\nll\n, called the norm of v, such that the following prop­\nerties are satisfied for all vectors u and v and all scalars c: \n1. \nll\nv\nll \n2 \n0, and \nll\nv\nll \n= 0   if and only \nif \nv \n= \n0. \n2. \nll\ncv\nll \n= \nl\nc\nl\nll\nv\nll \n3. \nll\nu +v\nii \ns; \nll\nu\nll \n+ \nll\nv\nll \nA vector space with a norm is called a normed linear space. \nShow that in an inner product space, \nll\nv\nll \n= v'(V,V/ defines a norm. \nSolulion","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":89180,"to":89244}}}}],[1489,{"pageContent":"if \nv \n= \n0. \n2. \nll\ncv\nll \n= \nl\nc\nl\nll\nv\nll \n3. \nll\nu +v\nii \ns; \nll\nu\nll \n+ \nll\nv\nll \nA vector space with a norm is called a normed linear space. \nShow that in an inner product space, \nll\nv\nll \n= v'(V,V/ defines a norm. \nSolulion \nClearly, v'(V,VJ 2 0. Moreover, \nv1i:V) = 0 � (v, v) = 0 � v = 0 \nby the definition of inner product. This proves property (1 ). \nFor property (2), we only need to note that \nll\ncv\nll \n= \nY\n(\ncv\n, \ncv\n) \n=\nWM\n= \n0-vfM \n= \nl\nc\nl\nll\nv\nll \nProperty (3) is just the Triangle Inequality, which we verified in Theorem 7.4. \nWe now look at some examples of norms that are not defined in terms of an inner \nproduct. Example 7.13 is the mathematical generalization to !R\nn \nof the taxicab norm \nthat we  explored in the Introduction to this chapter. \nThe sum norm \nll\nv\nll\ns \nof a vector v in !R\nn \nis the sum of the absolute values of its compo­\nnents. That is, if v = [ v\n1 \n·  ·  · v \nn\n] \nT\n, then \nShow that the sum norm is a norm. \nSolulion Clearly, \nll\nv\nll\ns \n= \nI \nv\n1 \nI \n+ \n·  ·  · \n+ \nI\nv \nn \nI \n2 \n0, and \nthe \nonly way to \nachieve \nequality \nis if \nI \nv\n1 \nI","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":89244,"to":89352}}}}],[1490,{"pageContent":"nents. That is, if v = [ v\n1 \n·  ·  · v \nn\n] \nT\n, then \nShow that the sum norm is a norm. \nSolulion Clearly, \nll\nv\nll\ns \n= \nI \nv\n1 \nI \n+ \n·  ·  · \n+ \nI\nv \nn \nI \n2 \n0, and \nthe \nonly way to \nachieve \nequality \nis if \nI \nv\n1 \nI \n= ·  ·  · = \nI\nv \nn \nI \n= 0. But this is so if and only if v\n1 \n= ·  ·  · = v \nn \n= 0   or, equivalently, \nv = 0, proving property (1). For property (2), we see that cv = [cv\n1 \n·  ·  · cv\nn\n]\nT\n, so","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":89352,"to":89404}}}}],[1491,{"pageContent":"Example J .14 \nSection 7.2 \nNorms and Distance Functions \n553 \nFinally, the Triangle Inequality holds, because if u = [u1 \n·  ·  · \nu\nn\nl \nr\n, then \nll\nu+v\nll\n,\n= \nl\nu\n1\n+v\n1\nI \n+\n···\n+ \nl\nu\nn\n+v\nn\nl \n:S \n(\nl\nu\n1\nI \n+ \nl\nv\n1\nI\n)  + \n· · · \n+  (\nl\nu\nn\nl \n+ \nl\nv\nn\nl\n) \n= \n(\nl\nu\n1\nI \n+ \n· · · \n+ \nl\nu\nn\nl\n)  +  (\nl\nv\n1\nI \n+ \n· · · \n+ \nl\nv\nn\nl\n) \n= \nll\nu\nll\n, \n+ \nll\nv\nll\n, \nThe sum norm is also known as the I-norm and is often denoted by \nll\nv\nll \n1 . On IR\n2\n, it \nis the  same as the taxicab norm. As Example 7.13 shows, it is possible to have several \nnorms on the same vector space. Example 7.14 illustrates another norm on !R\nn\n. \nThe max norm \nll\nv\nll\nm \nof a vector v in !R\nn \nis the largest number among the absolute \nvalues of its components. That is, if v = \n[ v\n1 \n·  ·  · v \nn\n] \nr\n, then \nShow that the max norm is a norm. \nSolution Again, it is clear that \nll\nv\nll\nm \n2: 0. If \nll\nv\nll\nm \n= \n0, then the \nlargest \nof \nI \nv\n1 \nI\n, ... , \nI \nv\nn \nI \nis zero, and so they all are. Hence, v\n1 \n= ·  ·  · = v\nn \n= 0, so v = 0. This verifies property (1). \nNext, we observe that for any scalar c, \nll\ncv\nll\nm \n= \nm\na\nx\n{\nl\ncv\n1\nI\n, ... , \nl\ncv\nn\nl\n}  = \nl\nc\nl\nm\na","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":89406,"to":89577}}}}],[1492,{"pageContent":"v\n1 \nI\n, ... , \nI \nv\nn \nI \nis zero, and so they all are. Hence, v\n1 \n= ·  ·  · = v\nn \n= 0, so v = 0. This verifies property (1). \nNext, we observe that for any scalar c, \nll\ncv\nll\nm \n= \nm\na\nx\n{\nl\ncv\n1\nI\n, ... , \nl\ncv\nn\nl\n}  = \nl\nc\nl\nm\na\nx\n{\nl\nv\n1\nI\n, ... , \nl\nv\nn\nl\n} \n= \nl\nc\nl\nll\nv\nll\nm \nFinally, for u = [u1 ·  ·  · u\nn\nl r, we have \nll\nu  +  v\nll\nm \n= \nm\na\nx\n{\nl\nu\n1 \n+\nV\ni\nl\n, ... , \nl\nu\nn \n+  v\nn\nl\n} \n:S \nm\na\nx\n{\nl\nu\n1\nI \n+ \nl\nv\n1\nI\n, .. \n·\n, \nl\nu\nn\nl \n+ \nl\nv\nn\nl\n} \n:S \nm\na\nx\n{\nl\nu\n1\nI\n, ... \n, \nl\nu\nn\nl\n}  + \nm\na\nx\n{\nl\nv\n1\nI\n, \n· · ·\n, \nl\nv\nn\nl\n} \n= \nll\nu\nll\nm \n+ \nll\nv\nll\nm \n..._... \n(Why is the second inequality true?) This verifies the Triangle Inequality. \nExample J .15 \nThe max norm is also known as the oo-norm or unifo rm  norm and is often \ndenoted by \nll\nv\nll\noo\n· \nIn general, it is possible to define a norm \nll\nv\nll\np \non !R\nn \nby \nll\nv\nll\np \n= \n(\nl\nv\n1\nI\nP \n+ \n· · ·  + \nl\nv\nn\nl\nP\n)\n1\n1\nP \nfor any real number p 2:  1. For p = 1, \nll\nv\nll\n1 \n= \nll\nv\nll\n,, justifying the term 1-norm. For \np \n= \n2, \nll\nv\nll\n2 \n= \n(\nl\nv\n1\n12 \n+ \n· · · \n+ \nl\nv\nn\nl2\n)\n1\n1\n2 \n= \nY\nv\ni \n+ \n· · · \n+  v\n� \nwhich is just the familiar norm on !R\nn \nobtained from the dot product. Called the","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":89577,"to":89815}}}}],[1493,{"pageContent":"ll\nv\nll\n1 \n= \nll\nv\nll\n,, justifying the term 1-norm. For \np \n= \n2, \nll\nv\nll\n2 \n= \n(\nl\nv\n1\n12 \n+ \n· · · \n+ \nl\nv\nn\nl2\n)\n1\n1\n2 \n= \nY\nv\ni \n+ \n· · · \n+  v\n� \nwhich is just the familiar norm on !R\nn \nobtained from the dot product. Called the \n2-norm or Euclidean norm, it is often denoted by \nll\nvk As p gets large, it can be \nshown using calculus that \nll\nv\nll\np \napproaches the max norm \nll\nv\nll\nm\n· \nThis justifies the use \nof the alternative notation \nll\nv\nll\n00 \nfo\nr \nthis \nno\nrm\n. \nFor a    vector v in Z�, define \nll\nv\nll\nH \nto be w\n(\nv)\n, the weight of v. Show that it  defines a \nnorm.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":89815,"to":89894}}}}],[1494,{"pageContent":"554 \nChapter 7 \nDistance and Approximation \nExample 1.16 \nSolution Certain ly, \nll\nv\nllH \n= w(v) 2: 0, and the only vector whose weight is zero is \nthe zero vector. Therefore, property (   1) is true. Since the only candidates for a scalar \nc are 0 and 1, property (2) is immediate. \nTo verify the Triangle Inequality, first observe that if u and v are vectors in Z'.�, \nthen w(u + v) counts the number of places in which u and v differ. [For example, if \nu = [l 0 1 0] r and  v = [ 0 1   lf \nthen u + v = [l 0 \n0 lf, so  w(u + v) = 3, in agreement with the fact that \nu and v differ in exactly three positions.] Suppose that both u and v have zeros in \nn\n0 \npositions and ls in \nn\n1 positions, u has a 0 and v has a 1 in \nn\n0\n1 positions, and u has a \n1 and v has a 0 in \nn\n1\n0 \npositions. (In the  example above, \nn\n0 \n= 0, \nn\n1 = 2, \nn\n0\n1 = 2, and \nn\n1\n0 \n= 1.) Now \nw(u) = \nn1 \n+ \nn1\n0, \nw(v) \n= \nn1 \n+ \nn\n0\n1\n, \nand \nw(u + v) \n= \nn1\n0 \n+ \nn\n0\n1 \nTherefore, \nll\nu + v\nllH \n= w(u + v) = \nn1\n0 \n+ \nn\n0\n1 \n(\nn1 \n+ \nn1\n0) \n+ \n(\nn1 \n+ \nno\n1\n) \n-\n2n1 \n:s  (\nn1 \n+ \nn1\n0) \n+ \n(\nn1 \n+ \nno\n1\n) \n= \nw(u) \n+","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":89896,"to":89999}}}}],[1495,{"pageContent":"n\n0\n1 = 2, and \nn\n1\n0 \n= 1.) Now \nw(u) = \nn1 \n+ \nn1\n0, \nw(v) \n= \nn1 \n+ \nn\n0\n1\n, \nand \nw(u + v) \n= \nn1\n0 \n+ \nn\n0\n1 \nTherefore, \nll\nu + v\nllH \n= w(u + v) = \nn1\n0 \n+ \nn\n0\n1 \n(\nn1 \n+ \nn1\n0) \n+ \n(\nn1 \n+ \nno\n1\n) \n-\n2n1 \n:s  (\nn1 \n+ \nn1\n0) \n+ \n(\nn1 \n+ \nno\n1\n) \n= \nw(u) \n+ \nw(v) \n= \nll\nu\nllH \n+ \nll\nv\nllH \nThe norm \nll\nv\nllH \nis called the Hamming norm. \nDistance Functions \nFor any  norm, we can  define a  distance function just  as we did  in the  last \nsection-namely, \nd(u, v) = \nll\nu -  vii \nLet u = \n[ \n_ �\n] \nand v \n= \n[ \n-\n�\n]\n. Compute d( u, v) relative to (a) the Euclidean norm, \n(b) the sum norm, and (c) the max norm. \nSolution Each calculation requires knowing that u - v = \n[ \n_\n:\n]\n. \n(a) As is by now quite familiar, \nd\nE\n(u, v) \n= \nll\nu - v\nll\nE \n= \nV\n4\n2 \n+  (-3)\n2 \n= \nV25 \n= \n5 \n(b) d\n,\n(u, v) = \nll\nu -v\nii\n,\n= \n1\n4\n1 \n+ \nl\n-3\n1 \n= 7 \n(c) d\nm\n(u,v) \n= \nll\nu - v\nll\nm \n= max{\nl\n4\nI\n, \nl\n-3\n1\n} = 4 \nThe distance function on Z'.� determined by the Hamming norm is called the \nHamming distance. We will explore its use in error-correcting codes in Section 8.5. \nExample 7 .17 provides an illustration of the Hamming distance.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":89999,"to":90160}}}}],[1496,{"pageContent":"Example 1.11 \nTheorem 1.5 \nSection 7.2 \nNorms and Distance Functions \n555 \nFind the Hamming distance between \nu \n=  [ 1 \n0 \no\nf \nand  v = [O \nSolution \nSince we are working over Z\n2\n, u - v = u + v. But \nd\nH\n(u, v)  = \nll\nu \n+ v\nllH \n= w(u \n+ v) \nAs we noted in Example 7.15, this is just the number of positions in which u and v \ndiffer. The given vectors are the same ones used in that example; the calculation is \ntherefore exactly the same. Hence, d\nH\n(u, v) =   3. \nTheorem 7.5 summarizes the most important properties of a distance function. \nLet d be a distance function defined on a normed linear space V. The following \nproperties hold for all vectors u, v, and w in V: \na.  d(u, v) 2 0, and d(u, v) = 0   if and only ifu =   v. \nb.  d(u,v) =\nd(v, u) \nc. \nd(u, w) \n:s \nd(u, v) \n+ \nd(v, w) \nProof (a)  Using property (1) from the definition of a norm, it is easy to check that \nd( u, v) = \nll\nu -  vii 2 0, with equality holding if and only if u - v = 0 or, equivalently, \nu =   v.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":90162,"to":90208}}}}],[1497,{"pageContent":"+ \nd(v, w) \nProof (a)  Using property (1) from the definition of a norm, it is easy to check that \nd( u, v) = \nll\nu -  vii 2 0, with equality holding if and only if u - v = 0 or, equivalently, \nu =   v. \n(b) You are asked to prove property (b) in Exercise 19. \n(c) We apply the Triangle Inequality to obtain \nd(u, v) \n+ \nd(v, w) \n= \nll\nu -\nvii \n+ \nll\nv -\nw\nll \n2 \nll\n(u \n- v) + (v - w)\nll \n= \nll\nu - w\nll \n= d(u, w) \nA function d satisfying the three properties of Theorem 7.5 is also called a metric, \nand a    vector space that possesses such a function is called a metric space. These are \nvery important in many branches of mathematics and are studied in detail in more \nadvanced courses. \nMalrix Norms \nWe  can define norms for matrices exactly as we defined norms for vectors in !R\nn\n. \nAfter all, the vector space M\nm\nn \nof all m X n matrices is isomorphic to !R\nm\nn\n, so  this is \nnot difficult to do. Of course, properties (1), (2), and (3) of a norm will also hold in","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":90208,"to":90254}}}}],[1498,{"pageContent":"n\n. \nAfter all, the vector space M\nm\nn \nof all m X n matrices is isomorphic to !R\nm\nn\n, so  this is \nnot difficult to do. Of course, properties (1), (2), and (3) of a norm will also hold in \nthe setting of matrices. It turns out that, for matrices, the norms that are most useful \nsatisfy an additional property. (We will restrict our attention to square matrices, but \nit is possible to generalize everything to arbitrary matrices.)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":90254,"to":90266}}}}],[1499,{"pageContent":"556 \nChapter 7 \nDistance and Approximation \nExample 1.18 \nDefinition \nA matrix norm on M\nnn \nis a mapping that associates with each \nn X n matrix A a real number \nll\nA\nll\n, \ncalled the norm of A, such that the following \nproperties are satisfied for all n X n matrices A and B and all scalars c. \n1. \nll\nA\nll \n2:  0 \nand \nll\nA\nll \n= \n0 \nif \nand only if \nA = \n0. \n2. \nll\ncA\nll \n= \nl\nc\nl\nll\nA\nll \n3. \nll\nA + B\nii \n:::; \nll\nA\nll \n+ \nll\nB\nll \n4. \nll\nAB\nll \n:::; \nll\nA\nll ll\nB\nll \nA matrix norm on M\nnn \nis said to be compatible with a vector norm \nll\nx\nll \non !R\nn \nif, for \nall n X n matrices A and all vectors x in !R\nn\n, we have \nll\nAx\nll \n:::; \nll\nA\nll ll\nx\nll \nThe Frobenius norm \nll\nA\nll\nF \nof a matrix A is obtained by stringing out the entries of the \nmatrix into a vector and then taking the Euclidean norm. In other words, \nll\nA\nll\nF \nis just \nthe square root of the sum of the squares of the entries of A. So, if A = [a;\nj\n], then \n(a)  Find the Frobenius norm of \nA= \n[\n� -:J \n(b) Show that the  Frobenius norm is compatible with the Euclidean norm.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":90268,"to":90371}}}}],[1500,{"pageContent":"j\n], then \n(a)  Find the Frobenius norm of \nA= \n[\n� -:J \n(b) Show that the  Frobenius norm is compatible with the Euclidean norm. \n( c)  Show that the Frobenius norm is a matrix norm. \nSolu\nlion \n(a) \nll\nA\nll\nF \n= \ny\n3\n2 \n+ \n(\n-1\n)\n2 \n+ \n2\n2 \n+ \n4\n2 \n= \nv30 \nBefore we continue, observe that if \nA\n1 \n= \n[ 3 \n-\n1] and \nA\n2 \n= \n[ 2 4] are the row \nvectors of A, then II \nA\n1 \nII \nE \n= \nY \n3\n2 \n+ \n(\n-1\n)\n2 \nand II \nA\n2 \nII \nE \n= \nV \n2\n2 \n+ 4\n2\n. Thus, \nSimilarly, if a\n1 \n= \n[ \n�\n] \nand a\n2 \n= \n[ \n-: \n] \nare the column vectors of A, then \nIt is easy to see that these facts extend to n X n matrices in general. We will use these \nobservations to solve parts (b) and (c). \n(b) Write","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":90371,"to":90455}}}}],[1501,{"pageContent":"Then \nSection 7.2 \nNorms and Distance Functions \n551 \n= \nV\nll\nA\n1\nx\nll\n� \n+ \n· · · \n+ \nll\nA\nn\nx\nll\n� \n:s \nV\nll\nA\n1 \nll\n�\nll\nx\nll\n� \n+ \n· · · \n+ \nA\nJ�\nll\nx\nll\n� \n= \n(\nV\nll\nA\n1\nll\n� \n+  ... + 11\nA\nn\n11\nD\nll\nx\nll\nE \n= \nll\nA\nll\nF\nll\nx\nll\nE \nwhere the  inequality arises from  the Cauchy-Schwarz Inequality applied to the \ndot products of the row vectors \nA\n; with the column vector x. (Do you see how \nCauchy-Schwarz has been applied?) Hence, the Frobenius norm is compatible with \nthe Euclidean norm. \n(c) Let h;   denote the ith column of B. Using the matrix-column representation of the \nprod uct AB, we have \nII AB \nll\nF \n= \nII [A\nh\nl\n· · · \nAb\nn \nJ \nll\nF \n= \nV\nll\nAb\n1\nll\n� \n+ \n· · · \n+ \nll\nAb\nn\nll\n� \n:S \nVll\nA\nll\n�\nll\nb\n1\nll\n� \n+ \n· · · \n+ \nll\nA\nll\n�\nll\nb\nn\nll\n� \nll\nA\nll\nF \nV\nll\nb\n1\nll\n� \n+ \n· · · \n+ \nll\nb\nn\nll\n� \nll\nA\nll\nF\nll\nB\nll\nF \nby part (b) \nwhich proves property ( 4) of the definition of a matrix norm. Properties ( 1) through \n(3) are true, since the Frobenius norm is derived from the Euclidean norm, which \nsatisfies these properties. Therefore, the Frobenius norm is a matrix norm. \n4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":90457,"to":90612}}}}],[1502,{"pageContent":"(3) are true, since the Frobenius norm is derived from the Euclidean norm, which \nsatisfies these properties. Therefore, the Frobenius norm is a matrix norm. \n4 \nFor many applications, the Frobenius matrix norm is not the best (or the easiest) \none to use. The most useful types of matrix norms arise from considering the effect \nof the matrix transformation corresponding to the square matrix A. This transfor­\nmation maps a vector x into Ax. One way to measure the \"size\" of A is to  compare \nll\nx\nll \nand \nll\nAx\nll \nusing any convenient (vector) norm. Let's think ahead. Whatever \ndefinition of II A II we arrive at, we know we are going to want it to be compatible with \nthe vector norm we are using; that is, we will need \nII \nA x\nii \n:S \nll\nA \n11 \nll\nx\nll \nor \nII A x\nii \n< \nll\nA II for x * 0 \nll\nx\nll -\nTh \n· \nll\nA\nx\nll \nh  \"    h\n. \nb\n·\n1\n· \n\"  fA If 1\n. \nh \ne express10n IGll measures t  e   stretc mg capa 1 ity  o   . we norma ize eac \n1 \nnonzero vector x by dividing it by its norm, we get unit vectors x \n= \nM\nx and thus","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":90612,"to":90668}}}}],[1503,{"pageContent":"558 \nChapter 7 \nDistance and Approximation \nFigure 1.8 \n-4 \nIf x ranges over all nonzero vectors in !R\nn\n, then x ranges over all unit vectors (i.e., the \nunit sphere) and the set of all vectors Ax determines some curve in !R\nn\n. For example, \nFigure 7.8 shows how the matrix A = \n[\n�  �\n] \naffects the unit circle in   IR\n2\n-it maps it \ninto an ellipse. With the Euclidean norm, the maximum value of II Ax II is clearly just \nhalf the length of the principal axis-in this case, 4 units. We express this by writing \nmax llAxll = 4. \nllxll=\n1 \nIn Section 7.4, we will see that this is not an isolated phenomenon. That is, \nII A xii \n, \nmax-\n11\n-\n1\n-\n1 \n= \nmaxllAxll \nx\n;eo \nx \nllx lH \nalways exists, and there is a particular unit vector y for which II A\nr \nII is maximum. \nNow we prove that llA II = max II Ax II defines a matrix norm. \nllxll=\n! \nTheorem 1.6 \nIf llxll is a  vectornorm on !R\nn\n, then llA II = max II Ax II defines a matrix norm onM\nnn \n-----------\nllxll=\nl \nthat is compatible with the vector norm that induces it.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":90670,"to":90722}}}}],[1504,{"pageContent":"llxll=\n! \nTheorem 1.6 \nIf llxll is a  vectornorm on !R\nn\n, then llA II = max II Ax II defines a matrix norm onM\nnn \n-----------\nllxll=\nl \nthat is compatible with the vector norm that induces it. \nProof (1) Certain ly,  llAxll 2: 0 for all vectors x, so, in particular, this inequality \nis \ntrue if llxll = 1. Hence, llA II = max II Ax II 2: 0 also. If llA II = 0, then we must have \nllxll=\nl \nllAxll = 0-and, hence, Ax = 0-for all x with llxll =  1. In particular, Ae; = 0   for \neach of the standard basis vectors e; in  !R\nn\n. But Ae; is just the ith column of A, so we \n� \nmust have A = 0. Conversely, if A = 0, it is clear that II A II = 0.   (Why?) \n(2) Let c be a scalar. Then \nllcAll  =  maxllcAxll = max\nl\nc\nl \nllAxll \nllxll=\nl \nllxll=\nl \nI \nc\nl \nmax II A xii \nllxll=\nl \nl\nc\nl \nllAll","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":90722,"to":90763}}}}],[1505,{"pageContent":"Section 7.2 \nNorms and Distance Functions \n559 \n(3) Let B be an n X n matrix and let y be a unit vector for which \nThen \nll\nA + B \nII \n= max \nII \n(A \n+ \nB\n)\nx\nll \n= \nII \n(A \n+ \nB\n)\ny\nll \nllxll=l \nll\nA + B\nii \nII \n(A\n+  B\n)\ny\nll \nll\nAy + By\nll \n::; \nll\nAy\nll \n+ \nll\nBy\nll \n::; \nll\nA\nll \n+ \nll\nB\nll \n� \n(Where does the second inequality come from?) Next, we show that our  definition is \ncompatible with the vector norm [property (  5)] and then use this fact to complete the \nproof that we have a matr  ix norm. \n(5)  Ifx \n= \n0, then the inequality \nII \nAx \nII \n::; \nll\nA 11 \nll\nx\nll \nis true, since both sides are zero. \nIf x * 0, then from the comments preceding this theorem, \nII \nA x\nii \nII \nA x\nii \nIGil\n::; \n�\n;; \nIGil \n= \nll\nA \nII \nHence, \nII \nA x\nii \n::; \nll\nA 11 \nll\nx\nll\n. \n(4)  Let z be a unit vector such that \nll\nAB\nll \n= max \nII \n(AB) x\nii\n= \nll\nABz\nll\n. Then \nllxll=l \nll\nAB\nll ll\nABz\nll \nll\nA (\nBz\n) \nII \n::; \nll\nA\nll ll\nBz\nll \n::; \nll\nA\nll ll\nB\nll ll\nz\nll \n= \nll\nA\nll ll\nB\nll \nby property (5) \nby property (5) \nThis completes the proof that \nII \nA \nII \n= max \nII \nAx \nII \ndefines a matrix norm on M\nnn \nthat is \nll\nx\nll=l","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":90765,"to":90921}}}}],[1506,{"pageContent":"ll \nll\nA (\nBz\n) \nII \n::; \nll\nA\nll ll\nBz\nll \n::; \nll\nA\nll ll\nB\nll ll\nz\nll \n= \nll\nA\nll ll\nB\nll \nby property (5) \nby property (5) \nThis completes the proof that \nII \nA \nII \n= max \nII \nAx \nII \ndefines a matrix norm on M\nnn \nthat is \nll\nx\nll=l \ncompatible with the vector norm that induces it. \nDefinition \nThe matrix norm \nII \nA \nII \nin Theorem 7.6 is   called the operator norm \ninduced by the vector norm \nII \nx\nii\n. \nThe term operator norm reflects the fact that a  matrix transformation arising from a \nsquare matrix is also called a linear operator. This norm is therefore a measure of the \nstretching capability of a linear operator. \nThe three most commonly used operator norms are those induced by the sum \nnorm, the Euclidean norm, and the max norm-namely, \nll\nA\nll\n1 \n=  max \nll\nAx\nll\n,\n, \nll\nA\nll\n2 \n=  max \nll\nAx\nll\nE\n, \nll\nA\nll\noo \n=  max \nll\nAx\nll\nrn \nllxll.= \n1 \nllxllE= \n1 \nllxll.= \n1 \nrespectively. The first and last of these turn out to have especially nice formulas that \nmake them very easy to compute.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":90921,"to":91016}}}}],[1507,{"pageContent":"560 \nChapter 7 \nDistance and Approximation \nTheorem 1.1 \nLet A be an n X n matrix with column vectors a; and row vectors A; for i = 1, ... , n. \na. \nll\nA\nll\n1 \n= \nJ\n=\n�\n.\nax\n_}\nll\na1\nll\nJ \n= \nJ\n=\n�\n.\na\n�J�\nl\na\niJ\nI\n} \nb. \nll\nA\nll\n,, \n= \ni\n=\n�\nax\n_}\nll\nA\n;\nll\n,\n} \n= \ni\n=��\nax\n,\nJ\n�\nl\na\niJ\nI\n} \nIn other words, \nll\nA 11\n1 \nis the largest absolute column sum, and \nll\nA \nll\noo is the largest \nabsolute row sum. Before we prove the theorem, let's look at an example to see how \neasy it is to  use. \nExample 1.19 \nLet \nA = \n[ \n: \n=\n� \n-\n�\n] \n-5 3 \nSolulion \nClearly, the largest absolute column sum is in the first column, so \nThe third row has the largest absolute row sum, so \nll\nA\nll\n,, \n= \nll\nA\nJ\n, \n=\nI\n-S\nI\n+ \nI\nl\nl\n+ \n1\n3\n1\n=9 \nWith reference to the definition 11 A 11\n1 \n= max 11Ax11 \n,\n, we  see that the maximum \nllxll.=l \nvalue of 10 is actually achieved when we take x = e\n1\n, for  then \nll\nAe\n1\nll\n, \n= \nll\na\n1\nll\n, \n= 10 = \nll\nA\nll\n1 \nFor \nll\nA\nll\n,, \n= max \nll\nAx\nll\nm\n,ifwe take \nllxllm=\nl \nwe obtain \nll\nAx\nll\nm \n� \nu \n=\n: \n-\n�\nr\n:\nJ \nm \n[ \n=\n�\n] \nm \n= max{\nl\n-2\n1\n, \nl\n-7\n1\n, \n1\n9\n1\n} \n=  9  = \nll\nA\nll\n,, \nWe will use these observations in proving Theorem 7.7.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":91018,"to":91203}}}}],[1508,{"pageContent":"Example 1.20 \nSection 7.2 \nNorms and Distance Functions \n561 \nProof Of Theorem 1.1 The strategy is the same in the case of bot  h the column sum \nand the row sum. If M represents the maximum value, we show that \nII A\nx \nII \n:s M \nfor all unit vectors x. Then we find a specific unit vector x for which equality occurs. \nIt is important to remember that for property (a) the vector norm is the sum norm \nwhereas for property (b) it is the max norm. \n(a)  To prove (a), let M  = . max {\nII \na\n1 II \nJ\n, the maximum absolute column sum, and let \n;=I, ... ' \nn \nll\nx\nll\n, \n= 1. Then \nl\nx\n1\nI \n+ ··· + \nl\nx\nn\nl \n= 1, so \nll\nA\nx\nll\n, \n= \nll\nx\n1\na\n1 \n+ \n· · · \n+ X\nn\na\nn\nll\n, \n:S \nl\nx\n1\nl \nll\na\n1\nll\n, \n+ \n· · · \n+ \nl\nx\nn\nl \nll\na\nn\nll\n, \n:S \nl\nx\n1\nI\nM + \n· · · \n+ \nl\nx\nn\nl\nM \n(\nl\nx\n1\nI \n+ \n· · · \n+ \nl\nx\nn\nl\n)M \n= \nl·\nM  = \nM \nIf the maximum absolute column sum occurs in column k, then with x = e\nk \nwe obtain \nll\nA\ne\nk\nll\n, \n= \nll\na\nk\nll\n, \n=  M \nTherefore, \nll\nA 11\n1 \n= max \nll\nAx\nll\n, \n= \nM \n= . max {\nII \na\n1\nll\n,\n}, as required. \nll\nx\nll,\n=\n1 \n;=l, ... \n, n \n(b)  The proof of property (b) is left as Exercise 32.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":91205,"to":91353}}}}],[1509,{"pageContent":"k \nwe obtain \nll\nA\ne\nk\nll\n, \n= \nll\na\nk\nll\n, \n=  M \nTherefore, \nll\nA 11\n1 \n= max \nll\nAx\nll\n, \n= \nM \n= . max {\nII \na\n1\nll\n,\n}, as required. \nll\nx\nll,\n=\n1 \n;=l, ... \n, n \n(b)  The proof of property (b) is left as Exercise 32. \nIn Section 7.4, we will discover a formula for the operator norm \nll\nA 11\n2\n, although \nit is not as computationally feasible as the formula for \nll\nA 11\n1 \nor \nll\nA \nI\nL,,. \nThe Condition Number of  a Matrix \nIn Exploration: Lies My Computer Told Me in Chapter 2, we encountered the notion \nof an ill-conditioned system of linear equations. Here is the  definition as it applies to \nmatrices. \nDefinition \nA matrix A is ill-conditioned if small changes in its entries can \nproduce large changes in the solutions to Ax = b. If small changes in the entries \nof A produce only small changes in the solutions to Ax = b, then A is called \nwell-conditioned. \nAlthough the definition applies to arbitrary matrices, we will restrict our atten­\ntion to square matrices. \nShow that A = \n[ \n� \n1 \n] is ill-conditioned. \n1.0005 \nSolution If we take b = \n[\n3 \n]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":91353,"to":91428}}}}],[1510,{"pageContent":"Although the definition applies to arbitrary matrices, we will restrict our atten­\ntion to square matrices. \nShow that A = \n[ \n� \n1 \n] is ill-conditioned. \n1.0005 \nSolution If we take b = \n[\n3 \n] \n, then the solution to Ax= bis x = \n[\n2\n1\n]\n. However, \n3.0010 \nif A changes to \nA' = \n[\n�  �.0010\n]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":91428,"to":91451}}}}],[1511,{"pageContent":"562 \nChapter 7 \nDistance and Approximation \n� \nthen the solution changes to x'  = [�]. (Check these assertions.) Therefore, a relative \nchange of 0.0005/1.0 005 \n= \n0.0005, or about 0.05%, causes a change of (2 -  1\n) / 1 = 1, \nor 100%, in x\n1 \nand \n(\n1 - 2)/2 = -0.5, or -50%, in x\n2\n• Hence, A is \nill-conditioned\n4 \nWe can  use  matrix norms to give a more precise way of determining when \na  matr  ix is ill-conditioned. Think of the change from A to A' as an error AA that, \nin turn, introduces an error Ax in the solution x to Ax = b. Then A' =A +AA and \nx' = x +Ax. In Example 7.20, \nAA = [\n0 \nO \n] and Ax = [ \n1\n] \n0 0.0005 \n-1 \nThen, since Ax = band A'x' = b, we have (A + AA) (x + Ax) = b. Expanding and \ncanceling off Ax = b, we obtain \nA(Ax) +  (AA)x +  (AA)(Ax) = 0 \nor  A(Ax) =  -  AA(x +  Ax) \nSince we are assuming that Ax = b has a solution, A must be invertible. Therefore, we \ncan rewrite the last equation as \nTaking norms of both sides (using a matr  ix norm that is compatible with a vector \nnorm), we have","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":91453,"to":91490}}}}],[1512,{"pageContent":"can rewrite the last equation as \nTaking norms of both sides (using a matr  ix norm that is compatible with a vector \nnorm), we have \nII Axil   11-A\n-'\n(\nAA\n)\nx' II =  llA\n-'\n(\nAA\n)\nx' II \ns; \nllA\n-'\n(\nAA\n) \n11 ll x' II \ns; llA\n-' ll ll AAll llx'll \n� \n(What is the justification for each step?) Therefore, \nThe expression llA\n-' \n11 llA II is called the condition number of A and is denoted by \ncond(A). If A is not invertible, we define cond(A) = oo. \nWhat are we to make of the inequality just above? The ratio II M II / llA II is a \nmeasure of the relative change in the matrix A,  which we are assuming to be small. \nSimilarly, II Ax II / II x' II is a measure of the relative error created in the solution to \nAx = b (although, in this case, the error is measured relative to the new solution, x', \nnot the original one, x). Thus, the inequality \nII Axil \nllAAll \n� s; cond\n(\nA\n)\nlfAll \n(\n1\n) \ngives an upper bound on how large the relative error in the solution can be in terms","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":91490,"to":91534}}}}],[1513,{"pageContent":"not the original one, x). Thus, the inequality \nII Axil \nllAAll \n� s; cond\n(\nA\n)\nlfAll \n(\n1\n) \ngives an upper bound on how large the relative error in the solution can be in terms \nof the relative error in the coefficient matrix. The larger the condition number, the \nmore ill-conditioned the matrix, since there is more \"room\" for the error to be large \nrelative to the solution.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":91534,"to":91548}}}}],[1514,{"pageContent":"Example 1.21 \nSection 7.2 \nNorms and Distance Functions \n563 \nRemarks \n• \nThe condition number of a matrix depends on the choice of norm. The most \ncommonly used norms are the operator norms llA 11\n1 \nand \nllA II\n,,,\n. \n• \nFor any norm, cond(A) 2: 1. (See Exercise 45.) \nFind the condition number of A = [ \n� \n1 \n] \nrelative to the oo-norm. \n1.0005 \nSolution We first compute \nA\n-\nI \n= [ \n2001 \n-2000 \n-2000] \n2000 \nTherefore, in the oo-norm (maximum absolute row sum), \nllAll\noo =  1 \n+  1.0005 = 2.0005 and \nllA\n-\n1\n11,,, =   2001  + \nl-20001 =  4001 \nso cond\n,,,\n(A) =  llA\n-\n1\nll\n\"'\nllAll\n\"' \n= 4001(2.0005) \n= \n8004. \nIt turns out that if the condition number is large relative to one compatible \nmatrix norm, it will be large relative to any compatible matrix norm. For example, \nit can be shown that for matrix A   in Examples 7.20 and 7.21, cond\n1 \n(A) \n= \n8004, \ncond\n2 \n(A) \n= \n8002 (relative to the 2-norm), and condp (A) \n= \n8002 (relative to the \nFrobenius norm). \nThe convergence of lteralive Melhods","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":91550,"to":91616}}}}],[1515,{"pageContent":"1 \n(A) \n= \n8004, \ncond\n2 \n(A) \n= \n8002 (relative to the 2-norm), and condp (A) \n= \n8002 (relative to the \nFrobenius norm). \nThe convergence of lteralive Melhods \nIn Section 2.5, we explored two iterative methods for solving a system oflinear equa­\ntions: Jacobi's method and the Gauss-Seidel method. In Theorem 2.9, we stated with­\nout proof that if A is a strictly diagonally dominant n X n matrix, then both of these \nmethods converge to the solution of Ax = b. We are now in a position to prove this \ntheorem. Indeed, one of the important uses of matrix norms is to  establish the con­\nvergence properties of various iterative methods. \nWe will deal only with Jacobi's method here. (The Gauss-Seidel method can be \nhandled using similar techniques, but it requires a bit more care.) The key is to  re­\nwrite the iterative process in terms of matrices. Let's revisit Example 2.3 7 with this in \nmind. The system of equations is \nso \nA = \n[\n7  -\nl\n] \nand \nb \n= \n[ \n5\n] \n3  -5 \n-7 \n(2)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":91616,"to":91653}}}}],[1516,{"pageContent":"564 \nChapter 7 \nDistance and Approximation \nWe rewrote Equation (2) as \nwhich is equivalent to \nor, in terms of matrices, \n5 + X\nz \nx =---\n1 \n7 \nX\nz \n= \nX\nz \n+ 5 \n[\n� \n-\n�\nJ \n[\n;\n:] \n[ \n_\n� �\nJ \n[\n;\n:] \n+ \n[ \n_\n�\nJ \n(3) \n(4) \n(5) \nStudy Equation (5) carefully: The matr  ix on the left-hand side contains the diagonal \nentries of A, while on the right-hand side we see the negative of the off-diagonal \nentries of A and the vector b. So, if we decompose A as \n[\n7 \n-\n1\nJ \n[\no o\nJ \n[\n7 \no\nJ \n[\no  -\n1\nJ \nA= = + + =L+D+U \n3  -5 3 0     0 -5 0   0 \nthen Equation (5) can be written as \nDx =  -\n(\nL +  U\n)\nx + b \nor, equivalently, \n(6) \nsince the matrix D is invertible. Equation (6) is the matrix version of Equation (3). \nIt is easy to see that we can do this in general: An n X n matrix A can be written as \nA = L + D + U, where D is the diagonal part of A and L and U are, respectively, the \nportions of A below and above the diagonal. The system Ax \n= \nb can then be writ­\nten in the form of Equation (6), provided Dis invertible-which it is if A is strictly \n�","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":91655,"to":91731}}}}],[1517,{"pageContent":"portions of A below and above the diagonal. The system Ax \n= \nb can then be writ­\nten in the form of Equation (6), provided Dis invertible-which it is if A is strictly \n� \ndiagonally dominant. (Why?) To simplify the  notation,let'sletM =   -D\n-\n1\n(L + U)   and \nc = D\n-\n1\nb so that Equation (6) becomes \nx =   Mx + c \n(7) \nRecall how we use this equation in Jacobi's method. We start with an initial vector \nx\n0 \nand plug it into the right-hand side of Equation (7) to get the first iterate x\n1\n-that \nis, x \n1 \n= \nMx\n0 \n+ c. Then we plug x\n1 \ninto the right-hand side of Equation (7) to get the \nsecond iterate x\n2 \n= \nMx\n1 \n+ c. In general, we have \nfork 2 0. For Example 2.37, we have \nM = -D\n-\n1\n(\nL +  U\n) \n=  -\nand \n[\n7 \no\nJ\n-\n1\n[\n0 \nl\nJ \n0 -5 \n3 0 \n(8)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":91731,"to":91787}}}}],[1518,{"pageContent":"so \nSection 7.2 \nNorms and Distance Functions \n565 \nX\ni\n= \n[\n1 \n�\n]\n[\n�\n] \n+ \n[t\nJ = \n[t\nJ \n= \n[\n�\n:\n:\n�\n�\n] \nX\nz \n= \n[\n� \n�\n] \n[\n0.714\n] \n+ \n[\n;\n] \n= \n[\n0.914\n] \n5 \n0  1.400 5 1.82\n9 \nand so on. (These are exactly the same calculations we did in Example 2.3\n7\n, but writ­\nten in matrix form.) \nTo show that Jacobi's method will converge, we need to show that the iterates x\nk \napproach the actual solution x of Ax= b. It is enough to show that the error vectors \nx\nk \n-  x approach the zero vector. From our calculations above, Ax = b is equivalent \nto x = Mx + c. Using Equation (8), we then have \nx\nk\n+\n1 \n-  x = \nMx\nk \n+  c  -   (\nMx \n+ c) \n=  M(\nx\nk \n-  x\n) \nNow we take the norm of both sides of this equation. (At this point, it is not impor­\ntant which norm we use as long as we choose a matrix norm that is compatible with \na vector norm.) We have \nll\nx\nk\n+J \n-  x\nii \n= \nll\nM\n(\nx\nk \n-  x\n)\nll\n:::; \nll\nM\nll ll\nx\nk \n-  x\nii \n(9) \nIfwe canshowthat \nll\nM\nll \n< 1, then we will have \nll\nx\nk\n+ i \n-  x\nii \n< \nll\nx\nk \n-  x\nii \nforallk \n2'. \n0, \nand it follows that 11 x\nk","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":91789,"to":91911}}}}],[1519,{"pageContent":"a vector norm.) We have \nll\nx\nk\n+J \n-  x\nii \n= \nll\nM\n(\nx\nk \n-  x\n)\nll\n:::; \nll\nM\nll ll\nx\nk \n-  x\nii \n(9) \nIfwe canshowthat \nll\nM\nll \n< 1, then we will have \nll\nx\nk\n+ i \n-  x\nii \n< \nll\nx\nk \n-  x\nii \nforallk \n2'. \n0, \nand it follows that 11 x\nk \n-  x 11 approaches zero, so the error vectors x\nk \n-  x approach \nthe zero vector. \nThe fact that strict diagonal dominance is defined in terms of the absolute values \nof the entries in the rows of a matrix suggests that the oo-norm of a matr  ix (the opera­\ntor norm induced by the max norm) is the one to choose. If A = [a\nij\n], then \n[\n- a\n,\n�\n/a\n,, \nM= \n. \n-a\nn1\n/a\nnn \n-a\n1\n2\nf \na\n11 \n0 \n-a\nn\n2\n/a\nnn \n- a\n,\n,\n/a\n., \nl \n-a\n2\nn\nf \na\n22 \n0 \n� \n(verify this), so, by Theorem 7.7, \nll\nM\nll\n\"' is the maximum absolute row sum of M. \nExample 1.22 \nSuppose it occurs in the kth row. Then \nl\n-a\nk, k\n+\n1\n1 \n1\n-a\nkn\nl \n+ \n+···+\n--\na\nkk \na\nkk \nl\na\nk\ni\nl \n+ \n· · · \n+ \nl\na\nk, k\n-\n1\n1 \n+ \nl\na\nk, k\n+\n1\nI \n+ \n· · · \n+ \nl\na\nkn\nl \n�����������������\n< \n1 \nI \na\nkk\nl \nsince A is strictly diagonally dominant.   Thus, \nll\nM\nll\n\"' \n< 1, so \nll\nx\nk \n-  x\nii \n--+ 0, as we \nwished to show. \nCompute \nll","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":91911,"to":92078}}}}],[1520,{"pageContent":"l\na\nk\ni\nl \n+ \n· · · \n+ \nl\na\nk, k\n-\n1\n1 \n+ \nl\na\nk, k\n+\n1\nI \n+ \n· · · \n+ \nl\na\nkn\nl \n�����������������\n< \n1 \nI \na\nkk\nl \nsince A is strictly diagonally dominant.   Thus, \nll\nM\nll\n\"' \n< 1, so \nll\nx\nk \n-  x\nii \n--+ 0, as we \nwished to show. \nCompute \nll\nM 1100 in Example 2.37 and use this value to find the number of iterations \nrequired to approximate the solution to three-decimal-place accuracy (after round­\ning) if the initial vector is x0 \n= 0.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":92078,"to":92131}}}}],[1521,{"pageContent":"566 \nChapter 7 \nDistance and Approximation \n.. \nSolulion We  have already computed M \n= [ � �\n], \nso  II MIL,, \n= \nt \n= \n0.6 < 1 \n(implying that Jacobi's method converges in Example 2.37, as we saw). The  approxi­\nmate solution x\nk \nwill be accurate to three decimal places if the error vector x\nk \n- x \nhas the property that each of its components is less than 0.0005 in absolute value. \n� \n(Why?) Thus, we need only guarantee that the maximum absolute component of \nx\nk \n- xis less than 0.0005. In other words, we need to find the smallest value of k such \nthat \nI \nExercises 1.2 \nII x\nk \n- xii \nm \n< 0.000\n5 \nUsing Equation (9) above, we see that \nllx\nk \n- xll\nm :S \nllMll\n\"'\nllx\nk\n-\n1 \n- xll\nm :S \nllMll\n;\nllx\nk\n-\n2 \n- xll\nm :S · · · :S \nllMll\n�\nllx\no \n- xll\nm \nNow llMll\n\"' \n= \n0.6 and  llx\no \n- xll\nm \n= \nllx\no \n- X\n1\n11\nm \n= \nllx\n1\n11\nm \n=\nII\n[\n�\n::\n�\n�\n] \nt \n= \n1.4, \nso \n(If we knew the exact solution in advance, we could use it instead of x\n1\n. In practice, \nthis is not the case, so we use an approximation to the solution, as we have done here.)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":92133,"to":92227}}}}],[1522,{"pageContent":"=\nII\n[\n�\n::\n�\n�\n] \nt \n= \n1.4, \nso \n(If we knew the exact solution in advance, we could use it instead of x\n1\n. In practice, \nthis is not the case, so we use an approximation to the solution, as we have done here.) \nTherefore, we need to find k such that \n(\n0.6\n)\nk\n(\nl.4\n) \n< 0.0005 \nWe can solve this inequality by taking logarithms (base 10) of both sides. We have \nlog\n10\n((\n0.6\n)\nk\n(\nl.4\n)) \n< log\n10\n(\n5 X 10\n-\n4\n) \n==> klog\n10\n(\n0.6\n) \n+ \nlog\n10\n(\n1.4\n) \n< log\n10\n5 - 4 \n::::? -0.222k \n+ \n0.146 < -3.301 \n::::? k > 15.5 \nSince k must be an integer, we can therefore conclude that k \n= \n16 will work and \nthat  16 iterations of Jacobi's method will give us three-decimal-place accuracy in \nthis example. (In fact, it appears from our calculations in Example 2.37 that we \nget this degree of accuracy sooner, but our goal here was only to come up with an \nestimate.) \nIn Emdm 1-3, /,tu� [ �:l andv � [-H \n4. (a) What does d\n,\n(u, v) measure? \n(b) \nWhat does d\nm\n(u, v) measure? \n1. Compute the Euclidean norm, the sum norm, and the \nmax norm of u. \n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":92227,"to":92304}}}}],[1523,{"pageContent":"estimate.) \nIn Emdm 1-3, /,tu� [ �:l andv � [-H \n4. (a) What does d\n,\n(u, v) measure? \n(b) \nWhat does d\nm\n(u, v) measure? \n1. Compute the Euclidean norm, the sum norm, and the \nmax norm of u. \n2\n. Compute the Euclidean norm, the sum norm, and the \nmax norm of v. \n3. Computed( u, v) relative to the Euclidean norm, the \nsum norm, and the max norm. \nIn Exercises 5 and 6, let u \n= \n[ 1  0 \nv \n= \n[O 1  1  0  1   1 l]� \n1  0  0 l] \nr\nand \n5. Compute the Hamming norms of u and v. \n6. Compute the Hamming distance between u and v. \n7. (a) For whichvectorsvis llvll\nE \n= \nllvll\nm\n?Explain \nyour answer.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":92304,"to":92337}}}}],[1524,{"pageContent":"(b) For which vectors vis \nll\nv\nll\n, \n= \nll\nv\nll \nm\n? Explain your \nanswer. \n(c\n) \nFor which \nvectors \nvis \nll\nv\nll\n, \n= \nll\nv\nll\nm \n= \nll\nv\nll\nE\n? \nExplain your answer. \n8. (a) Under what conditions on u and vis \nII \nu +   vii \nE \n= \nII \nu \nII E \n+ \nII \nv \nII E\n? Explain your answer. \n(b) Under what conditions on u and vis \nllu +vii\n,\n=  llull\n,\n+llvll\n,\n?Explain\ny\nour answer\n. \n(c) Under what conditions on u and vis \nII \nu +   vii \nm \n= \nll\nu\nll \nm \n+ \nll\nv\nll \nm\n? Explain \nyour \nanswer. \n9. \nShow that \nfor all v in \nIKr, II \nv \nII m \n::::; II \nvii \nE· \n10. Showt\nhat for all v in ll�r\n, \nll\nv\nll\nE  s \nll\nv\nll\n,\n. \n11. \nShow that \nfor all vin !R\nn\n, \nll\nv\nll\n,  ::::; \nn\nll\nv\nll\nm· \n12. \nShow that for \nall v in !R\nn\n, \nll\nv\nll\nE::::; \nVn\nll\nv\nll \nm· \n13. Draw the unit circles in IR\n2 \nrelative to the sum norm \nand the max norm. \n14. By showing that the identity of Exercise 33 in \nSection 7.1 fails, show that the sum norm does not \narise from any inner product. \nIn Exercises 15-18, prove that \nII II \ndefines a norm on the \nvector space V \n15. V= IR\n2\n, \n11\n[\n�\n]\n11 \n= max{\nl\n2a\nl\n, \nl\n3b\nl\n} \n16. V = M\nm n ' \nll\nA\nll \n= \nm\na\nx\n{\nl\na\ni\nj\nl\n} \n1\n,\nj \n�","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":92339,"to":92509}}}}],[1525,{"pageContent":"arise from any inner product. \nIn Exercises 15-18, prove that \nII II \ndefines a norm on the \nvector space V \n15. V= IR\n2\n, \n11\n[\n�\n]\n11 \n= max{\nl\n2a\nl\n, \nl\n3b\nl\n} \n16. V = M\nm n ' \nll\nA\nll \n= \nm\na\nx\n{\nl\na\ni\nj\nl\n} \n1\n,\nj \n�\n17. \nv \n= \nC{b\n[ O, \nI\nL \nll\nJ\nll \n= \nr \nI\nJ\n(\nx\n)\nI \ndx \n0 \n18. \nII! II \n=   max \nI\nf \n(\nx\n) \nI \nO:sx:s \n1 \n19. Prove Theorem 7.5\n(\nb\n)\n. \nIn \nExercises 20-25, \ncompute \nll\nA \nII p, \nll\nA \nII \n1\n, and \nll\nA \nll\noo-\n20. A = \n[\n� \n�\n] \n21. A= \n[\n_\n� \n-\n�\n] \n22. A = \n[\n_\n� \n-\n�\nJ \n23. A = \n[\n: \n3 \n�\n] \nu \n-5 \n-\n:\n] \n[\n: \n-2 \n-\n�\n] \n24. A = \n25. A = \n-1 \n-4 \n-3 \nSection 7.2 \nNorms and Distance Functions \n561 \nInExercises26-31,findvectorsxandy with \nll\nx\nll\n, \n= 1 and \nll\nr\nll\nm \n= 1 such that \nll\nA\nll\n1 \n= \nll\nAx\nll\n,\nand \nll\nA\nll\noo \n= \nll\nA\nr\nll\nm > \nwhere A is the matrix in th e given exercise. \n26.  Exercise 20 \n27. Exercise 21 \n28. Exercise 22 \n29. Exercise 23 \n30. Exercise 24 \n31. Exercise 25 \n32. Prove Theorem 7.7\n(\nb\n)\n. \n33. (a) If \nII \nA \nII \nis an operator norm, prove that \nII \nI\nll \n= 1, \nwhere I is an identity matrix. \n(b) Is there a vector norm that induces the \nFrobenius norm as an operator norm? Why \nor why not? \n34. Let \nll\nA \nII","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":92509,"to":92705}}}}],[1526,{"pageContent":"II \nA \nII \nis an operator norm, prove that \nII \nI\nll \n= 1, \nwhere I is an identity matrix. \n(b) Is there a vector norm that induces the \nFrobenius norm as an operator norm? Why \nor why not? \n34. Let \nll\nA \nII \nbe a matrix norm that is compatible with a \nvector norm 11x11. Prove that 11 A 11 2: \nI\n,\\ \nI \nfor every \neigenvalue ,\\ of A. \nIn Exercises 35-40, find cond\n1\n(A ) \nand cond\noo\n(A\n)\n. State \nwhether th e given matrix is ill-conditioned. \n35. A = \n[\n! \n�\n] \n36. A= \n[\n_\n� \n-\n!\n] \n[\n� \n�\n.99\n] \n[ \n150 \n200\n] \n37. A = \n38. A= \n3001 \n4002 \n[\ni \n1 \n�\n] \n[\ni \n1 \n!\nl \n2 \n39. A = \n5 \n40. A= \n1 \n3 \n0 \n1 \n4 \n41. Let A = \n[\n1\n1 \nk\nl\n]\n. \n(a) Find a formula for cond\n00\n(A) in terms of k. \n(b) What happens to cond\n00\n(A) as k approaches 1? \n42. Consider the linear system Ax = b, where A is invert­\nible. Suppose an error Lib changes b to b' = b + Lib. \nLet x' be the solution to the new system; that is, \nAx' = b'. Let x' = x +Ax so that Ax represents \nthe resulting error in the solution of the system. \nShow that \nll\nii\nx\nll \nll\nii\nb\nll \nW\n::::; cond(A)lfbll \nfor any compatible matrix norm.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":92705,"to":92811}}}}],[1527,{"pageContent":"Ax' = b'. Let x' = x +Ax so that Ax represents \nthe resulting error in the solution of the system. \nShow that \nll\nii\nx\nll \nll\nii\nb\nll \nW\n::::; cond(A)lfbll \nfor any compatible matrix norm. \n43. Let A \n= \n[10    10] \nand b = \n[100]\n. \n10 \n9 \n99 \n(a) Compute cond\noo\n(A). \n(b) Suppose A is changed to \nA' = \n[ \n1 O \n10 \n10\n] \n.How \n11 \nlarge a relative change can this change produce in \nthe solution to Ax = b? [Hint: Use inequality (1) \nfrom this section.]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":92811,"to":92848}}}}],[1528,{"pageContent":"568 \nChapter 7 \nDistance and Approximation \n(c) Solve the systems using A and A' and determine \nthe actual relative error. \n(d) Suppose bis changed to \nb' = [ \n100\n]\n. How large a \n101 \nrelative change can this  change produce  in the \nsolution to Ax = b? [Hint: Use Exercise 42.] \n(e) Solve the systems using b and b' and determine \nthe actual relative error. \n44. Let A � [ � \n_ \n� � l '\n\"\nd b � m \n(a) Compute cond\n1\n(A). \n(b) Suppose A is changed to A' = \n[ \nl\n� \n1 \n5 \n-1 \nnHow \nlarge a relative change can this change produce in \nthe solution to Ax = b? [Hint: Use inequality (1) \nfrom this section.] \n(c) Solve the systems using A and A' and determine \nthe actual relative error. \n(d) Suppose b iS <h<mg<d to b' � []How l\"g\" \nrelative change can this change produce in the \nsolution to Ax = b? [ Hint: Use Exercise 42.] \n(e) Solve the systems using b and b' and determine \nthe actual relative error. \n45. Show that if A is an invertible matrix, then \ncond\n(\nA\n) \n2:: 1 with respect to any matrix norm.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":92850,"to":92896}}}}],[1529,{"pageContent":"(e) Solve the systems using b and b' and determine \nthe actual relative error. \n45. Show that if A is an invertible matrix, then \ncond\n(\nA\n) \n2:: 1 with respect to any matrix norm. \n46. Show that if A and B are invertible matrices, then \ncond(AB) :s cond(A) cond(B) with respect to any \nmatrix norm. \n47. Let A be an invertible matrix and let A\n1 \nand A\nn \nbe the \neigenvalues with the largest and smallest absolute val­\nues, respectively. Show that \nI\nA\n1\nI \ncond (A) 2:: -\nI\nA\nn\nl \n[Hint: See Exercise 34 and Theorem 4.18(b) in \nSection4.3.] \ncAs \nIn Exercises 48-51, write the given syste m in th e form \nof Equation (7). Then use th e method of Example 7.22 to \nestimate the number of iterations of Jacobi's method that \nwill be needed to approximate the solution to three-decimal­\nplace accuracy. (Use Xo = 0.) Compare your answer with \nthe solution computed in th e given exercise from Section 2.5. \n48. Exercise 1, Section 2.5 \n50. Exercise 4, Section 2.5 \n49. Exercise 3, Section 2.5 \n51. Exercise 5, Section 2.5","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":92896,"to":92935}}}}],[1530,{"pageContent":"the solution computed in th e given exercise from Section 2.5. \n48. Exercise 1, Section 2.5 \n50. Exercise 4, Section 2.5 \n49. Exercise 3, Section 2.5 \n51. Exercise 5, Section 2.5 \nExercise 52( c) refers to th e Leontief model of an open econ­\nomy, as discussed in Sections 2.4 and 3.7. \n52. Let A be an n X n matrix such that \nll\nA\nll \n< 1, where \nthe norm is either the sum norm or the max norm. \n(a) Prove that A\nn\n---+ \n0 as n\n---+ \noo\n. \n(b)  Deduce from (a) that I - A   is invertible and \n(\nI - A)\n-\n1 \n=I+ A \n+ \nA\n2 \n+ \nA\n3 \n+ \n·   ·   · \n[ Hint: See the proof of Theorem 3.34.] \n( c) Show that (b) can be used to prove Corollaries \n3.35 and 3.36. \nII \nLeast Squares Approximation \nIn many branches of science, experimental data are used to infer a mathematical rela­\ntionship among the variables being measured. For example, we might measure the height \nof a tree at various points in time and try to deduce a function that expresses the tree's","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":92935,"to":92976}}}}],[1531,{"pageContent":"tionship among the variables being measured. For example, we might measure the height \nof a tree at various points in time and try to deduce a function that expresses the tree's \nheight h in terms of time t. Or, we might measure the size p of a population over time and \ntry to find a rule that relates p to t. Relationships between variables are also of interest in \nbusiness; for example, a company producing widgets may be interested in knowing the \nrelationship between its total costs c and the number n of widgets produced. \nIn each of these examples, the data come in the form of two measurements: \none for the independent variable and one for the (supposedly) dependent variable. \nThus, we have a set of data points \n(\nx;, y;\n)\n, and we are looking for a function that best \napproximates the relationship between the independent variable x and the dependent \nvariable y. Figure 7.9 shows examples in which experimental data points are plotted, \nalong with a curve that approximately \"fits\" the data.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":92976,"to":92991}}}}],[1532,{"pageContent":"y \ny \nSection 7.3 \nLeast Squares Approximation \n569 \ny \nfigure 1.9 \nCurves of\"best fit\" \nRoger Cotes ( 1682-1716) was an \nEnglish mathematician who, while \na fellow at Cambridge, edited the \nsecond edition of Newton's Prin­\ncipia. Although he published little, \nhe made important discoveries in \nthe theory oflogarithms, integral \ncalculus, and numerical methods. \nThe method of least squares, which we are about to consider, is attributed to \nGauss. A new asteroid, Ceres, was discovered on New Year's Day,  1801, but it disap­\npeared behind the sun shortly after it was observed. Astronomers predicted when \nand where Ceres would reappear, but their calculations differed greatly from those \ndone, independently, by Gauss. Ceres reappeared on December 7, 1801, almost ex­\nactly where Gauss had predicted it would be. Although he did not disclose his meth­\nods at the time, Gauss had used his least squares approximation method, which he \ndescribed in a paper in 1809. The same method was actually known earlier; Cotes","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":92993,"to":93016}}}}],[1533,{"pageContent":"ods at the time, Gauss had used his least squares approximation method, which he \ndescribed in a paper in 1809. The same method was actually known earlier; Cotes \nanticipated the method in the early 18th century, and Legendre published a paper \non it in 1806. Nevertheless, Gauss is generally given credit for the method of least \nsquares approximation. \nWe begin our exploration of approximation with a more general result.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":93016,"to":93021}}}}],[1534,{"pageContent":"510 \nChapter 7 \nDistance and Approximation \nTheorem 1.8 \nThe Best Approximation Theorem \nIn the sciences, there are many problems that can be phrased generally as \"What is \nthe best approximation to X of type Y?\" X might be a set of data points, a function, \na vector, or many other things, while Y might be a particular type of function, a ve c­\ntor belonging to a certain vector space, etc. A typical example of such a problem is \nfinding the vector win a  subspace W of a vector space V that best approximates (i.e., \nis closest to) a given vector v in V. This problem gives rise to the following definition. \nDefinition \nIf Wis a  subspace of a normed linear space V and ifv is a vector \nin V, then the best approximation to v in Wis the vector v in W such that \nll\nv -\nvii \n< \nll\nv - w\nll \nfor every vector win W  different from v. \nIn IR\n2 \nor IR\n3\n, we are used to th  inking of \"shortest distance\" as corresponding to \n\"perpendicular distance:' In algebraic terminology, \"shortest distance\" relates to the","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":93023,"to":93050}}}}],[1535,{"pageContent":"In IR\n2 \nor IR\n3\n, we are used to th  inking of \"shortest distance\" as corresponding to \n\"perpendicular distance:' In algebraic terminology, \"shortest distance\" relates to the \nnotion of orthogonal projection: If Wis a  subspace of !R\nn \nand vis a  vector in !R\nn\n, then \nwe expect projw(v) to be the vector in W that is closest to v (Figure 7.10). \nSince orthogonal projection can be defined in any inner prod uct space, we have \nthe following theorem. \nll\nv \n-\nvii \nw \nw \nFigure 1.10 \nIf v = projw\n(\nv\n)\n, then \nll\nv \n-\nv\nii  < \nll\nv - w\nll \nfor all\nw =fa \nv \nThe Best Approximation Theorem \nIf Wis a  finite-dimensional subspace of an inner product space V and if vis a  vec­\ntor in V, then projw (v) is the best approximation to v in W. \nProof Let w be a vector in W different from projw(v).   Then projw(v)  - w is also \nin W, so v -  projw(v) \n= \nperpw(v) is orthogonal to projw(v) -   w, by   Exercise 43 in \nSection 7.1. Pythagoras' Theorem now implies that \nll\nv -  proj\nw\n(\nv\n)\nll\n2 \n+ \nll\nproj\nw\n(\nv\n) \n- w\nll\n2 \n= \nll\n(\nv -  proj\nw\n(\nv\n)) \n+ \n(\nproj\nw\n(\nv\n) \n- w\n)\nll","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":93050,"to":93130}}}}],[1536,{"pageContent":"= \nperpw(v) is orthogonal to projw(v) -   w, by   Exercise 43 in \nSection 7.1. Pythagoras' Theorem now implies that \nll\nv -  proj\nw\n(\nv\n)\nll\n2 \n+ \nll\nproj\nw\n(\nv\n) \n- w\nll\n2 \n= \nll\n(\nv -  proj\nw\n(\nv\n)) \n+ \n(\nproj\nw\n(\nv\n) \n- w\n)\nll\n2 \n= \nll\nv - w\nll\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":93130,"to":93174}}}}],[1537,{"pageContent":"Example 1.23 \nSection 7.3 \nLeast Squares Approximation \n511 \nas Figure 7.10 illustrates. However, \nll\nproj\nw\n(\nv\n) \n- w\nll\n2 \n> 0, since w * projw(v),  so \nll\nv -  proj\nw\n(\nv\n)\nll\n2 \n< \nll\nv -  proj\nw\n(\nv\n)\nll\n2 \n+ \nll\nproj\nw\n(\nv\n) \n- w\nll\n2 \n= \nll\nv - w\nll\n2 \nor, equivalently, \nll\nv -  proj\nw\n(\nv\n)\nll \n< \nll\nv - w\nll \nLet u\n, \n� u J u, � [ -n and F m Hnd th< b\"t apprnrimation to v in th< \nplane W = span(u\n1\n, u\n2\n) and find the Euclidean distance from v to W. \nSolution The vector in W that best approximates vis projw(v).   Since u\n1 \nand u\n2 \nare \northogonal, \nproj\nw\n(v) = \n_I\n_ \nU1 \n+ \n_\n2\n_ \nU\n2 \n(u·v) (u·v) \nU1 \n• \nU1 \nU\n2 \n• \nU\n2 \nThe distance from v to Wis the  distance from v to the point in W closest to v. But this \ndistance is just \nll\nperp\nw\n(\nv\n)\nll \n= \nll\nv -  proj\nw\n(\nv\n)\nll\n· \nWe \ncompu\nte \nso \nll\nv -  proj\nw\n(v)\nll \n= \n\\/\n0\n2 \n+ (\nlf-\n)\n2 \n+ \nes1\n)\n2 \n=\nVilt-\n= 12\nVs\n/5 \nwhich is the distance from v to W. \nIn Section 7 .5, we will look at other examples of the Best Approximation Theorem \nwhen we explore the problem of approximating functions.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":93176,"to":93316}}}}],[1538,{"pageContent":")\n2 \n+ \nes1\n)\n2 \n=\nVilt-\n= 12\nVs\n/5 \nwhich is the distance from v to W. \nIn Section 7 .5, we will look at other examples of the Best Approximation Theorem \nwhen we explore the problem of approximating functions. \nRemark The orthogonal projection of a vector v onto a subspace W is defined \nin terms of an orthogonal basis for W. The Best Approximation Theorem gives us an \nalternative proof that projw(v) does not depend on the choice of this basis, since there \ncan be only one vector in W that is closest to v-namely, projw(v). \nLeasl Squares Approximalion \nWe now turn to the problem of finding a curve that \"best fits\" a set of data points. Be­\nfore we can proceed, however, we need to define what we mean by \"best fit:' Suppose \nthe data points \n(\n1, 2\n)\n, \n(\n2, 2\n)\n, and \n(\n3, 4\n) \nhave arisen from measurements taken during \nsome experiment. Also suppose we have reason to believe that the x and y values are \nrelated by a   linear function; that is, we expect the points to   lie on some line with equa­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":93316,"to":93351}}}}],[1539,{"pageContent":"some experiment. Also suppose we have reason to believe that the x and y values are \nrelated by a   linear function; that is, we expect the points to   lie on some line with equa­\ntion y = a + bx. If our measurements were accurate, all three points would satisfy this \nequation and we would have \n2=a+b·l 2=a+b·2 4=a+b·3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":93351,"to":93355}}}}],[1540,{"pageContent":"512 \nChapter 7 \nDistance and Approximation \nThis is a system of three linear equations in two variables: \na+ b \n= \n2 \na  + 2b \n= \n2 \nor \na+ 3b \n= \n4 \nUnfortunately, this system is inconsistent (since the three points do not  lie on a \nstraight line). So we will settle for a line that comes \"as close as possible\" to passing \nthrough our points. For any line, we will measure the vertical distance from each data \npoint to the line (representing the errors in they-direction), and then we will try to \nchoose the line that minimizes the total error. Figure 7 .11 illustrates the situation. \ny \n6 \n5 \ny \n=a+ bx \n4 \n3 \n2 \n�-1-�+-�+--�1-----i�--+�--+�--+---+X \n-1 \n2 \n3 \n4 \n5 \n6 \n-1 \nFigure 1.11 \nFinding the line that minimizes ei + e� + e� \nIf the errors are denoted by 81, 8\n2\n, and 8\n3\n, then we can form the error vector \nWe want e to be as small as possible, so \nllell \nmust be as close to zero as possible. Which \nnorm should we use? It turns out that the familiar Euclidean norm is the best choice.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":93357,"to":93402}}}}],[1541,{"pageContent":"We want e to be as small as possible, so \nllell \nmust be as close to zero as possible. Which \nnorm should we use? It turns out that the familiar Euclidean norm is the best choice. \n(The sum norm would also be a sensible choice, since \nII \nell\ns \n= \nI \n8\n1 \nI \n+ \nI \n8\n2 \nI \n+ \nI \n8\n3 \nI \nis the \nactual sum of the errors in Figure 7 .11. However, the absolute value signs are hard to \nwork with, and, as you will soon see, the choice of the Euclidean norm leads to some \nvery nice formulas.) So we are going to minimize \nllell \n= \nV \n8\ni \n+ \n8\n� \n+ \n8\n� \nor, equivalent\nly, \nllell\n2 \n= \n8\ni \n+ \n8\n� \n+ \n8\n� \nThis is where the term \"least squares\" comes from: We need to find the smallest sum \nof squares, in the sense of the foregoing equation. The number \nllell \nis called the least \nsquares error of the approximation.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":93402,"to":93457}}}}],[1542,{"pageContent":"Example 1.24 \nSection 7.3 \nLeast Squares Approximation \n513 \nFrom Figure 7.11, we also obtain the following formulas for 8\n1\n, 8\nz\n, and 83 in our \nexample: \n81 =  2  -( a \n+ \nb · 1\n) \n8\nz \n=  2  -( a \n+ \nb · 2\n) \n83 \n= 4 -(\na \n+ \nb · 3\n) \nWhich of the following lines gives the smallest least squares error for the data points \n(1, 2), (2, 2), and \n(\n3, 4\n)\n? \n(a) y = 1 + x \n(b) y = -2 + 2x \n(c) y = \n� \n+ x \nSolulion \nTable 7.1 shows the necessary calculations. \nTable 1.1 \n8\n1 \n8\nz \n83 \ny=I+x \n2-(1\n+\n1) = 0 \n2-(1\n+\n2)=-\nl \n4 -  (1 \n+ \n3\n) \n= \n0 \ny = -2 + 2x \n2  -(-2 \n+ \n2) =  2 \n2  -(-2 \n+ \n4\n) \n= 0 \n4 -  (-2 \n+ \n6\n) = 0 \n2 \n-\n(\nt + \n1\n) \n= \nt \n2 \n-\n(\nt + \n2\n) \n= \n-\nt \n4 -\n(\nt + \n3\n) \n= \nt \n8\ni \n+ \nd \n+ \n8\n� \noz \n+ \n( -1) z \n+ \noz = \n1 \n2z \n+\noz\n+ \noz= 4 \n(t)Z \n+ \n( \n-\nt\n)Z \n+ \n(t)Z \n= \nt \nllell \n2 \nv1=\n0.81\n6 \ny \nFigure 1.12 \nWe see that the line y = \nt \n+ \nx produces the smallest least squares error among \nthese three lines. Figure 7.12 shows the data points and all three lines.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":93459,"to":93594}}}}],[1543,{"pageContent":"514 \nChapter 7 \nDistance and Approximation \nIt turns out that the line y = � + x in Example 7 .24 gives the smallest least squares \nerror of any line, even though it passes through none of the given points. The rest of \nthis section is devoted to illustrating why this is so. \nIn general, suppose we haven data poi nts (x\n1\n, y\n1\n), ... , (x\nn\n, Y\nn\n) and a line y = a + bx. \nOur error vector is \n[\n�\nl \nl \ne= \n: \nS\nn \nwheres; = y; - (a + bx;). The line y = a + bx that minimizes sf + ·  ·  · + s� is called \nthe least squares approximating line (or the line of best fit) for the points (x\n1\n, y\n1 \n), ... , \n(x\nn\n,Y\nn\n). As noted prior to Example 7.24, we can express this problem in matrix form. If \nthe given points were actually on the line y = a + bx, then the n linear equations \na+ bx\nn \n= \nYn \nwould all be true (i.e., the system would be consistent). Our interest is in the case \nwhere the points are not collinear, in which case the system is inconsistent. In matr  ix \nform, we have \nwhich is of the form Ax = b, where \nA = \n[ \n: :: \nl","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":93596,"to":93643}}}}],[1544,{"pageContent":"where the points are not collinear, in which case the system is inconsistent. In matr  ix \nform, we have \nwhich is of the form Ax = b, where \nA = \n[ \n: :: \nl\n' \nx = \n[\n: l \nh \n= \n[\n;: \nl \n1 X\nn \nY\nn \n� \nThe error vector e is just b -Ax (check this), and we want to minimize \nll\ne\nll\n2 \nor, equiv­\nalently, \nll\ne\nll\n· We can therefore rephrase our problem in terms of matrices as follows. \nDefinition \nIf A is an m x n matrix and bis in !R\nm\n, a least squares solution of \nAx = b   is a vector x in !R\nn \nsuch that \nll\nb \n-\nA\nx\nil \n:::; \nll\nb \n-\nAx\nil \nfor all x in !R\nn\n.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":93643,"to":93696}}}}],[1545,{"pageContent":"Theorem 1.9 \nSection 7.3 \nLeast Squares Approximation \n515 \nSolulion of lhe Leasl Squares Problem \nAny vector of the form Ax is in  the column space of A, and as x varies over all vec­\ntors in IJ�r, Ax varies over all vectors in col(A). A least squares solution of Ax = bis \ntherefore equivalent to a vector yin col(A) such that \nli\nb \n-\nY\nll \n::; \nli\nb \n-\nY\nll \nfor all y in col(A). In other words, we need the closest vector in col(A) to b. By the \nBest Approximation Theorem, the vector we want is the orthogonal projection of b \nonto col(A). Thus, ifx is a least squares solution of Ax = b, we have \n(1) \nIn order to find x, it would appear that we  need to first compute projcol\n(\nA\n)\n(b) and then \nsolve the system ( 1). However, there is a better way to proceed. \nWe know that \nb -\nAx = b -\nproj\nco\nl(\nA\n)\n(\nb\n) \n= perp\nco\nl(\nA\n)\n(\nb\n) \nis  orthogonal  to  col(A).  So  b - Ax is  in \n(\ncol\n(\nA\n)) \n_j_ \n= null\n(\nA\nT)\n.  Therefore \nA\nT \n(b - Ax) = O, which, in turn, is equivalent to A \nT\nb - A\nT \nAx = O or","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":93698,"to":93764}}}}],[1546,{"pageContent":"b -\nAx = b -\nproj\nco\nl(\nA\n)\n(\nb\n) \n= perp\nco\nl(\nA\n)\n(\nb\n) \nis  orthogonal  to  col(A).  So  b - Ax is  in \n(\ncol\n(\nA\n)) \n_j_ \n= null\n(\nA\nT)\n.  Therefore \nA\nT \n(b - Ax) = O, which, in turn, is equivalent to A \nT\nb - A\nT \nAx = O or \nThis represents a system of equations known as the normal equations for x. \nWe have just established that the solutions of the normal equations for x are pre­\ncisely the least squares solutions of Ax = b. This proves the first part of the following \ntheorem. \nThe Least Squares Theorem \nLet A be an m X n matrix and let b be in !R\nm\n. Then Ax = b   always has at least one \nleast squares solution x. Moreover: \na. xis a  least squares solution of Ax = b  if and only ifx is a solution of the normal \nequations A\nT \nAx = A \nT\nb. \nb. A has linearly independent columns if and only if A\nT \nA is invertible. In this case, \nthe least squares solution of Ax = b is unique and is given by \nProof We have already established property (a). For property (b), we note that the","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":93764,"to":93820}}}}],[1547,{"pageContent":"T \nA is invertible. In this case, \nthe least squares solution of Ax = b is unique and is given by \nProof We have already established property (a). For property (b), we note that the \nn columns of A are linearly independent if and only if rank(A) = n. But this is true if \nand only if A\nT \nA is invertible, by Theorem 3.28. If A\nT \nA is invertible, then the unique \nsolution of A\nT \nAx = A \nT\nb is clearly x =   (A\nT \nA)\n-\n1 \nA \nT\nb.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":93820,"to":93841}}}}],[1548,{"pageContent":"516 \nChapter 7 \nDistance and Approximation \nExample 1.25 \nFind a least squares solution to the inconsistent system Ax = b, where \nSolulion \nWe compute \nA\nT\nA = \n[\n� \n2 \n-2 \nand \nA\nr\nb= \n[\n� \n-\n:\ni\n[ \n� \n-�: \n-\n1 \n1 \n2 \n-\n:\ni\n[\n�\n] \n-2 \nThe normal equations A\nr \nAx = A \nT\nb are just \n[\n� \n3\n�\n] \n[\n1!\n] \nwhich yield  x = \n[ \n1\n]\n. The  fact  that this solution is unique was  guaranteed by \nTheorem 7.9(b), since the columns of A are clearly linearly independent. \nRemark We could have phrased Example 7.25 as follows: Find the best approxi­\nmation to bin the  column space of A. The resulting equations give the system Ax = b \n� \nwhose least squares solution we just found. (Verify this.) In this case, the components \nof x are the coefficients of that linear combination of the columns of A that produces \nthe best approximation to b-namely, \nExample 1.26 \nThis is exactly the result of Example 7.23. Compare the two approaches. \nFind the least squares approximating line for the data points (1, 2), (2, 2), and (3, 4) \nfrom Example 7.24. \nSolulion","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":93843,"to":93909}}}}],[1549,{"pageContent":"This is exactly the result of Example 7.23. Compare the two approaches. \nFind the least squares approximating line for the data points (1, 2), (2, 2), and (3, 4) \nfrom Example 7.24. \nSolulion \nWe have already seen that the corresponding system Ax = b is","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":93909,"to":93913}}}}],[1550,{"pageContent":"Section 7.3 \nLeast Squares Approximation \n511 \nwhere y = \na + bx is the line we seek. Since the columns of A are clearly linearly inde­\npendent, there will be a unique least squares solution, by part (b) of the Least Squares \nTheorem. We compute \n2 2 \nHence, we can solve the normal equations AT A\nx \n= A Tb, using Gaussian elimination \nto obtain \nSo x= \n[\n[\n]\n, from which we see that \na \n= t, b = \nsquares approximating line: y = t + x. \n1 are the coefficients of the least \nThe line we just found is the line in Example 7.24(c), so we have justified our \nclaim that this line produces the smallest least squares error for the data points (1, 2), \n(2, 2), and (3, 4). Notice that ifx is a least squares solution of Ax = b, we may com­\npute the least squares error as \nll\ne\nll \n= \nll\nb \n-\nA\nx\nil \nSince Ax = projcol(A\n)\n(b ), this is just the length of perpcol(A\n)\n(b )-that is, the distance \nfrom b to the column space of A. In Example 7.26, we had \nso, as in   Example 7.24(c), we have a least squares error \nof \nll\ne\nll \n= \nv1 \n= \n0.816.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":93915,"to":93964}}}}],[1551,{"pageContent":")\n(b )-that is, the distance \nfrom b to the column space of A. In Example 7.26, we had \nso, as in   Example 7.24(c), we have a least squares error \nof \nll\ne\nll \n= \nv1 \n= \n0.816. \nRemark Note that the  columns of A in Example 7.26 are linearly independent, \nso (AT A\n)\n-\n1 \nexists, and we could calculate x as x = (AT A\n)\n-\n1\nA T\nb\n. However, it is almost \nalways easier to solve the normal equations using Gaussian elimination (or to let your \nCAS do it for you!). \nIt is interesting to look at Example 7.26 from two different geometric points of \nview. On the one hand, we have the least squares approximating line y = t + x, \nwith corresponding errors 8\n1 \n= L 8\n2 \n= \n-\nt, and 8\n3 \n=Las shown in Figure 7.13(a). \nEquivalently, we have the projection of b onto the column space of A, as shown in \nFigure 7.13(b). Here, \nP \n= proj\nco\nl(\nA)\n(\nb\n) \n= A\nx \n= \n[\nl\n� \n�\nl\n]\n[\nt\nl\n] \n[\nL\n!: l\nl","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":93964,"to":94027}}}}],[1552,{"pageContent":"518 \nChapter 7 \nDistance and Approximation \ny \n6 \n5 \n4 \n3 \n2 \n-1 \n2 \n-I \nFigure 1.13 \n3 \n4 \n5 \n(a) \n6 \ne \n5 \n3 \n8 \np \n= \n3 \nII \n3 \nW =col( A) \n(b) \n� \nand the least squares error vectoris e = \n[;\n,31]. \n[What would Figure 7.13(b) look like \nif the data points were collinear?] \n0 \nExample 1.21 \nFind the least squares approximating line and the least squares error for the points \n(1, 1), (2, 2), (3, 2), and (4, 3). \nSolulion Let y = \na \n+ \nbx be the equation of the line we seek. Then, substituting the \nfour points into this equation, we obtain \na\n+ \nb=l [1  1\n]    [\n1\n] \na \n+ \n2b  = 2 \nor \n1  2 \n[\na\n] \n2 \na \n+ \n3b  = 2 \n1  3   b     2 \na \n+ \n4b = 3 \n1  4 \n3 \nSo we want the least squares solution of Ax = \nb\n, where \nSince the columns of A are linearly independent, the solution we want is \nx = \n(\nA\nT\nA)-\nI\nA\nT\nb \n= \n[\ni\n] \n.-.... \n(Check this calculation.) Therefore, we take a = \nt \nand b = t producing the least \nsquares approximating line y = \nt \n+ \n�x, as sh  own in Figure 7.14.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":94029,"to":94121}}}}],[1553,{"pageContent":"Example 1.28 \ny \n5 \n4 \n3 \n2 \nFigure 1.14 \nSince \nSection 7.3 \nLeast Squares Approximation \n519 \n2 \n3 \n4 \n5 \nthe least squares error is \nll\ne\nll \n= \nVs\n/5 \n= \n0.447. \nWe  can use the method of least squares to approximate data points by curves \nother than straight lines. \nFind the parabola that gives the best least squares approximation to the points (-1, 1), \n(O, -1), (1,  O), and (2, 2). \nSolution The equation of a parabola is a quadratic y = a + bx + c:\n2\n. Substituting \nthe given points into this quadratic, we obtain the linear system \na -b + c \na \na+ b + c \na+ 2b + 4c \nThus, we want the least squares approximation of Ax = b, where","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":94123,"to":94159}}}}],[1554,{"pageContent":"580 \nChapter 7 \nDistance and Approximation \nWe compute \n2 \n6 \n8 \nso the normal equations are given by \nwhose solution is \nThus, the least squares approximating parabola has the equation \ny = -fa -�x + \nx\n2 \nas shown in Figure 7.15. \n-3 -2 \nFigure 1.15 \ny \n-1 \n(\n0\n, \n-1\n) \n-2 \nA least squares approximating parabola \n3 \nOne of the important uses of least squares approximation is to  estimate constants \nassociated with various processes. The next example illustrates this application in the \ncontext of population growth. Recall from Section 6.7 that a population that is grow­\ning (or decaying) exponentially satisfies an equation of the form p(t)  = c\ni\nt\n, where \np(t) is the size of the population at time t and c and k are constants. Clearly, c = p\n( \n0 \n), \nbut k is not so easy to determine. It is easy to see that \np'(t) \nk=­\np(t) \nwhich explains why k is sometimes referred to as the relative growth rate of the popu­\nlation: It is the ratio of the growth rate p' \n(\nt\n) \nto the size of the population p(t).","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":94161,"to":94207}}}}],[1555,{"pageContent":"CAS \nExample 1.29 \nTable 1.2 \nPopulation \nYear \n(in billions) \n1950 2.56 \n1960 \n3.04 \n1970 3.71 \n1980 \n4.46 \n1990 \n5.28 \n2000 \n6.08 \nSection 7.3 \nLeast Squares Approximation \n581 \nTable 7.2 gives the population of the world at 10-year intervals for the second half of \nthe 20th century. Assuming an exponential growth model, find the relative growth \nrate and predict the world's population in 2010. \nSolution Let's agree to measure time t in 10-year intervals so that t =  0 is 1950, \nt =  1    is 1960, and so on. Since c = p(O) = 2.56, the equation for the growth rate of \nthe population is \np =   2.56e\nk\nt \nHow can we use the method ofleast  squares on this equation? Ifwe take the natural \nlogarithm of both sides, we convert the equation into a linear one: \nln p =   ln\n(\n2.56e\nk\nt\n) \n=  ln 2.56 + ln\n(\ni\nt\n) \n= 0.94 +kt \nSource: U.S. Bureau of the Census, Inter­\nnational Data Base \nPlugging in the values oft and p from Table 7 .2 yields the following system (where we \nhave rounded calculations to three decimal places): \n0.94 = 0.94","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":94209,"to":94255}}}}],[1556,{"pageContent":"national Data Base \nPlugging in the values oft and p from Table 7 .2 yields the following system (where we \nhave rounded calculations to three decimal places): \n0.94 = 0.94 \nk =   0.172 \n2k =   0.371 \n3k = 0.555 \n4k = 0.724 \n5k = 0.865 \nWe  can ignore the first equation (it just corresponds to the initial condition c = \np(O) = 2.56). The remaining equations correspond to a system Ax= b, with \n1 \n0.172 \n2 0.371 \nA=  3 \nand \nb= \n0.555 \n4 \n0.724 \n5 \n0.865 \nSince A\nT \nA =  55 and A \nT\nb = 9.80, the corresponding normal equations are just the \nsingle equation \n55x = 9.8o \nTherefore, k = x = 9.80\n/\n55 = 0.178. Consequently, the least squares solution has \nthe form p = 2.56e\n0\n·\n17\n3\nt \n(see Figure 7.16). \nThe world's population in 2010 corresponds to t= 6, from which we obtain \np\n(\n6\n) \n= 2.56e\n0\n·\n1\n7\n8\n(\n6\nJ \n= 7.448 \nThus, if our model is accurate, there will be approximately 7.45 billion people on \n_.... \nEarth in the year 2010. (The U.S. Ce nsus Bureau estimates that the global population","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":94255,"to":94311}}}}],[1557,{"pageContent":"= 2.56e\n0\n·\n1\n7\n8\n(\n6\nJ \n= 7.448 \nThus, if our model is accurate, there will be approximately 7.45 billion people on \n_.... \nEarth in the year 2010. (The U.S. Ce nsus Bureau estimates that the global population \nwill be \"only\" 6.82 billion in 2010. Why do you think our estimate is higher?)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":94311,"to":94324}}}}],[1558,{"pageContent":"582 \nChapter 7 \nDistance and Approximation \nTheorem 1.10 \np\n(t) \n7 \n� \n6 \nVJ \nc::: \n.Sl \n� \n5 \n:.s \n3 \n4 \nc::: \n0 \n·.= \n3 \nc:j \n3 \na. \n0 \n2 \n0... \n2  3  4  5  6 \nDecades since 1950 \nFigure 1.16 \nLeast Squares via the OB Factorization \nIt  is often the case that the normal equations for a  least squares problem are \nill-conditioned. Therefore, a small numerical error in performing Gaussian elimina­\ntion will result in a large error in the least squares solution. Consequently, in practice, \nother methods are usually used to compute least squares approximations. \nIt turns out that the QR factorization of A yields a more reliable way of computing \nthe least squares approximation of Ax = \nb\n. \nLet A be an m X n matr  ix with linearly independent columns and let \nb \nbe in !R\nm\n. \nIf A = QR is a QR factorization of A, then the unique least squares solution x of \nAx= \nb\nis \nProof Recall from Theorem 5.16 that the QR factorization A  = QR involves an \nm X n matrix Q with orthonormal columns and an invertible upper triangular","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":94326,"to":94375}}}}],[1559,{"pageContent":"Ax= \nb\nis \nProof Recall from Theorem 5.16 that the QR factorization A  = QR involves an \nm X n matrix Q with orthonormal columns and an invertible upper triangular \nmatrix R. From the Least Squares Theorem, we have \nArA\nx \n=Ar\nb \n==> (QRfQRx = (QRfb \n==> Rr\nQ\nr\nQ\nR\nx \n= Rr\nQ\nr\nb \n==> RrRx. = Rr\nQ\nr\nb \n...,.. \nsince Q\nr \nQ = I. (Why?) \nSince R is invertible, so is R\nr\n, and hence we have \nR\nx\n= \nQ\nr\nb \nor, equivalently, \nx \n= R\n-\n1\nQ\nr\nb \nRemark Since R is upper triangular, in practice it is easier to solve Rx = Q\nr\nb \ndirectly than to invert Rand compute R\n-\n1\nQ\nr\nb.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":94375,"to":94429}}}}],[1560,{"pageContent":"Example 1.30 \nTheorem 1.11 \nSection 7.3 \nLeast Squares Approximation \n583 \nUse the QR factorization to find a least squares solution of Ax = b, where \nA = [-\n� \n� \n�] and  b = [-�] \n-1  0  1 \n-2 \n1 \n2 \n0 \nSolution \nFrom Example 5.15, \n3Vs/10 \n1 \n[ \n1\n/\n2 \n-V6/�]\n[\n: \n-1\n/\n2  3Vs/10 \nVs \nA= QR = \nVs/10 \n-1\n/\n2 \nV6/\n6 \n1\n/\n2 \nVs/10 \nV6/\n3 \n0 \nWe have \n[ 1\n/\n2 \nQ\nT\nb = 3Vs/10 \n-V6/6 \n-1\n/\n2 \n3Vs/10 \n0 \n-1\n/\n2 \nVs/10 \nV6/\n6 \nso we   require the solution to Rx = Q\nr\nb, or \n[\n2   1 \n0  Vs \n0 \n0 \nBack substitution quickly yields \n1\n/\n2 l   [ 7 \n/\n2 l \n3Vs/2  x = -Vs/2 \nV6/2 \n-2V6/3 \n[ 4/3] \nx \n= \n3\n/2 \n-4\n/\n3 \nOrthogonal Proieclion Revisiled \n0 \n1\n/\n2 l \n3Vs/2 \nV6/\n2 \n[ 7 \n/\n2] \n-Vs/2 \n-2V6/3 \nOne of the nice byproducts of the least squares method is a new formula for the or­\nthogonal projection of a vector onto a subspace of !R\nm\n. \nLet W be a subspace of !R\nm \nand let A be an m X n matrix whose columns form a \nbasis for W. If v is any vector in !R\nm\n, then the orthogonal projection of v   onto W \nis the vector \nproj\nw\n(\nv\n) \n= A\n(\nA\nT\nA\n)-\n'A\nr\nv \nThe linear transformation P: !R\nm\n� !R\nm \nthat projects !R\nm","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":94431,"to":94562}}}}],[1561,{"pageContent":"basis for W. If v is any vector in !R\nm\n, then the orthogonal projection of v   onto W \nis the vector \nproj\nw\n(\nv\n) \n= A\n(\nA\nT\nA\n)-\n'A\nr\nv \nThe linear transformation P: !R\nm\n� !R\nm \nthat projects !R\nm \nonto Whas A(A\nT\nA)\n-\n1\nA\nr \nas its standard matrix. \nProof Given the way we have constructed A, its column space is W.  Since the \ncolumns of A are linearly independent, the Least Squares Theorem guarantees that \nthere is a unique least squares solution to Ax = v given by \nx \n= \n(\nA\nT\nA\n)-\n1\nA\nr\nv","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":94562,"to":94607}}}}],[1562,{"pageContent":"584 \nChapter 7 \nDistance and Approximation \nExample 1.31 \nBy Equation (1), \nTherefore, \nas required. Since this equation holds for all v in !R\nm\n, the last statement of the theorem \nfollows immediately. \nWe will illustrate Theorem 7 .11 by revisiting Example 5.11. \nFind th, orthogonal prnj,dion ofv \n� \n[ \n-�\n] \nonto th, plan, Win Ul' with 'q oation \nx -y \n+ \n2z = 0, and give the standard matrix of the orthogonal projection transfor­\nmation onto W. \nSolulion \nAs in Example 5.11, we will take as a basis for W the set \nm\ni\nr\nrn \nWe form the matrix \n[ \n1   -1\n] \nA= \n�     � \nwith these basis vectors as its columns. Then \nso \n1 \n1 \n[\n� \n�\n] \nBy Theorem 7.11, the  standard matrix of the orthogonal projection transformation \nonto Wis \nso the orthogonal projection of v onto Wis \nwhich agrees with our solution to Example 5.11. \n1 \n1 \n�\n]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":94609,"to":94657}}}}],[1563,{"pageContent":"Section 7.3 \nLeast Squares Approximation \n585 \nRemark Since the projection of a vector onto a subspace Wis unique, the stan­\ndard matrix of this linear transformation (as given by Theorem 7 .11) cannot depend \non the choice of basis for W. In other words, with a different basis for W, we have a \n......,.. \ndifferent matrix A,  but the matrix A(A \nT \nA)\n-\n1\nA \nT\nwill be the same! (You are asked to \nverify this in Exercise 43.) \nExample 1.32 \nThe Pseudoinverse of a  Malrix \nIf A is an n X n matrix with linearly independent columns, then it is invertible, and \nthe unique solution to Ax = b   is x = A \n-\nl\nb. If m > n and A is m X n with linearly \nindependent columns, then Ax = b   has no exact solution, but the best approximation \nis given by the unique least squares solution x = (A\nT\nA)\n-\n1\nA\nT\nb. The matrix (A\nT\nA)\n-\n1\nA\nT \ntherefore plays the role of an \"inverse of   A\" in this situation. \nDefinition \nIf A  is a  matrix with linearly independent columns, then the \npseudoinverse of A is the  matrix A\n+ \ndefined by \nA\n+\n= \n(\nA\nT\nA\n)\n-\n1\nA","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":94659,"to":94713}}}}],[1564,{"pageContent":"Definition \nIf A  is a  matrix with linearly independent columns, then the \npseudoinverse of A is the  matrix A\n+ \ndefined by \nA\n+\n= \n(\nA\nT\nA\n)\n-\n1\nA\nr \nObserve that if A is m X n, then A\n+ \nis n X m. \nHnd the p\n<\neudoinvem of A � \n[ \n: n \nSolution We have already done most of the calculations in Example 7 .26. Using our \nprevious work, we have \n[ \nt \n-\n�\n]\n[l \n-1 \n2 \n1 \n1 \n2 \n�\n] \nThe  pseudoinverse is a  convenient shorthand notation for  some of the  con­\ncepts we have been exploring. For example, if A is m X n with linearly independent \ncolumns, the least squares solution of Ax = b   is given by \nand the standard matr  ix of the orthogonal projection P from !R\nm \nonto col(A) is \n[P] =AA\n+ \nIf A  is actually a square matr  ix, then it is easy to show  that A+  =  A \n-\nl \n(see \nExercise 53). In this case, the least squares solution of Ax = bis the exact solution, \nsince","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":94713,"to":94766}}}}],[1565,{"pageContent":"586 \nChapter 7 \nDistance and Approximation \n� \nThe projection matrix becomes [P] = AA\n+ \n= AA\n-\n1 \n= I. (What is the geometric \ninterpretation of this equality?) \nTheorem 7.12 summarizes the key properties of the pseudoinverse of a matrix. \n.......... \n(Before reading the proof of this theorem, verify these properties for the matrix in \nExample 7.32.) \nTheorem 1.12 \nLet A be a matrix with linearly independent columns. Then the pseudoinverse \nA\n+ \nof A satisfies the following properties, called the Penrose conditions for A: \na. AA\n+\nA =A \nb. A\n+\nAA\n+ \n=A\n+ \nc.  AA\n+ \nand A\n+ \nA are symmetric. \nProof We prove condition (a) and half of condition ( c) and leave the proofs of the \nremaining conditions as Exercises 54 and 55. \n(a) We compute \nAA\n+\nA \n= A\n((\nA\nT\nA\n)-\n1\nA\nT)\nA \n= A\n(\nA\nT\nA\n)-\n1\n(\nA\nT\nA\n) \n=AI= A \n( c)  By Theorem 3 .4, A\nT \nA is sy  mmetric. Therefore, (A\nT \nA )\n-\n1 \nis also symmetric, by \nExercise 46 in Section 3.3. Taking the transpose of AA\n+\n, we  have \n(\nAA\n+\n)T \n= \n(\nA\n(\nA\nT\nA\n)-\n1\nA\nTf \n= \n(\nA\nTf((\nA\nT\nA\n)-\n1\nf\nA\nT \n= A\n(\nA\nT\nA\n)-\nI\nA\nT \n=AA+","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":94768,"to":94878}}}}],[1566,{"pageContent":"T \nA is sy  mmetric. Therefore, (A\nT \nA )\n-\n1 \nis also symmetric, by \nExercise 46 in Section 3.3. Taking the transpose of AA\n+\n, we  have \n(\nAA\n+\n)T \n= \n(\nA\n(\nA\nT\nA\n)-\n1\nA\nTf \n= \n(\nA\nTf((\nA\nT\nA\n)-\n1\nf\nA\nT \n= A\n(\nA\nT\nA\n)-\nI\nA\nT \n=AA+ \nExercise 56 explores further properties of the pseudoinverse. In the next section, \nwe will see how to extend the definition of A+ to handle all matrices, whether or not \nthe columns of A are linearly independent. \n.. \nI Exercises 1.3 \nGAS \nIn Exercises 1-3, consider the data points (1, O), (2, 1), and \n(3, 5). Compute the least squares error for the given line. \nIn each case, plot the points and the line. \n1. y = -2 + 2x 2. y = x 3. y = -3  + \n& \nx \nIn Exercises 4-6, consider the data points (-5, 3), (0, 3), \n(5, 2), and (10, 0). Compute th e least squares error for th e \ngiven line. In each case, plot th e points and the line. \n4. y = 3 -\nk\nx \n6\n. y = 2 -kx \nIn Exercises 7-14,find the least squares approximating line \nfor the given points and compute th e corresponding least \nsquares error. \n7. (1, O), (2, 1), (3, 5)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":94878,"to":94948}}}}],[1567,{"pageContent":"4. y = 3 -\nk\nx \n6\n. y = 2 -kx \nIn Exercises 7-14,find the least squares approximating line \nfor the given points and compute th e corresponding least \nsquares error. \n7. (1, O), (2, 1), (3, 5) \n8. (1, 6), (2, 3), (3, 1) \n9. (O, 4), (1, 1), (2, O) \n10. (O, 3), (1, 3), (2, 5) \n11. (-5, -1), (O, 1), (5, 2), (10, 4)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":94948,"to":94960}}}}],[1568,{"pageContent":"12. (-5, 3), (0, 3), (5, 2 ) , (10, 0) \n13. (1, 1), \n(\n2, 3), (3, 4), (4, 5), (5, 7) \n14. (1, 10), \n(\n2, 8), (3, 5), (4, 3), (5, O) \nIn Exercises 15-18,find th e least squares approximating \nparabola for the given points. \n15. (1, 1), \n(\n2, -2\n)\n, (3, 3), (4, 4) \n16. (1, 6), \n(\n2, O), (3, O), (4, 2\n) \n17. \n(\n-2, 4), (-1, 7), (O, 3), (1, O), \n(\n2, -1) \n18. \n(\n-2, 0), (-1, -11), (0, -10), (1, -9), \n(\n2, 8) \nIn Exercises 19-22, find a least squares solution of Ax = b \nby constructing and solving the normal equations. \nIn Exercises 23 and 24, show that the least squares solution \nof Ax = b is not unique and solve the normal equations to \nfind all the least squares solutions. \n23. A � \n[ \n� \n= \n� \nO \n] b � \n[\n-\n; l \n24. A � \n[\n� \n-\n� -�Jb � \n[\n-\n�] \nIn Exercises 25 and 26,find the best approximation to a \nsolution of the given system of equations. \n25. x + y -\nz = 2 \n26. 2x + 3y + z = 21 \n-y \n+ 2z =  6 \nx + y + z =  7 \n3x + 2y -\nz =  11 \n-x + y -z =  14 \n-x + \nz =  0 \n2y + z =  0 \nSection 7.3 \nLeast Squares Approximation \n581","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":94962,"to":95029}}}}],[1569,{"pageContent":"25. x + y -\nz = 2 \n26. 2x + 3y + z = 21 \n-y \n+ 2z =  6 \nx + y + z =  7 \n3x + 2y -\nz =  11 \n-x + y -z =  14 \n-x + \nz =  0 \n2y + z =  0 \nSection 7.3 \nLeast Squares Approximation \n581 \nIn Exercises 27 and 28, a QR factorization of A is given. \nUse it to find a least squares solution of Ax = b. \n27. A = \n28. A = \nO\nJ \n[ 1\n/V6 \n-1 \n'\nQ =    2\n/V6 \n1 \n-1/V6 \nR=\n[\nV6 \n- V6/2\nJ\nb=\n[\n�\n] \n0 \n1\n/\\/2 \n, \n1 \n1\n] \nb = \n1 \n, \n1\n/\\/2] \n0 \n, \n1\n/\\/2 \nU\nl \n29. A tennis ball is dropped from various heights, and the \nheight of the ball on the first bounce is measured. Use \nthe data in Table 7.3 to find the least squares approxi­\nmating line for bounce height b as a linear function \nof initial height h. \nTable 1.3 \nh (cm) \nb (cm) \n20 \n14.5 \n40 \n31 \n48 \n36 \n60 \n45.5 \n80 \n59 \n100 \n73.5 \n30. Hooke's Law states that the length L of a spring is \na linear function of the force F applied to it. (See \nFigure 7.17 and Example 6.92.) Accordingly, there \nare constants a and b such that \nL \n=  a  + bF \nTable 7.4 shows the results of attaching various \nweights to a spring. \nF \nFioure 1.11 \nTable 1.4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":95029,"to":95115}}}}],[1570,{"pageContent":"Figure 7.17 and Example 6.92.) Accordingly, there \nare constants a and b such that \nL \n=  a  + bF \nTable 7.4 shows the results of attaching various \nweights to a spring. \nF \nFioure 1.11 \nTable 1.4 \nF (oz) \nL (in.) \n2 \n7.4 \n4 \n9.6 \n6 \n11.5 \n8 \n13.6","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":95115,"to":95133}}}}],[1571,{"pageContent":"588 \nChapter 7 \nDistance and Approximation \nTable 1.5 \nYear of Birth \nLife Expectancy (years) \n1920 \n54.1 \n1930 \n59.7 \n1940 \n62.9 \n1950 \n68.2 \n1960 \n69.7 \n1970 \n70.8 \n1980 \n73.7 \n1990 \n75.4 \nSource: World Almanac and Book of Facts. New York: World Almanac Books, 1999 \n(a) Determine the constants \na \nand b by finding the \nleast squares approximating line for these data. \nWhat does \na \nrepresent? \n(b) Estimate the length of the spring when a weight of \n5 ounces is attached. \n31. Table 7.5 gives life expectancies for people born in the \nUnited States in the given years. \n(a) Determine the least squares approximating line \nfor these data and use it to predict the life expec­\ntancy of someone born in 2000. \n(h) How good is this model? Explain. \n32. When an object is thrown straight up into the air, \nNewton's Second Law of Motion states that its height \ns (t) at time tis given by \ns(t) \n= \ns\n0 \n+ v\n0\nt + \nt\ngt\n2 \nwhere v\n0 \nis its initial velocity and g    is the constant of \nacceleration due to gravity. Suppose we take the mea­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":95135,"to":95189}}}}],[1572,{"pageContent":"s (t) at time tis given by \ns(t) \n= \ns\n0 \n+ v\n0\nt + \nt\ngt\n2 \nwhere v\n0 \nis its initial velocity and g    is the constant of \nacceleration due to gravity. Suppose we take the mea­\nsurements shown in Table 7.6. \nTable 1.6 \nTime (s) \nHeight (m) \n0.5 \n11 \n17 \n1.5 \n21 \n2 \n23 \n3 \n18 \n(a) Find the least squares approximating quadratic for \nthese data. \n(h) Estimate the height at which the object was \nreleased (in m), its initial velocity (in mis), and its \nacceleration due to gravity (in m/s\n2\n). \n(c) Approximately when will the object hit the \nground? \n33. Table 7.7 gives the population of the United States at \n10-year intervals for the years 1950-2000. \n(a) Assuming an exponential growth model of the \nform p(t) \n= \nclt, where p(t) is the  population at \ntime t, use least squares to find the equation for \nthe growth rate of the population. [Hint: Let t \n= \n0 \nbe 1950.] \n(b) Use the equation to estimate the U.S. population \nin 2010. \nTable 1.1 \nPopulation \nYear \n(in millions) \n1950 150 \n1960 \n179 \n1970 \n203 \n1980 \n227 \n1990 250 \n2000 \n281","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":95189,"to":95252}}}}],[1573,{"pageContent":"= \n0 \nbe 1950.] \n(b) Use the equation to estimate the U.S. population \nin 2010. \nTable 1.1 \nPopulation \nYear \n(in millions) \n1950 150 \n1960 \n179 \n1970 \n203 \n1980 \n227 \n1990 250 \n2000 \n281 \nSource: U.S. Bureau of the Census \n34. Table 7.8 shows average major league baseball salaries \nfor the years 1970-2005. \n(a) Find the least squares approximating quadratic for \nthese data. \n(b) Find the least squares approximating exponential \nfor these data. \n(c) Which equation gives the better approximation? \nWhy? \n(d) What do you estimate the average major league \nbaseball salary will be in 2010 and 2015? \nTable 1.8 \nA\nverage Salary \nYear \n(thousands of dollars) \n1970 \n29.3 \n1975 44.7 \n1980 143.8 \n1985 \n371.6 \n1990 \n597.5 \n1995 \n1110.8 \n2000 \n1895.6 \n2005 \n2476.6 \nSource: Major League Baseball Players Association","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":95252,"to":95301}}}}],[1574,{"pageContent":"35. A 200 mg sample ofradioactive polonium-210 is ob­\nserved as it decays. Table 7.9 shows the mass remain­\ning at various times. \nAssuming an exponential decay model, use least \nsquares to find the half-life ofpolonium-210. (See \nSection 6.7.) \nTable 1.9 \nT\ni\nm\ne (da\ny\ns) \nMass (mg) \n0 \n200 \n30 \n172 \n60 \n148 \n90 \n128 \n36. Find the plane z = a + bx + cy that best fits the data \npoints (O, -4, O), (5, 0, O), (4, -1, 1), (1, -3, 1), and \n(-1, -5, -2). \nIn Exercises 37-42,find the standard matrix of the \northogonal projection onto the subspace W. Then use this \nmatrix to find the orthogonal projection of v onto W. \n43. Verify that the standard matrix of the projection onto \nWin Example 7.31 (as constructed by Theorem 7.11) \ndoes not depend on the choice of basis. Take \nm\nH�ll \nSection 7.3 \nLeast Squares Approximation \n589 \nas a basis for W and repeat the calculations to show \nthat the resulting projection matrix is the  same. \n44. Let A be a matrix with linearly independent columns \nand let P = A \n(\nA\nr \nA\n)-\n1 \nA\nr","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":95303,"to":95350}}}}],[1575,{"pageContent":"589 \nas a basis for W and repeat the calculations to show \nthat the resulting projection matrix is the  same. \n44. Let A be a matrix with linearly independent columns \nand let P = A \n(\nA\nr \nA\n)-\n1 \nA\nr \nbe the matrix of orthogonal \nprojection onto col(A). \n(a) Show that Pis symmetric. \n(b) Show that Pis idempotent. \nIn Exercises 45-52, compute th e pseudo inverse of A. \n45. A = \n[\n2\n1\n] \n47. A \n� \nH \n�\n] \n46. A\n�\n[-�: \n48. A\n�\n[� \n�\n] \n49. A = \n[\n0\n1  1\n1\n] \n50. A= \nSJ.A\n�\n[\n�\n�\n�] \n52. A= \n[\n3\n1 \n4\n2\n] \n[\n� \n� \n-\n�1 \n1  1 -2 \n0  0 \n2 \n53. (a) Show that if A is a square matrix with linearly \nindependent columns, then A+ = A \n-\n1\n• \n(b)  If A is an m X n matrix with orthonormal \ncolumns, what is A+? \n54. Prove Theorem 7.12(b). \n55. Prove the remaining part of Theorem 7.12(c). \n56. Let A be a matr  ix with linearly independent columns. \nProve the following: \n(a) \n(\ncA\n)\n+  = \n(\nl/c\n)\nA+ forall scalarsc:;i:O. \n(b) \n(\nA+\n)\n+  = A if A is a square matrix. \n(c) \n(\nA\nT)\n+  = \n(\nA +\nf \nif A is a square matrix. \n57. Let n data points (x\n1\n, y\n1\n), ... , \n(\nx\nn\n, Y\nn\n) be given. Show","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":95350,"to":95458}}}}],[1576,{"pageContent":"(a) \n(\ncA\n)\n+  = \n(\nl/c\n)\nA+ forall scalarsc:;i:O. \n(b) \n(\nA+\n)\n+  = A if A is a square matrix. \n(c) \n(\nA\nT)\n+  = \n(\nA +\nf \nif A is a square matrix. \n57. Let n data points (x\n1\n, y\n1\n), ... , \n(\nx\nn\n, Y\nn\n) be given. Show \nthat if the points do not all lie on the same vertical \nline, then they have a unique least squares approxi­\nmating line. \n58. Let n data points (x\n1\n, y\n1\n), ... , \n(\nx\nn\n, Y\nn\n) be given. \nGeneralize Exercise 57 to show that if at least k + 1 \nof \nx1, ... , x\nn \nare distinct, then the given points have \na unique least squares approximating polynomial of \ndegree at most k.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":95458,"to":95512}}}}],[1577,{"pageContent":"590 \nChapter 7 \nDistance and Approximation \n• \nExample 1.33 \nThe Singular Value Decomposition \nIn Chapter 5, we saw that every symmetric matrix A  can be factored as A =   PDP \nT\n' \nwhere Pis an orthogonal matrix and Dis a  diagonal matr  ix displaying the eigenval­\nues of A. If  A is not symmetric, such a factorization is not possible, but as we learned \nin Chapter 4, we may still be able to factor a square matrix A  as A =   PDP\n-\n1\n, where \nD is as before but P is now simply an invertible matrix. However, not every matrix \nis diagonalizable, so it   may surprise you that we  will now show that every matrix \n(symmetric or not, square or not) has a    factorization of the form A = PDQ \nT\n' \nwhere \nP and Q are orthogonal and D is a diagonal matrix! This remarkable result is the sin­\ngular value decomposition (SVD), and it is one of the most important of all matrix \nfactorizations. \nIn this section, we will show how to compute the SVD of a matrix and then con­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":95514,"to":95538}}}}],[1578,{"pageContent":"gular value decomposition (SVD), and it is one of the most important of all matrix \nfactorizations. \nIn this section, we will show how to compute the SVD of a matrix and then con­\nsider some of its many applications. Along the way, we will tie up some loose ends by \nanswering a few questions that were left open in previous sections. \nThe Singular Values of a  Malrix \nFor any m X n matrix A, the n X n matrix A\nT \nA is symmetric and hence can be or­\nthogonally diagonalized, by the Spectral Theorem. Not only are the eigenvalues of \nA\nT \nA all real (Theorem 5.18), they are all nonnegative. To show this, let A be an eigen­\nvalue of A\nT \nA with corresponding unit eigenvector v. Then \n0\n:::::: \n11\nAv\nll\n2 \n= \n(\nAv\n)\n· \n(\nAv\n)\n= \n(\nAv\nf\nAv = v\nT\nA\nT\nAv \n= v\nT \nAv \n= A\n(\nv \n· v\n) \n= A\nll\nv\nll\n2 \n= A \nIt therefore makes sense to take (positive) square roots of these eigenvalues. \nDefinition \nIf A is an m x n matrix, the singular values of A are the square \nroots of the eigenvalues of A\nT \nA and are denoted by u \n1\n, ... , u n-It is conventional","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":95538,"to":95598}}}}],[1579,{"pageContent":"Definition \nIf A is an m x n matrix, the singular values of A are the square \nroots of the eigenvalues of A\nT \nA and are denoted by u \n1\n, ... , u n-It is conventional \nto arrange the singular values so that u \n1 \n2:: u \n2 \n2:: ·  ·  · 2:: u n-\nFind the singular values of \nSolulion The matrix \n0 \n�\n] \nhas eigenvalues A\n1 \n= 3   and A\n2 \n=  1. Consequently, the singular values of A are u\n1 \n= \nVAi = \nv'3 \nand u\n2 \n=  VA;\" = 1.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":95598,"to":95626}}}}],[1580,{"pageContent":"y \n2 \n-2 \nFioure 1.18 \nSection 7.4 \nThe Singular Value Decomposition \n591 \nTo  understand the significance of the singular values of an m X n matrix A, \nconsider the eigenvectors of A\nT \nA. Since A\nT \nA is symmetric, we know that there is an \northonormal basis for !R\nn \nthat consists of eigenvectors of A\nT \nA. Let {v\n1\n, ..• , v\nn\n} be such \na basis corresponding to the eigenvalues of A\nT \nA, ordered so that ,\\\n1 \n2: ,\\\n2 \n2: ·  ·  · 2: Aw \nFrom our calculations just before the definition, \nA\n;\n= \nll\nAv\n;\nll\n2 \nTherefore, \nO\"\n; \n= \n'\\IA; \n= \nll\nAv;\nll \nIn other words, the singular values of A are the lengths of the vectors Av\n1\n, .•• , Av\nn\n­\nGeometrically, this result has a nice interpretation. Consider Example 7.33 again. \nIf x lies on the unit circle in IR\n2 \n(i.e., \nll\nx\nll \n= 1), then \n11\nAx\nll\n2 \n= \n(\nAx\n)\n· \n(\nAx\n) \n= \n(\nAx\nf(\nAx\n) \n= x\nT \nA \nT\nAx \n= \n[x\n1 \nx\n2\n] \n[\n�  �\n] [\n:\n:] \n= \n2x\ni \n+ 2x\n1\nx\n2 \n+ 2x\n� \nwhich we recognize is a quadratic form. By Theorem 5.25, the maximum and mini­\nmum values of this quadratic form, subject to the constraint \nll\nx\nll \n= 1, are A\n1 \n= 3 and \n,\\\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":95628,"to":95740}}}}],[1581,{"pageContent":"�  �\n] [\n:\n:] \n= \n2x\ni \n+ 2x\n1\nx\n2 \n+ 2x\n� \nwhich we recognize is a quadratic form. By Theorem 5.25, the maximum and mini­\nmum values of this quadratic form, subject to the constraint \nll\nx\nll \n= 1, are A\n1 \n= 3 and \n,\\\n2 \n= 1, respectively, and they occur at the corresponding eigenvectors of A\nT \nA-that \nis, when x = v\n1 \n= \n[ \n� � �\n] \nand x = v\n2 \n= \n[ \n-\n� � �\nl \nrespectively. Since \n11\nAv\n;\nll\n2 \n= \nvTA\nT\nA\nv\n; \n=\nA\n; \nfor i = 1, 2, we   see that 0'\n1 \n= \nll\nAv\n1ll \n= \nV3 \nand 0'\n2 \n= \n11\nAv\n2\nll \n= 1 are the maximum \nand minimum values of the lengths \nll\nAx\nll \nas x traverses the unit circle in   IR\n2\n• \nNow, the linear transformation corresponding to A maps IR\n2 \nonto the plane in IR\n3 \nwith equation x -y -z = 0 (verify this), and the image of the unit circle under this \ntransformation is an ellipse that lies in this plane. (We will verify this fact in general \nshortly; see Figure 7 .18.) So O' \n1 \nand O' \n2 \nare the lengths of half of the major and minor \naxes of this ellipse, as sh  own in Figure 7.19.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":95740,"to":95828}}}}],[1582,{"pageContent":"shortly; see Figure 7 .18.) So O' \n1 \nand O' \n2 \nare the lengths of half of the major and minor \naxes of this ellipse, as sh  own in Figure 7.19. \nWe can now describe the singular value decomposition of a matrix. \nz \nmultiplication \nb\ny \nA \n.......------. \n2 \nr \ny \n-2 \n-2 \nThe matrix A transforms the unit circle in !ffi\n2 \ninto an ellipse in !ffi3 \nFigure 1.19","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":95828,"to":95849}}}}],[1583,{"pageContent":"592 \nChapter 7 \nDistance and Approximation \nThe Singular Value Decomposition \nWe want to show that an m X n   matrix A  can be factored as \nA= u�vr \nwhere U is an m X m orthogonal matrix, V is an n X n orthogonal matr  ix, and L is \nan m X n   \"diagonal\" matrix. If the nonzero singular values of A are \nand u ,.+ \n1 \n= u \nr\n+ 2 = \n·   ·   · \n= u \nn \n= 0, then L will have the block form \nr \nn-r \n� = ----j-----,  where D = \n[\nD i 0 ] J\nr \n0 i 0 ) m - r \n[\n�I � \nl \n0 \nu,. \n(1) \nand each matrix 0 is a zero matrix of the appropriate size. (If r = m or r = n, some of \nthese will not appear.) Some examples of such a matrix L  with r = 2   are \n0 \n3 \n� \n(What is D in each case?) \n0 \n3 \n0 \nTo  construct  the  orthogonal  matrix V, we  first  find  an orthonormal  basis \n{v\n1\n, ... , v\nn\n} for !R\nn \nconsisting of eigenvectors of the n X n symmetric matrixA \nr \nA. Then \nis an orthogonal n X n matrix. \nFor the orthogonal matrix U, we first note that {Av\n1\n, ... , Av\nn\n} is an orthogonal set \nof vectors in !R\nm","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":95851,"to":95907}}}}],[1584,{"pageContent":"r \nA. Then \nis an orthogonal n X n matrix. \nFor the orthogonal matrix U, we first note that {Av\n1\n, ... , Av\nn\n} is an orthogonal set \nof vectors in !R\nm\n. To see this, suppose that v; is the eigenvector of A\nr \nA corresponding \nto the eigenvalue A;. Then, for i  * j, we have \n(\nAv\n;\n)\n· \n(\nAv\n) \n= \n(\nAv\n;\nVAv\nj \n= v\nT\nA\nT\nAv \nI \n) \n= ,\\.\n(\nv .\n. \nv.\n) \n= 0 \n) \nI \n) \nsince the eigenvectors V; are orthogonal. Now recall that the  singular values satisfy \n<I; \n= \nll\nAv\n;\nll \nand that the first r of these are nonzero. Therefore, we can normalize \nAv\n1\n, ... , Av,. by setting \n1 \nu  =   -Av \nfor i =  1, ... , r \nz ai \n' \nThis guarantees that {u\n1\n, ... , u,.} is an orthonormal set in !R\nm\n, but if r < m it will not \nbe a basis for !R\nm\n. In this case, we extend the set {u\n1\n, ... , u,.} to an orthonormal basis \n{u\n1\n, ... , u\nm\n} for !R\nm\n. (This is the only tricky part of the construction; we will describe \ntechniques for carrying it out in the examples below and in the exercises.) Then we set","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":95907,"to":95985}}}}],[1585,{"pageContent":"Theorem 1.13 \nExample 1.34 \nSection 7.4 \nThe Singular Value Decomposition \n593 \nAll that remains to be shown is that this works; that is, we  need to verify that with \nU, V, and Las described, we have A =   ULV\nT\n. Since V\nT\n= v\n-\n1\n, this is equivalent to \nshowing that \nAV =  U!i \nWe know that \nAv\n; = CT\n;U; \nfor i =  1, ... , r \nand \nll\nAv\n;\nll \n= CT\n; \n= 0 for i = r + 1, ... , n. Hence, \nTherefore, \nas required. \nAv\n; \n= 0 \nfor i = r +  1, ... , n \nAV = A\n[\nv\n1 \n[\nAv\n1 \n[\nAv\n1 \nOJ \nOJ \n[\nCT\nI \nQ \n! \nl \nu \nJ \n: \n·\n. \n: \n! \n0 \nm \n, \n0 \n·   ·   · \nCT \ni \n---------\n0·-------�-1--0\n-\nWe have just proved the following extremely important theorem. \nThe Singular Value Decomposition \nLet A be an m  X  n matr  ix with singular values CT \n1 \n2 CT \n2 \n2 ·  ·  · 2 CT r > 0 and \nCT,\n+\n1 \n= CT,.\n+\n2 \n= ··· = CT\nn \n=  0. Then there exist an m  X  m orthogonal matrix U, \nan n  X  n orthogonal matrix V,  and  an m  X  n matr  ix L   of the form shown in \nEquation ( 1) such that \nA factorization of A as in Theorem 7.13 is called a singular value decomposition","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":95987,"to":96075}}}}],[1586,{"pageContent":"an n  X  n orthogonal matrix V,  and  an m  X  n matr  ix L   of the form shown in \nEquation ( 1) such that \nA factorization of A as in Theorem 7.13 is called a singular value decomposition \n(SVD) of A. The columns of U are called left  singular vectors of A, and the columns \nof V are called right singular vectors of A. The matrices U and V are not uniquely \ndetermined by A, but L must contain the singular values of A, as in  Equation (1). (See \nExercise 25.) \nFind a singular value decomposition for the following matrices: \n(a) A �\n[\n� � \n�\n] (b) A �\n[\ni \n�\n]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":96075,"to":96091}}}}],[1587,{"pageContent":"594 \nChapter 7 \nDistance and Approximation \nSolution (a)  We compute \nand find that its eigenvalues are ,\\\n1 \n=  2, ,\\\n2 \n=  1, and ,\\\n3 \n=  0, with corresponding \neigenvectors \nm. \n[\n�\n]\nf i\nl \n� \n(Verify this.) These vectors are orthogonal, so we   normalize them to obtain \nThe singular values of A are \n<T1 \n= Vl, \n<T\n2 \n=  VI= 1, and \n<T\n3 \n=  Vo= 0. Thus, \nand \n[ \nl/Vl 0 \nv =   1\n/\n:2 \n0 \nTo find U, we compute \nu\n,  � \n:\n,\nAv\n,   � \n�\n[\n� \n� \n�\n] \n[\n:;�\n] \n� \n[\n�\n] \nu\n, \n�\n:\n,\nAv\n,� \nH\n�  � \n�l [\n�\n] \n� \n[\n�\n] \nThese vectors already form an orthonormal basis (the standard basis) for IR\n2\n, so we \nhave \nu = \n[\n� \n�] \nThis yields the SVD \n110 \nlOVl  OO \n�r \n[ \nl/Vl  l/Vl O\nJ \nA\n-\n-\n0 0    1 -U.'\"' V \n-[\no  o  1\n] \n-[\no 1\n][ \no   1 \no\n] \n_\n1\n10 \n1\n10 \n0 \n-\n.,-.. \nwhich can be easily checked. (Note that V had to be transposed. Also note that the \nsingular value \n<T\n3 \ndoes not appear in 2:.\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":96093,"to":96206}}}}],[1588,{"pageContent":"Section 7.4 \nThe Singular Value Decomposition \n595 \n(b) This is the matrix in Example 7.33, so we already know that the singular values are \n. \n[ \nl/Vl\n] \n[\n-1\n/Vl\n] \nu\n1 \n= \nV3 \nand u\n2 \n= \nl, correspondmg to \nv\n1 \n= \nl\n/Vl \nand \nv2 \n= \nl\n/Vl \n.  So \n[\n\\13 \nOJ \n}2 \n= \n� \n� \nand \nV \n= \n[l/Vl -1/Vl\n] \nl/Vl l/Vl \nFor U, we compute \nand \nu\n1 \n= \n_!_\nAv\n1 \n=\n, \nl\n;::;-\n[\n� \n<T\n1 \nv 3 \n0 \nl \nl \n[l/Vl\n] \n-\n[\nl/V6\n] \n0 \nl/Vl \n-\nl/V6 \n1 l/V6 \n[\nl   l\nl \n[ \n0 \nl \n1 1 \n-1/Vl \nu2 \n= \n�\nAv\n2 \n= \nl \n1 \n0 \n[ \nl/Vl\n] \n= \n-1/Vl \n2 \n0 \n1 l/Vl \nThis time, we need to extend {u\n1 , u2\n} to an orthonormal basis for IR\n3\n. There are \nseveral ways to proceed; one method is to use the Gram-Schmidt Process, as in \nExample 5.14. We first need to find a linearly independent set of three vectors that con­\ntains u1 and u2. If e\n3 \nis the  third standard basis vector in IR\n3\n, it is clear that {u\n1 ,  u2, e\n3\n} \nis linearly independent. (Here, you should be able to determine this by inspection, but \na reliable method to use in general is to  row reduce the matrix with these vectors as","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":96208,"to":96324}}}}],[1589,{"pageContent":"1 ,  u2, e\n3\n} \nis linearly independent. (Here, you should be able to determine this by inspection, but \na reliable method to use in general is to  row reduce the matrix with these vectors as \nits  columns and use the Fundamental Theorem.) Applying Gram-Schmidt  (with \nnormalization) to {u\n1 ,  u2, e\n3\n} (only the last step is needed), we  find \nso \nand we have the SVD \n[l   l\nl \n[2/V6 \nA= \n1 0 = \n1/\n\\/6 \n0 1 \nl/V6 \n[\n-1M\nl \nU\n3 \n= \n1/\\13 \n1/\\13 \n[\n2/V6 \n0 \n-1/\nv'3\n] \nu \n= \nl/V6 \n-1/Vl \n1\n/\\13 \nl/V6 \nl/Vl \n1/\\13 \n0 \n-1/Vl \nl/Vl \n-l/\n\\13\n]\n[\n\\13 \n°\n]\n[ \nl/\\/2 \n1/\n\\13 \n0 \n1 \n/\n' \n;::;-\n-1 \nv2 \n1\n/\n\\13 \n0     0 \nl/Vl\n] \n= \nU)2V\nT \nl/Vl \n-+ \nThere is another form  of the  singular value decomposition, analogous to the \nspectral decomposition of a  symmetric matr  ix. It is obtained from the SVD by \nan outer product expansion and is very useful in applications. We can obtain this \nversion of the SVD by imitating what we did to obtain the spectral decomposition.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":96324,"to":96403}}}}],[1590,{"pageContent":"596 \nChapter 7 \nDistance and Approximation \nTheorem 1.14 \nAccordingly, we have \ni\n[\n7 \n·\n. \n� i \no]\n[\n�r\n] \nU\nm \n-\n�\n-----:_:_: ____ �_\n�\n_;_ _ ___ _ �T \n0 \ni \n0 \nn \nu,\nt \nu,\nt \nlf,u,\n] \n[-\nnm\n+ \n[\nu,\n,\n, \nrnr \n= \nu\n1\nu\n1\nv\nf \n+ · · · \n+ \nO\"\nr\nu,\nv\n'! \nv\nf \nusing block multiplication and the column-row representation of the product. The \nfollowing theorem summarizes the process for obtaining this outer product fo rm of \nthe SVD. \nThe Outer Product Form of the SVD \nLet \nA be an m X  n matrix with singular values O\" 1 2: O\" 2 2: \n· · · \n2: O\" r > 0 and O\" r\n+ \n1 \n= \nO\" r\n+\nz \n= \n· · · = O\" \nn \n= 0\n. Let u1, ... , \nU\nr be left singular vectors and let v1 , ... , Vr be right \nsingular vectors of A corresponding to these singular values. Then \nA \n= \nu\n1\nu\n1\nv\nf + · · · + \nu\n,\nu,\nv\n'! \nRemark If A is a positive definite, symmetric matrix, then Theorems 7.13 and \n7.14 both reduce to results that we  already know. In this case, it is not hard to show \nthat the SVD generalizes the Spectral Theorem and that Theorem 7 .14 generalizes the \nspectral decomposition. (See Exercise 27.)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":96405,"to":96499}}}}],[1591,{"pageContent":"that the SVD generalizes the Spectral Theorem and that Theorem 7 .14 generalizes the \nspectral decomposition. (See Exercise 27.) \nThe SVD of a matr  ix A   contains much important information about A, as out­\nlined in the crucial Theorem 7.15.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":96499,"to":96502}}}}],[1592,{"pageContent":"Theorem 1.15 \nSection 7.4 \nThe Singular Value Decomposition \n591 \nLet A= U�V\nT\nbe a singular value decomposition of an m  X  n matrix A. Let u1, ... , \nu,. be all the nonzero singular values of A. Then: \na. The rank of A is r. \nb. \n{u1, ... , u,.} is an orthonormal basis for col(A). \nc. {u ,.\n+ \n1 ,  ... , u\nm\n} is an  orthonormal basis for null(A \nT\n). \nd. {v1, ... , v,.} is an orthonormal basis for row(A). \ne. {v,\n+\n1 ,  ... , v\nn\n} is an orthonormal basis for null(A). \nProof \n(a)  By Exercise 61 in Section 3.5, we have \nrank\n(\nA\n) \n= rank\n( \nU!i V\nT) \n= rank\n(\n!, V\nT) \n= rank\n(\n!,\n) \n= r \n(b) We already know that {u1, ... , \nu\n,.} is an orthonormal set. Therefore, it is linearly \nindependent, by Theorem 5.1. Since u\ni \n= \n(\n1\n/ u)Av\n; \nfor i = 1, ... , r, each u\ni \nis in the \n� \ncolumn space of A. (Why?) Furthermore, \nr = rank\n(\nA\n) \n= dim\n(\ncol\n(\nA\n)) \nTherefore, {u1, ... , u,.} is an  orthonormal basis for col(A), by Theorem 6.lO(c). \n(c) Since {u1, ... , u\nm\n} is an  orthonormal basis for !R\nm \nand {u1, ... , u,.} is a basis for","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":96504,"to":96577}}}}],[1593,{"pageContent":"(\nA\n) \n= dim\n(\ncol\n(\nA\n)) \nTherefore, {u1, ... , u,.} is an  orthonormal basis for col(A), by Theorem 6.lO(c). \n(c) Since {u1, ... , u\nm\n} is an  orthonormal basis for !R\nm \nand {u1, ... , u,.} is a basis for \ncol(A), by property (b ), it follows that {u,\n+ \n1 ,  ... , u\nm\n} is an orthonormal basis for the \northogonal complement of col(A). But (col(A)) = null(A \nT\n), by Theorem 5.10. \n(e) Since \nAv\n,\n+\n1 \n= \n·   ·   · \n= Av\nn \n= \n0 \nthe set {v,.\n+\n1 ,  ... , v\nn\n} is an orthonormal set contained in the null space of A. Therefore, \n{v,.\n+ \n1, ... , v\nn\n} is a linearly independent set of n -r vectors in null(A). But \ndim\n(\nnull\n(\nA\n)) \n= n -r \nby the Rank Theorem, so {v,.\n+ \n1 ,  ... , v\nn\n} is an orthonormal basis for null(A), by Theo­\nrem 6.lO(c). \n(d) Property (d) follows from property (e) and Theorem 5.10. (You are asked to \nprove this in Exercise 32.) \nThe SVD provides new geometric insight into the effect of matrix transforma­\ntions. We have noted several times (without proof) that an m  X  n matr  ix transforms \nthe unit sphere in !R\nn","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":96577,"to":96639}}}}],[1594,{"pageContent":"The SVD provides new geometric insight into the effect of matrix transforma­\ntions. We have noted several times (without proof) that an m  X  n matr  ix transforms \nthe unit sphere in !R\nn \ninto an ellipsoid in !R\nm\n. This point arose, for example, in our \ndiscussions of Per  ron's Theorem and of operator norms, as well as in the introduction \nto singular values in this section. We now prove this result.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":96639,"to":96647}}}}],[1595,{"pageContent":"598 \nChapter 7 \nDistance and Approximation \nTheorem 1.16 \nLet A be an m  X n matrix with rank r. Then the image of the unit sphere in !R\nn \nunder the matrix transformation that maps x to Ax is \na.  the surface of an ellipsoid in !Rm if r  = n. \nb. a solid ellipsoid in !Rm if r < n. \nProof Let A = U2. V\nT \nbe a singular value decomposition of the m X n matrix A. Let \nthe left and right singular vectors of A be u1, ... , u\nm \nand v1, ... , v\nn\n, respectively. Since \nrank(A) = r, the singular values of A satisfy \nby Theocem 7 .15( a). Let x � [ JJ he a unH vedodn fll\". Now, '\n'\"\n\" \nV ;, an orthogonal \nmatrix, so is v\nT\n, and hence v\nT \nxis a unit vector, by Theorem 5.6. Now \nso (v\nf \nx)\n2 \n+ .. · + (v\n�\nx)\n2 \n= I. \nBy the outer product form of the SVD, we have A \nTherefore, \nwhere we are lettingy; denote the scalar <.T;vTx. \n(a)  If r  = n, then we must have n :s m and \nAx \n= \nY\n1 \nU\n1 \n+ \n... \n+ \nY\nnUn \n=Uy \nwhe<e y \n� \n[\n;J Thecef\n°\n''· \nagaill \nby The\n°\n'em 5.6, \nll\nAx\nll \n� \nll\nU\nrll \n� \nllrll\n. \n'\n'\"\n\" \nU\n\" \northogonal. But \n(\n;1\nJ\n2 \n+ \n· · ·  + \n(\n;:)\n2 \n= \n(\nv\nf\nx\n)\n2 \n+","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":96649,"to":96743}}}}],[1596,{"pageContent":"Ax \n= \nY\n1 \nU\n1 \n+ \n... \n+ \nY\nnUn \n=Uy \nwhe<e y \n� \n[\n;J Thecef\n°\n''· \nagaill \nby The\n°\n'em 5.6, \nll\nAx\nll \n� \nll\nU\nrll \n� \nllrll\n. \n'\n'\"\n\" \nU\n\" \northogonal. But \n(\n;1\nJ\n2 \n+ \n· · ·  + \n(\n;:)\n2 \n= \n(\nv\nf\nx\n)\n2 \n+ \n· · ·  + \n(\nv\n�\nx\n)\n2 \n= \n1 \n� \nwhich shows that the  vectors Ax form the surface of an ellipsoid in !Rm. (Why?) \n(b) If r < n, the only difference in the above steps is that the equation becomes \n(\n;1\nJ\n2 \n+ \n... \n+ \n(\n�J\n2 \n:s 1 \nsince we are missing some terms. This inequality corresponds to a solid ellipsoid \nin !Rm.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":96743,"to":96822}}}}],[1597,{"pageContent":"Example 1.35 \nSection 7.4 \nThe Singular Value Decomposition \n599 \nDescribe the image of the unit sphere in IR\n3 \nunder the action of the matrix \nA = \n[ \n1  1  O\nJ \n0  0  1 \nSolution In Example 7.34(a), we found the following SVD of A: \n[ 1/v'2 \n[ \n1  1  O\nJ = \n[ \n1  O\nJ \n[ \nVl  0  O\nJ \nO \n0  0  1     0  1   0   1  0 \n/\n' \nh \n-1 \nv2 \n1\n/\n:2 \n0\n0\n1\n] \n1\n/Vl \nSince r = rank(A) = 2 < 3 = n, the second part of Theorem 7.16 applies. The image \nof the unit sphere will satisfy the inequality \n2 \nY\n1 \n+ \ny\n� \n::::; \n1 \n2 \nrelative to y1y\n2 \ncoordinate axes in IR\n2 \n(corresponding to the left singular vectors u1 \nand u\n2\n). Since \nu1 \n= e\n1 \nand \nu\n2 \n= e\n2\n, the image is as shown in Figure 7.20. \nz \nY\n2 \n-1 \nx \nFigure 1.20 \nIn general, we can describe the effect of an m  X  n matr  ix A   on the unit sphere \nin !R\nn \nin terms of the effect of each factor in its SVD, A = u� V\nT\n, from right to left. \nSince V\nT \nis an orthogonal matrix, it maps the unit sphere to itself. The m X  n matrix \n� does two things: The diagonal entries u r+ \n1 \n= u r+z = \n· · · \n= u \nn","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":96824,"to":96911}}}}],[1598,{"pageContent":"T\n, from right to left. \nSince V\nT \nis an orthogonal matrix, it maps the unit sphere to itself. The m X  n matrix \n� does two things: The diagonal entries u r+ \n1 \n= u r+z = \n· · · \n= u \nn \n= 0 collapse n - r \nof the dimensions of the unit sphere, leaving an r-dimensional unit sphere, which the \nnonzero diagonal entries u 1 , ... , u r then distort into an ellipsoid. The orthogonal \nmatrix U then aligns the axes of this ellipsoid with the orthonormal basis vectors \nu1, ... , u\nr in !R\nm\n. (See Figure 7.21.) \nAPPliCalions Of lhe SUD \nThe singular value decomposition is an  extremely useful tool, both practically and \ntheoretically. We will look at just a few of its many applications.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":96911,"to":96932}}}}],[1599,{"pageContent":"600 \nChapter 7 \nDistance and Approximation \nFigure 1.21 \nJ \nRank Until now, we have not worried about calculating the rank of a matrix from \na computational point of view. We compute the rank of a matrix by row reducing \nit to echelon form and counting the number of nonzero rows. However, as we have \nseen, roundoff errors can affect this process, especially if the matrix is ill-conditioned. \nEntries that should be zero may end up as very small nonzero numbers, affecting our \nability to accurately determine the rank and other quantities associated with the ma­\ntrix. In practice, the SVD is often used to find the rank of a matrix, since it is much \nmore reliable when roundoff errors are present. The basic idea behind this approach \nis that the orthogonal matrices U and Vin the SVD preserve lengths and thus do not \nintroduce additional errors; any errors that occur will tend to show up in the matrix 2:. \ncAs \nExample 1.36 \nLet \n[8.1650 \nA= 4.0825 \n4.0825 \n-0.0041 \n-3.9960 \n4.0042 \n-0.0041 l \n4.0042 \n-3.9960","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":96934,"to":96960}}}}],[1600,{"pageContent":"introduce additional errors; any errors that occur will tend to show up in the matrix 2:. \ncAs \nExample 1.36 \nLet \n[8.1650 \nA= 4.0825 \n4.0825 \n-0.0041 \n-3.9960 \n4.0042 \n-0.0041 l \n4.0042 \n-3.9960 \n[8.17 \nand  B = 4.08 \n4.08 \n0 \n-4 \n4 \nThe matrix B has been obtained by rounding off the entries in A to two decimal \nplaces. If we compute the ranks of these two approximately equal matrices, we find \nthat rank(A) = 3 but rank(B) = 2. By the Fundamental Theorem, this implies, among \nother things, that A is invertible but B is not. \nThe explanation for this critical difference between two matrices that are approxi­\nmately equal lies in their SVDs. The singular values of A are 10, 8, and 0.01, so A has \nrank 3. The singular values of B are 10, 8, and 0, so B has rank 2. \nIn practical applications, it is  often assumed that if a singular value is computed to \nbe close to zero, then roundoff error has crept in and the actual value should be zero.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":96960,"to":96987}}}}],[1601,{"pageContent":"In practical applications, it is  often assumed that if a singular value is computed to \nbe close to zero, then roundoff error has crept in and the actual value should be zero. \nIn this way, \"noise\" can be filtered out. In this example, if we compute A= UL:V\nT \nand \nreplace \n�= \n[10  0  0 l \n� \n� \n�.01 \n..-.. \nthen UL:'V\nT \n= B. (Try it!) \nby �I= \nMatrix Norms and lhe Condilion Number The SVD can provide simple formulas \nfor certain expressions involving matrix norms. Consider, for example, the Frobenius \nnorm of a matrix. The following theorem sh   ows that it  is completely determined by \nthe singular values of the matrix.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":96987,"to":97006}}}}],[1602,{"pageContent":"Theorem 1.11 \ncAs \nEx\na\nm\nple \n1.3\n1 \nSection 7.4 \nThe Singular Value Decomposition \n601 \nLet A be an m  X  n matrix and let u 1, ... , u r be all the nonzero singular values of \nA. Then \nThe proof of this result depends on the following analogue of Theorem 5.6: \nIf A is an m  X  n matr  ix and Q is an m X  m orthogonal matrix, then \nTo show that this is true, we compute \nll\nO\nA\nll\n� = \nII \n[ \nQa\n1 \n· · · \nQa\n\" \nl \nII\n� \n= \nll\nO\na\n1ll\n� + \n· · · \n+ \nll\nQa\nnll\n� \n= \nll\na\n1ll\n� + \n· · · \n+ \nll\na\nnll\n� \n= \nll\nA\nll\n� \n(\n2\n) \nProof of Theorem 1.11 Let A = U2. V\nT \nbe a   singular value decomposition of A.   Then, \nusing Equation (2) twice, we have \nll\nA\nll\n� = \nl\niU\nk\nV\nT\nll\n� \n= \nll\nk\nV\nT\nll\n� = \nll\n(\nk\nV\nT)T\nll\n� \n= \nll\nV\nk \nT\nll\n� = \nI\nl\nk \nT\nll\n� = u\nf \n+ \n· · · \n+ u\n� \nwhich establishes the result. \nVerify Theorem 7 .17 for the matrix A in Example 7 .18. \nSolution The  matrix A = [� \ncheck that \n-\n1] \n4 \nhas  singular values 4.5150  and 3.1008. We \nV \n4.5150\n2 \n+ \n3.100\n8\n2 \n= \nV3Q \n= \nll\nA\nll\nF \nwhich agrees with Example 7.18. \nIn Section 7.2, we commented that there is no easy formula for the operator","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":97008,"to":97140}}}}],[1603,{"pageContent":"-\n1] \n4 \nhas  singular values 4.5150  and 3.1008. We \nV \n4.5150\n2 \n+ \n3.100\n8\n2 \n= \nV3Q \n= \nll\nA\nll\nF \nwhich agrees with Example 7.18. \nIn Section 7.2, we commented that there is no easy formula for the operator \n2-norm of a matr  ix A. Although that is true, the SVD of A provides us with a very nice \nexpression for \nll\nA\nll\n2\n. \nRec\na\nll \nth\nat \nll\nA\nll\n2 \n= max\nll\nAx\nll \nll\nx\nll\n�\nl \nwhere the vector norm is the ordinary Euclidean norm. By Theorem 7.16, for \nll\nx\nll \n=  1, \nthe set of vectors \nll\nAx\nll \nlies on or inside an ellipsoid whose semi-axes have lengths","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":97140,"to":97194}}}}],[1604,{"pageContent":"602 \nChapter 7 \nDistance and Approximation \nequal to the nonzero singular values of A. It follows immediately that the largest of \nthese is u \n1\n, so \nThis provides us with a neat way to express the condition number of a (square) \nmatrix with respect to the operator 2-norm. Recall that the condition number (w  ith \nrespect to the operator 2-norm) of an invertible matrix A is defined as \nAs you will be asked to show in Exercise 28, if A  = \nU\nL:V\nT\n, then A-\n1 \n= V2:-\n1\nU\nT\n. \n� \nTherefore, the singular values of A \n-\nl \nare 1/ u\n1\n, ... , 1/ u \nn \n(why?), and \nExample 1.38 \nE. H. Moore (1862-1932) was an \nAmerican mathematician who \nworked in group theory, number \ntheory, and geometry. He was \nthe first head of the mathemat­\nics department at the University \nof Chicago when it opened in \n1892. In 1920, he introduced a \ngeneralized matrix inverse that \nincluded rectangular matrices. \nHis work did not receive much \nattention because of his obscure \nwriting style. \nl/u\nn \n2: · · · 2: l/u\n1 \nIt follows that \nll\nA \n-\nl \n11\n2 \n= \n1 \nI \n(J\" \nn\n> \nso","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":97196,"to":97257}}}}],[1605,{"pageContent":"included rectangular matrices. \nHis work did not receive much \nattention because of his obscure \nwriting style. \nl/u\nn \n2: · · · 2: l/u\n1 \nIt follows that \nll\nA \n-\nl \n11\n2 \n= \n1 \nI \n(J\" \nn\n> \nso \nFind the 2-condition number of the matrix A in Example 7.36. \nSolulion \nSince u\n1 \n= 10 and u\n3 \n= 0.01, \nU\n1 \n10 \ncond\n2\n(\nA\n) \n=  -  =  -  = 1000 \nU\n3 \n0.01 \nThis value is large enough to suggest that A may be ill-conditioned and we should be \nwary of the effect of roundoff errors. \n4 \nThe Pseudoinverse  and leasl Squares  Approximalion In Section 7.3, we pro­\nduced the formula A+ =  (A\nT \nA)-\n1\nA \nT \nfor the pseudoinverse of a matrix A. Clearly, this \nformula is valid only if A\nT \nA is invertible, as we noted at the time. Equipped with the \nSVD, we can now define the pseudoinverse of any matrix, generalizing our previous \nformula. \nDefinition Let A \n= \nU\nL:V\nT \nbe an SVD for an m x n matr  ix A, where 2: = \n[ \n� �\n] \nand Dis an r X r diagonal matrix containing the nonzero singular values \nu\n1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":97257,"to":97325}}}}],[1606,{"pageContent":"formula. \nDefinition Let A \n= \nU\nL:V\nT \nbe an SVD for an m x n matr  ix A, where 2: = \n[ \n� �\n] \nand Dis an r X r diagonal matrix containing the nonzero singular values \nu\n1 \n2: u2 2: ·  ·  · 2: ur > 0 of A. The pseudoinverse (or Moore-Penrose inverse) of A \nis the n X  m matr  ix A+ defined by \nwhere 2:+ is the n X  m matrix \nA +   = v�+\nur \n�+ = \n[\nv\n-\n1 \nO\nJ \n0   0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":97325,"to":97350}}}}],[1607,{"pageContent":"Example 1.39 \nOne of those who was unaware of \nMoore's work on matrix inverses \nwas Roger Penrose (b. 1931), who \nintroduced his own notion of \na generalized matrix inverse \nin 1955. Penrose has made many \ncontributions to geometry and \ntheoretical physics. He is also the \ninventor of a type of nonperiodic \ntiling that covers the plane with \nonly two different shapes of tile, \nyet has no repeating pattern. He has \nreceived many awards, including the \n1988 Wolf Prize in Physics, which \nhe shared with Stephen Hawking. In \n1994, he was knighted for services \nto science. Sir Roger Penrose is \ncurrently the Emeritus Rouse Ball \nProfessor of Mathematics at the \nUniversity of Oxford. \nSection 7.4 \nThe Singular Value Decomposition \n603 \nFind the pseudoinverses of the matrices in Example 7 .34. \nSolulion \n(a) \nFrom the SVD \n[\n� \n1 \n�\n] \n= \n[\n� \nA= \n0 \nwe form \nThen \n[1\n/V2 \nA+ \n= v�+ur  = l\n/\n[2 \n(b) \nWe have the SVD \n�\n]\n[\n� \n0 \n�l\n[ \nl\n�V2 \n1 \n-1/\\/2 \n['\nM \"] \n�+ = 0 \n1 \n0    0 \n0 \n-1\n/v'\nl\n1\n/V2 \n0 \n1 \n0 \n1\n/\\/2 \n0 \n0 0 \n1/\\/2 \n0 \n1/\\/2 \n�\n]\n[\n� \n�\n] \n�\n] \n[ \n1\n/\n2 \n1\n/\n2 \n0 \n�\n] \nA\n� \n[","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":97352,"to":97452}}}}],[1608,{"pageContent":"[\n� \n1 \n�\n] \n= \n[\n� \nA= \n0 \nwe form \nThen \n[1\n/V2 \nA+ \n= v�+ur  = l\n/\n[2 \n(b) \nWe have the SVD \n�\n]\n[\n� \n0 \n�l\n[ \nl\n�V2 \n1 \n-1/\\/2 \n['\nM \"] \n�+ = 0 \n1 \n0    0 \n0 \n-1\n/v'\nl\n1\n/V2 \n0 \n1 \n0 \n1\n/\\/2 \n0 \n0 0 \n1/\\/2 \n0 \n1/\\/2 \n�\n]\n[\n� \n�\n] \n�\n] \n[ \n1\n/\n2 \n1\n/\n2 \n0 \n�\n] \nA\n� \n[\ni \n'\n] \n[2\n/\nv'6 \n0 = 1\n/\n\\/6 \n-1\n/\\/2 \n-iM] \n[\nv'3 \"]\n[ \n1M \n1/\\/3 0   1 \n/\n\\/2 \n-1 \n2 \n1\n/\\/2] \n1\n/\\/2 \nu�vr \nso \nand \n1 \n1\n/\\/6 \n1\n/\\/2 \n1/\\/3 0   0 \n�+ = \n[l\n/\n[3 \n0 \n�\n] \n1 \n-1\n/\\/2] [\n1/\\/3 0 \n1\n/\\/2 \n0 \n�\nJ\n[ \n2\n/\n:6 \n-1\n/\\/3 \n[1/\n3 \n1/3 \n2/3 \n-1/3 \n-\n1/3\n] \n2/3 \n1\n/\\/6 \n-1/\\/2 \n1/\\/3 \n1\n/\\/6\n] \n1/\\/2 \n1/\\/3 \nIt is straightforward to check that this new definition of the pseudoinverse general­\nizes the old one, for if the m X  n matrix A  = U� V\nT \nhas linearly independent columns, \nthen direct substitution shows that \n(\nA\nT \nA\n)\n-IA\nT \n= v�+ u\nT\n. (You are asked to verify this \nin Exercise 50.) Other properties of the pseudoinverse are explored in the exercises. \nWe have seen that when A has linearly independent columns, there is a unique \nleast squares solution x to A\nx \n= b; that is, the normal equations A\nT \nA\nx \n= A \nT\nb have the \nunique solution \nx = \n(\nA\nT\nA\n)\n-\n1\nArb  = A+b","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":97452,"to":97634}}}}],[1609,{"pageContent":"least squares solution x to A\nx \n= b; that is, the normal equations A\nT \nA\nx \n= A \nT\nb have the \nunique solution \nx = \n(\nA\nT\nA\n)\n-\n1\nArb  = A+b \nWhen the columns of A are linearly dependent, then A\nT \nA is not invertible, so the nor­\nmal equations have infinitely many solutions. In this case, we will ask for the solution \nx of minimum length (i.e., the one closest to the origin). It turns out that this time we \nsimply use the general version of the pseudoinverse.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":97634,"to":97658}}}}],[1610,{"pageContent":"604 \nChapter 7 \nDistance and Approximation \nTheorem 1.18 \nExample 1.40 \nThe least squares problem A\nx \n=  b has a unique least squares solution x of minimal \nlength that is given by \nProof Let A be an m  X  n matrix of rank r with SVD A  = \nU\n:2:V\nT \n(so that A\n+   = \nv:2:+ u\nT\n)\n. Let y  =  v \nT\nX and let c  =  u\nT\nb. Write y and c in block form as \ny  = \n[\n;\n:\n] \nand c  = \n[\n:\n:\n] \nwhere y\n1 \nand c\n1 \nare in !Pt. \nWe wish to minimize \nll\nb  -  A\nx\ni\nl \nor, equivalently, \nll\nb  -  A\nx\nll\n2\n• Using Theorem 5.6 \nand the fact that \nU\nT \nis orthogonal (because \nU \nis), we have \nThe only part of this expression that we have any control over is y\n1\n, so the mini­\nmum value occurs when c\n1 \n-Dy\n1 \n= 0 or, equivalently, when y\n1 \n= D-\n1\nc\n1\n. So all least \nsquares solutions x are of the form \nSet \n[D-\n1 \n] \nx \n=Vy= v \nY\n2\nC\n1 \nWe claim that this xis the  least squares solution of minimal length. To show this, let's \nsuppose that \nis a different least squares solution (hence, y\n2 \n-=fa \n0). Then \nllxll \n= \nll\nV\nYll \n= \nllrll \n< \nllY'll \n= \nll\nV\nY'll \n= \nll\nx\n'll \nas claimed.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":97660,"to":97765}}}}],[1611,{"pageContent":"suppose that \nis a different least squares solution (hence, y\n2 \n-=fa \n0). Then \nllxll \n= \nll\nV\nYll \n= \nllrll \n< \nllY'll \n= \nll\nV\nY'll \n= \nll\nx\n'll \nas claimed. \nWe still must show that xis equal to A+ b. To do so, we simply compute \nFind the minimum length least squares solution of A\nx \n= b, where \nA  = \n[ \n�   �\n] \nand b  = \n[ \n�\n]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":97765,"to":97799}}}}],[1612,{"pageContent":"Section 7.4 \nThe Singular Value Decomposition \n605 \nSolution The corresponding equations \nx+y=O \nx+y=l \nare clearly inconsistent, so a   least squares solution is our only hope. Moreover, the \ncolumns of A are linearly dependent, so there will be infinitely many least squares \nsolutions-among which we want the one with minimal length. \nAn SVD of A is given by \nA= \n[\n� \nl\nJ = \n[l/V2 \n1 \nl/V2 \nl/V2\nJ \n[\n2 \n-1/V2 0 \nO\nJ \n[l/Vl \n0  l   /V2 \nl/V2\nJ \n-1/V2 \n� \n(Verify this.) It follows that \nTheorem 1.19 \nA+ = v�+ur = \n[1\n/V2 \nl/V2 \nso \nl/V2\nJ \n[\n1/2 \n-1/V2 0 \nO\nJ \n[l/Vl \n0  l   /V2 \nl/V2\nJ \n-l/V2 \n[1\n/\n4 \n1/\n4 \nl/4\nJ \n1/4 \nYou can see that the  minimum least squares solution in Example 7.40 satisfies \nx + y = �. In a sense, this is a compromise between the two equations we started \nwith. In Exercise 49, you are asked to solve the normal equations for this problem \ndirectly and to verify that this solution really is the one closest to the origin. \nThe Fundamenlal  Theorem of  Invertible Malrices It is appropriate to conclude","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":97801,"to":97863}}}}],[1613,{"pageContent":"directly and to verify that this solution really is the one closest to the origin. \nThe Fundamenlal  Theorem of  Invertible Malrices It is appropriate to conclude \nby  revisiting the  Fundamental Theorem of Invertible Matrices one  more time. \nNot surprisingly, the singular values of a square matrix tell us when the matrix is \ninvertible. \nThe Fundamental Theorem of Invertible Matrices: Final Version \nLet A be an n  X  n matrix and let T : V ---+ W be a linear transformation whose \nmatrix [TJ\nc\n,_8 with respect to bases B and C of V and W, respectively, is A. The \nfollowing statements are equivalent: \na.  A   is invertible. \nb. Ax = b has a unique solution for every b in !R\nn\n. \nc.  Ax = 0 has only the trivial solution. \nd. The reduced row echelon form of A is I\nn\n-\ne.  A   is a product of elementary matrices. \nf.  rank(A) = n \ng.  nullity(A) = 0 \nh.  The column vectors of A are linearly independent. \ni.  The column vectors of A span !R\nn\n. \nj.  The column vectors of A form a basis for !R\nn\n.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":97863,"to":97891}}}}],[1614,{"pageContent":"f.  rank(A) = n \ng.  nullity(A) = 0 \nh.  The column vectors of A are linearly independent. \ni.  The column vectors of A span !R\nn\n. \nj.  The column vectors of A form a basis for !R\nn\n. \nk.  The row vectors of A are linearly independent. \n1. The row vectors of A span !R\nn\n.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":97891,"to":97903}}}}],[1615,{"pageContent":"606 \nChapter 7 \nDistance and Approximation \nm. The row vectors of A form a basis for IJ�r. \nn. det A of-0 \no.  0   is not an eigenvalue of A. \np.  T is invertible. \nq.  Tis one-to-one. \nr.  Tis onto. \ns.  ker(T) = {O} \nt.  range(T) = W \nu.  0   is not a    singular value of A. \nProof First note that, by the definition of singular values, 0 is a singular value of A if \nand only if 0 is an eigenvalue of A\nT \nA. \n(a) => ( u) If A is invertible, so is A\nT\n, and hence A\nT \nA is as  well. Therefore, property ( o) \nimplies that 0 is not an eigenvalue of A\nT \nA, so 0 is not a singular value of A. \n(u) => (a)  If 0 is not a singular value of A, then 0 is not an eigenvalue of A\nT \nA. \nTherefore, A\nT \nA is invertible, by the equivalence of properties (a) and ( o ). But then \nrank(A) = n, by Theorem 3.28, so A is invertible, by the equivalence of properties (a) \nand (f).","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":111452,"to":111483}}}}],[1616,{"pageContent":"Figure 1.22 \nVignette \nDigital Image Compression \nAmong the many applications of the SVD, one of the most impressive is its use in \ncompressing digital images so that they can be efficiently transmitted electronically \n(by satellite, fax, Internet, or the like). We have already discussed the problem of \ndetecting and correcting errors in such transmissions. The problem we now wish to \nconsider has to do   with reducing the amount of information that has to be transmit­\nted, without losing any essential information. \nIn the case of digital images, let's suppose we have a grayscale picture that is \n340 X 280 pixels in size. Each pixel is one of 256 shades of gray, which we can repre­\nsent by a number be  tween 0 and 255. We can store this information in a 340 X 280 \nmatrix A, but transmitting and manipulating these 95,200 numbers is very expensive. \nThe idea behind image compression is that some parts of the picture are less interest­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":111485,"to":111498}}}}],[1617,{"pageContent":"matrix A, but transmitting and manipulating these 95,200 numbers is very expensive. \nThe idea behind image compression is that some parts of the picture are less interest­\ning than others. For example, in a photograph of someone standing outside, there \nmay be a lot of sky in the background, while the person's face contains a lot of detail. \nWe can probably get away with transmitting every second or third pixel in the back­\nground, but we would like to keep all the pixels in the region of the face. \nIt turns out that the small singular values in the SVD of the matrix A come from \nthe \"boring\" parts of the image, and we can ignore many of them. Suppose, then, that \nwe have the SVD of A in outer product form \nA = \nu\n1\nu\n1\nv\nf \n+ · · · + u,u,v; \nLet k :::::  rand define \nA\nk \n= \nu\n1\nu\n1\nv\nf \n+ · · · + u\nk\nu\nk\nv\n[ \nThen Ak is an approximation to A   that corresponds to keeping only the first k singu­\nlar values and the corresponding singular vectors. For our 340 X 280 example, we may","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":111498,"to":111532}}}}],[1618,{"pageContent":"A\nk \n= \nu\n1\nu\n1\nv\nf \n+ · · · + u\nk\nu\nk\nv\n[ \nThen Ak is an approximation to A   that corresponds to keeping only the first k singu­\nlar values and the corresponding singular vectors. For our 340 X 280 example, we may \ndiscover that it is enough to transmit only the data corresponding to the first 20 singular \nvalues. Then, instead of transmitting 95,200 numbers, we need only send 20 singular values \nplus the 20 vectors u\n1\n, ... , u\n2\n0 \nin IR\n3\n4\n0 \nand the 20 vectors \nv\n1\n, ... , v\n2\n0 \nin IR\n2\n8\n0\n, for a total of \n20  + 20\n. \n340  + 20\n. \n280 = 12,420 \nnumbers. This represents a substantial saving! \nThe picture of the mathematician Gauss in Figure 7 .22 is a 340 X 280 pixel image. \nIt has 256 shades of gray, so the corresponding matrix A is 340 X 280, with entries \nbetween 0 and 255. \nIt turns out that the matrix A has rank 280. If we approximate A by Ak, as de­\nscribed above, we get an image that corresponds to the first k singular values of A.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":111532,"to":111581}}}}],[1619,{"pageContent":"between 0 and 255. \nIt turns out that the matrix A has rank 280. If we approximate A by Ak, as de­\nscribed above, we get an image that corresponds to the first k singular values of A. \nFigure 7 .23 shows several of these images for values of k from 2 to 256. At first, the \nimage is very blurry, but fairly quickly it takes shape. Notice that A\n32 \nalready gives \na pretty good approximation to the actual image (which comes from A \n= \nA\n2\n8\n0\n, \nas \nshown in the upper left-hand corner of Figure 7.23). \nSome \nof the singular values of A are u\n1 \n= 49,096, u\n1\n6 \n= 22,589, u\n32 \n= 10,187, \n<T6\n4 \n= 484,u\n1\n2\n8 \n= 182,u\n2\n56 \n= 5,andu\n2\n8\n0 \n= 0.5. Thesmallersingularvaluescontributevery \nlittle to the image, which is why the approximations quickly look so close to the original. \n601","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":111581,"to":111621}}}}],[1620,{"pageContent":"Original, k = r = 280 \nk=2 \nk=4 \nk=8 \nk =  16 \nk =  32 \nk = 64 \nk \n= 128 \nk \n= 256 \nFigure 7.23 \n608","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":111623,"to":111635}}}}],[1621,{"pageContent":".. \n1 \nExercises 1.4 \nIn Exercises 1-10, find the singular values of th e given matrix. \n1. A= \n[\n� \n�\n] \n2. A= \n[\n� \n�\n] \n3.A  = \n[\n� �\n] \n[\nY2 \n4. A= \nO \n�\n] \n5.A  = \n[\n!\n] \n6. A= \n[3 \n4] \n7.A  = \nu �\n] \n8. A= \n[\n: \n-\n�\nl \n9. A  = \n[\n� \n�\n] \n[\n1 \n0 \n:\nJ \n0 \n10. A= \n-\n3 \n2 \n0 \nIn Exercises 11-20, find an SVD of the indicated matrix. \n11. A in Exercise 3 \n13.A  = \n[ \n0 \n-\n2\n] \n-\n3 \n0 \n15. A in Exercise 5 \n17. A in Exercise 7 \n19. A in Exercise 9 \n12. A= \n[\n1\n1 \n1\n1\n] \n14. A= \n[\n1\n1 \n-\n1\n1\n] \n16. A in Exercise 6 \n18. A in Exercise 8 \n20. A= \n[\n� \n�\n] \nIn Exercises 21-24,find the outer product form of the SVD \nfor th e matrix in the given exercises. \n21. Exercises 3 and 11 \n22. Exercise 14 \n23. Exercises 7 and 17 24. Exercises 9 and 19 \n25. Show that the matrices \nU \nand Vin the SVD are not \nuniquely determined. [Hint: Find an example in which \nit would be possible to make different choices in the \nconstruction of these matrices.] \n26. Let A be a symmetric matrix. Show that the singular \nvalues of A are: \n(a) the absolute values of the eigenvalues of A.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":111637,"to":111742}}}}],[1622,{"pageContent":"construction of these matrices.] \n26. Let A be a symmetric matrix. Show that the singular \nvalues of A are: \n(a) the absolute values of the eigenvalues of A. \n(b) the eigenvalues of A if A is positive definite. \n27. (a) Show that, for a positive definite, symmetric \nmatrix A, Theorem 7 .13 gives the orthogonal \ndiagonalization of A, as guaranteed by the Spectral \nTheorem. \nSection 7.4 \nThe Singular Value Decomposition \n609 \n(b) Show that, for a positive definite, symmetric \nmatrix A, Theorem 7 .14 gives the spectral \ndecomposition of A. \n28. If A is an  invertible matrix with SVD A  = \nU\n!i vr, \nshow that !, is invertible and that A \n-\nI \n= V!i \n-\nI \nu\nT \nis \nan SVD of A\n-\n1\n• \n29. Show that if A  = U!i V\nT \nis an SVD of A, then the left \nsingular vectors are eigenvectors of AA r. \n30. Show that A and A\nT \nhave the same singular values. \n31. Let Q be an   orthogonal matrix such that QA makes \nsense. Show that A and QA have the same singular \nvalues. \n32. Prove Theorem 7.lS(d). \n33. What is the image of the unit circle in IR\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":111742,"to":111785}}}}],[1623,{"pageContent":"31. Let Q be an   orthogonal matrix such that QA makes \nsense. Show that A and QA have the same singular \nvalues. \n32. Prove Theorem 7.lS(d). \n33. What is the image of the unit circle in IR\n2 \nunder the \naction of the matr  ix in Exercise 3? \n34. What is the image of the unit circle in IR\n2 \nunder \nthe action of the matrix in Exercise 7? \n35. What is the  image of the unit sphere in IR\n3 \nunder the \naction of the matr  ix in Exercise 9? \n36. What is the image of the unit sphere in IR\n3 \nunder \nthe action of the matrix in Exercise 10? \nIn Exercises 37-40, compute (a) \nll\nA\nll\nz \nand (b) cond\n2\n(A\n) \nfor \nthe indicated matrix. \n37. A in Exercise 3 \n[ \n1  0.9\n] \n39.A  = \n1  1 \n38. A in Exercise 8 \n[ \n10 \n40. A= \n100 \n10 \n100 \n�\n] \nIn Exercises 41-44, compute the pseudoinverse A\n+ \nof A in \nth e given exercise. \n41. Exercise 3 \n42. Exercise 8 \n43. Exercise \n9 \n44. Exercise 10 \nIn Exercises 45-48, find A\n+ \nand use it to compute th e mini­\nmal length least squares solution to Ax = b. \n45.A  = \n[\nl \n2\n] b -\n[\n3\n] \n2 4 ' \n-\n5 \n46.A  = \n[\n� � \n�\n]\n,b = \n[\n�\n] \n47.A  = \n[","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":111785,"to":111865}}}}],[1624,{"pageContent":"43. Exercise \n9 \n44. Exercise 10 \nIn Exercises 45-48, find A\n+ \nand use it to compute th e mini­\nmal length least squares solution to Ax = b. \n45.A  = \n[\nl \n2\n] b -\n[\n3\n] \n2 4 ' \n-\n5 \n46.A  = \n[\n� � \n�\n]\n,b = \n[\n�\n] \n47.A  = \n[\n: } \n� \n[\n�\n:","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":111865,"to":111898}}}}],[1625,{"pageContent":"610 \nChapter 7 \nDistance and Approximation \n48. A \n� \n[ \n� \n� + \n� \nm \n49. (a) Set up and solve the normal equations for the \nsystem of equations in Example 7.40. \n(b) Find a parametric expression for the length of a \nsolution vector in part (a). \n(c) Find the solution vector of minimal length and \nverify that it is the  one produced by the method \nof Example 7.40. [Hint: Recall how to find the \ncoordinates of the vertex of a parabola.] \n56. Let Q be an orthogonal matrix such that QA makes \nsense. Show that (QA)\n+ \n= A\n+ \nQ\nT\n. \n57. Prove that if A is a positive definite matrix with SVD \nA= u2v\nT\n, then u = v. \n58. Prove that for a diagonal matrix, the 1-, 2-, and \nClO-norms are the same. \n59. \nProve \nthat for any \nsquare \nmatrix A, \nll\nA\nll\n� :::::: \nll\nA\nll1ll\nA\nll\n00\n· \n[Hint: \nll\nA \nII\n� \nis  the square of the largest singular value of \nA and hence is equal to the largest eigenvalue of A\nT \nA. \nNow use Exercise 34 in Section 7.2.] \n50. Verify that when A has linearly independent col-\n�\nEvery \ncomplex number can  be written in polar form as","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":111900,"to":111961}}}}],[1626,{"pageContent":"A and hence is equal to the largest eigenvalue of A\nT \nA. \nNow use Exercise 34 in Section 7.2.] \n50. Verify that when A has linearly independent col-\n�\nEvery \ncomplex number can  be written in polar form as \numns, the definitions of pseudoinverse in this section \nz = re\n;\n8, where r =  lzl is a nonnegative real number \nand in Section 7.3 are the same. \nand e is its argument, with \nk\n0\n1 \n=  1. \n(S\nee \nAp\npendi\nx \nC.) \n51. Verify that the pseudoinverse (as defined in this \nThus, z has been decomposed into a stretching factor rand \nsection) satisfies the Penrose conditions for A \na rotation factor e\n;\n0\n•  There is an analogous decomposition \n(Theorem 7.12 in Section 7.3). \nA = RQfor square matrices, called th e polar \n52. Show that A\n+ \nis the only matr  ix that satisfies the \nPenrose conditions for A. To do this, assume that \nA' is a matr  ix satisfying the Penrose conditions: \n(a) AA'A =A, (b) A'AA' =  A', and (c) AA' and A' A \nare symmetric. Prove that A' =A\n+\n. [Hint: Use the \nPenrose conditions for A\n+ \nand A' to show that","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":111961,"to":112005}}}}],[1627,{"pageContent":"A' is a matr  ix satisfying the Penrose conditions: \n(a) AA'A =A, (b) A'AA' =  A', and (c) AA' and A' A \nare symmetric. Prove that A' =A\n+\n. [Hint: Use the \nPenrose conditions for A\n+ \nand A' to show that \nA\n+\n= A'AA \n+\nand A' = A'AA \n+\n.It is helpful to note \nthat condition ( c) can be written as AA' =  (A') \nT\nA\nT \nand \nA' A = A\nT \n(A') \nT\n,  with similar versions for A\n+\n.] \n53. Show that (A\n+ \nt =   A. [ Hint: Show that A  satisfies the \nPenrose conditions for A\n+\n. By Exercise 52, A must \ntherefore be (A +t.\nJ \n54. Show that (A +\n)\nT \n= (A\nT\n)\n+\n. [Hint: Show that (A\n+ \n)\nT \nsatisfies the Penrose conditions for A\nT\n. By Exercise 52, \n(A+ ) \nT \nmust therefore be (A\nT\n)\n+.] \n55. Show that if A is a symmetric, idempotent matr  ix, then \nA+   =A. \nAPPiications \ndecomposition. \n60. Show that every square matrix A  can be factored as \nA = RQ, where R is symmetric, positive semidefinite \nand Q is orthogonal. [Hint: Show that the SVD can be \nrewritten to give \nA = \nU2\nV\nT \n= \nl/2(\nU\nT\nU\n)\nV\nT \n= \n(\nU2,\nU\nT)\n(\nU\nV\nT) \nThen show that R = \nU2 \nU\nT \nand Q = \nU\nV\nT \nhave the","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":112005,"to":112097}}}}],[1628,{"pageContent":"and Q is orthogonal. [Hint: Show that the SVD can be \nrewritten to give \nA = \nU2\nV\nT \n= \nl/2(\nU\nT\nU\n)\nV\nT \n= \n(\nU2,\nU\nT)\n(\nU\nV\nT) \nThen show that R = \nU2 \nU\nT \nand Q = \nU\nV\nT \nhave the \nright properties.] \nFind a polar decomposition of th e matrices in \nExercises 61-64. \n61. A in Exercise 3 \n63. A = \n[ \n1    2\n] \n-3  -1 \n62. A in Exercise 14 \n64. A= \nH -� -:\nl \nAnnroximalion or  Funclions \nIn many applications, it is  necessary to approximate a given function by a \"nicer\" \nfunction. For example, we might want to approximate f (x) = e\nx \nby a linear func­\ntion g(x) = c + dx on some interval [a, b]. In this case, we have a continuous \nfunctionf, and we want to approximate it as closely as possible on the interval [a, b]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":112097,"to":112148}}}}],[1629,{"pageContent":"Example 1.41 \nSection 7.5 Applications \n611 \nby a  function g in the  subspace <;JP\n1\n.  The general  problem can  be phrased as \nfollows: \nGiven a continuous function f on an interval [a, b] and a    subspace W of� [a, b] , \nfind the function \"closest\" to fin W. \nThe problem is analogous to the least squares fitting of data points, except now we \nhave infinitely many data points-namely, the points on the graph of the function f \nWhat should \"approximate\" mean in this context? Once again, the Best Approxima­\ntion Theorem holds the answer. \nThe given function f lives in the vector space � [a, b] of continuous functions on \nthe interval [a, b] . This is an inner product space, with inner product \n(f, g) = r f\n(\nx\n)\ng\n(\nx\n) \ndx \na \nIf Wis a  finite-dimensional subspace of� [a, b] , then the best approximation to fin W \nis given by the projection off onto W, by Theorem 7 .8. Furthermore, if \n{ u\n1\n, ... , u\nk\n} is \nan orthogonal basis for W, then \n. \n(u\n1\n,f) \n(u\nk\n>\nf) \npro\nJ\nw\n(f\n) \n= \n-\n( \n-\n-\n)\nu\n1 \n+ · · · + \n-\n( \n-\n-\n)\nu\nk \nU\n1\n, \nU\n1 \nU\nk\n, \nU\nk","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":112150,"to":112221}}}}],[1630,{"pageContent":"is given by the projection off onto W, by Theorem 7 .8. Furthermore, if \n{ u\n1\n, ... , u\nk\n} is \nan orthogonal basis for W, then \n. \n(u\n1\n,f) \n(u\nk\n>\nf) \npro\nJ\nw\n(f\n) \n= \n-\n( \n-\n-\n)\nu\n1 \n+ · · · + \n-\n( \n-\n-\n)\nu\nk \nU\n1\n, \nU\n1 \nU\nk\n, \nU\nk \nFind the best linear approximation to f\n(\nx) = e\nx \non the interval [ -1, l]. \nSolution Linear functions are polynomials of degree 1, so  we use  the subspace \nW =<;If i[ -1, l] of� [-1, l] with the inner product \n(f,g) \n= \nr \nf\n(\nx\n)\ng\n(\nx\n)\ndx \n-\n] \nA basis for <;If i[-1, l] is given by {l, x}. Since \n(1, x) \n= \nr \nx dx \n= 0 \n-] \nthis is an orthogonal basis, so the best approximation to fin Wis \nr \n( 1 . ex) dx \nr \nxex dx \n-] \n-] \n1 \n+ \n1 \nx \nJ \n( l \n· \nl\n) dx \nJ \nx\n2\ndx \n-] \n-] \n2e\n-\n1 \n-\n-\n2\n-+-z-x \n3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":112221,"to":112324}}}}],[1631,{"pageContent":"612 \nChapter 7 \nDistance and Approximation \na \nFigure J.25 \n� \n':\nher e we  ha\n�\ne used integration by parts to evaluate \nr \nxe\nx \ndx. (Check these calcula-\ntions.) See Figure 7.24. \n-I \nb \ny \nf\n(\nx\n) = eX \n-1 \nFigure J.24 \nThe error in approximating f by g is the one specified by the Best Approxima­\ntion Theorem: the distance \nll\nJ -  g\nll \nbetween f and g relative to the inner prod uct on \nCtS [ -1, l] . This error is just \nll\nJ-g\nll \nand is often called the root mean square error. With the aid of a CAS, we find that the \nroot mean square error in Example 7.41 is \nII \ne\nx \n-\n(\nt\n(\ne  -  e\n-\n1\n) \n+ \n3e\n-\n1\nx ) \nII \n= \n� \nf \n1 \n(\ne\nx \n-\nt\n(\ne  -  e\n-\n1\n)  -\n3e\n-\n1\nx\n)\n2 \ndx \n= \n0.2\n3 \nRemark The root mean square error can be thought of as analogous to the area \nbetween the graphs off and g on the specified interval. Recall that the area between \nthe graphs off and g on the interval [a, b] is given by \nr \nI\nJ \n(x\n) \n-  g\n(\nx\n) \nI \ndx \na \n(See Figure 7.25.) \nAlthough the equation in the above Remark is a sensible measure of the \"error\"","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":112326,"to":112419}}}}],[1632,{"pageContent":"the graphs off and g on the interval [a, b] is given by \nr \nI\nJ \n(x\n) \n-  g\n(\nx\n) \nI \ndx \na \n(See Figure 7.25.) \nAlthough the equation in the above Remark is a sensible measure of the \"error\" \nbetween f and g, the absolute value sign makes it hard to work with. The root mean \nsquare error is easier to use and therefore preferable. The square root is necessary to \n\"compensate'' for the squaring and to keep the unit of measurement the same as it \nwould be for the area between the curves. For comparison purposes, the area between \nthe graphs off andgin Example 7.41 is \nr \nl\nex \n-\nt\n(e \n-  e\n-\n1\n) \n-\n3e\n-\n1\nx\nl \ndx \n= \n0.28 \n-]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":112419,"to":112458}}}}],[1633,{"pageContent":"Example 4.30 \nSection 7.5 Applications \n613 \nFind the best quadratic approximation to f(x) = ex   on the interval [ -1, l]. \nSolution A quadratic function is a polynomial of the form g(x) = a + bx + cx\n2 \nin \nW = <!J> \n2 \n[ -1, l]. This time, the standard basis { 1, x, x\n2\n} is not orthogonal. However, \nwe can construct an orthogonal basis using the Gram-Schmidt Process, as we did in \nExample 7.8. The result is the  set of Legendre polynomials \n{1,x,x\n2\n-H \nUsing this set as our basis, we compute the best approximation to fin Was g(x) = \nprojw(ex ). The linear terms in this calculation are exactly as in Example 7.41, so we \nonly require the additional calculations \nand (x\n2 \n-\nt\n,x\n2 \n-\nt\ni\n= \nr \n(\nx\n2 \n-\nt\n)\n2\ndx = \nr \n(\nx\n4 \n-\nt\nx\n2 \n+ \n!\n)\ndx = \n!s \n-I -I \nThen the best quadratic approximation to f(x) = ex   on the interval [-1, l] is \n-\n. \nx \n-\n(1, \nex\n/    (x, \nex\n/ \n(x\n2 \n- t\n, ex\n/ \n2 \nI \ng\n(\nx\n) \n-\npro\nJ\nw\n(\ne \n) \n-\n-\n( \n-\n, \n1  + \n-\n( \n-\n, \nx + \n2 \n1 \n2 \n1 \n(\nx \n- 3\n) \n1, \n11 \nX,  Xf \n(x  -\n3\n,\nX \n- 3\nl \nI \nI I \nt\n(\ne -  7e\n-\nl\n) \n2 \nI \n= 2\n(\ne - e\n- ) \n+ 3e\n-\nx + \n8 \n(\nx - 3\n) \n(See Figure 7.26.) \n45 \n3\n(","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":112460,"to":112593}}}}],[1634,{"pageContent":"x \n-\n(1, \nex\n/    (x, \nex\n/ \n(x\n2 \n- t\n, ex\n/ \n2 \nI \ng\n(\nx\n) \n-\npro\nJ\nw\n(\ne \n) \n-\n-\n( \n-\n, \n1  + \n-\n( \n-\n, \nx + \n2 \n1 \n2 \n1 \n(\nx \n- 3\n) \n1, \n11 \nX,  Xf \n(x  -\n3\n,\nX \n- 3\nl \nI \nI I \nt\n(\ne -  7e\n-\nl\n) \n2 \nI \n= 2\n(\ne - e\n- ) \n+ 3e\n-\nx + \n8 \n(\nx - 3\n) \n(See Figure 7.26.) \n45 \n3\n(\n1le-\n1 \n- e\n) \nlS\n(\ne -  7e\n-\n1\n) \n-----\n+ 3e\n-\n1\nx + \nx\n2 \n= \n1.00 + l.lOx +   0.54x\n2 \n4 4 \n-2 \n-1 \nFigure 1.26 \n5 \n4 \n3 \n2 \ny \nf\n(\nx\n) \n=ex \ng(\nx\n) \n= \n1\n.00 + l.lOx +  0.54x\n2 \n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":112593,"to":112712}}}}],[1635,{"pageContent":"614 \nChapter 7 \nDistance and Approximation \nExample 1.43 \nNotice how much better the quadratic approximation in Example 7.42 is than the \nlinear approximation in Example 7.41. It turns out that, in the quadratic case, the root \nmean square error is \nll\nex \n- g(x)\nll \nIn general, the higher the degree of the approximating polynomial, the smaller the \nerror and the better the approximation. \nIn many applications, functions are approximated by combinations of sine and \ncosine functions. This method is particularly useful if the function being approxi­\nmated displays periodic or almost periodic behavior (such as that of a sound wave, an \nelectrical impulse, or  the motion of a vibrating system). A  function of the form \np \n(\nx\n) \n= \na\n0 \n+ a\n, \ncos x + a\n2 \ncos 2x + · · · + a\nn \ncos nx + b\n, \nsin x \n+ b\n2 \nsin 2x + · · · + b\nn \nsin nx \n(1) \nis called a trigonometric polynomial; if \na\nn \nand b\nn \nare not both zero, then p(x) is said \nto have order n. For example, \np\n(\nx\n) \n= 3 -  cos x +   sin 2x + 4   sin 3x","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":112714,"to":112764}}}}],[1636,{"pageContent":"+ b\n2 \nsin 2x + · · · + b\nn \nsin nx \n(1) \nis called a trigonometric polynomial; if \na\nn \nand b\nn \nare not both zero, then p(x) is said \nto have order n. For example, \np\n(\nx\n) \n= 3 -  cos x +   sin 2x + 4   sin 3x \nis a trigonometric polynomial of order 3. \nLet's restrict our attention to the vector space � [  -7f, 7f] with the inner prod uct \n(f,g\n/ \n= r j\n(\nx\n)\ng\n(\nx\n)\ndx \n-\nrr \nThe trigonometric polynomials of the form in Equation (1) are linear combinations \nof the set \nB = {l, cos x, ... , cos  nx, sin x, ... , sin  nx} \nThe best approximation to a function fin c€ [ - 7f, 7f] by a trigonometric polynomial \nof order n will therefore be projw(f), where W = span(B). It turns out that Bis an \northogonal set and, hence, a basis for W.  Verification of this fact involves showing \nthat any two distinct functions in B are orthogonal with respect to the given inner \nproduct. Example 7.43 presents some of the necessary calculations; you are asked to \nprovide the remaining ones in Ex  ercises 17-19.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":112764,"to":112805}}}}],[1637,{"pageContent":"product. Example 7.43 presents some of the necessary calculations; you are asked to \nprovide the remaining ones in Ex  ercises 17-19. \nShow that sin jx is orthogonal to cos kx in C{i; \n[ -7f, 7f] for j, k 2: 1. \nSolulion \nUsing a trigonometric identity, we compute as follows: If j * k, then \nr sin jx cos kx dx = \nt\nr [ sin\n(\nj + k\n)\nx +   sin\n(\nj - k\n)\nx l dx \n-TT \n-TT \n= \n_\n1\n[\ncos(\nj \n+ k\n)\nx \n+ \ncos(\n) \n- k\n)\nx\n]\nrr \n2 \n·+k ·-k \n} } \n-rr \n=O \nsince the cosine function is periodic with period 27f.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":112805,"to":112846}}}}],[1638,{"pageContent":"Example 1.44 \nSection 7.5 Applications \n615 \nIfj = k, then \nf \n1T \n1 \nsin kx cos kx dx =  -\n[ \nsin\n2 \nkx] rr_\n1T \n= 0 \n-\n1T \n2k \nsince sin br = 0 for any integer k. \nIn order to find the orthogonal projection of a function fin Cfb [ -n, 1T] onto the \nsubspace W spanned by the orthogonal basis B, we need to know the squares of the \nnorms of the basis vectors. For example, using a half-angle formula, we have \n(sin kx, sin kx) = r sin\n2 \nkx dx \n-\nrr \n= \nt\nf \n(\n1 \n- cos \n2kx\n)\ndx \n-\n1T \n= \nt\n[\nx \n_ \nsin \n2kx\n]\nrr \n2k \n-\n1T \n= 1T \nIn \nExercise 20, you are asked to show that (cos kx, cos kx) = 1T and (1, 1) = 2n. \nWe now have \nproj\nw\n(f\n) \n= \na\n0 \n+ a\n1 \ncos x + · · · + a\n\" \ncos nx + b\n, \nsin x + · · · + b\" sin nx \n(2) \nwhere \n(l,f) \n1 f \n1T \na\n0 \n= \n-\n(\n-\n1 \n= \n-\nf(x) dx \n1, \n11 \n21T \n-\nrr \n(cos kx,f) \n1 \nJ\n\" \na\nk \n= \n(   k     k \n1 \n= \n-\nf \n(\nx\n) \ncos kx dx \ncos  X, COS X \n/ \n1T \n-\nrr \n(sin kx,f) \n1 \nJ\n\"    . \nb\nk \n= \n( \n. \nk \n. \nk \n1 \n= \n-\nf \n(\nx\n) \nsm kx dx \nsm  x, sm \nX1 \n1T \n-\nrr \n(3) \nfork 2 1. The approximation to f given by Equations (2) and (3) is   called the nth-order \nFourier approximation to f on \n[","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":112848,"to":112984}}}}],[1639,{"pageContent":"1 \nJ\n\"    . \nb\nk \n= \n( \n. \nk \n. \nk \n1 \n= \n-\nf \n(\nx\n) \nsm kx dx \nsm  x, sm \nX1 \n1T \n-\nrr \n(3) \nfork 2 1. The approximation to f given by Equations (2) and (3) is   called the nth-order \nFourier approximation to f on \n[ \n-n, 1T]. The coefficients a\n0\n,  a1, ••• , a\nn\n, b1, ... ,  b\n\" \nare called the Fourier coefficients off \nFind the fourth-order Fourier approximation to f(x) = x on \n[ \n-n, 1T]. \nSolution \nUsing formulas (3), we obtain \n1 \nJ\nrr \n1 \n[\nx\n2\n]\n1T \na\n0 \n= \n-\nx dx = \n-\n-= 0 \n21T \n-\nrr \n21T  2 \n-\nrr \nand for k 2 1, integration by parts yields \nl\nf\n1T \nl\n[\nx \n1 \n]\nrr \na\nk \n= \n1T \n_\n1T \nx cos kx dx = \n1T \nk sin kx + kl cos kx \n_\n1T \n= 0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":112984,"to":113067}}}}],[1640,{"pageContent":"616 \nChapter 7 \nDistance and Approximation \nand \n1 \nJ\n\" \n1 [ x 1 \n]\n\" \nb\nk \n= \n-\nx sin  kx dx = \n-\n-\n- cos kx + 2 sin kx \n7T \n-rr \n7T \nk k \n-rr \n{\n1 \nif k is even \nif k is odd \n2\n(\n-l\n)\nk\n+\nl \nk \nIt follows that the  fourth-order Fourier approximation to f(x) = x on \n[ \n-7T, 7T] is \nJean-Baptiste Joseph Fourier \n(1768-1830) was a French \nmathematician and physicist who \ngained prominence through his \ninvestigation into the theory of \nheat. In his landmark solution of \nthe so-called heat equation, he \nintroduced techniques related to \nwhat are now known as Fourier \nseries, a tool widely used in many \nbranches of mathematics, physics, \nand engineering. Fourier was a \npolitical activist during the French \nrevolution and became a favorite \nof Napoleon, accompanying him \non his Egyptian campaign in 1798. \nLater Napoleon appointed Fourier \nPrefect ofisere, where he oversaw \nmany important engineering \nprojects. In 1808, Fourier was made \na baron. He is commemorated by \na plaque on the Eiffel Tower. \n2\n(\nsin x -  �sin 2x + \nt \nsin 3x - i   sin 4x\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":113069,"to":113134}}}}],[1641,{"pageContent":"many important engineering \nprojects. In 1808, Fourier was made \na baron. He is commemorated by \na plaque on the Eiffel Tower. \n2\n(\nsin x -  �sin 2x + \nt \nsin 3x - i   sin 4x\n) \nFigure 7.27 shows the first four Fourier approximations to f(x) = x on \n[ \n-7T, 7T]. \ny \ny \ny\n=x \nn = l \nn=2 \ny \ny \ny\n=x \ny\n=x \nn=3 \nn=4 \nFigure 1.21","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":113134,"to":113161}}}}],[1642,{"pageContent":"Section 7.5 Applications \n611 \nYou can clearly see the approximations in Figure 7.27 improving, a fact that can \nbe confirmed by computing the root mean square error in each case. As the order of \nthe Fourier approximation increases, it can be  shown that this error approaches zero. \nThe trigonometric polynomial then becomes an infinite series, and we write \nf \n(\nx\n) = a\n0 \n+ \n2: \n(\na\nk \ncos kx \n+ \nb\nk \nsin kx\n) \nk\n=\nl \nThis is called the Fourier series off on \n[ \n- n, n] . \nMariner 9 used the Reed-Muller code R\ns\n, whose minimum distance is 2\n4 \n= 16. \nBy Theorem 1, this code can correct k errors, where 2k \n+ \n1 \n:::::: \n16. The largest value \nof k for which this inequality is true is k =  7. Thus, R\ns \nnot only contains exactly the \nright number of code vectors for transmitting 64 shades of gra  y but also is capable \nof correcting up to 7 errors, making it quite reliable. This explains why the images \ntransmitted by Mariner 9 were so sharp! \n1 \nExercises 1.5 \nApproximation of Functions","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":113163,"to":113209}}}}],[1643,{"pageContent":"of correcting up to 7 errors, making it quite reliable. This explains why the images \ntransmitted by Mariner 9 were so sharp! \n1 \nExercises 1.5 \nApproximation of Functions \nIn Exercises 1-4, find the best linear approximation to f on \nth e interval \n[ \n-1, l]. \n1. f(x\n) = x\n2 \n3. f\n(\nx\n) = x\n3 \n2.  f \n(\nx) = x\n2 \n+ \n2x \n4. f\n(\nx\n) = sin\n( \nnx/2\n) \nIn Exercises 5 and 6, find the best quadratic approximation \nto f on th e interval \n[ \n-1, l]. \n5. f\n(\nx\n) = \nl\nx\nl \n6.  f\n(\nx\n) = cos(nx/2\n) \n7. Apply the Gram-Schmidt Process to the basis {l, x} to \nconstruct an orthogonal basis for<!/' 1 \n[ \n0, l]. \n8. Apply the Gram-Schmidt Process to the basis \n{ 1, x, x \n2\n} to construct an orthogonal basis for <!/' 2 \n[ \n0, l] . \nIn Exercises 9-12, find th e best linear approximation to f on \nth e interval \n[\nO, l]. \n9. f(x\n) = x\n2 \n10. f \n(x\n) = \nVx \n11. f(x\n) = e x \n12. f(x) = sin\n( \nnx/2) \nIn Exercises 13-16, find the best quadratic approximation \nto f on the interval \n[ \n0, l]. \n13. f(x\n) = \nx\n3 \n14. f(x) = \nVx \n15. f(x) = e\nx \n16. f(x) = sin\n( \nnx/2)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":113209,"to":113295}}}}],[1644,{"pageContent":"(x\n) = \nVx \n11. f(x\n) = e x \n12. f(x) = sin\n( \nnx/2) \nIn Exercises 13-16, find the best quadratic approximation \nto f on the interval \n[ \n0, l]. \n13. f(x\n) = \nx\n3 \n14. f(x) = \nVx \n15. f(x) = e\nx \n16. f(x) = sin\n( \nnx/2) \n17. Show that 1 is orthogonal to cos kx and sin kx in \nCf6 [-n, 7T] for k 2: 1. \n18. Show that cos jx is orthogonal to cos kx in Cf6 \n[ \n-n, n] \nforj * k,j, k 2: 1. \n19. Show that sin jx is orthogonal to sin kx in Cf6 \n[ \n-n, n] \nforj * k,j,  k 2: 1. \n20. Show that \n11\n1\n11\n2 \n= \n2n and \nII \ncos kx\nll \n2 \n= \n7T in Cf6 \n[ - n, n] . \nIn Exercises 21 and 22, find the third-order Fourier approxi­\nmation to f on \n[ \n- n, n] . \n21. f\n(\nx\n) = \nl\nx\nl \n22. f\n(\nx\n) = x\n2 \nIn Exercises 23-26, find th e Fourier coefficients a\n0\n, a\nk > \nand \nb\nk \noff on [-n, n]. \n{\nO if-n  ::=:::x<O \n23\" f \n(\nx\n) = \n1 if 0 \n:::::: \nx \n:::::: \nn \n24. f\n(\nx\n) = \n{\n-\n1\n1 \n25. f \n(\nx\n) =  n  -x \nif- n :::::: x < 0 \nifO ::::: x ::=::: n \n26.  f\n(\nx\n) = \nl\nx\nl \nRecall that a function f is an even fu nction if f\n( \n- x\n) = \nf\n(\nx\n) \nfor \nall x;f is called an odd fu nction if f ( - x\n) = \n-f (x) for all x.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":113295,"to":113409}}}}],[1645,{"pageContent":"if- n :::::: x < 0 \nifO ::::: x ::=::: n \n26.  f\n(\nx\n) = \nl\nx\nl \nRecall that a function f is an even fu nction if f\n( \n- x\n) = \nf\n(\nx\n) \nfor \nall x;f is called an odd fu nction if f ( - x\n) = \n-f (x) for all x. \n27. (a) Prove that r f\n(\nx\n) \ndx = 0 if f is an odd function. \n-rr \n(b) \nProve that the Fourier coefficients a\nk \nare all zero if \nf is odd. \n28. (a) Prove that r f(x) dx =  2 rf(x) dx if f is an even \n-rr \n0 \nfunction. \n(b) Prove that the Fourier coefficients b\nk \nare all zero if \nf is even.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":113409,"to":113448}}}}],[1646,{"pageContent":"Chapter Review \nKev Definitions and Concepts \nBest Approximation Theorem, 570 \nCauchy-Schwarz Inequality, 539 \ncondition number of a matrix, 562 \ndistance, 535 \nleast squares error,  572 \northonormal set of \nvectors, 537 \npseudoinverse of a matrix, \n585, 602 \nleast squares solution, 574, 604 \nLeast Squares Theorem, 575 \nmatrix norm, 556 \nEuclidean norm (2-norm), 553 \nFrobenius norm, 556 \nFundamental Theorem of Invertible \nmax norm ( \noo \n-norm, uniform \nsingular value decomposition \nnorm), 553 \nnorm, 535, 552 \n(SVD),  593 \nsingular values, 590 \nMatrices, 605 \nHamming distance, 554 \nHamming norm, 554 \nill-conditioned matrix, 561 \ninner product, 531 \nnormed linear space, 552 \noperator norm, 559 \nsingular vectors, 593 \nsum norm (1-norm),  552 \nTriangle Inequality, 540 \nunit sphere, 535 \ninner prod uct space, 531 \northogonal basis, 537 \northogonal projection, 538, 583 \northogonal (set of) vectors,  537 \northonormal basis, 537 \nunit vector, 535 \nwell-conditioned matrix, 561 \nReview Questions","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":113450,"to":113493}}}}],[1647,{"pageContent":"inner prod uct space, 531 \northogonal basis, 537 \northogonal projection, 538, 583 \northogonal (set of) vectors,  537 \northonormal basis, 537 \nunit vector, 535 \nwell-conditioned matrix, 561 \nReview Questions \n1. Mark each of the following statements true or false: \n618 \n(a) Ifu = \n[\n�\n:] \nand v = \n[ \n:J then <u, v) = u\n1\nv1 + \n'1Tu\n2\nv\n2 \ndefines an inner product on IR\n2\n. \n2u\n1\nv\n2 \n-2u\n2\nv 1 + 4u\n2\nv\n2 \ndefines an inner product \non IR\n2\n. \n(c) <A, B) = tr(A) + tr(B) defines an inner product on \nM\n22\n· \n(d) If u and v are vectors in an   inner product space \nwith \nllull = 4, llvll = \nVs\n, \nand <u, v) = 2, then \nllu +  vll =5. \n(e) The sum norm, max norm, and Euclidean norm \non !R\nn \nare all equal to the absolute value function \nwhen n = 1. \n(f) If a matrix A is well-conditioned, then cond(A) is \nsmall. \n(\ng) If cond(A) is sm all, then the matrix A is \nwell-conditioned. \n(h) Every linear system has a unique least squares \nsolution. \n(i) \nIf A is a matrix with orthonormal columns, then \nthe standard matrix of an orthogonal projection","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":113493,"to":113558}}}}],[1648,{"pageContent":"well-conditioned. \n(h) Every linear system has a unique least squares \nsolution. \n(i) \nIf A is a matrix with orthonormal columns, then \nthe standard matrix of an orthogonal projection \nonto the column space of A is P = AA \nT\n. \n(j)  If A is a symmetric matrix, then the singular \nvalues of A are the same as the eigenvalues of A. \nIn Questions 2-4, determine whether th e definition gives an \ninner product. \n2. \n(p\n(x\n)\n, \nq\n(\nx\n)\n) \n= p\n(\nO\n)q(\nl\n) + p\n(\nl )q(\nO\n) \nfor p\n(\nx\n) , q (\nx\n) \nin <!/'\n1 \n3. <A, B) = tr(A \nT\nB) for A, Bin M\n22 \n4. \n<J, g\n) \n= \n(\n0\n��\\aI/\n(\nx\n))\n(\n0\n1Jl\n:\n:\n/\n(\nx\n)) for \nf, \ngin \nC(b [\nO, 1] \nIn Questions 5 and 6, compute the indicated quantity using \nth e specified inner product. \n5. 111  + x + x\n2\nll \nif\n<a\n0 \n+ a 1x + a\n2\nx\n2\n, b\n0 \n+ b\n1\nx + b\n2\nx\n2\n) \n= \na\n0\nb\n0 \n+ a\n1\nb 1 + a\n2\nb\n2 \n6. \nd(x, x\n2\n) \nif \n(p(\nx\n)\n, \nq\n(\nx\n)\n) \n= \nf �\np\n(x\n)\nq\n(x\n) \ndx \nIn Questions 7 and 8, construct an orthogonal set of vectors \nby applying th e Gram-Schmidt Process to th e given set of \nvectors using the specified inner product. \n7. \n{ \n[\n�\nl \n[\n�\n]\n} \nif\n<u, \nv) = \nu\nT \nAv, \nwhere \nA \n= \n[\n:  !\n] \n8. {1, \nX\n, \nx\n2\n} if","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":113558,"to":113710}}}}],[1649,{"pageContent":"by applying th e Gram-Schmidt Process to th e given set of \nvectors using the specified inner product. \n7. \n{ \n[\n�\nl \n[\n�\n]\n} \nif\n<u, \nv) = \nu\nT \nAv, \nwhere \nA \n= \n[\n:  !\n] \n8. {1, \nX\n, \nx\n2\n} if \n(p(\nx\n)\n, \nq\n(\nx\n)\n) \n= \nr\np (x\n)\nq (x) dx \n0 \nIn Questions 9 and 10, determine whether th e definition \ngives a norm. \n9. llvll \n= v\nT\nvforvin !R\nn \n10. \nll\np (x) II = \nl\np(\no\n) I + \nl\np(l) -p(\no\n) I for p\n(\nx\n) \nin <!/'\n1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":113710,"to":113776}}}}],[1650,{"pageContent":"11. Show that the  matrix A \n= \n[ \n�. l \n0.11 \nill-conditioned. \n0.1 \n0.11 \n0.111 \n0.11 \nl \n0.111 \nis \n0.1111 \n12. Prove that if Q is an orthogonal n X  n matrix, then its \nFrobenius norm is II Q II F \n= \nVn\n. \n13. Find the line of best fit through the points (1, 2), (2, 3), \n(3, \n5), and ( 4, \n7). \n14. Find the least squares solution of \n[\n� -�\n]\n[\n::\n] \n[\n-\n!l \n15. \nflnd the mthogona! pmjection of x  � \n[ \n�\n] \nonto the \ncolumn sp�e of A \n� \n[ \n� \niJ \nSection 7.5 Applications \n619 \n16. If u and v are orthonormal ve ctors, show that \nP \n= \nuur \n+ \nvvr is the standard matrix of an or­\nthogonal projection onto span (u, v) . [Hint: Show that \nP \n= \nA(A\nT\nA)\n-\n1\nA\nT \nfor some matrix A.] \nIn Questions 17 and 18,find (a) th e singular values, (b) a \nsingular value decomposition, and (c) th e pseudoinverse of \nth e matrix A. \n17. A \n= \n[\n�  �\ni \n1   -1 \n18. A \n= \n[\n� \n-1\n] \n-1 \n19. If P and Q are orthogonal matrices for which PA Q is \ndefined, prove that PA Q has the same singular values \nas A. \n20. If A is a square matrix for which A2 \n= \n0, prove that \n(A +)\n2 \n= \n0.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":113778,"to":113866}}}}],[1651,{"pageContent":"A1 \nPlease, sir, I want some more. \n-Oliver \nCharles Dickens, Oliver Tw ist \nAnyone who understands algebraic \nnotation reads at a glance in an \nequation results reached \narithmetically only \nwith great labour and pains. \n-Augustin Cournot \nResearches into the Mathematical \nPrinciples of  the Theory of  We alth \nTranslated by Nathaniel T. Bacon \nMacmillan, 1897, p. 4 \nAppendix A* \nMathematical Notation \nand Methods of Proof \nIn this book, an   effort has been made to use \"mathematical English\" as much as pos­\nsible, keeping mathematical notation to a minimum. However, mathematical nota­\ntion is a convenient shorthand that can greatly simplify the amount of writing we \nhave to do. Moreover, it is commonly used in every branch of mathematics, so  the \nability to read and write mathematical notation is an essential ingredient of mathe­\nmatical understanding.  Finally,  there are some theorems whose proofs  become \n\"obvious\" if the right notation is used.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":113868,"to":113891}}}}],[1652,{"pageContent":"matical understanding.  Finally,  there are some theorems whose proofs  become \n\"obvious\" if the right notation is used. \nProving theorems in mathematics is as much an art as a science. For the beginner, \nit is often hard to know what approach to use in proving a theorem; there are many \napproaches, any one of which might turn out to be the best. To become proficient at proofs, \nit is important to study as many examples as possible and to get plenty of practice. \nThis appendix summarizes basic mathematical notation applied to sets. Summa­\ntion notation, a useful shorthand for dealing with sums, is also discussed. Finally, \nsome approaches to proofs are illustrated with generic examples. \nSet Notation \nA set is a collection of objects, called the elements (or members) of the set. Examples \nof sets include the set of all words in this text, the set of all books in your college \nlibrary, the set of positive integers, and the set of all vectors in the plane whose equa­\ntion is 2x + 3y -z = 0.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":113891,"to":113904}}}}],[1653,{"pageContent":"library, the set of positive integers, and the set of all vectors in the plane whose equa­\ntion is 2x + 3y -z = 0. \nIt is often possible to list the elements of a set, in which case it is conventional to \nenclose the list within braces. For example, we have \n{l, 2, 3}, \n{a, t, x, z}, \n{2, 4, 6,  ... , 100}, \n{ \nn  2n  n  4n \nSn} \n-----\n4\n'\n5\n'\n2\n'\n7\n' \"\"\"'\n6 \n.-. \nNote that ellipses ( ... ) denote elements omitted when a pattern is present. (What is \nthe pattern in the last two examples?) Infinite sets are often expressed using ellipses. \nFor example, the set of positive integers is usually denoted by N or z+, so \nN \n= \nz\n+ \n= \n{l, 2, 3, ... } \nThe set of all integers is denoted by \"ll._, so \n\"1l._ \n= \n{ ... , -2, -1, 0,  1,2, ... } \nTwo sets are considered to be equal if they contain exactly the same elements. The \norder in which elements are listed does not matter, and repetitions are not counted. Thus, \n*Exercises and selected odd-numbered answers for this appendix can be found on the student companion \nwebsite.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":113904,"to":113941}}}}],[1654,{"pageContent":"Example A.1 \nAppendix A \nMathematical Notation and Methods of Proof \n12 \n{l,2, 3} =  {2, 1,3} =  {l,3, 2,  l} \nThe symbol E means \"is an element of\" or \"is in;' and the symbol fl_ denotes the \nnegation-that is, \"is   not an element of\" or \"is not in:' For example, \ns E z+ \nbut \no \nfi-z+ \nIt is often more convenient to describe a set in terms of a rule satisfied by all of its \nelements. In such cases, set builder notation is appropriate. The format is \n{x : x  satisfies P} \nwhere P represents a property or a collection of properties that the element x must \nsatisfy. The colon is pronounced \"such that:' For example, \n{n :nEZ,n>O} \nis read as \"the set of all n such that n is an integer and n is greater than zero:' \nThis is just another way of describing the positive integers z+. (We could also \nwrite z+ = {n E Z: n > O}.) \nThe empty set is the set with no elements. It is denoted by either 0 or { } . \nDescribe in words the following sets: \n(a) A = {n : n = 2k, k E  Z} \n(c)  C =   {x E IR: 4x\n2 \n- 4x -3 =  O}","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":113943,"to":113968}}}}],[1655,{"pageContent":"The empty set is the set with no elements. It is denoted by either 0 or { } . \nDescribe in words the following sets: \n(a) A = {n : n = 2k, k E  Z} \n(c)  C =   {x E IR: 4x\n2 \n- 4x -3 =  O} \n(b) B = {m/n: m, n E Z, n  * O} \n(d) D = {x E Z: 4x\n2 \n- 4x - 3 = O} \nSolution (a) A is the set of numbers n that are integer multiples of2. Therefore, A is \nthe set of all even integers. \n(b) B is the  set of all expressions of the form m / n, where m and n are integers and n \nis nonzero. This is the set of rational numbers, usually denoted by Q. (Note that this \nway of describing Q produces many repetitions; however, our convention, as noted \nabove, is that we include only one occurrence of each element.  Thus, this expression \nprecisely describes the set of all rational numbers.) \n(c)  C is the set of all real solutions of the equation 4x\n2 \n-\n4x \n-\n3 =  0. By factoring or \nusing the quadratic formula, we find that the roots of this equation are -� and �. \n� (Verify this.) Therefore, \nJohn Venn (1834-1923) was an","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":113968,"to":113993}}}}],[1656,{"pageContent":"2 \n-\n4x \n-\n3 =  0. By factoring or \nusing the quadratic formula, we find that the roots of this equation are -� and �. \n� (Verify this.) Therefore, \nJohn Venn (1834-1923) was an \nEnglish mathematician who studied \nat Cambridge University and later \nlectured there. He worked primarily \nin mathematical logic and is best \nknown for inventing Venn diagrams. \nc =  {  -t\n.\nn \n(d)  From the solution to (c) we see that there are no solutions to 4x\n2 \n- 4x - 3 = 0 \nin IR that are integers. Therefore, D is the empty set, which we can express by writing \nD\n=0\n. \nIf every element of a   set A  is also an element of a   set B, then A is called a \nsubset of B,   denoted A <: B. We  can represent this situation schematically using \na Ve nn diagram, as shown in Figure A. l. (The rectangle represents the universal \nset, a set large enough to contain all of the other sets in   question-in this case, \nA and B.)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":113993,"to":114020}}}}],[1657,{"pageContent":"A3 \nAppendix A \nMathematical Notation and Methods of Proof \nExample A.2 \nExample A.3 \nFigure A.1 \nAC: B \n(a)  {1, 2, 3} (;::; {1, 2, 3, 4, 5} \n(b) z+ (;::;  Z  (;::;  IR \n(c)  Let A be the set of all positive integers whose last two digits are 24 and let B be \nthe set of all positive integers that are evenly divisible by 4. Then if n is in A, it is of \nthe form \nn \n= \nlOOk \n+ \n24 \nfor some integer k. (For example, 36,524 \n= \n100 · 365 \n+ \n24.) But then \nn \n= \nlOOk \n+ \n24 \n= \n4(25k \n+ \n6) \nso n/ 4 \n= \n25k \n+ \n6, which is an integer. Hence, n is evenly divisible by 4, so it is in B. \nTherefore, A (;::; B. \n4 \nWe can show that two sets A and B are equal by showing that each is a subset of \nthe other. This  strategy is particularly useful if the sets are defined abstractly or if it is \nnot easy to list and compare their elements. \nLet A be the set of all positive integers whose last two digits form a number that is evenly \ndivisible by 4. In the case of a one-digit number, we take its tens digit to be 0.   Let B be","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":114022,"to":114064}}}}],[1658,{"pageContent":"Let A be the set of all positive integers whose last two digits form a number that is evenly \ndivisible by 4. In the case of a one-digit number, we take its tens digit to be 0.   Let B be \nthe set of all positive integers that are evenly divisible by 4.   Show that A \n= \nB. \nSolution As in   Example A.2( c ), it is easy to see that A (;::; B. If n is in  A, then we can \nsplit off the number m formed by its last two digits by writing \nn \n= \nlOOk \n+ \nm \nfor \nsome integer k. But, since m is divisible by 4, we have m = 4r for some integer r. \nTherefore, \nn \n= \nlOOk \n+ \nm \n= \nlOOk \n+ \n4r \n= \n4(25k \n+ \nr) \nso n is also evenly divisible by 4. Hence, A (;::; B. \nTo show that B (;::; A, let   n be in B. That is, n is evenly divisible by 4. Let's say that \nn \n= \n4s, where s is an integer. If m is the number formed by the last two digits of n, \nthen, as above, n \n= \nlOOk \n+ \nm for some integer k. But now \nm \n= \nn -  lOOk \n= \n4s - lOOk \n= \n4(s - 25k) \nwhich implies that m is evenly divisible by 4, since s -  25k is an integer. Therefore, n","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":114064,"to":114109}}}}],[1659,{"pageContent":"then, as above, n \n= \nlOOk \n+ \nm for some integer k. But now \nm \n= \nn -  lOOk \n= \n4s - lOOk \n= \n4(s - 25k) \nwhich implies that m is evenly divisible by 4, since s -  25k is an integer. Therefore, n \nis in A, and we have shown that B (;::;   A. \nSince A (;::; B and B (;::; A, we must have A \n= \nB.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":114109,"to":114125}}}}],[1660,{"pageContent":"Fiuure A.4 \nDisjoint sets \nExample A.4 \nG \nL: is the capital Greek letter sigma, \ncorresponding to S (for \"sum\"). \nSummation notation was \nintroduced by Fourier in 1820 and \nwas quickly adopted by the \nmathematical community. \nAppendix A \nMathematical Notation and Methods of Proof \n14 \nThe intersection of sets A and B is denoted by A n B and consists of the elements \nthat A and B have in common. That is, \nAnB={x:xEA and  xEB} \nFigure A.2 shows a Venn diagram of this case. The union of A and Bis denoted by \nA U B and consists of the elements that are in either A or B (or both). That is, \nAUB={x:xEA or  xEB} \nSee Figure A.3. \nFigure A.2 \nAnB \nFigure A.3 \nAUB \nLet A= {n\n2\n: n E z+, 1 ::s n ::s 4} and let B = {n E z+ : n ::s 10 and n is odd}. Find \nAn B andA U B. \nSolution We see that \nA= {1\n2\n, 2\n2\n, 3\n2\n, 4\n2\n} = {l, 4, 9,  16} \nand  B =  {l, 3, 5, 7, 9} \nTh\nerefore, An B = {l, 9} andA U B \n= {l, 3, 4, 5, 7, 9, 16}. \nIf A n B = 0, then A and B are called disjoint sets. (See Figure A.4.) For example,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":114127,"to":114169}}}}],[1661,{"pageContent":"2\n, 3\n2\n, 4\n2\n} = {l, 4, 9,  16} \nand  B =  {l, 3, 5, 7, 9} \nTh\nerefore, An B = {l, 9} andA U B \n= {l, 3, 4, 5, 7, 9, 16}. \nIf A n B = 0, then A and B are called disjoint sets. (See Figure A.4.) For example, \nthe set of even integers and the set of odd integers are disjoint. \nsummalion Nolalion \nSummation notation is a convenient shorthand to use to write out a sum such as \n1  + 2 +  3 + ... +  100 \nwhere we want to leave out all but a few terms. As in   set notation, ellipses ( ... ) convey \nthat we have established a pattern and have simply left out some intermediate terms. \nIn the above example, readers are expected to recognize that we are summing all of \nthe positive integers from 1to 100. However, ellipses can be ambiguous. For example, \nwhat would one make of the following sum? \n1  + 2 + ... + 64 \nIs this the sum of all positive integers from 1 to 64 or just the powers of two, 1 + 2 + \n4 + 8 + 16 + 32 + 64? It is often clearer (and shorter) to use summation notation \n(or sigma notation).","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":114169,"to":114192}}}}],[1662,{"pageContent":"15 \nAppendix A \nMathematical Notation and Methods of Proof \nExample A.5 \nWe can abbreviate a sum of the form \na\nl \n+ a\n2 \n+ ... + \na\nn \n(1) \nas \n(\n2\n) \nwhich tells us to   sum the terms \na\nk \nover all integers k ranging from 1 to n. An alterna­\ntive version of this expression is \nThe subscript k is called the index of summation. It is a \"dummy variable\" in the sense \nthat it does not appear in the actual sum in expression (1). Therefore, we can use any \nletter we like as the index of summation (as long as it doesn\n'\nt already appear somewhere \nelse in the expressions we are summing). Thus, expression \n(\n2\n) \ncan also be written as \nThe index of summation need not start at 1. The sum \na\n3 \n+ a\n4 \n+ ·  ·  · + \na\n99 becomes \n99 \n2:a\nk \nk\n= 3 \nalthough we can arrange for the index to begin at 1 by rewriting the expression as \n9\n7 \n2:a\nk\n+\n2\n· \nk\n=\nl \nThe key to using summation notation effecti vely is being able to recognize patterns. \nWrite the following sums using summation notation. \n(a) 1 + 2 + 4 + .\n. \n· + 64 (b) 1 + 3 + 5 + \n..\n. +  99","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":114194,"to":114256}}}}],[1663,{"pageContent":"k\n+\n2\n· \nk\n=\nl \nThe key to using summation notation effecti vely is being able to recognize patterns. \nWrite the following sums using summation notation. \n(a) 1 + 2 + 4 + .\n. \n· + 64 (b) 1 + 3 + 5 + \n..\n. +  99 \n(c) 3 + 8 + 15 + \n.. \n· \n+ 99 \nSolution \n(a)  We recognize this expression as a sum of powers of 2: \n1  + \n2 \n+ 4 \n+  ... + 64 \n= \n2\n° \n+ \n2\n1 \n+ \n2\n2 \n+ \n... + \n2\n6 \n6 \nTherefore, the index of summation appears as the exponent, and we have 2: 2\nk\n. \nk\n=O \n(b) This expression is the sum of all the odd integers from 1 to 99\n. \nEvery odd integer is of \nthe form 2k +  1, so the sum is \n1  + 3 +  5 + ... + 99 \n= \n(\n2\n. \n0 +   1) + \n(\n2\n. 1  +   1) + \n(\n2\n. \n2 +  1) +  ... + \n(\n2\n. \n49 +  1) \n4\n9 \n= \n2:C2k +  1) \nk\n=O \n(c) The pattern here is less clear, but a little reflection reveals that each term is l less than \na perfect square: \n3 + 8 + 15 +  ... + 99 \n= \n(\n2\n2 \n- 1) + \n(\n3\n2 \n- 1) + \n(\n4\n2 \n- 1) +  ... +  (10\n2 \n- 1) \n10 \n2:Ck\n2 \n- 1) \nk\n=\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":114256,"to":114350}}}}],[1664,{"pageContent":"Example A.6 \nAppendix A \nMathematical Notation and Methods ofrroof \nA6 \nRewrite each of the sums in Example A.5 so that the index of summation starts at 1. \nSolution (a)  If we use the change of variable i \n= \nk + 1, then, as k goes from 0 to 6, \ni goes from 1 to 7. Since k \n= \ni - 1, we  obtain \n6 \n7 \n:L2\nk \n= \n:Li-\nI \nk=O \ni=l \n(b)  Using the same substitution as in   part (a), we get \nfi \n� � \n2:(2k +  1) \n= \n2:(2\n(\ni - 1) +  1) \n= \n2:(2i - 1) \nk=O \ni=l i=l \n� \n(c) The substitution i = k -2 will work (try it), but it is easier just to add a term corre­\nsponding to k = l, since 12 - 1 = 0. Therefore, \nExample A.1 \nIO IO \n2:(\nk\n2 \n- 1) \n= \n2:(\nk\n2 \n- 1) \nk=2 \nk=I \nMultiple summations arise when there is more than one index of summation, as \nthere is with a matr  ix. The notation \nn \n:L\na\nu \n(3) \ni,j=I \nmeans to sum the terms \na\n;\n1 \nas i and j each range independently from 1 to n. The sum \nin expression (3) is equivalent to either \nn \nn \n:L :L\na\ni) \ni=l j=I \nwhere we sum first over j and  then over i (we always work from the inside out), or \nn \nn \n:L :L","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":114352,"to":114423}}}}],[1665,{"pageContent":"in expression (3) is equivalent to either \nn \nn \n:L :L\na\ni) \ni=l j=I \nwhere we sum first over j and  then over i (we always work from the inside out), or \nn \nn \n:L :L\na\niJ \nj=I i=l \nwhere the order of summation is reversed. \n3 \nWrite out :L ji using both possible orders of summation. \ni,j=I \nSolution \n3 3 \n:L :Li\nj \ni=l j=l \nn \n:L \n(\ni\ni \n+ i\n2 \n+ i\n3\n) \ni=l \n= \n(1 +  1  + 1) + \n(\n2 + 4 +  8)  +  (3  + 9 + 27) \n= \n56","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":114423,"to":114462}}}}],[1666,{"pageContent":"A1 \nAppendix A \nMathematical Notation and Methods of Proof \nHow to Solve It is the title of a book \nby the mathematician George \nP6lya (1887-1985). Since its publi­\ncation in 1945, How to Solve It has \nsold over a million copies and has \nbeen translated into 17 languages. \n� \nP6lya was born in Hungary, but \nbecause of the political situation in \nEurope, he moved to the United \nStates in 1940. He subsequently \ntaught at Brown and Stanford \nUniversities, where he did mathe­\nmatical research and developed a \nwell-deserved reputation as an \noutstanding teacher. The P6lya \nPrize is awarded annually by the \nSociety for Industrial and Applied \nMathematics for major contribu-\ntions to areas of mathematics close \nto those on which P6lya worked. \nThe Mathematical Association \nof America annually awards \nP6lya Lectureships to math-\nematicians demonstrating \nthe high-quality exposition \nfor which P6lya was known. \nExample A.8 \nand \n3  3 \nn \n2: 2:i\nj \n= 2:0\nj \n+ 2\nj \n+ 3\nj\n) \nj\n�\nl\ni�\nI \nj\n�\nI \n(1\n1 \n+ 2\n1 \n+ 3\n1\n)  +   (1\n2 \n+ 2\n2 \n+ 3\n2\n)  +   (1\n3 \n+ 2\n3 \n+ 3\n3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":114464,"to":114532}}}}],[1667,{"pageContent":"ematicians demonstrating \nthe high-quality exposition \nfor which P6lya was known. \nExample A.8 \nand \n3  3 \nn \n2: 2:i\nj \n= 2:0\nj \n+ 2\nj \n+ 3\nj\n) \nj\n�\nl\ni�\nI \nj\n�\nI \n(1\n1 \n+ 2\n1 \n+ 3\n1\n)  +   (1\n2 \n+ 2\n2 \n+ 3\n2\n)  +   (1\n3 \n+ 2\n3 \n+ 3\n3\n) \n(1 + 2 +  3)  + (1 + 4 +  9)  + (1 + 8 + 27) =  56 \nRemark Of course, the value of the sum in Example A.7 is the sa   me no matter \nwhich order of summation we choose, because the sum is finite. It is also possible to \nconsider infinite sums (known as infinite series in calculus), but such sums do not \nalways have a value and great care must be taken when rearranging or manipulating \ntheir terms. For example, suppose we let \nThen \nS=l +2+4+8+ \n1  +   2(1 + 2 + 4 + .. ·) \n1  +   2S \nfrom which it follows that S  = -1. This is clearly nonsense, since S is a sum of \nnon-negative terms! (Where is the error?) \nMelhods of Proof \nThe notion of proof is at the very heart of mathematics. It is one thing to know what \nis true; it is quite another to know why it is true and to be able to demonstrate its truth","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":114532,"to":114589}}}}],[1668,{"pageContent":"Melhods of Proof \nThe notion of proof is at the very heart of mathematics. It is one thing to know what \nis true; it is quite another to know why it is true and to be able to demonstrate its truth \nby means of a logically connected sequence of statements. The intention here is not to \ntry to teach you how to do proofs; you will become better at doing proofs by studying \nexamples and by practicing-something you should do often as you work through \nthis text. The intention of this brief section is simply to provide a few elementary ex­\namples of some types of proofs. The proofs of theorems in the text will provide fur­\nther illustrations of \"how to solve it:' \nRoughly speaking, mathematical proofs fall into two categories: direct proofs \nand indirect proofs . Many theorems have the structure \"if P, then Q;' where P (the \nhypothesis, or premise\n) \nand Q (the conclusion\n) \nare statements that are either true or \nfalse. We denote such an implication by P \n=> \nQ. A direct proof proceeds by establish­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":114589,"to":114607}}}}],[1669,{"pageContent":"hypothesis, or premise\n) \nand Q (the conclusion\n) \nare statements that are either true or \nfalse. We denote such an implication by P \n=> \nQ. A direct proof proceeds by establish­\ning a chain of implications \nP => P\n1 \n=> P\n2 \n=> ·   ·   · => P\nn \n=> Q \nleading directly from P to Q. \nProve that any  two consecutive perfect squares differ by an odd number. This instruc­\ntion can be rephrased as \"Prove that if a and b are consecutive perfect squares, then \na -bis odd:' Hence, it has the form P \n=> \nQ, with P being \n\"\na and b are consecutive \nperfect squares\" and Q being \n\"\na - bis odd:'","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":114607,"to":114633}}}}],[1670,{"pageContent":"Example A.9 \nAppendix A \nMathematical Notation and Methods of Proof \nAB \nSolulion \nAssume that a and b are consecutive perfect squares, with a > b. Then \na \n= \n(\nn +  1)\n2 \nand b \n= \nn\n2 \nfor some integer n. But now \na -b \n= \n(\nn +  1)\n2 \n-n\n2 \n= \nn\n2 \n+ 2n +  1 -n\n2 \n= \n2n +  1 \nso a -b is odd. \nThere are two types of indirect proofs that can be used to establish a conditional \nstatement of the form P =;. Q. A proof by contradiction assumes that the hypothesis \nPis true, just as in   a direct proof, but then supposes that the conclusion Q is fals e. The \nstrategy then is to show that this is not possible (i.e., to rule out the possibility that \nthe conclusion is false) by finding a contradiction to the truth of P. It then follows \nthat Q  must be true. \nLet n be a positive integer. Prove that if n\n2 \nis even, so is n. (Take a few minutes to try \nto find a direct proof of this assertion; it will help you to appreciate the indirect proof \nthat follows.) \nSolulion Assume that n is a positive integer such that n\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":114635,"to":114678}}}}],[1671,{"pageContent":"to find a direct proof of this assertion; it will help you to appreciate the indirect proof \nthat follows.) \nSolulion Assume that n is a positive integer such that n\n2 \nis even. Now suppose that \nn is not even. Then n is odd, so \nn \n= \n2k +  1 \nfor some integer k. But if so, we have \nn\n2 \n= \n(2k +  1)\n2 \n= \n4k\n2 \n+ 4k +  1 \nso n\n2 \nis odd, since it is 1 more than the even number 4k\n2 \n+ 4k. This contradicts our \nhypothesis that n\n2 \nis even. We conclude that our supposition that n was not even must \nhave been false; in other words, n must be even. \nClosely related to the method of proof by contradiction is proof by  contraposi­\ntive. The negative of a statement Pis the statement \"it is not the case that P;' abbrevi­\nated symbolically as -,p and pronounced \"not P:' For example, if Pis \n\"\nn is even;' then \n-,p is \"it is not  the case that n is even\" -in other words, \n\"\nn is odd:' \nThe contrapositive of the statement P =;. Q is the statement •Q =;. •P. A conditional","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":114678,"to":114714}}}}],[1672,{"pageContent":"\"\nn is even;' then \n-,p is \"it is not  the case that n is even\" -in other words, \n\"\nn is odd:' \nThe contrapositive of the statement P =;. Q is the statement •Q =;. •P. A conditional \nstatement P =;. Q and its contrapositive •Q =;. -,pare logically equivalent in the sense that \n� \nthey are either both true or both false. (For example, if P =;. Q is a theorem, then so is •Q \n=;. •P. To see this, note that if the hypothesis •Q is true, then Q is false. The conclusion -,p \ncannot be false, for if it were, then P would be true and our known theorem P =;. Q would \nimply the truth of Q, giving us a contradiction. It follows that -,p is true and we have \nproved •Q =;. •P.) Here is a contrapositive proof of the assertion in Example A.9.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":114714,"to":114726}}}}],[1673,{"pageContent":"19 \nAppendix A \nMathematical Notation and Methods of Proof \nExample A.10 \nExample A.11 \nI \nI \nI \nI \nI \nI \nI \nI \nI \nI \nFigure A.5 \nLet n be a positive integer. Prove that if n\n2 \nis even, so is n. \nSolulion \nThe contrapositive of the given statement is \n\"If n is not even, then n\n2 \nis not even\" \nor \n\"If n is odd, so is n\n2\n\" \nTo prove this contrapositive, assume that n is odd. Then n \n= \n2k +  1 for some inte­\nger k. As before, this means that n\n2 \n= \n(2k +  1)\n2 \n= \n4k\n2 \n+  4k +  1   is odd, which \ncompletes the proof of the contrapositive. Since the contrapositive is true, so is the \noriginal statement. \nAlthough we do not require a new method of proof to handle it, we will briefly \nconsider how to prove an \"if and only if\" theorem. A  statement of the form \"P if and \nonly if Q\" signals a dou ble implication, which we denote by P � Q. To prove such a \nstatement, we must prove P =;. Q and Q =;. P. To do so, we can use the techniques \ndescribed earlier, where appropriate. It is important to notice that the \"if\" part of","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":114728,"to":114774}}}}],[1674,{"pageContent":"statement, we must prove P =;. Q and Q =;. P. To do so, we can use the techniques \ndescribed earlier, where appropriate. It is important to notice that the \"if\" part of \nP � Q is \"P  if Q;' which is Q =;.  P; the \"only if\" part of P � Q is \"P  only if Q;'  mean­\ning P =;.  Q. The implication P =;. Q is sometimes read as \"P is sufficient for Q\" or \"Q \nis necessary for P\"; Q =;.Pis read \"Q is sufficient for P\" or \"P is   necessary for Q:' \nTaken together, they are P � Q, or \"P is necessary and sufficient for Q\" and vice \nversa. \nA pawn is placed on a chessboard and is allowed to move one square at a time, either \nhorizontally or vertically. A pawn's to ur of a chessboard is a path taken by a pawn, moving \nas described, that visits each square exactly once, starting and ending on the same square. \nProve that there is a pawn's tour of an n X n chessboard if and only if n is even. \nSolulion [ {:::: ] (\"if\") Assume that n is even. It is easy to see that the strategy illus­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":114774,"to":114785}}}}],[1675,{"pageContent":"Prove that there is a pawn's tour of an n X n chessboard if and only if n is even. \nSolulion [ {:::: ] (\"if\") Assume that n is even. It is easy to see that the strategy illus­\ntrated in Figure A.5 for a 6 X 6 chessboard will always give a pawn's tour. \n[ \n=;. ]   (\"only if\") Suppose that there is a pawn's tour of an n X n chessboard. We \nwill give a proof by contradiction that n must be even. To this end, let's assume that n \nis odd. At each move, the pawn moves to a square of a different color. The total num -\nber of moves in its tour is n\n2\n, which is also an odd number, according to the proof in \nExample A.10. Therefore, the pawn must end up on a square of the opposite color \nfrom that of the square on which it started. (Why?) This is impossible, since the pawn \nends where it started, so we have a contradiction. It follows that n cannot be odd; \nhence, n is even and the proof is complete. \nSome theorems assert that several statements are equivalent. This means that","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":114785,"to":114799}}}}],[1676,{"pageContent":"hence, n is even and the proof is complete. \nSome theorems assert that several statements are equivalent. This means that \neach is true if and only if all of the others are true. Showing that n statements are \n(n) n! n\n2 \n-  n \n\" \n,, \nequivalent requires \n= \n(     ) \n= \n---if and only if  proofs. In practice, \n2 2! n - 2 ! 2 \nhowever, it is often easier to establish a \"ring\" of n implications that links all of the \nstatements. The proof of the Fundamental Theorem of Invertible Matrices provides \nan excellent example of this approach.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":114799,"to":114815}}}}],[1677,{"pageContent":"Appendix B* \n0 \nGreat fleas have little fleas \nup on their backs to  bite 'em, \nAnd little fleas have lesser fleas, \nand so ad infinitum. \n-Augustus De Morgan \nA Budget of  Paradoxes \nLongmans, Green, and Company, \n1872, p. 377 \n........... \nMathematical Induction \nThe ability to spot patterns is one of the keys to success in mathematical problem \nsolving. Consider the following pattern: \n1  =  1 \n1  +  3 = 4 \n1+3+5=9 \n1  + 3 + 5 +  7 = 16 \n1  + 3  + 5 + 7 + 9 = 25 \nThe sums are all perfect squares: 12, 22, 32, 42, \n52•  It seems reasonable to conjecture that \nthis pattern will continue to hold; that is, the sum of consecutive odd numbers, start­\ning at 1, will always be a perfect square. Let's try to be more precise. If the sum is n2, \nthen the last odd number in the sum is 2n - 1. (Check this in the five cases above.) In \nsymbols, our conjecture becomes \n1  +  3 +  5 + ·   ·   · +  (2n - 1) =  n\n2 \nfor all n 2 1 \n(\n1\n) \nNotice that Equation (1) is really an infinite collection of statements, one for each","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":114817,"to":114848}}}}],[1678,{"pageContent":"symbols, our conjecture becomes \n1  +  3 +  5 + ·   ·   · +  (2n - 1) =  n\n2 \nfor all n 2 1 \n(\n1\n) \nNotice that Equation (1) is really an infinite collection of statements, one for each \nvalue of n 2 1. Although our conjecture seems reasonable, we cannot assume that the pat­\ntern continues-we need to prove it. This is where mathematical induction comes in. \nFirst Principle of Mathematical Induction \nLet S(n) be a statement about the positive integer n. If \n1. S (1) is true and \n2. for all k 2 1, the truth of S(k) implies the truth of S(k + 1) \nthen S(n) is true for all n 2 1. \nVerifying that S(l) is true is called the basis step. The assumption that S(k) is true \nfor some k 2 1 is called the induction hypothesis. Using the induction hypothesis to \nprove that S(k + 1) is then true is called the induction step. Mathematical induction \nhas been referred to as the domino principle because it is analogous to showing that a","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":114848,"to":114866}}}}],[1679,{"pageContent":"prove that S(k + 1) is then true is called the induction step. Mathematical induction \nhas been referred to as the domino principle because it is analogous to showing that a \nline of dominoes will fall down if ( 1) the first domino can be knocked down (the basis \nstep) and ( 2) knocking down any domino (the induction hypo  thesis) will knock over \nthe next domino (the induction step). See Figure B.l. \nWe now use the principle of mathematical induction to prove Equation (1). \n*Exercises and selected odd-numbered answers for this appendix can be found on the student companion \nwebsite. \n81","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":114866,"to":114874}}}}],[1680,{"pageContent":"82 \nAppendix B  Mathematical Induction \nExample B.1 \n-� ..... \nI \nI \n'7,, \nI \nI \n/ \nI\n\\ \nI \nI \n/ \n/ \\ \nI \nI \n/ \n/ \n). \n/\n/\n/\n/\n,/'\n) \n1-t\"\"j''x\nL \n;::--:;··'\\ \n// \n/_,\n,..\nY \n.,,.. ,,.... \n\\ \nA \n?-\n,,,,Y \n\\ \n;.-4 / \\ \\ \n�:-\n_-\n_- _-  � _- _-\n_-\n�J \nIf  the first domino falls, and ,  ,  , \neach  domino that falls knocks down the next  one,  ,  ,  , \n� \n'.A \n'�� \n• \n!!Iii • \n• \n• \n• \n• \n• \n• \n-\n• \n• \n� \n� \n• \n• \n• \n• \n• \n• \n• \n• \n• \n• \n• \n• \n• \n• \n• \nthen all the dominoes can be made to fall b\ny \npushing over the first one, \nFigure B.1 \nUse mathematical induction to prove that \n1  + 3 + 5 + ·   ·   · + (2n - 1) \n= \nn\n2 \nfor all n 2: 1. \nSolulion For n \n= \n1, the sum on the left-hand side is just 1, while the right-hand side \nis 12• Since 1 \n= \n12, this completes the basis step. \nNow assume that the formula is true for some integer k 2: 1. That is, assume that \n1 \n+  3 + 5  + ... + (\n2k -1) \n= \nk\n2 \n(This is the induction hypothesis.) The induction step consists of proving that the \nformula is true when n \n= \nk +  1. We see that when n \n= \nk +  1, the left-hand side of \nformula (1) is","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":114876,"to":114984}}}}],[1681,{"pageContent":"2k -1) \n= \nk\n2 \n(This is the induction hypothesis.) The induction step consists of proving that the \nformula is true when n \n= \nk +  1. We see that when n \n= \nk +  1, the left-hand side of \nformula (1) is \n1 + 3 + 5 + ... + \n(\n2\n(\nk + 1) - 1) = \n1 + 3 + 5 + ... + \n(\n2k +  1) \n= \n1 + 3 + 5 + ... + \n(\n2k - 1) + \n(\n2k + 1) \nk2 \n= \n(\nk + 1)\n2 \nwhich is the right-hand side ofEquation (1) when n = k + 1. \n+ 2k + l� \nby the induction \nhypothesis \nThis completes the induction step, and we conclude that Equation (1) is true for \nall n 2: 1, by the principle of mathematical induction.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":114984,"to":115019}}}}],[1682,{"pageContent":"Example B.2 \nAppendix B  Mathematical Induction \n83 \nThe next example gives a proof of a useful formula for the sum of the first n \npositive integers. The formula appears several times in the text; for example, see the \nsolution to Exercise 51 in Section 2.4. \nProve that \nfor all n 2 1. \n1 \n+ 2 +  · · · +  n = \nn(n +   1) \n2 \nSolution \nThe formula is true for n =   1, since \n1(1 +  1) \nl=---\n2 \nAssume that the formula is true for n = k; that is, \nk(k +   1) \n1+2+ .. \n·+k=\n---\n2 \nWe need to show that the  formula is true when n = k + l; that is, we must prove that \nBut we see that \n(k +  l)[(k +   1) +  l] \n1 \n+ 2 + ... +  (k +  1) = \n-------\n2 \n1 \n+ 2 + ... +  (k + 1) =  (1 + 2  + ... + k) +  (k + 1) \nk(k +   1) \n= +  (k + 1) \nby the induction hypothesis \n2 \nk(k +   1) + 2(k +  1) \n2 \nk\n2 \n+  3k + 2 \n2 \n(k +   l)(k +   2) \n2 \n(k + l)[(k+l)+l] \n2 \nwhich is what we needed to show. \nThis completes the induction step, and we conclude that the  formula is true for all \nn 2 1, by the principle of mathematical induction.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":115021,"to":115069}}}}],[1683,{"pageContent":"2 \n(k + l)[(k+l)+l] \n2 \nwhich is what we needed to show. \nThis completes the induction step, and we conclude that the  formula is true for all \nn 2 1, by the principle of mathematical induction. \nIn a similar vein, we can prove that the sum of the squares of the first n positive \nintegers satisfies the formula \n2 2 2 \n2 \nn (n +  1)(2n +  1) \n1  +2 +3 + .. ·+n \n=\n------\n6 \n� \nfor all n 2 1. (Verify this for yourself.)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":115069,"to":115085}}}}],[1684,{"pageContent":"84 \nAppendix B  Mathematical Induction \nExample B.3 \nExample B.4 \nThe basis step need not be for n \n= \n1, as the next two examples illustrate. \nProve that n ! > 2\" for all integers n 2: 4. \nSolution \nThe basis step here is when n \n= \n4. The inequality is clearly true in this case, \nsince \n4! \n= \n24 > 16 \n= \n2\n4 \nAssume that kl > 2\nk \nfor some integer k 2: 4.   Then \n(k + l)! \n= \n(k + l)k! \n> (k +  1)2\nk \n2: 5\n. \n2\nk \n> 2 \n• \n2\nk \n= \n2\nk\n+I \nby the induction hypothesis \nsince k 2:: 4 \nwhich verifies the inequality for n \n= \nk + 1 and completes the induction step. \nWe conclude that n ! > 2\" for all integers n 2: 4, by the principle of mathematical \ninduction. \nIf a is a nonzero real number and n 2: 0   is an integer, we can give a recursive \ndefinition of the power a\" that is compatible with mathematical induction. We define \na0 \n= \n1 and, for n 2: 0, \nn \ntimes \n(This form avoids the ellipses used in the version a\nn \n= \na\n�\n.) \nWe can now use \nmathematical induction to verify a  familiar property of exponents.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":115087,"to":115147}}}}],[1685,{"pageContent":"a0 \n= \n1 and, for n 2: 0, \nn \ntimes \n(This form avoids the ellipses used in the version a\nn \n= \na\n�\n.) \nWe can now use \nmathematical induction to verify a  familiar property of exponents. \nLet a be a nonzero real number. Prove that ama\" \n= \nam+n for all integers m, n 2: 0. \nSolution At first glance, it is not clear how to proceed, since there are two variables, \nm and n. But we simply need to keep one of them fixed and perform our induction \nusing the other. So, let m 2: 0   be a fixed integer. When n \n= \n0, we have \nusing the definition a0 = 1. Hence, the basis step is true. \nNow assume that the formula holds when n = k, where k 2: 0. Then ama\nk \n= am+\nk\n. \nFor n = k + 1, using our recursive definition and the fact that addition and multipli­\ncation are associative, we see that \na\nm\na\nk\n+I \n= \na\nm\n(\na\nk\na\n) \n= \n(a\nm\na\nk\n)a \n= \na\n(\nm\n+\nk\n)+I \n= \na\nm\n+\n(\nk\n+I) \nby definition \nby the induction hypothesis \nby definition","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":115147,"to":115211}}}}],[1686,{"pageContent":"Appendix B  Mathematical Induction \n85 \nTherefore, the formula is true for n \n= \nk +   1, and the induction step is complete. \nWe conclude that \na\n'\"\na\n\" \n= \na\nm+n for all integers m, n 2 0, by the principle of \nmathematical induction. \nIn Examples B. l through B.4, the use of the induction hypothesis during the \ninduction step is relatively straightforward. However, this is not always the case. An \nalternative version of the principle of mathematical induction is often more useful. \nSecond Principle of Mathematical Induction \nLet 5(n) be a statement about the positive integer n. If \n1. 5 (1) is true and \n2. the truth of 5(1), 5(2), ... , 5(k) implies the truth of 5(k + 1) \nthen 5(n) is true for all n 2 1. \nThe only difference between the two principles of mathematical induction is in the \ninduction hypo  thesis: The first version assumes that 5(k) is true, whereas the second \nversion assumes that all of 5(1), 5(2), ... , 5(k) are true. This makes the second prin­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":115213,"to":115237}}}}],[1687,{"pageContent":"induction hypo  thesis: The first version assumes that 5(k) is true, whereas the second \nversion assumes that all of 5(1), 5(2), ... , 5(k) are true. This makes the second prin­\nciple seem weaker than the first, since we need to assume more in ordertoprove5(k +  1) \n(although, paradoxically, the second principle is sometimes called strong induction). \nIn fact, however, the two principles are logically equivalent: Each one implies the \n� \nother. (Can you see why?) \nThe next example presents an instance in which the second principle of mathe­\nmatical induction is easier to use than the first. Recall that a prime number is a posi­\ntive integer whose only positive integer factors are 1 and itself. \nExample B.5 \nProve that every positive integer n 2 2 either is prime or can be factored into a prod­\nuct of primes. \nSolution The result is clearly true when n \n= \n2, since 2 is prime. Now assume that for \nall integers n between 2 and k, n either is prime or can be factored into a product of \nprimes. Let n \n=","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":115237,"to":115255}}}}],[1688,{"pageContent":"Solution The result is clearly true when n \n= \n2, since 2 is prime. Now assume that for \nall integers n between 2 and k, n either is prime or can be factored into a product of \nprimes. Let n \n= \nk +   1. If k + 1 is prime, we are done. Otherwise, it must factor into a \nproduct of two smaller integers-say, \nk +  1 \n= \na\nb \n� \nSince 2 ::; \na\n, b::; k (why?), the induction hypothesis applies to \na \nand b. Therefore, \na \n= \np\n1 \n•   •   • \nP\nr and \nb \n= \nq 1 \n•   •   • \nq, \nwhere the \np\n's and \nq\n's are all prime. Then \na\nb \n= \nP1 \n· · · p\n,q\n1 \n· · · \nq, \ngives a factorization of \na\nb into primes, completing the induction step. \nWe conclude that the result is true for all integers n 2 2, by the second principle \nof mathematical induction.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":115255,"to":115303}}}}],[1689,{"pageContent":"86 \nAppendix B  Mathematical Induction \n� \nDo you see why the first principle of mathematical induction would have been \nExample B.6 \ndifficult to use here? \nWe conclude with a highly nontrivial example that involves a combination of in­\nduction and b\na\nckw\nard induction.  The result is the Arithmetic Mean-Geometric \nMean Inequality, discussed in Chapter 7 in Exploration: Geometric Inequalities and \nOptimization Problems. The clever proof in Example B.6 is due to Cauchy. \nLet x\nI\n,  ... ,  x\nn \nbe nonnegative real numbers. Prove that \nfor all integers n 2 2. \n�\nn\nl \nX\n1 \n+ \nX\nz \n+ \n... \n+ \nXn \nV \nX\n1\nX2\n'' \n'\nX\nn :S \nn \nSolulion For n \n= \n2\n, \nthe inequality becomes \\/XY :s (x \n+ \ny\n)\n/2. You are asked to \nverify this in Problems 1 and 2 of the Exploration mentioned above. \nIf S(n\n) \nis the stated inequality, we will prove that S(k\n) \nimplies S(2k\n)\n. Assume that \nS(k\n) \nis true; that is, \nfor all nonnegative real numbers x\nI\n,  ... ,  x\nk\n. Let \nThen \nwhich verifies S\n(\n2k\n)\n. \n:s \nk \ne\n1\n:\nY\n2\n> \n.. \ne\n2\nk\n-\n1\n2\n+\nY\n2\nk\n) \n= \n'\\fl \nX\n1 \n... \nX\nk \nX\n1 \n+ \nX\nz \n+ \n... \n+ \nx\nk \n:s \n�������-\nk","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":115305,"to":115415}}}}],[1690,{"pageContent":")\n. Assume that \nS(k\n) \nis true; that is, \nfor all nonnegative real numbers x\nI\n,  ... ,  x\nk\n. Let \nThen \nwhich verifies S\n(\n2k\n)\n. \n:s \nk \ne\n1\n:\nY\n2\n> \n.. \ne\n2\nk\n-\n1\n2\n+\nY\n2\nk\n) \n= \n'\\fl \nX\n1 \n... \nX\nk \nX\n1 \n+ \nX\nz \n+ \n... \n+ \nx\nk \n:s \n�������-\nk \n(\nY\nI \n+ \nY\n2\n) \n(\nY\n2\nk\n-\n1 \n+ \nY\n2\nk\n) \n2 \n+ \n... \n+ \n2 \nY1 \n+ · · · + \nY\n2\nk \n2k \nk \nby S(2) \nby S(k) \nThus, the Arithmetic Mean-Geometric Mean Inequality is true for n \n= \n2\n, \n4, \n8, ... -the powers of 2. In order to complete the proof, we need to \"fill in the gaps:' \nWe will use backward induction to prove that S\n(\nk\n) \nimplies S (\nk -1). Assuming S\n(\nk\n) \nis \ntrue, let \nk -1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":115415,"to":115520}}}}],[1691,{"pageContent":"Then \nAppendix B  Mathematical Induction \nBJ \n(\nX1 \n+ \nX\nz \n+  ... + \nX\nk\n-\n1\n) \nx + x +  ... + \n1 \n(\nx\ni \n+ \nX\nz \n+ ... + \nX\nk\n-\n1\n) \nI \n2 \nk - 1 \nk xx \n... \nx \n< \n----------------\n1 \n2 \nk\n-\n1 \nk - 1 \n-\nk \nkx\n1 \n+ kx\n2 \n+  · · · + kx\nk\n-\ni \nk \n(\nk - 1) \nX\n1 \n+ \nX\nz \n+  · · · + x\nk\n-\n1 \nk - 1 \nEquivalently, \nor \n. . . \n(\nX\ni \n+ \nX\nz \n+  ... + \nX\nk\n-\n1\n) \n< (\nX\ni \n+ \nX\nz \n+  ... + \nX\nk\n-\nl\n)\nk \nX1X\n2 \nX\nk\n-\n1 \nk \n-\nk - 1 - 1 \n< \n(\nX\ni \n+ \nX\nz \n+ ... + \nX\nk\n-\nl\n)\nk\n-\nI \nX1X\n2 \n... \nX\nk\n-\n1 \n-\nk - 1 \nTaking the \n(\nk - l)th root of both sides yields S(k - 1). \nThe two inductions, taken together, show that the Arithmetic Mean-Geometric \nMean Inequality is true for all n 2 2. \nRemark Although mathematical induction is a powerful and indispensable tool, \nit cannot work miracles. That is, it cannot prove that a  pattern or formula holds if it \ndoes not. Consider the diagrams in Figure B.2, which show the maximum number of \nregions R(n\n) \ninto which a circle can be subdivided by n straight lines. \nR\n(\nO\n) \n= 1 = 2\n° \nFigure B.2 \nR\n(l) = 2 = 2\n1 \nR\n(\n2\n) \n= 4 = 2\n2 \nBased on the evidence in Figure B.2, we might conjecture that \nR(n\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":115522,"to":115678}}}}],[1692,{"pageContent":"regions R(n\n) \ninto which a circle can be subdivided by n straight lines. \nR\n(\nO\n) \n= 1 = 2\n° \nFigure B.2 \nR\n(l) = 2 = 2\n1 \nR\n(\n2\n) \n= 4 = 2\n2 \nBased on the evidence in Figure B.2, we might conjecture that \nR(n\n) \n= 2\" for n 2  0 \nand try to prove this conjecture using mathematical induction. We would not succeed, \nsince this formula is not correct! If we had considered one more case, we would have \ndiscovered that R(3) \n= \n7 * 8 \n= \n23, thereby demolishing our conjecture. In fact, the \ncorrect formula turns out to be \nn\n2 \n+ n + 2 \nR(n\n) \n= \n----\n2 \n� \nwhich c\na\nn be verified by induction. (Can you do it?) \nFor other examples in which a pattern appears to be true, only to disappear when \nenough cases are considered, see Richard K. Guy's delightful article \"The Strong Law of \nSmall Numbers\" in the Americ\na\nn Mathematic\na\nl Monthl\ny\n, Vol. 95 (1988), pp. 697-712.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":115678,"to":115729}}}}],[1693,{"pageContent":"[The J extension of  th e number \nconcept to include the irrational,  and \nwe will at once add, the imaginary,  is \nthe greatest fo rward step  which pure \nmathematics has  ever taken. \n-Hermann Hankel \nTheorie der Complexen \nZahlensysteme \nLeipzig, 1867, p. 60 \nThere is nothing \"imaginary\" \nabout complex numbers-they are \njust as \"real\" as the real numbers. \nThe term imaginary arose from the \nstudy of polynomial equations \nsuch as x\n2 \n+ 1 = 0, whose solu­\ntions are not \"real\" (i.e., real num­\nbers). It is worth remembering that \nat one time negative numbers were \nthought of as \"imaginary\" too. \nJean-Robert Argand (1768-1822) \nwas a French accountant and ama­\nteur mathematician. His geometric \ninterpretation of complex numbers \nappeared in 1806 in a book that he \npublished privately. He was not, \nhowever, the first to give such an \ninterpretation. The Norwegian­\nDanish surveyor Caspar Wessel \n(1745-1818) gave the same version \nof the complex plane in 1787, but his \npaper was not noticed by the","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":115731,"to":115763}}}}],[1694,{"pageContent":"however, the first to give such an \ninterpretation. The Norwegian­\nDanish surveyor Caspar Wessel \n(1745-1818) gave the same version \nof the complex plane in 1787, but his \npaper was not noticed by the \nmathematical community until after \nhis death. \nC1 \nAppendix C* \nComplex Numbers \nA complex number is a number of the form \na + bi, where \na \nand b are real numbers and \ni is a symbol with the property that i\n2 \n= \n-1. The real number \na \nis considered to be a \nspecial type of complex number, since \na \n= \na + Oi. If z \n= \na + bi is a complex number, \nthen the real part of z, denoted by Re z, is \na\n, and the imaginary part of z, denoted by \nIm z, is b. Two complex numbers \na + bi and c + di are equal if their real parts are equal \nand their imaginary parts are equal-that is, if \na \n= \nc and b \n= \nd. A complex number \na \n+ \nbi can be identified with the point \n(a\n, b\n) \nand plotted in the plane (called the complex \nplane, or the Argand plane), as shown in Figure C.l. In the complex plane, the horizon­","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":115763,"to":115808}}}}],[1695,{"pageContent":"d. A complex number \na \n+ \nbi can be identified with the point \n(a\n, b\n) \nand plotted in the plane (called the complex \nplane, or the Argand plane), as shown in Figure C.l. In the complex plane, the horizon­\ntal axis is called the real axis and the vertical axis is called the imaginary axis. \n-4 + 3i \n• \nIm \n6i \n4i \n3 +  2i \n2i \n• \n�-+--+--+--+--+�+--+--+--+---+--<,__.,_-+--+\nRe \n-\n6 -4  -2 2 \n• \n-2i \n-\n3 \n-\n2i \n-4i \n• \n1 - 4i \n-6i \nFigure C.1 \nThe complex plane \nOperations on  Complex Numbers \n4 \n6 \nThe sum of the complex numbers \na \n+ bi and c + di is defined as \n(a \n+ bi\n) \n+ \n(\nc + di\n) \n= \n(a + c\n) \n+ \n(\nb + d\n)\ni \nNotice that, with the identification of \na + bi with \n(a\n,  b\n)\n,  c + di with \n( c, d )\n, and \n(a \n+ c\n) \n+ \n(\nb + \nd )\ni with \n(a + c, b + \nd )\n, addition of complex numbers is   the same \nas vector addition. The product of \na \n+ bi and c + di is \n(a \n+ bi\n)(\nc + di\n) \n= \na(\nc + di\n) \n+ bi\n(\nc + di\n) \n= \na\nc + adi + bci + bd\ni\n2 \n*Exercises and selected odd-numbered answers for this appendix can be found on the  student companion \nwebsite.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":115808,"to":115904}}}}],[1696,{"pageContent":"Im \nbi \nExample C.1 \nz \n=a+ bi \nT \nI \n..L \n-1-----+----+Re \nl\na \nI \nT \nI \n-bi \n� \nz =a -bi \nFigure C.2 \nComplex conjugates \nExample C.2 \nAppendix C  Complex Numbers \nC2 \nSince i\n2 \n= -1, this expression simplifies to (ac - bd) +  (ad +   bc)i.   Thus, we have \n(a + bi)(c +  di)  =  (ac - bd) +  (ad +  bc)i \nObserve that,   as a special case, a(c +  di)  =  ac + adi, so the negative of c +   di is \n- ( c + di) = (    -1) ( c + di) =  -c -di. This fact allows us to compute the difference \nofa +   bi and c +  di as \n(a +  bi) - (c + di) =  (a +  bi) +  (-l)(c +   di) \n=(a+ (-c)) +  (b +  (-d))i \n=  (a - c) +  (b - d)i \nFind the sum, difference, and product of 3 - 4i and - 1 + 2i. \nSolution The sum is \n(3 \n- 4i) +  (-1 + 2i)  =   (3 - 1) \n+  (-4 + 2)i = 2 -2i \nThe difference is \n(3 - 4i) - (-1 + 2i)  =   (3 - (-1)) +  (-4 - 2)i = 4 -6i \nThe product is \n(3 \n- 4i)( -1 + 2i) =  -3 + 6i + 4i \n- 8i\n2 \n-3 +  1 Oi -8( -1) =  5 + lOi \nThe conjugate of z = a + bi is the complex number \nz = a -bi \n(z is pronounced \n\"","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":115906,"to":115954}}}}],[1697,{"pageContent":"The product is \n(3 \n- 4i)( -1 + 2i) =  -3 + 6i + 4i \n- 8i\n2 \n-3 +  1 Oi -8( -1) =  5 + lOi \nThe conjugate of z = a + bi is the complex number \nz = a -bi \n(z is pronounced \n\"\nz bar:')  Figure  C.2 gives the geometric interpretation of the \nconjugate. \nTo find the quotient of two complex numbers, we multiply the numerator and the \ndenominator by the conjugate of the denominator. \n-1 + 2i \nExpress \nin the form a +   bi. \n3 +   4i \nSolution We multiply the numerator and denominator by 3 +   4i = 3 -4i. Using \nExample C.l, we obtain \n-1 + 2i \n3 +   4i \n-1 + 2i  3 -  4i   5 +   lOi \n---·---\n= \n---\n3 +  4i   3 - 4i   3\n2 \n+ 4\n2 \n5 +   lOi 1   2 \n---\n=-+-i \n25 \n5   5 \n......,.. \nOn the following page is a summary of some of the properties of conjugates. The \nproofs follow from the definition of conjugate; you should verify them for yourself.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":115954,"to":115991}}}}],[1698,{"pageContent":"C3 \nAppendix C  Complex Numbers \nIm \nz \n=a+ bi \na \nFigure C.3 \nIm \na+ bi \nr \nb \ne \nFigure C.4 \n1. z = z \n2. z + w = z + w \n3. zw = zw \n4.  Ifz *   0, then (w\n/\nz) = w/\nz. \n5.  z is real if and only ifz =  z. \nThe absolute value (or modulus) lzl of a complex number z = a + bi is its dis­\ntance from the origin. As Figure C.3 shows, Pythagoras' Theorem gives \nl\nz\nl \n= \nl\na +   bi\nl \n= \nV\na\n2 \n+ b\n2 \nObserve that \nzz =  (a  + bi)(a - bi) = a\n2 \n- abi +  bai - b\n2\ni\n2 \n= a\n2 \n+  b\n2 \nHence, \nzz = \nl\nz\nl\n2 \nThis gives us an alternative way of describing the division process for the quotient of \ntwo complex numbers. If w and z * 0 are two complex numbers, then \n-    -    -\nw   w  z   wz wz \nz \n-\nzz \nI \nz\nl \n2 \nz \nz \n........... \nBelow is a summary of some of the properties of absolute value. (You should try \nto prove these using the definition of abs  olute value and other properties of complex \nnumbers.) \n1. \nl\nz\nl \n= Oifand only ifz=O. \n2. \nl\nz\nl \n= \nl\nz\nl \n3. \nl\nzw\nl \n= \nl\nz\nl l\nw\nl \n4.  Ifz *   O, then \nl�I \nl\nz\nl\n' \n5. \nl\nz + w\nl  :::;  l\nz\nl \n+ \nl\nw\nl \nPolar Form","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":115993,"to":116102}}}}],[1699,{"pageContent":"numbers.) \n1. \nl\nz\nl \n= Oifand only ifz=O. \n2. \nl\nz\nl \n= \nl\nz\nl \n3. \nl\nzw\nl \n= \nl\nz\nl l\nw\nl \n4.  Ifz *   O, then \nl�I \nl\nz\nl\n' \n5. \nl\nz + w\nl  :::;  l\nz\nl \n+ \nl\nw\nl \nPolar Form \nAs you have seen, the complex number z = a + bi can be represented geometrically \nby the point (a, b). This point can also be expressed in terms of polar coordinates \n(\nr, 8), where r 2': 0, as sh  own in Figure C.4. We have \nso \na = r cos 8 \nand  b = r sin 8 \nz = a +  bi = r cos 8  + (r sin 8)i \nThus, any complex number can be written in the polar fo rm \nz =   r(cos 8  +  i sin 8)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":116102,"to":116152}}}}],[1700,{"pageContent":"Example C.3 \nIm \nz \n= l + i \n} \n--l'---r-'---+----Re \n,2 \n-\n-\n1 \n2 \nce3 \n-2 \nw= \n-\n,\n3i \nFigure C.5 \nAppendix C  Complex Numbers \nC4 \nwhere r \n= \nI \nz\nl \n= \nV \na\n2 \n+  b\n2 \nand tan (J \n= \nb /a. The angle (J is called an argument of z \nand is denoted by arg z. Observe that arg z is not unique: Adding or subtracting any \ninteger multiple of 2\n7T gives another argument of z. However, there is only one argu­\nment (J that satisfies \n-7T < (J :s 7T \nThis is called the principal argument of z and  is denoted by Arg z. \nWrite the following complex numbers in polar form using their principal arguments: \n(a) z \n= \n1 + i \n(b) w \n= \n1  - VJi \nSolution \n(a) We compute \nr \n= \nI \nz \nI \n= \nV \n1\n2 \n+  1\n2 \n= \nv'2  and \ntan (J \n= \n_1_ \n= \n1 \n7T \nTherefore, Arg z \n= \n(J \n= \n-( \n= \n45°), and we have \n4 \nas shown in Figure C.5. \n(b) We  have \n-\nV3 \nr \n= \nl\nw\nl \n= \n\\/\n1\n2 \n+  (-V3)\n2 \n= V4 = \n2 \nand  tan fJ \n= \n--= \n-V3 \n1 \n7T \nSince w lies in  the  fourth quadrant, we  must  have Arg  z \n= \n(J \n= \n- -( \n= \n-60°). \nTherefore, \n3 \nw \n= \n2( cos( -\n�\n) \n+  i   sin( -\n�\n)) \nSee Figure C.5.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":116154,"to":116270}}}}],[1701,{"pageContent":"w\nl \n= \n\\/\n1\n2 \n+  (-V3)\n2 \n= V4 = \n2 \nand  tan fJ \n= \n--= \n-V3 \n1 \n7T \nSince w lies in  the  fourth quadrant, we  must  have Arg  z \n= \n(J \n= \n- -( \n= \n-60°). \nTherefore, \n3 \nw \n= \n2( cos( -\n�\n) \n+  i   sin( -\n�\n)) \nSee Figure C.5. \nThe polar form of complex numbers can be used to give geometric interpretations \nof multiplication and division. Let \nz\n1 \n= \nr\n1\n(cos fJ\n1 \n+ i   sin fJ\n1\n) \nand  z\n2 \n= \nr\n2\n(cos fJ\n2 \n+  i sin fJ\n2\n) \nMultiplying, we obtain \nz\n1\nz\n2 \n= \nr\n1\nr\n2\n(cos fJ\n1 \n+  i   sin fJ\n1\n)(cos fJ\n2 \n+  i   sin fJ\n2\n) \n= \nr\n1\nr\n2 \n[(cos fJ\n1 \ncos fJ\n2 \n- sin fJ\n1 \nsin fJ\n2\n)  +  i(sin fJ\n1 \ncos fJ\n2 \n+ cos fJ\n1 \nsin fJ\n2\n)] \nUsing the trigonometric identities \ncos(fJ\n1 \n+ fJ\n2\n) \n= \ncos fJ\n1 \ncos fJ\n2 \n- sin fJ\n1 \nsin fJ\n2 \nSin(fJ\nI \n+ fJ\n2\n) \n= \nSin (J\nI \nCOS (J\n2 \n+ COS (J\nl \nSin (J\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":116270,"to":116395}}}}],[1702,{"pageContent":"C5 \nAppendix C  Complex Numbers \nIm \nfigure C.6 \nIm \n-\nr \nfigure C.1 \nwe obtain \n(1) \nwhich is the  polar form of a complex number with absolute value r1r\n2 \nand argument \ne\nl \n+ e\nz\n. This shows that \nEquation (1) says that to multi\np\nl\ny \ntwo complex numbers,  we multi\np\nl\ny \ntheir \na\nbsolute \nvalues and add their a\nrguments. See Figure C.6. \nSimilarly, using the subtraction identities for sine and cosine, we can show that \n� (Verify this.) Therefore, \nr \nl \nz \nz \nExample C.4 \nand we see that to divide two complex numbers, we divide their \na\nbsolute values \na\nnd \nsubtr\na\nct their \na\nrguments. \nAs a special case of the last result, we obtain a formula for the reciprocal of a com­\nplex number in polar form. Setting \nZ\n1 \n= \n1 (and therefore e\nl \n= \nO) and \nZ\nz \n= \nz (and \ntherefore e\n2 \n= \ne), we obtain the following: \nSee Figure C.7. \nIf z \n= \nr(cos e  + i sin e) is nonzero, then \n1 \n1 \n-\n= \n-(cos e -i sin e) \nz \nr \nFind the product of 1 + i and 1 -v3i in polar form. \nSolulion From Example C.3, we have \nTherefore, \nSee Figure C.8.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":116397,"to":116476}}}}],[1703,{"pageContent":"Im \n1 + i \n2 \n-2i \n1 -\n,\n3i \nFigure C.8 \nAppendix C  Complex Numbers C6 \n(\n1 + i\n)(\nl -\n,\n3i\n) = \n-\n-\n(\nl +\n,\n3)\n+i\n(\nl-\n,\n3) \nRemark \nSince (1 + i\n) \n(1 -  \\13i\n) \n= \n(1 + \\13\n) \n+ i\n(\nl -\\13\n) \n(check this), we \nmust have \n1 \n+  V3 \n= \n2V2 \ncos\n(\n-\n�\n) \n= \n-\n2V2 \ncos\n(\n�\n) \nand \n1  -\nV3 \n= \n2V2 \nsin\n(\n-\n�\n) \n= \n-\n2V2 \nsin\n(\n�\n) \n� \n(Why?) This implies that \nAbraham De Moivre (1667-1754) was \na French mathematician who made \nimportant contributions to trigonom­\netry, analytic geometry, probability, \nand statistics. \n(\n'TT\n) \n1+\\/3 \n(\n'TT\n) \n\\/3-\n1 \ncos \nii \n= \nlW \nand sin \nii \n= \nlW \nWe therefore have a method for finding the sine and cosine of an angle such as n/12 \nthat is not a special angle but that can be obtained as a sum or difference of special \nangles. \nDe Moivre's Theorem \nIf n is a positive integer and z = r (cos 8 + i sin 8), then repeated use of Equation (1) \nyields formulas for the powers of z: \nz\n2 \n= r\n2\n(cos 28 + i sin 28) \nz\n3 \n= \nzz\n2 \n= \nr\n3\n( cos 38 + i sin 38) \nz\n4 \n= \nzz\n3 \n= r\n4\n(cos 48 + i sin 48) \nIn general, we have the following result, known as De Moivre's Theorem.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":116478,"to":116604}}}}],[1704,{"pageContent":"CJ \nAppendix C  Complex Numbers \nTheorem C.1 \nExample C.5 \nIm \n-2 + 2i \n2i \n-4i \nl + i \nDe Moivre's Theorem \nIf z \n= \nr(cos () + i sin ()) and n is a positive integer, then \nz\n\" \n= \nr\nn\n(cosn()  + isinn()) \nStated differently, we have \nl\nz\n\"\nI \n= \nl\nz\nl\n\" \nand  arg(z\n\"\n) \n= \nn arg z \nIn words, De Moivre's Theorem says that to t\na\nke the nth \np\nower o\nf \na \ncomplex number, \nwe t\na\nke the nth \np\nower o\nf \nits \na\nbsolute value \na\nnd multipl\ny \nits \na\nrgument b\ny \nn. \nFind (1 + i\n) \n6• \nSolution \nFrom Example C.3(a), we have \n1  + i \n= \nv2 \n(cos \n: \n+ i sin \n: \n) \nHence, De Moivre's Theorem gives \n(1 + \ni)\n6 \n= \n(\n\\/2\n)\n6\n( \ncos \n6\n: \n+ \ni \nsin \n6\n:\n) \n( \n317 317\n) \n= \n8  cos 2 + i sin 2 \n= \n8(0 + i (-\n1\n)) \n= \n-Si \n-+----+---+--+--+--+-.......... \nRe \nSee Figure C.9, which shows 1 + i, (1 + i\n)\n2\n, (1 + i\n)\n3\n, •.. , (1 + i\n)\n6• \n-4-4i \nFigure C.9 \nPowers of 1 + i \n-8i \nWe can also use De Moivre's Theorem to find nth roots of complex numbers. An \nnth root of the complex number z is any complex number w such that \nw\n\" \n= \nz \nIn polar form, we have \nw \n= \ns(cos <p + i sin <p\n) \nand  z \n= \nr(cos ()  + i sin ())","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":116606,"to":116736}}}}],[1705,{"pageContent":"nth root of the complex number z is any complex number w such that \nw\n\" \n= \nz \nIn polar form, we have \nw \n= \ns(cos <p + i sin <p\n) \nand  z \n= \nr(cos ()  + i sin ()) \nso, by De Moivre's Theorem, \ns\n\"\n(cos n<p + i sin n<p\n) \n= \nr(cos ()  + i sin ()) \nEquating the absolute values, we see that \nWe must also have \ns\n\" \n= r or s = \nr\n1\n1\n\" \n= \\(;: \ncos n<p \n= \ncos () \nand  sin n<p \n= \nsin () \n� \n(Why?) Since the sine and cosine functions each have period 217, these equations \nimply that n<p and () differ by an integer multiple of 217; that is, \n() +   2k17 \nn<p \n= \n()  +   2k17 \nor <p \n= \n----\nn","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":116736,"to":116782}}}}],[1706,{"pageContent":"Example C.6 \nIm \nFigure C.10 \nThe cube roots of -27 \nAppendix C  Complex Numbers CB \nwhere k is an integer. Therefore, \n[ ((J + 2br\n) \n((J + 2k1T\n)] \nw  = r\n1\n1\n\" \ncos \nn \n+  i   sin \nn \ndescribes the possible nth roots of z as k ranges over the integers. It is not hard to \nshow that k = 0, 1, 2, ... , n - 1 produce distinct values of w, so there are exactly n \ndifferent nth roots of z = r(cos (J + i sin fJ). We summarize this result as follows: \nLet z = r(cos (J + i sin fJ) and let n be a positive integer. Then z has exactly n dis­\ntinct nth roots given by \n1\n/ \n[ ((J + 2k1T\n) \n.. ((J + 2k1T\n)] \nr \nn \ncos \n+ l sm \nn n \nfork = 0, 1, 2, ... , n -   1. \nFind the three cube roots of -27. \n(2) \nSolution In polar form, -27 \n= \n27(cos 1T + i    sin 7T). It follows that the cube roots of \n-27 are given by \n(-2\n7)\n1\n/\n3 \n= \n27\n1\n/\n3\n[ \ncos\n( \n1T \n+\n3 \n2k1T\n) \n+  i \nsin\n( \n1T \n+\n3 \n2k\n1T\n)\n] \nUsing formula (2) with n = 3, we obtain \nfor k = 0, 1, 2 \n27\n1\n/\n3\n[ \ncos; \n+  i \nsin;\n] \n[ \n(\n7T +   21T\n) \n(\n1T + 27T\n)\n] \n27\n1\n/\n3 \ncos \n3 \n+  i sin \n3 \n[ \n(\n7T + 4\n7T\n) \n(\n7T + 47T\n)\n] \n27\n1\n/\n3 \ncos \n3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":116784,"to":116894}}}}],[1707,{"pageContent":"+\n3 \n2k1T\n) \n+  i \nsin\n( \n1T \n+\n3 \n2k\n1T\n)\n] \nUsing formula (2) with n = 3, we obtain \nfor k = 0, 1, 2 \n27\n1\n/\n3\n[ \ncos; \n+  i \nsin;\n] \n[ \n(\n7T +   21T\n) \n(\n1T + 27T\n)\n] \n27\n1\n/\n3 \ncos \n3 \n+  i sin \n3 \n[ \n(\n7T + 4\n7T\n) \n(\n7T + 47T\n)\n] \n27\n1\n/\n3 \ncos \n3 \n+  i sin \n3 \n= 3\n(\n_1_ + \nV3\ni\n) \n= i + \n3V3\nj \n2    2 \n2    2 \n= 3(cos 1T +  i sin 7T) = -3 \n( 5\n7T \n5\n7T\n) \n= 3  cos 3 +  i sin 3 \nAs Figure C. l 0 shows, the three cube roots of - 27  are equally spaced 21T /3   radians \n(120°) apart around a circle of radius 3 centered at the origin. \nIn general, formula (2) implies that the nth roots of z = r(cos (J + i   sin fJ) will lie \non a circle of radius r\n1\n1\n\" \ncentered at the origin. Moreover, they will be equally spaced \n21T!n radians (360/n°) apart. (Verify this.) Thus, if we can find one nth root of z, the \nremaining nth roots of z can be obtained by rotating the first root through successive \nincrements of21T/n radians. Had we known this in Example C.6, we could have used \nthe fact that the real cube root of -27 is -3  and then rotated it twice through an","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":116894,"to":116981}}}}],[1708,{"pageContent":"increments of21T/n radians. Had we known this in Example C.6, we could have used \nthe fact that the real cube root of -27 is -3  and then rotated it twice through an \nangle of27T /3 radians (120°) to get the other two cube roots.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":116981,"to":116983}}}}],[1709,{"pageContent":"C9 \nAppendix C  Complex Numbers \nLeonhard Euler ( 1707-1783) was the most prolific mathematician of all time. He has over 900 pub­\nlications to his name, and his collected works fill over 70 volumes. There are so many results attrib­\nuted to him that \"Euler's formula\" or \"Euler's Theorem\" can mean many different things, depending \non the context. \nEuler worked in so many areas of mathematics, it is difficult to list them all. His contribu­\ntions to calculus and analysis, differential equations, number theory, geometry, topology, me­\nchanics, and other areas of applied mathematics continue to be influential. He also intro­\nduced much of the notation we currently use, including 7T, e, i, � for summation, � for \ndifference, and f( x) for a function, and was the first to treat sine and cosine as functions. \nEuler was born in Switzerland but spent most of his mathematical life in Russia and Germany. \nIn  1727, he  joined the St. Petersburg Academy of  Sciences, which had been founded by Catherine","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":116985,"to":116997}}}}],[1710,{"pageContent":"Euler was born in Switzerland but spent most of his mathematical life in Russia and Germany. \nIn  1727, he  joined the St. Petersburg Academy of  Sciences, which had been founded by Catherine \nI, the wife of Peter the Great. He went to Berlin in 1741 at the invitation of Frederick the Great, but \nreturned in 1766 to St. Petersburg, where he remained until his death. When he was young, he lost \nthe vision in one eye as the result of an illness, and by 177 6 he had lost the vision in the other eye \nand was totally blind. Remarkably,  his mathematical output did not diminish, and he continued to \nbe productive until the day he died. \nEuler's Formula \nIn calculus, you learn that the function \ne\n2 \nhas a power series expansion \nz\n2 \nz\n3 \ne\n2 \n= \n1 +  z  +  - +  - + \n2! \n3! \nthat converges for every real number z. It can be shown that this expansion also \nworks when z is a complex number and that the complex exponential function \ne\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":116997,"to":117022}}}}],[1711,{"pageContent":"z\n2 \nz\n3 \ne\n2 \n= \n1 +  z  +  - +  - + \n2! \n3! \nthat converges for every real number z. It can be shown that this expansion also \nworks when z is a complex number and that the complex exponential function \ne\n2 \nobeys the usual rules for exponents. The sine and cosine functions also have power \nseries expansions: \nx\n3 \nx\ns \nx\n7 \nsin x \n= \nx - - +  - - - + \n3! \n5! \n7! \nx\n2 \nx\n4 \nx\n6 \ncos x \n= \n1  -- +  -\n-\n- + \n2! \n4! \n6! \nIf we  let z = ix, where x is a real number, then we have \n(\nix\n)\n2 \n(\nix\n)\n3 \n1 +ix+--+--+ \n2! \n3! \nUsing the fact that i\n2 \n= -1, i\n3 \n= -i, i\n4 \n1, i\n5 \n= i, and so on, repeating in a cycle of \nlength 4, we  see that \ne\nix \n= \nx\n2 \nix\n3 \nx\n4 \nix\n5 \nx\n6 \nix\n7 \n1 \n+  ix - - - - +  - +  - - - - - +  + -   -.. · \n2!   3! 4! 5! 6! 7! \n= \ncos x +  i sin x \nThis remarkable result is known as Euler's fo rmula.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":117022,"to":117106}}}}],[1712,{"pageContent":"Theorem C.2 \nExample C.1 \nAppendix C  Complex Numbers \nC10 \nEuler's Formula \nFor any real number x, \ne\ni\nx =  cos x +  i   sin x \nUsing Euler's formula, we see that the  polar form of a complex number can be \nwritten more compactly as \nz = r(cos e  +   isin O) = re\n;\ne \nFor example, from Example C.3(a), we have \n1 \n+  i = \n\\/2\n( \ncos \n: \n+  i sin \n: \n) \n= \n\\/2e\ni\n\"\n/4 \nWe can also go in the other direction and convert a complex exponential back \ninto polar or standard form. \nWrite the following in the form a + bi: \n(a) e;\" (b) e\nz\n+\ni\n7T\n/4 \nSolution \n(a) Using Euler's formula, we have \ne\n;\n\" =  cos n + i   sin n =  -1 +  i • 0 =   -1 \n(If we write this equation as e\ni\n\" +  1  = 0, we obtain what is surely one of the most \nremarkable equations in mathematics. It contains the fundamental operations of \naddition, multiplication, and exponentiation; the additive identity 0 and the multipli­\ncative identity 1; the two most important transcendental numbers, n and e; and the \ncomplex unit i-all in one equation!)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":117108,"to":117157}}}}],[1713,{"pageContent":"cative identity 1; the two most important transcendental numbers, n and e; and the \ncomplex unit i-all in one equation!) \n(b) Using rules for exponents together with Euler's formula, we obtain \ne\nz\n\\/2 e\nz\n\\/2 \n=--+--i \n2 2 \nIf z = re\ni\n8 = r(cos e + i    sin e), then \nz = r(cos e  -  isin O) \nThe trigonometric identities \ncos(-8) = cos e  and sin(-8) = -sin e \n(3)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":117157,"to":117173}}}}],[1714,{"pageContent":"C11 \nAppendix C  Complex Numbers \nallow us to   rewrite Equation (3) as \nz \n= \nr(cos(-e) +  isin(-e)) \n= \nre\ni(\n-\ne\n) \nThis gives the following useful formula for the conjugate: \nIf z \n= \nre\ni\n0\n, then \nNote \nEuler's formula gives a quick, one-line proof of De Moivre's Theorem: \n(\nr(COS e  + i sin e) \ni\nn \n= \n(re\ni\ne\nr \n= \nr\nn\ne\ni\nn\nO \n= \nr\nn\n(\nCOS ne + \ni sin \nne)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":117175,"to":117218}}}}],[1715,{"pageContent":"Appendix D* \n0 \nEuler gave the most algebraic of  the \nproofs  of the existence of  the roots of \n[a polynomial] equation .... I regard \nit as unjust to ascribe this proof \nexclusively to  Gauss, who merely \nadded the finishing touches. \n-Georg Frobenius, 1907 \nQuoted on the MacTutor History of \nMathematics archive, \nhttp://www-history.mcs \n.st-and.ac. uk/history/ \nExample D.1 \nPolynomials \nA polynomial is a function p of a single variable x that can be written in the form \n(\n1\n) \nwhere \na\n0\n, \na\n1, ... , \na\nn are constants (\na\nn \n* O), called the coefficients of p. With the con­\nvention that x\n0 \n= \n1, we can use summation notation to write pas \nn \np(x) \n= \n2:a\nk\nx\nk \nk\n=O \nThe integer n is called the degree of p, which is denoted by writing deg p \n= \nn. A \npolynomial of degree zero is called a constant polynomial. \nWhich of the following are polynomials? \n(a) 2  -tx + v'2x\n2 \n(b) \n1 \n2--\n3x\n2 \n(c)� \n(d) \nln(\n2e\n5\nx\n') \ne\n3\nx \n(e) \nx\n2 \n- 5x + 6 \nx - 2 \n(f) Vx \n(g)  cos(2 cos-\n1\nx) \n(h) \ne\nx \nSolution (a)  This is the only one that is obviously a polynomial.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":117220,"to":117297}}}}],[1716,{"pageContent":"(a) 2  -tx + v'2x\n2 \n(b) \n1 \n2--\n3x\n2 \n(c)� \n(d) \nln(\n2e\n5\nx\n') \ne\n3\nx \n(e) \nx\n2 \n- 5x + 6 \nx - 2 \n(f) Vx \n(g)  cos(2 cos-\n1\nx) \n(h) \ne\nx \nSolution (a)  This is the only one that is obviously a polynomial. \n(b)  A polynomial of the form shown in Equation \n(\n1\n) \ncannot become infinite as \nx \napproaches a finite value [lim p(x) \n*  ±oo], whereas 2 - 1/3x\n2 \napproaches -oo as \nx\n�c \nx approaches zero. Hence, it is not a polynomial. \n(c)  We have \nwhich is equal to v'2x when x 2 0 and to -\\/2x when x < 0. Therefore, this expres­\nsion is formed by \"splicing together\" two polynomials (a \np\niecewise \np\nol\ny\nnomial\n)\n, but it \nis not a   polynomial itself. \n*Exercises and selected odd-numbered answers for this appendix can be found on the student companion \nwebsite. \n01","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":117297,"to":117354}}}}],[1717,{"pageContent":"02 \nAppendix D \nPolynomials \n(d)  Using properties of exponents and logarithms, we have \n(2e\n5\nx') \n, , \nln \n--= \nln(2e\n5\nx \n-\n3\nx) \n= \nln 2  +   ln(e\n5\nx \n-\n3\nx) \ne\n3\nx \n= \nln 2 +   5x\n3 \n- 3x \n= \nln 2 -  3x + 5x\n3 \nso this expression is a polynomial. \n(e)  The domain of this function consists of all real numbers x *   2. For these values \nof x, the function simplifies to \nx\n2 \n- 5x + 6 \nx - 2 \n(x - 2)(x -  3) \n------\n=x-3 \nx - 2 \nso we can say that it is a polynomial on its domain. \n(f)  We  see that this function cannot be a polynomial (even on its domain x 2 O), \nsince repeated differentiation of a polynomial of the form shown in Equation ( 1) \n� \neventually results in zero and Vx does not have this property. (Verify this.) \n(g)  The domain of this expression is -1 :S x :S 1\n. \nLet 8 = cos \n-\ni \nx so that cos 8 = x. Using \na trigonometric identity, we see that \ncos(2 cos\n-\n1 \nx) \n= \ncos 2() \n= \n2 cos\n2 \n8  - 1 \n= \n2x\n2 \n- 1 \nso this expression is a polynomial on its domain.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":117356,"to":117426}}}}],[1718,{"pageContent":". \nLet 8 = cos \n-\ni \nx so that cos 8 = x. Using \na trigonometric identity, we see that \ncos(2 cos\n-\n1 \nx) \n= \ncos 2() \n= \n2 cos\n2 \n8  - 1 \n= \n2x\n2 \n- 1 \nso this expression is a polynomial on its domain. \n(h)  Analyzing this expression as we did the one in (f), we conclude that it  is not a \npolynomial. \nTwo polynomials are equal if the coefficients of corresponding powers of x are all \nequal. In particular, equal polynomials must have the same degree. The sum of two \npolynomials is obtained by adding together the coefficients of corresponding pow­\ners of x. \nExample D.2 \nFind the sum of2 -4x + x\n2 \nand 1 +  2x - x\n2 \n+ 3x\n3\n. \nSolution We compute \n(2 -4x + x\n2\n)  +   (1 + 2x - x\n2 \n+ 3x\n3\n) \n= \n(2 +  1) +  (-4 + 2)x \n+ (1 +  (-l))x\n2 \n+  (O +  3)x\n3 \n= \n3 -  2x + 3x\n3 \nwhm we have \"padded\" the fast polynom;al by g;vmg ;t an x\n' \ncoeffident of mo\n\".-+ \nWe  define the difference of  two  polynomials analogously,  subtracting \ncoefficients instead of adding them. The product of two polynomials is obtained","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":117426,"to":117483}}}}],[1719,{"pageContent":"' \ncoeffident of mo\n\".-+ \nWe  define the difference of  two  polynomials analogously,  subtracting \ncoefficients instead of adding them. The product of two polynomials is obtained \nby repeatedly using the distributive law and then gathering together correspond­\ning powers of x.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":117483,"to":117489}}}}],[1720,{"pageContent":"Example D.3 \nExample D.4 \nAppendixD \nPolynomials \nFind the product of2 -  4x + x\n2 \nand 1 +  2x - x\n2 \n+ 3x\n3\n• \nSolution We obtain \n(2 - 4x + x\n2\n)(1 + 2x - x\n2 \n+ 3x\n3\n) \n= \n2(1 + 2x - x\n2 \n+ 3x\n3\n)  -  4x(l + 2x - x\n2 \n+ 3x\n3\n) \n+ x\n2\n(1 + 2x - x\n2 \n+ 3x\n3\n) \n= \n(2  +   4x - 2x\n2 \n+ 6x\n3\n)  +   (-4x - 8x\n2 \n+ 4x\n3 \n- 12x\n4\n) \n+  (x\n2 \n+ 2x\n3 \n- x\n4 \n+ 3x\n5\n) \n= \n2 +   (4x -  4x) + (-2x\n2 \n- 8x\n2 \n+ x\n2\n) +  (6x\n3 \n+ 4x\n3 \n+ 2x\n3\n) \n+ (-12x\n4 \n- x\n4\n)  +  3x\n5 \n= \n2 -  9x\n2 \n+  12x\n3 \n- 13x\n4 \n+ 3x\n5 \nObserve that for  two polynomials p and q, we have \ndeg(pq) \n= \ndeg p +   deg q \n03 \nIf p and q are polynomials with deg q ::s deg p, we can divide q into p, using long \ndivision to obtain the quotient plq. The next example illustrates the procedure, which \nis the  same as for long division of one integer into another. Just as the quotient of two \nintegers is not, in general, an integer, the quotient of two polynomials is not, in gen­\neral, another polynomial. \nCompute \n1  +   2x - x\n2 \n+ 3x\n3 \n2 -  4x + x\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":117491,"to":117593}}}}],[1721,{"pageContent":"integers is not, in general, an integer, the quotient of two polynomials is not, in gen­\neral, another polynomial. \nCompute \n1  +   2x - x\n2 \n+ 3x\n3 \n2 -  4x + x\n2 \nSolution We will perform long division. It is helpful to write each polynomial with \ndecrea sing powers of x. Accordingly, we have \nx\n2 \n- 4x + \n2)3x\n3 \n- x\n2 \n+ 2x +  1 \nWe begin by dividing x\n2 \ninto 3x\n3 \nto obtain the partial quotient 3x. We then multiply \n3x by the divisor x\n2 \n- 4x + 2, subtract the result, and bring down the next term from \nthe dividend (3x\n3 \n- x\n2 \n+ 2x + 1): \n3x \nx\n2 \n- 4x + \n2\nh\nx\n3 \n-\nx\n2 \n+ 2x +  1 \n3x\n3 \n- 12x\n2 \n+ 6x \nl lx\n2 \n- 4x +  1 \nThen we repeat the  process with l lx\n2\n, multiplying 11 by \nx\n2 \n- 4x + 2   and subtracting \nthe result from llx\n2 \n- 4x + 1. We obtain \n3x +  11 \nx\n2 \n- 4x + \n2\nh\nx\n3 \n-\nx\n2 \n+  2x +  1 \n3x\n3 \n- 12x\n2 \n+  6x \nllx\n2 \n- 4x + \nllx\n2 \n- 44x + 22 \n40x - 21","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":117593,"to":117677}}}}],[1722,{"pageContent":"04 \nAppendix D \nPolynomials \nTheorem D.1 \nTheorem D.2 \nWe  now have a  remainder 40x - 21. Its  degree is less than that of the divisor \nx\n2 \n- 4x + 2, so the process stops, and we have found that \nor \n3x\n3 \n- x\n2 \n+ 2x +  1 \n= \n(x\n2 \n- 4x + 2)(3x +   11)  +   (40x - 21) \n3x\n3 \n- x\n2 \n+ 2x + \nx\n2 \n- 4x + 2 \n40x -21 \n= \n3x +  11 + \n2 \nx  -4x+2 \nExample D.4 can be generalized to give the following result, known as the division \nalgorithm. \nThe Division Algorithm \nIf f and g are polynomials with deg g :s deg\nf, \nthen there are polynomials q and r \nsuch that \nf(x) \n= \ng(x)q(x) +  r(x) \nwhere either r \n= \n0 or deg r < deg g. \nIn Example D.4, \nf(x) \n= \n3x\n3 \n- x\n2 \n+ 2x +  1,  g(x) \n= \nx\n2 \n- 4x + 2, \nq(x) \n= \n3x +  11, \nand \nr(x) \n= \n40x - 21 \nIn the division algorithm, if the remainder is zero, then \nf(x) \n= \ng(x)q(x) \nand we say that g is a fa ctor off (Notice that q is also a factor off) There is a cl ose \nconnection between the factors of a polynomial and its zeros. A zero of a polynomial \nf is a number a such that\nf\n(\na\n) \n=","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":117679,"to":117754}}}}],[1723,{"pageContent":"connection between the factors of a polynomial and its zeros. A zero of a polynomial \nf is a number a such that\nf\n(\na\n) \n= \n0. [The number a is also called a root of the polyno­\nmial equation f(x) \n= \nO.] The following result, known as the Factor Theorem, estab­\nlishes the connection between factors of a polynomial and  its zeros. \nThe Factor Theorem \nLet f be a polynomial and let a be a constant.   Then a is a zero off if and only if \nx - a is a factor of f(x). \nProof By the division algorithm, \nf(x) \n= \n(x - a)q(x) + r(x) \nwhere either r(x) \n= \n0 or deg r < deg(x - a) \n= \n1. Thus, in either case, r(x) \n= \nr is a \nconstant. Now, \nj(a) \n= \n(a  - a)q(a) + r \n= \nr","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":117754,"to":117785}}}}],[1724,{"pageContent":"Theorem D.3 \nAppendixD \nso f(a\n) \n= 0   if and only if r = 0,   which is equivalent to \nj(x) =  (x - a)q(x) \nas we needed to prove. \nPolynomials \n05 \nThere is no method that is guaranteed to find the zeros of a given polynomial. \nHowever, there are some guidelines that are useful in special cases. The case of a poly­\nnomial  with inte\ng\ner coefficients is particularly interesting. The  following result, \nknown as the Rational Roots Theorem, gives criteria for a zero of such a polynomial \nto be a rational number. \nThe Rational Roots Theorem \nLet \nj(x) = \nao + a\n1\nX + \n· · · \n+ a\nn\nX\nn \nbe a   polynomial with integer coefficients and let alb be a   rational number writ­\nten in lowest terms. If alb is a zero of\nf, \nthen a\n0 \nis a multiple of a and an is a multiple \nof b. \nProof If a/ b is a zero of\nf, \nthen \n(\na\n) \n(\na\n)n\n-\n1 \n(\na\n)n \na \n+a \n-  +\n···\n+a_ \n-\n+a \n-\n=O \nO \nl\nb \nni\nb \nn\nb \nMultiplying through by b\nn\n, we  have \naob\nn \n+ a\n1\nab\nn\n-I \n+ \n· · · \n+ a\nn\n-I\na\nn\n-\nl\nb + a\nn\na = 0 \nwhich implies that \n(1) \n(2) \nThe","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":117787,"to":117876}}}}],[1725,{"pageContent":"Proof If a/ b is a zero of\nf, \nthen \n(\na\n) \n(\na\n)n\n-\n1 \n(\na\n)n \na \n+a \n-  +\n···\n+a_ \n-\n+a \n-\n=O \nO \nl\nb \nni\nb \nn\nb \nMultiplying through by b\nn\n, we  have \naob\nn \n+ a\n1\nab\nn\n-I \n+ \n· · · \n+ a\nn\n-I\na\nn\n-\nl\nb + a\nn\na = 0 \nwhich implies that \n(1) \n(2) \nThe \nleft-hand side of Equation (2) is   a multiple of b, so ana\nn \nmust be a multiple of b \nalso. Since a/b is in lowest terms, a and b have no common factors greater than  1. \nTherefore, an must be a multiple of b. \nWe can also write Equation (1) as \n� \nand a    similar argument shows that a\n0 \nmust be a multiple of a. (Show this.) \nExample D.5 \nFind all the rational roots of the equation \n6x\n3 \n+ 13x\n2 \n- 4 = 0 \n(3) \nSolution If a/bis a  root of this equation, then 6 is a multiple of b and -4 is a multiple \nof a\n, \nby the Rational Roots Theorem. Therefore, \naE{:±:: l, :±:: 2, :±:: 4}  and bE{:±:: l, :±:: 2, :±:: 3, :±:: 6}","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":117876,"to":117954}}}}],[1726,{"pageContent":"06 Appendix D \nPolynomials \nForming all possible rational numbers a/b with these choices of a and b, we see that \nthe only possible rational roots of the given equation are \nSubstituting these values into Equation (3) one at a time, we find that -2, \n-\nt\n, \nand ! \n� \nare the only values from this list that are actually roots. (Check these.) As we will see \nshortly, a polynomial equation of degree 3 cannot have more than three roots, so \nthese are not only all the r\na\ntion\na\nl roots of Equation (3) but also its onl\ny \nroots. \nWe can improve on the trial-and-error method of Example D.5 in various ways. \nFor example, once we find one root a of a given polynomial equation f(x) =  0, we \nknow that x - a is a factor ofj(x)-say, f(x) = (x - a)g(x). We can therefore divide \nf(x) by x - a (using long division) to findg(x).  Since degg < deg\nf, \nthe roots of g(x) = 0 \n[which are also roots of f(x) = OJ may be easier to find. In particular, if g(x) is a qua­\ndratic polynomial, we have access to the quadratic fo rmula.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":117956,"to":117982}}}}],[1727,{"pageContent":"f, \nthe roots of g(x) = 0 \n[which are also roots of f(x) = OJ may be easier to find. In particular, if g(x) is a qua­\ndratic polynomial, we have access to the quadratic fo rmula. \nSuppose \nax\n2 \n+bx+ c = 0 \n(We may assume that a is positive, since multiplying both sides by -1 would produce \nan equivalent equation otherwise.) Then, completing the square, we have \na  x\n2 \n+ -x + -  = - - c \n( b    b\n2\n) b\n2 \na \n4a\n2 \n4a \n� (Verify this.) Equivalently, \nTherefore, \nor \nb\n2 \n- 4ac \n4a\n2 \nb \n+\n�\n2 \n- 4ac \n= \n±\nV\nb\n2 \n- 4ac \nx+-= \n2a \n-\n4a\n2 \n2a \nx\n= \n-b ± \nV\nb\n2 \n- 4ac \n2a \nLet's revisit the equation from Example D.5 with the quadratic formula in mind. \nExample D.6 \nFind the roots of 6x\n3 \n+ 13x\n2 \n- 4 = 0. \nSolution Let's suppose we use the Rational Roots Theorem to discover that x =  -2 \nis a rational root of 6x\n3 \n+ 13x\n2 \n- 4 = 0. Then x + 2 is a factor of 6x\n3 \n+ 13x\n2 \n- 4,   and \nlong division gives \n6x\n3 \n+  13x\n2 \n- 4 = (x + 2)(6x\n2 \n+ x -2)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":117982,"to":118060}}}}],[1728,{"pageContent":"AppendixD \nPolynomials \nDJ \n� \n(Check this.) We can now apply the quadratic formula to the second factor to find \nthat its zeros are \nx = \n- 1 ± \nv\n1\n2 \n- 4(6)(-2) \n2·6 \n-1 ±  v'49 \n12 \n-1 ± 7 \n12 \nor, in lowest terms, \nt \nand -\nt\n. Thus, the three roots of Equation (3) are -2, \nt, \nand -\nt\n, \nas we determined in Example D.S. \nRemark The Factor Theorem establishes a connection between the zeros of a \npolynomial and its line\na\nr factors. However, a polynomial without linear factors may \nstill have factors of higher degree. Furthermore, when asked to factor a polynomial, \nwe need to know the number system to which the coefficients of the factors are sup­\nposed to belong. \nFor example, consider the polynomial \np(x) \n=  x\n4 \n+ 1 \nOver the r\na\ntion\na\nl numbers Q, the only possible zeros of p are 1 and  -1, by the Rational \nRoots Theorem. A  quick check shows that neither of these actually works, so \np(x\n) \nhas \nno line\na\nr factors with rational coefficients, by the Factor Theorem. However, \np(x\n) \nmay \nstill factor into a product of two \nq\nuad\nr","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":118062,"to":118119}}}}],[1729,{"pageContent":"p(x\n) \nhas \nno line\na\nr factors with rational coefficients, by the Factor Theorem. However, \np(x\n) \nmay \nstill factor into a product of two \nq\nuad\nr\natics. We will check for quadratic factors using \nthe method of undetermined coefficients. \nSuppose that \nx\n4 \n+ 1 = \n(x\n2 \n+  a\nx +  b\n)(\nx\n2 \n+  ex+ \nd) \n� \nExpanding the right-hand side and comparing coefficients, we obtain the equations \na\n+  c \n= 0 \nb  + \na\nc  +  d \n= 0 \nbe+  ad\n= 0 \nbd = 1 \nIf \na \n= 0, then c = 0 and d = -b. This gives - b1 = 1, which has no solutions in Q. \nHence, we may assume that \na \n=fa 0. Then c = \n-a, and we obtain d \n= b. It now follows \nthat b1 = 1, so b = 1 orb = -1. This implies that \na\n2 \n= 2 or \na\n2 \n= -2, respectively, \nneither of which has solutions in Q. It follows that x \n4 \n+ 1 cannot be factored over Q. \nWe say that it is irreducible over Q. \nHowever, over the re\nal   numbers IR, x\n4 \n+ 1 does factor. The calculations we have \njust done show that \nx\n4 \n+ 1 = \n(\nx1 +  v2x +  l\n)(\nx1 -  v2x + 1) \n�","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":118119,"to":118190}}}}],[1730,{"pageContent":"We say that it is irreducible over Q. \nHowever, over the re\nal   numbers IR, x\n4 \n+ 1 does factor. The calculations we have \njust done show that \nx\n4 \n+ 1 = \n(\nx1 +  v2x +  l\n)(\nx1 -  v2x + 1) \n� \n(Why?) To  see whether we can factor further, we  apply the quadratic formula. We see \nthat the first factor has zeros \nx= \n-vz \n± \nV\n(v2)\n2 \n- 4 \n2 \n-v2 ±  V=2   v2 \n-----\n= \n-(-1 ± i\n) \n= \n2 2 \n1 \n1 \n---+ \n--i \nv2\n-\nv2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":118190,"to":118227}}}}],[1731,{"pageContent":"DB \nFigure D.1 \nAppendix D \nPolynomials \nIm \nwhich are in IC but not in IR. Hence, x\n2 \n+ Vlx +  1   cannot be factored into linear fac­\ntors over IR. Similarly, x\n2 \n-  Vlx +  1   cannot be factored into linear factors over IR. \nOur calculations show that a complete factorization of x\n4 \n+ 1   is possible over the \ncomplex numbers IC. The four zeros of x\n4 \n+ 1   are \n1 1 \n()' \n= \n-\n--+ -i \nV2   \\12\n' \n1 1 \n()' \n= \n-\n--\n-\n--\ni \nV2   \\12\n' \n1 1 \n-a\n=  --\n- -i \nV2   V2 \n-\n1 1 \n-a=-+ \n--\ni \nV2   \\12\n' \nwhich, as Figure D.l shows, all lie on the unit circle in the complex plane. Thus, the \nfactorization of x\n4 \n+ 1   is \nx\n4 \n+  1 \n= \n(x -a)(\nx -\na\n)(x + \na\n)(x + a) \nThe preceding Remark illustrates several important properties of polynomials. \nNotice that the polynomial \np(x\n) \n= \nx\n4 \n+  1   satisfies deg \np \n= \n4 and has exactly four \nzeros in IC. Furthermore, its complex zeros occur in conjug\na\nte \npa\nirs\n; \nthat is, its com­\nplex zeros can be paired up as \n{\na\n, \na\n}  and {-a\n, -\na\n}","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":118229,"to":118315}}}}],[1732,{"pageContent":"p(x\n) \n= \nx\n4 \n+  1   satisfies deg \np \n= \n4 and has exactly four \nzeros in IC. Furthermore, its complex zeros occur in conjug\na\nte \npa\nirs\n; \nthat is, its com­\nplex zeros can be paired up as \n{\na\n, \na\n}  and {-a\n, -\na\n} \nThese last two facts are true in general. The first is an instance of the Fundamental \nTheorem of Algebra (FTA), a result that was first proved by Gauss in 1797. \nTheorem D.4 \nThe Fundamental Theorem of Algebra \nEvery polynomial of degree n with real or complex coefficients has exactly n zeros \n(counting multiplicities) in IC. \nThis important theorem is sometimes stated as \n\"Every polynomial with real or complex coefficients has a zero in IC.\" \nLet's call this statement PTA'. Certainly, PTA implies PTA'. Conversely,  if PTA' is \ntrue, then if we have a polynomial \np \nof degree n, it has a zero \na \nin IC. The Factor \nTheorem then tells us that x -\na \nis a factor of \np(x\n)\n, so \np(\nx\n) \n= \n(x - a)q(x) \nwhere q is a polynomial of degree n - 1   (also with real or complex coefficients). We","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":118315,"to":118365}}}}],[1733,{"pageContent":"a \nin IC. The Factor \nTheorem then tells us that x -\na \nis a factor of \np(x\n)\n, so \np(\nx\n) \n= \n(x - a)q(x) \nwhere q is a polynomial of degree n - 1   (also with real or complex coefficients). We \ncan now apply PTA' to q to get another zero, and so on, making PTA true. This argu­\n� \nment can be made into a nice induction proof. (Try it.) \nIt is not possible to give a formula (along the lines of the quadratic formula) for \nthe zeros of polynomials of degree 5 or more. (The work of Abel and Galois confirmed \nthis; see page 311.) Consequently, other methods must be used to prove PTA. The \nproof that Gauss gave uses topological methods and can be found in more advanced \nmathematics courses. \nNow suppose that","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":118365,"to":118387}}}}],[1734,{"pageContent":"Descartes' stated this rule in his \n1637 book La Geometrie, but did \nnot give a proof. Several mathema­\nticians later furnished a proof, and \nGauss provided a somewhat \nsharper version of the theorem in \n1828. \nTheorem 0.5 \nExample 0.1 \nAppendixD \nPolynomials \n09 \nis a polynomial with real coefficients. Let a be a complex zero of \np \nso that \nThen, using properties of conjugates, we have \n= \np(a) \n= \n0 \n= \n0 \nThus, a is also a  zero of \np. This proves the following result: \nThe complex zeros of a polynomial with real coefficients occur in conjugate pairs. \nIn some situations, we do not need to know wh\na\nt the zeros of a polynomial are­\nwe only need to know where they are located. For example, we might only need to \nknow whether the zeros are positive or negative (as in Theorem 4.35). One theorem \nthat is useful in this regard is Descartes' Rule of  Signs. It allows us to make certain \npredictions about the number of positive zeros of a polynomial with real coefficients \nbased on the signs of these coefficients.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":118389,"to":118421}}}}],[1735,{"pageContent":"predictions about the number of positive zeros of a polynomial with real coefficients \nbased on the signs of these coefficients. \nGiven a polynomial \na\n0 \n+ a1x + ·  ·  · + a\nn\nx\nn\n, write its nonzero coefficients in order. \nReplace each positive coefficient by a plus sign and each negative coefficient by a \nminus sign. We will say that the  polynomial has k sign changes if there are k places \nwhere the coefficients change sign. For example, the polynomial 2  -3x + 4x\n3 \n+ x\n4 \n- 7x\n5 \nhas the sign pattern \n+-++-\n._,_,._,_, ._,_, \nso it has three sign changes, as indicated. \nDescartes' Rule of Signs \nLet \np \nbe a polynomial with real coefficients that has k sign changes. Then the \nnumber of positive zeros of \np \n(counting multiplicities) is at most k. \nIn words, Descartes' Rule of Signs says that a  real polynomial cannot have more \npositive zeros than it has sign changes. \nShow that the polynomial \np(\nx\n) \n= \n4 + 2x\n2 \n- 7x\n4 \nhas exactly one positive zero. \nSolution The coefficients of \np","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":118421,"to":118463}}}}],[1736,{"pageContent":"positive zeros than it has sign changes. \nShow that the polynomial \np(\nx\n) \n= \n4 + 2x\n2 \n- 7x\n4 \nhas exactly one positive zero. \nSolution The coefficients of \np \nhave the sign pattern +  + -,  which has only one sign \nchange. So, by Descartes'  Rule  of Signs, \np \nhas at most one positive  zero. But \np(O) \n= \n4 and \np(l) \n= \n-1, so there is a zero somewhere in the interval \n(O\n, 1). Hence, \nthis is the  only positive zero of \np.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":118463,"to":118489}}}}],[1737,{"pageContent":"010 AppendixD \nPolynomials \nExample D.8 \nWe can also use Descartes' Rule of Signs to give a bound on the number of nega ­\ntive zeros of a polynomial with real coefficients. Let \np(x) \n= \na\n0 \n+ a\n1\nx + a\n2\nx\n2 \n+ · · · + a\nn\nx\n\" \nand let b be a negative zero of p. Then b \n= \n- c for c > 0, and we have \n0 \n= \np(b) \n= \na\n0 \n+ a\n1\nb + a\n2\nb\n2 \n+· · · + a\nn\nb\n\" \nBut \nso c is a positive zero of p ( -  x). Therefore, p (x) has exactly as many negative zeros as \np(-x) has  positive zeros. Combined with Descartes' Rule of Signs, this observation \nyields the following: \nLet p be a polynomial with real coefficients. Then the number of negative zeros of \npis at  most the number of sign changes of p(-x). \nShow that the zeros of p(x) \n= \n1 +  3x + 2x\n2 \n+ x\n5 \ncannot all be real. \nSolulion The coefficients of p(x) have no sign changes, so  p has no positive zeros. \nSince p( -x) \n= \n1 -  3x + 2x\n2 \n- x\n5 \nhas three sign changes among its coefficients, p has \nat most three negative zeros. We note that 0  is not a zero of p either, sop has at most","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":118491,"to":118550}}}}],[1738,{"pageContent":"Since p( -x) \n= \n1 -  3x + 2x\n2 \n- x\n5 \nhas three sign changes among its coefficients, p has \nat most three negative zeros. We note that 0  is not a zero of p either, sop has at most \nthree real zeros. Therefore, it has at least two complex (nonreal) zeros.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":118550,"to":118558}}}}],[1739,{"pageContent":"Chapter 1 \nExercises 1.1 \n1. \n3. (a), (b) \n(\nc) \nx \nx \ny \nz \n3 \n-3 \nz \n3 \ny \nAnswers to Selected \nOdd- Num bered  Exercises \n3 \nAnswers are easy. It's asking \nthe right questions [that's J hard. \n-Doctor Who \n\"The Face of Evil;' \nBy Chris Boucher \nBBC, 1977 \n(d) \n3 \nx \nz \n3 \n5. (a) \ny \n3 \n� \n2 \n2 \n3 \n-1 \n(b) \ny \ny \n2 \n1 \n/ \n2 \n-1 \n-2 \ny \nx \n4 \nx \n3 \nANS1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":118560,"to":118611}}}}],[1740,{"pageContent":"ANS2 \nAnswers to Selected Odd-Numbered Exercises \n(c) \ny \n2 \" \n--+----+----+-----'1----t--1r--.--- X \n(d) \n-2 \nI \n6 \nl \n2 \nI \n3 \ny \n7. a + b = [\n5, 3\n] \ny \n4 \n2  4 \n9. d -c = [5, -\n5\n] \ny \n11. [3, -2, 3\n] \nI \n6 \n6 \n2 \nI \n3 \n[ \n1/2\n] \n[\n-\nv'3\n/2\n] \n1\n3\n.u= \nv'3\n/2 \n, v= \n_\n1\n1\n2 \n, u+v= \n[\n(1 - V3)/2\n] \nu - v = \n[\n(1 + V3)/2\n] \n(V3 - 1)/2 \n, \n(1 + V3)/2 \n15.\n5\na \n17. x = 3a \n19. \ny \n21.w=-2u +\n4\nv \ny \n25. u + v = \n[\n�\n] \n27.u+v\n= \n[\n0, 1, 0,0] \n29. + \n0 \n2 \n3 \n0  0 \n2 \n3 \n2  3 \n0 \n2  2 3 \n0  1 \n3 \n3 \n0 \n2 \n3\n1. 0 \n3\n5. 0 \n3\n9.\n5 \n43. [O, 0, 2, \n2\n] , [2, \n3, 1, 1 ] \n47. No solution \n51. No solution \n55. x = 1, or x = \n5 \n57. (a) All a * 0 \n0 \n2 \n3 \n0  0  0  0  0 \n0 \n2 \n3 \n2 \n0 \n2 \n0 \n2 \n3 \n0 \n3 \n2 \n1 \n33\n. 1 \n37. 2, 0,3 \n41. [\n1, 1, OJ \n45. x = 2 \n49. x = 3 \n5\n3\n. x = 2 \n(b)a=\nl ,  5 \n(c)   a and m can have no common factors other than 1 \n[i.e., the greatest common divisor (gcd) of a and \nmis l]. \nExercises 1.2 \n1. -1 \n3\n. 11 \n[\n-1/\nVs\n] \n7. \nVs\n, \n2/\nVs \n5. 2 \n[ 1/Vi4\n] \n9. Vi4, 2/Vi4 \n3/Vi4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":118613,"to":118773}}}}],[1741,{"pageContent":"11\n. \n\\16, [ \n1 \nI \n\\16, 1 \nI \n\\13\n, \n1 \nI \n\\/2\n, o \nl \n1\n3\n. \nvu \n15\n. \nV6 \n17\n. \n(a) u · vis a  scalar, not a   vector. \n(c) v \n· \nw is a scalar and u is a vector. \n1\n9\n. \nAcute \n2\n1\n. \nAcute \n2\n3\n. \nAcute \n2\n5\n. \n60° \n2\n7\n. \n=88.10° \n2\n9\n. \n=1\n4\n.\n3\n4\n° \n3\n1\n. \ns\n.\nince AB· \nAC\n= \n[\n-\n�\ni \n· \n[ \n�\n1 = 0\n, \nLBAC is a \nnght angle. \n-\n1 \n-\n3 \n33\n. \nIf we take the cube to be a unit cube (as in Figure 1.\n3\n4\n)\n, \nthe four diagonals are given by the vectors \nSince d\n; \n· \nd\nj \nof. 0 for all i of. j (six possibilities), no two \ndiagonals are perpendicular. \n3\n5\n. \nD  = \n(\n-\n2\n, \n1\n, \n1) \n3\n7\n. \n5 mi/h at an angle of = 5\n3\n.1\n3\n° to the bank \n3\n9\n. \n60° \n4\n1\n. \n[ \n-u \n43\n.\nu: \n4\n7\n. \nA\n= \n\\/45\n/2 \n4\n9\n. \nk = \n-\n2\n, 3 \n[\n-\n0.\n3\n01\n] \n4\n5\n. \n0.0\n33 \n-\n0.252 \n51\n. \nv is of the form k \n[ _ \n� l where k is a scalar. \n5\n3\n. \nThe Cauchy-Schwarz Inequality would be violated. \nExercises 1.3 \n1\n. \n(a) \n[\n�\n] \n· \n[;] \n= 0 (b) 3x \n+ \n2y \n= 0 \n3\n. \n(a) \n[;] \n[\n�\n] \n+ \nt\n[\n-\n�\n] \n(b) x = 1 -  t \ny = \n3\nt \n5\n. \n(a) \nx = \nt \n(b)\ny\n=-t \nz = \n4\nt \n7\n. \n(a) \nm \n· \n[\n�\n] \n� \n2 (h) 3x \n+ \n2\ny \n+ \nF \n2 \nAnswers to Selected Odd-Numbered Exercises ANS3 \n(b) x = 2s \n-\n3\nt \ny  = \ns \n+ \n2t \nz = 2s \n+ \nt \n11\n. \n[;] \n= \n[ \n-\n�\n] \n+ \nt \n[ \n�\n] \nl\n3\n. \n[ \n� \n1 \n�","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":118775,"to":119024}}}}],[1742,{"pageContent":"y = \n3\nt \n5\n. \n(a) \nx = \nt \n(b)\ny\n=-t \nz = \n4\nt \n7\n. \n(a) \nm \n· \n[\n�\n] \n� \n2 (h) 3x \n+ \n2\ny \n+ \nF \n2 \nAnswers to Selected Odd-Numbered Exercises ANS3 \n(b) x = 2s \n-\n3\nt \ny  = \ns \n+ \n2t \nz = 2s \n+ \nt \n11\n. \n[;] \n= \n[ \n-\n�\n] \n+ \nt \n[ \n�\n] \nl\n3\n. \n[ \n� \n1 \n� \nrn \n+ \n{ \n-\n: \n1 \n+ \n{ \n� \n� \n1 \n15\n. \n(a) x = \n[;] [ _\n�\n] \n+ \nt\n[\n�\n] \ny \n= \n-\n1 \n+ \n3\nt \n17\n. \nDirection vectors for the two lines are given by \nd\n1 \n= \n[ \n�J and dz = \n[ \n�J. The lines are perpendicular \nif and only if d\n1 \nand d\nz \nare orthogonal. But d\n1 \n·d\nz \n= 0 \nif and only if 1 \n+ \nm\n1\nm\nz \n= \n0 or, equivalently, \nm\n1\nm\nz \n= \n-\n1. \n1\n9\n. \n(a) Perpendicular \n(c) Perpendicular \n2\n1\n. \n[;] \n= \n[ \n_\n�\n] \n+ \nt\n[�] \n2\n3\n. \n[ \n� \nl \n� \n[\n-\n� l \n+ \n{ \n� � l \n(b) Parallel \n(d) Perpendicular \n2\n5\n. \n(\na) x = O,x  = 1\n,y \n= O,y = 1,z  = \nO,z = 1 \n(b) x \n-\ny \n= 0 \n(c) x \n+ \ny \n-\nz = 0 \n2\n7\n. \n3\n\\/2/2 \n2\n9\n. \n2\\13/\n3 \n3\n1\n. \n(\nt, t\n) \n33\n. \n(�, �, �) \n3\n5\n. \n18VD/1\n3 \n3\n7\n. \n� \n43\n. \n=\n7\n8.9° \n4\n5\n. \n=80.\n4\n° \nExercises 1.4 \n1\n. \n1\n3 \nNat approx N 6\n7\n.\n3\n8 E \n3\n. \n3\\13 N at an angle of \n3\n0° to f\n1 \n5\n. 4 \nN at an angle of 60° to f\nz \n7\n. \n5 Nat an angle of 60° to the given force, 5\\13 N \nperpendicular to the 5 N force \n9\n. \n7\n50\\/2 N \n11\n. \n980 N \n1\n3\n. \n= 11\n7","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":119024,"to":119272}}}}],[1743,{"pageContent":"7\n.\n3\n8 E \n3\n. \n3\\13 N at an angle of \n3\n0° to f\n1 \n5\n. 4 \nN at an angle of 60° to f\nz \n7\n. \n5 Nat an angle of 60° to the given force, 5\\13 N \nperpendicular to the 5 N force \n9\n. \n7\n50\\/2 N \n11\n. \n980 N \n1\n3\n. \n= 11\n7\n.6 Nin the 15 cm wire, = 88.2 Nin the 20 cm \nwire","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":119272,"to":119303}}}}],[1744,{"pageContent":"ANS4 \nAnswers to Selected Odd-Numbered Exercises \nReview \nQ\nuestions \n1. (a) T \n(\nc) \nF \n(e) T \n(\ng) \nF \n(i)  T \n3. x= \n[\n1\n8\n1\n] \n[\n-2M\nl \n5. 120° \n7. \nl/\nVs \n0 \n11. \nV6/\n2 \n9. 2x + 3\ny -  z = 7 \n13. The Cauchy-Schwarz Inequality would be violated. \n15. 2\nV6/\n3 \n17. x = 2 19. 3 \nChapter 2 \nExercises 2.1 \n1. Linear 3. Not linear because of the x\n-\n1 \nterm \n5. Not linear \n7. 2x + 4y = 7 \n9. \nx + y =   4(x, \ny i=   O) \n15. Unique solution, x = 3, y = -3 \ny \n17. No solution \ny \n19. [7, 3] \n23. [\n5, -2, 1, l] \n27. \n[\n� \n-\n�\nI \n�\nJ \n21. \n[�\n, \nt\n, -t \nl \n25. [\n2, -7, - 32] \n29. \n[\n-\n� \n� \n=\n�\n] \n2  4 \n4 \n31. \ny \n+ z =  1 \n3\n3. \n[\nl, \nl] \nx-y \n=l \n2x - y + z =  1 \n35. [ 4,  -1] \n37. No solution \n39. \n(a) 2x + y = 3 \n(b) x = \n� \n-}s \n4x + 2y = 6 \ny = s \n1 1 \nI \nJ \n41. \nLet u = - and v =-.The solution is x = 3,y = -2. \nx y \n43. Let u = tan x, v = sin   y, w = cos z. One solution is \nx = n/4, y =   -n/6, z = n/3. (There are infinitely \nmany solutions.) \nExercises 2.2 \n1. No \n3. Reduced row echelon form \n5. No \n7. No \n[\n: \n1 \n:\nJ \n11. (b) \n[\n: \n�\n] \n9. (a) \n1 \n0 \n[\ni \n0 \n-1 \ni \n13. (b) \n1 \n-1 \n0    0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":119305,"to":119442}}}}],[1745,{"pageContent":"x = n/4, y =   -n/6, z = n/3. (There are infinitely \nmany solutions.) \nExercises 2.2 \n1. No \n3. Reduced row echelon form \n5. No \n7. No \n[\n: \n1 \n:\nJ \n11. (b) \n[\n: \n�\n] \n9. (a) \n1 \n0 \n[\ni \n0 \n-1 \ni \n13. (b) \n1 \n-1 \n0    0 \n15. Perform elementary row opera  tions in the order \nR\n4 \n+ 29R\n3\n, 8R\n3\n, R\n4 \n- 3R\n2\n, R\n2 \n� \nR\n3\n, R\n4 \n- R\nP \nR\n3 \n+ 2Rp \nand, finally, R\n2 \n+ 2R\n1\n• \n17. One possibility is to  perform elementary row \noperations on A in the order R\n2 \n- 3Rp } R\n2\n, R\n1 \n+ 2R\n2\n, \nR\n2 \n+ 3Rp R\nI\n� \nR\n2\n. \n19. Hint: Pick a   random 2 X 2 matrix and try this­\ncarefully! \n21. This is really two elementary row opera  tions \ncombined: 3R\n2 \nand R\n2 \n-2R\n1\n• \n23. Exercise 1: 3\n; \nExercise 3: 2; Exercise 5: 2;   Exercise 7: 3 \n25. \n[\nr \n27.\n{\nJ \n29. \n[ \n_\n�\n] \n24 \n6 \n0 \n12 \n- 10 \n-2 \n6 \n-6 \n31. \n0 +  r 1 \n+s \n0 \n+  t \n0 \n0 0 \n1 \n0 \n0 0 0 \n33. No solution \n35. Unique solution","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":119442,"to":119560}}}}],[1746,{"pageContent":"37. Infinitely many solutions \n39. Hint: Show that if ad - be * 0, the rank of \n[\n: �] \nis 2. (There are two cases: a = 0 and a *   0.) Use \nthe Rank Theorem to deduce that the given system \nmust have a unique solution. \n41. (a) No solution if k =   -1 \n(b) A unique solution if k *  ± 1 \n(c) Infinitely many solutions if k = 1 \n43. (a) No solution if k = 1 \n(b) A unique solution if k *   -2, 1 \n( c) Infinitely many solutions if k =  -2 \n49. No intersection \n51. Thnequfred mtm x \n� \n[\n::\nJ \nace the rn lutionr nf \nthe homogeneous system with augmented matrix \n[\nu\n, U\n2 \nU\n3 \nI \n0\n] \nV\n1 \nV\n2 \nV\n3 \n0 \nBy Theorem 3, there are infinitely many solutions. If \nu\n1 \n* 0   and u\n1 \nv\n2 \n-  u\n2 \nv\n1 \n*-0, the  solutions are given by \nt\n[\n�::: \n= \n�:::\n] \nU\n1\nV\n2 \n-\nU\nz\nV\n1 \nBut a direct check shows that these are still solutions \neven if u\n1 \n= 0   and/or u\n1 \nv\n2 \n-  u\n2\nv\n1 \n= 0. \n53. \n[\n�\n] \nExercises 2.3 \n1. Yes \n55. \n[\n:\nJ \n3. No \n5. Yes \n57. \n[\n�\n] \n7. Yes \n9. We need to show that the  vector equation x\n[ \n�\n] \n+ \ny \n[ \n_ \n�\n] \n= \n[ \n�\n] \nhas a    solution for all values of a and b.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":119562,"to":119669}}}}],[1747,{"pageContent":"1 \nv\n2 \n-  u\n2\nv\n1 \n= 0. \n53. \n[\n�\n] \nExercises 2.3 \n1. Yes \n55. \n[\n:\nJ \n3. No \n5. Yes \n57. \n[\n�\n] \n7. Yes \n9. We need to show that the  vector equation x\n[ \n�\n] \n+ \ny \n[ \n_ \n�\n] \n= \n[ \n�\n] \nhas a    solution for all values of a and b. \nThis vector equation is equivalent to the linear system \nwhose augmented matrix is \n[ \n1    1 \nI \na\n]\n. \nRow \n1   -1  b \nreduction yields \n[ \n1    1 \nI \na \n] \n, from which we can \n0  -2  b - a \nsee that there is a (unique) solution. \nAnswers to Selected Odd-Numbered Exercises ANS5 \n[Further row operations yield x = (a + b)/2, \ny =(a -  b)/2.] Hence, \n�\n2 \n=\nspan\n(\n[\n�\n]\n, \n[ \n_\n�\n]\n)\n. \n11. We need to 'how th'1 the ve<tm equation x \n[ \n�\n] \n+ \n{\n] \n+ z \n[\n:\nJ \n� \nrn h� a r\nnlut ion focall valu\" \nof a, b, and c. This vector equation is equivalent to \nthe linear system whose augmented matrix is \n[ \n� \n� \nO \nn Row cedu<tion �dd' \n[\ni \n1 \n� \n� \n] \n, from which we can see \n0  2 b+c-a \nthat there is a (unique) solution. [Further row \noperations yield x =  (a - b + c)/2, \ny =(a+ b -  c)/\n2, z= (-a +  b + c)/2.] \nHence,\n�\n'\n� \n'pan\n( \n[ \nH \n[J \n[\n:\n]\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":119669,"to":119791}}}}],[1748,{"pageContent":"[\ni \n1 \n� \n� \n] \n, from which we can see \n0  2 b+c-a \nthat there is a (unique) solution. [Further row \noperations yield x =  (a - b + c)/2, \ny =(a+ b -  c)/\n2, z= (-a +  b + c)/2.] \nHence,\n�\n'\n� \n'pan\n( \n[ \nH \n[J \n[\n:\n]\n) \n13. (a) The line through the origin with direction \nvector \n[\n-\n�\n] \n(b) The line with general equation 2x + y = 0 \n15. (a) The plane through the origin with direction \nvedu\" [H U\n] \n(b) The plane with general equation 2x - y + 4z = 0 \n17. Substitution yields the linear system \na    + 3c = 0 \n-a +  b -3c = 0 \nwho\" 'olution i' {-� l It folluw' that the;e are \ninfinitely many solutions, the simplest perhaps being \na= -3, b =  0, c =  1. \n19. u = u + O(u + v) + O(u + v + w\n) \nv \n= (-l)u + (u + v) + O(u + v + \nw\n) \nw \n= \nOu \n+ \n(-l)(u \n+ \nv\n) \n+ \n(u \n+ \nv \n+ \nw\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":119791,"to":119853}}}}],[1749,{"pageContent":"ANS& \nAnswers to Selected Odd-Numbered Exercises \n21. (c) We must show that span(ep e\n2\n, e\n3\n) =  span(ep e\n1 \n+ \ne\n2\n, e\n1 \n+ e\n2 \n+ e\n3\n). We know that span(ep e\n1 \n+ e\n2\n, \ne\n1 \n+ e\n2 \n+ e\n3\n) � IR\n3 \n= span(ep e\n2\n, e\n3\n). From \nExercise 19, e\nl' \ne\n2\n, and e\n3 \nall belong to \nspan(ep e\n1 \n+ e\n2\n' \ne\n1 \n+ e\n2 \n+ e\n3\n). Therefore, by \nExercise 2l(b), span(ep e\n2\n, e\n3\n) =  span( e\nl' \ne\n1 \n+ \ne\n2\n, e\n1 \n+ e\n2 \n+ e\n3\n). \n23. Linearly independent \n25. Lmeedy dependent\n. \n-\nm \n+ \nm  m \n27. Linearly dependent, since the set contains the zero vector \n29. Linearly independent \n31. Lmeedy dependent\n. \n[ \nj\n] \n+ \nu \nl \n+ \n[ \n= \ni\nl \n� \nrn \n43. (a) Yes \n(b) No \nExercises 2.4 \nI\n.x\n1\n= 160,x\n2\n= 120,x\n3 \n= 160 \n3. two small, three medium, four large \n5. 65 bags of house blend, 30 bags of special blend, \n45 bags of gourmet blend \n7. 4FeS\n2 \n+  110\n2 \n-----+ 2Fe\n2\n0\n3 \n+ 850\n2 \n9. 2C\n4\nH\n10 \n+  130\n2 \n-----+ 8C0\n2 \n+  lOH\n2\n0 \n11. 2CsH\n11\n0H +  150\n2 \n-----+ 12H\n2\n0 +   10C0\n2 \n13. Na\n2\nC0\n3 \n+ 4C  + N\n2 \n-----+ 2NaCN + 3CO \n15. (a) \n1\n1 \n= \n30 - t (b) 1\n1 \n= \n15, 1\n3 \n=  15 \nf\n2\n=\n-\n10 +t \nf\n3 \n= \nt \n(c) \n0 -:s 1\n1 \n-:s 20 \n0 -::; 1\n2 \n-::; 20 \n10 -::; l \n3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":119855,"to":120028}}}}],[1750,{"pageContent":"2\n0 \n11. 2CsH\n11\n0H +  150\n2 \n-----+ 12H\n2\n0 +   10C0\n2 \n13. Na\n2\nC0\n3 \n+ 4C  + N\n2 \n-----+ 2NaCN + 3CO \n15. (a) \n1\n1 \n= \n30 - t (b) 1\n1 \n= \n15, 1\n3 \n=  15 \nf\n2\n=\n-\n10 +t \nf\n3 \n= \nt \n(c) \n0 -:s 1\n1 \n-:s 20 \n0 -::; 1\n2 \n-::; 20 \n10 -::; l \n3 \n-::; 30 \n(d) Negative flow would mean that water was flowing \nbackward, against the direction of the arrow. \n17. (a)  1\n1 \n=  -200 + s +  t (b) 200-:s1\n3 \n-:s 300 \n1\n2 \n= \n300 -  s \n-\nt \n1\n3 \n= \ns \n1\n4 \n=   150 - t \nls = \n(c) Ifl\n3 \n= s = 0,   thenls =   t2 200 (from theJ; equa­\ntion), but ls = t -:s 150 (from the 1\n4 \nequation). This is \na contradiction. \n( d) 50 -::; 1\n3 \n-::; 300 \n19. I\n1 \n= 3   amps, I\n2 \n= 5   amps, I\n3 \n= 2   amps \n21. (a) I= 10 amps, I\n1 \n= I\ns \n= 6   amps, I\n2 \n= I\n4 \n= 4   amps, \nI\n3 \n= 2   amps \n(b) Reff = �   ohms \n(c) Yes; change it to 4 ohms. \n23. Farming : Manufacturing = 2 : 3 \n25. The painter charges $39/hr, the plumber $42/hr, the \nelectrician $54/hr. \n27. (a) Coal should produce $100 million and steel $160 \nmillion. \n(b) Coal should reduce production by \"' $4.2 million and","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":120028,"to":120131}}}}],[1751,{"pageContent":"25. The painter charges $39/hr, the plumber $42/hr, the \nelectrician $54/hr. \n27. (a) Coal should produce $100 million and steel $160 \nmillion. \n(b) Coal should reduce production by \"' $4.2 million and \nsteel should increase production by \"' $5.7 million. \n29. (a) Yes; push switches 1, 2, and 3 or switches 3, 4, and 5. \n(b) No \n31. The states that can be obtained are represented by \nthose vectors \nX\n1 \nX\n2 \nX\n3 \nX\n4 \nXs \nin .:£'.� for which x\n1 \n+ x\n2 \n+ x\n4 \n+ Xs = 0. \n(There are 16 such possibilities.) \n33. If 0 =  off, 1 =  light blue, and 2 =  dark blue, then the \nlinear system that arises has augmented matrix \n1  1  0  0  0 \n2 \n1  1   1 \n0  0 \n1 \n0  1   1 0 \n2 \n0  0 \n1 \n0  0  0 \n2 \nwhich reduces over .l'.\n3 \nto \n1  0  0 \n0 \n0  1  0  0 \n2 \n0  0  1  0  0 \n2 \n0  0  0 \n2 \n0  0  0  0  0 \n0 \nThis yields the solutions \nX\n1 \n2 \nX\n2 \n1 \nX\n3 \n2 \n+ t \n0 \nX\n4 \n2 2 \nXs \n0 1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":120131,"to":120200}}}}],[1752,{"pageContent":"where tis in 2\n3\n. Hence, there are exactly three solutions: \n0 \n2 \n1 \n2 \n0 \n2  , 2  , 2 \n2 \n0 \n0 \n2 \nwhere each entry indicates the number of times the \ncorresponding switch should be pushed. \n35. (a)  Push squares 3 and 7. \n(b) The 9 X 9 coefficient matrix A is row equivalent to \n2\n2\n, so for any b in 2�, Ax = b has a unique \nsolution. \nExercises 2.5 \n1. \nn \n0 \nI \n2 \n3 \n4 \n5 \nx\n, \n0 \n0.8\n5\n71 \n0.971\n4 \n0.99\n5\n9 \n0.9991 \n0.9998 \nX\nz \n0 \n0.8000 \n0.971\n4 \n0.99\n4\n3 \n0.9992 \n0.9998 \nExact solution: x\n1 \n= 1, x\n2 \n= \n1 \n5. \nn \n0 \nI \n2 \n3 \n4 5 \nx\n, \n0 \n0.3333 \n0.2\n5\n00 \n0.30\n55 \n0.2916 \n0.3009 \nX\nz \n0 0.2\n5\n00 \n0.083\n4 \n0.12\n5\n0 \n0.0972 \n0.10\n4\n2 \nX\n3 \n0 \n0.3333 \n0.2\n5\n00 \n0.30\n55 \n0.2916 \n0.3009 \nExact solution: x\n1 \n= 0.3, x\n2 \n= 0.1, x\n3 \n= 0.3 \n7. n \n0 \nI \n2 \n3 \n4 \n0 \n0 \n0.8\n5\n71 \n0.971\n4 \n0.99\n5\n9 \n0.9992 \n0.9998 \n1.0000 \n1.0000 \n1.0000 \nAfter three iterations, the Gauss-Seidel method is \nwithin 0.001 of the exact solution. Jacobi's method \ntook four iterations to reach the same accuracy. \nAnswers to Selected Odd-Numbered Exercises \nANS1 \n37. Grace is 1\n5\n, and Hans is \n5\n. \n39. 1200 and 600 square yards \n41. (a) a= \n4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":120202,"to":120345}}}}],[1753,{"pageContent":"took four iterations to reach the same accuracy. \nAnswers to Selected Odd-Numbered Exercises \nANS1 \n37. Grace is 1\n5\n, and Hans is \n5\n. \n39. 1200 and 600 square yards \n41. (a) a= \n4 \n-d, b = \n5 \n-d, c = -2 + d, dis arbitrary \n(b) No solution \n43. (a) No solution \n(b) [a, b, c, d, e,f] = \n[\n4\n,\n5\n,6, -3, -1,0] + \nj[-1, -1, -1, 1, 1, l] \n45. (a) y \n= x\n2 \n-2x + 1 \n(b) y \n= x\n2 \n+ 6x + 10 \n47.A=l,B=2 \n49. A  =  -LB\n= \nt\n, C = 0, D = \n-f5, \nE = -\nt \n51. a = \nt, \nb = \nt, \nc = \no \n3. \nn \n0 \nI \n2 \n3 \n4 \n5 \nx\n, \n0 \n0.2222 \n0.2\n5\n39 \n0.2610 \n0.2620 \n0.2622 \nX\nz \n0 \n0.28\n5\n7 0.3\n4\n92 \n0.3\n5\n82 0.3603 0.3606 \n6 \n0.2623 \n0.3606 \nExact solution (to four decimal places): x\n1 \n= 0.2623, \nX\n2 \n= 0.3606 \n6 \n0.2986 \n0.0996 \n0.2986 \n9. n \n0 \n0 \n0 \n7 \n8 \n0.3001 \n0.2997 \n0.1008 0.1000 \n0.3001 \n0.2997 \nI \n2 \n0.2222 \n0.3\n4\n92 \n0.2610 \n0.3603 \n3 \n0.2622 \n0.3606 \n4 \n0.2623 \n0.3606 \nAfter three iterations, the Gauss-Seidel method is \nwithin 0.001 of the exact solution. Jacobi's method \ntook four iterations to reach the same accuracy.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":120345,"to":120459}}}}],[1754,{"pageContent":"ANSB Answers to Selected Odd-Numbered Exercises \n11. \nn \n0 \n1 \n2 \n3 \n4 \nX\n1 \n0 \n0.3333 \n0.2777 \n0.2962 \n0.2993 \nX\nz \n0 \n0.1667 \n0.1112 \n0.1020 \n0.1004 \nX\n3 \n0 \n0.2777 0.2962 0.2993 \n0.2998 \nAfter four iterations, the Gauss-Seidel method is within \n5 \n6 \n1\n3\n. \n0.2998 \n0.3000 \n0.1000 0.1000 \n0.3000 0.3000 \n0.5 \n0.001 of the exact solution. Jacobi's method took seven \niterations to reach the same accuracy. \n�t--r-+-+-+-+-...--+--+-+oot-t--+--.Xj \n0.5 \n1 \nI \n15. n \n0 \n1 \n2 \n3 \n4 \n17. \n0 \n0 \n3 \n-4 \n-5 \n8 \n19 \n-28 \n-53 \n80 \nIf the equations are interchanged and the Gauss-Seidel \nmethod is applied to the equivalent system \n3x\n1 \n+ 2x\n2 \n= 1 \nX\n1 \n- 2x\n2 \n= 3 \nwe obtain \nn \n0 \n1 \n2 \n3 \nX\n1 \n0 \n0.3333 \n1.2222 \n0.9260 \n4 \n5 \n1.0247 \n0.9918 \nX\nz \n0 \n-1.3333 \n-0.8889 \n-1.0370 \n-0.9876 \n-1.0041 \nAfter seven iterations, the process has converged to \nwithin 0.001 of the exact solution x\n1 \n= 1, x\n2 \n= -1. \nX\n2 \n19. \nn \n0 1 \n2 \n3 \n-\nX\n1 \n0    -1.6 \n14.97 8.550 \n� \nX\nz \n0 25.9 \n11.408 \n14.051 \nX] \nX\n3 \n0 \n-10.35 \n-9.311 \n-11.200 \n10 \n20 \nn \n7 \n8 \n9 \n-30 \nX\n1 \n9.989 \n10.022 \n10.002 \nX\nz \n11.187 \n11.082 \n11.052 \nX\n3 \n-11.912 -11.948 \n-11.973 \n6 \n7","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":120461,"to":120611}}}}],[1755,{"pageContent":"X\n2 \n19. \nn \n0 1 \n2 \n3 \n-\nX\n1 \n0    -1.6 \n14.97 8.550 \n� \nX\nz \n0 25.9 \n11.408 \n14.051 \nX] \nX\n3 \n0 \n-10.35 \n-9.311 \n-11.200 \n10 \n20 \nn \n7 \n8 \n9 \n-30 \nX\n1 \n9.989 \n10.022 \n10.002 \nX\nz \n11.187 \n11.082 \n11.052 \nX\n3 \n-11.912 -11.948 \n-11.973 \n6 \n7 \n1.0027 \n0.9991 \n-0.9986 \n-1.0004 \n4 \n5 \n10.740 \n9.839 \n11.615 \n11.718 \n-11.322 \n-11.721 \n10 \n11 \n10.005 \n10.001 \n11.026 \n11.015 \n-11.985 -11.992 \nAfter 12 iterations, the Gauss-Seidel method has converged to \nwithin 0.01 of the exact solution x\n1 \n= 10, x\n2 \n= 11, x\n3 \n= -12. \n8 \n1.0003 \n-0.9998 \n6 \n10.120 \n11.249 \n-11.816 \n12 \n10.001 \n11.008 \n-11.996","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":120611,"to":120696}}}}],[1756,{"pageContent":"21. \nn \n13 \n14 \n15 16 \nx\n, \n10.0004 10.0003 10.0001    10.0001 \nX\n2 \n11.0043 \n11.0023 11.0014 \n11.0007 \nX\n3 \n-11.9976 \n-11.9986 \n-11.9993 -11.9996 \n23. The Gauss-Seidel method produces \nn \n0 \n1 \n2 \n3 \n4 \nx\n, \n0 0 \n12.5 \n21.875 \n24.219 \nX\nz \n0 0 \n18.75 \n21.438 24.609 \nX\n3 \n0 50 \n68.75 \n73.438 \n74.609 \nX\n4 \n0 \n62.5 \n71.875 74.219 \n74.805 \nThe exact solution is x\n1 \n= 25, x\n2 \n= 25, x\n3 \n= 75, x\n4 \n= 75. \n25. The Gauss-Seidel method produces the following iterates: \nn \n0 \n1 \n2 \n3 \n4 \nt\n, \n0 20 \n21.25 22.8125 \n23.3301 \nt\n2 \n0 \n5 \n11.25 \n13.3203 \n14.6386 \nt\n3 \n0 \n21.25 \n24.6094 \n26.9873 \n27.7303 \nt\n4 \n0 \n2.5 \n5.8594 \n8.2373 8.9804 \nt\ns \n0 \n7.1875 \n14.6289 \n16.2829 \n16.7578 \nt\n6 \n0 \n23.0469 \n24.9072 \n25.3207 25.4394 \nn \n7 \n8 \n9 \n10 \nt\n, \n23.8093 \n23.8206 \n23.8242 \n23.8252 \nt\n2 \n15.2824 \n15.2966 \n15.3010 \n15.3024 \nt\n3 \n28.0579 \n28.0650 28.0671 \n28.0678 \nt\n4 \n9.3079 \n9.3150 \n9.3172 \n9.3178 \nt\ns \n16.9633 16.9677 \n16.9690 16.9695 \nt\n6 \n25.4908 \n25.4919 \n25.4922 \n25.4924 \n27. (a) \nn \n0 1 \n2 \n3 \n4 \n5 \n6 \nx\n, \n0 0 \nl l \n5 5 \n2\n1 \n4 4 16 16 64 \nX\n2 \nl l 3 3 ll \nll \n2 2 \n8 8 32 32 \nAnswers to Selected Odd-Numbered Exercises \nANS9 \n5 \n6 \n7 \n8 \n9 \n24.805 24.951","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":120698,"to":120867}}}}],[1757,{"pageContent":"t\n6 \n25.4908 \n25.4919 \n25.4922 \n25.4924 \n27. (a) \nn \n0 1 \n2 \n3 \n4 \n5 \n6 \nx\n, \n0 0 \nl l \n5 5 \n2\n1 \n4 4 16 16 64 \nX\n2 \nl l 3 3 ll \nll \n2 2 \n8 8 32 32 \nAnswers to Selected Odd-Numbered Exercises \nANS9 \n5 \n6 \n7 \n8 \n9 \n24.805 24.951 \n24.988 \n24.997 \n24.999 \n24.902 \n24.976 \n24.994 \n24.998 24.999 \n74.902 \n74.976 \n74.994 \n74.998 \n74.999 \n74.951 \n74.988 74.997 \n74.999 \n75.000 \n5 \n6 \n23.6596 \n23.7732 \n15.0926 \n15.2732 \n27.9626 \n28.0352 \n9.2126 \n9.2852 \n16.9036 16.9491 \n25.4759 \n25.4873 \n11 \n12 \n23.8256 23.8257 \n15.3029 \n15.3029 \n28.0681 \n28.0681 \n9.3181 \n9.3181 \n16.9696 \n16.9696 \n25.4924 \n25.4924 \nX\n2 \n1.2 \n0.8 \n0.4 \nXj","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":120867,"to":120950}}}}],[1758,{"pageContent":"ANS10 \nAnswers to Selected Odd-Numbered Exercises \n(b) \n2x\n1 \n+ \nX\n2 \n=  1 \nX\n1 \n+ 2x\n2 \n=  1 \n(c) \nn \n0 \n1 2 \n3 4 5 \nX\n1 \n0   0 \n0.25 \n0.3125   0.3281 \n0.3320 \nX\n2 \n0.5 \n0.375 \n0.3438 \n0.3360 \n[Columns 1, 2, and 3 of this table are the \nodd-numbered columns 1, 3, and 5 from the table \nin part (a).] The iterates are converging to \nX\n1 \n= X\n2 \n= 0.3333. \n(d) X\n1 \n= X\nz \n= \nt \nReview \nQ\nuestions \n1. (a) F \n(c) F \n(e) T \n(g) T \n(i)  F \n3.\n[\nJ \n5\n. \n[\n�\n] \n7. k = -1 \n9. (O, 3, 1\n) \n11. x -2y \n+ \nz  = 0 13. (a)  Yes 15\n. \n1or2 \n17. If c\n1\n(u \n+ \nv) \n+ \nc\n2\n(u - v) = 0, then (c\n1 \n+ \nc\n2\n)u \n+ \n( c \n1 \n- c \n2\n)v = 0. Linear independence of u and v \nimplies c \n1 \n+ \nc\n2 \n= 0and c\n1 \n- c\n2 \n= 0. Solving this \nsystem, we get c\n1 \n= c\n2 \n= 0. Hence u \n+ \nv and u - v \nare linearly independent. \n19. Their ranks must be equal. \nChapter 3 \nExercises 3.1 \n1. \n[ \n3 -6\n] \n-5 \n7 \n[ \n12 \n5\n. \n-4 \n9. [10] \n13. \n[\n� \n0 \n�\n] \n0 \n0 \n17. \n[\n� \n�\n] \n[ \n1 .\n5\n0 \n19. B = \n1.75 \n3. Not possible \n7. \n[ \n3 \n19 \n2�\n] \n[\n-4 \n11. \n8 \n-\n!\n] \n[ \n27 \n12�\n] \n15. \n-49 \n1.00  2.00\n] \nBA \n= \n[650.00 462.50\n] \n1.50  1.00 \n, \n675.00 406.25 \n0.3340","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":120952,"to":121122}}}}],[1759,{"pageContent":"-5 \n7 \n[ \n12 \n5\n. \n-4 \n9. [10] \n13. \n[\n� \n0 \n�\n] \n0 \n0 \n17. \n[\n� \n�\n] \n[ \n1 .\n5\n0 \n19. B = \n1.75 \n3. Not possible \n7. \n[ \n3 \n19 \n2�\n] \n[\n-4 \n11. \n8 \n-\n!\n] \n[ \n27 \n12�\n] \n15. \n-49 \n1.00  2.00\n] \nBA \n= \n[650.00 462.50\n] \n1.50  1.00 \n, \n675.00 406.25 \n0.3340 \nColumn i corresponds to warehouse i, row 1 contains \nthe costs of shipping by truck, and row 2 contains the \ncosts of shipping by train. \n6 \n7 \n0.3330 \n0.3332 \n0.3335 0.3334 \n2\n1\n. \n[\n� \n-2 \n_\n:J\n[\n::: \n[\n�\n] \n23. AB= \n[2a\n1 \n+ \na\n2 \n- a\n3 \n3a\n1 \n- a\n2 \n+ 6  a3 \na\n2 \n+ \n4\na3] \n(where a\n; \nis the ith column of A\n) \n2\n5\n.\nH \n3 \n�\n] \n[\n: \n0 \n:\nJ\n+\nH \n-12 \n-\n:\ni \n-9 \n+ \n-1 \n6 \n6 \n0 \n-6  -4 \n[ \n2\nA\n1 \n+ 3A\n2 \nl \n27. BA = \nA\n1 \n-\nA\n2 \n+ A\n3 \n(where A\n; \nis the ith row \n-\nA\n1 \n+ 6A\n2 \n+ \n4A\n3 \nof A\n) \n29. Ifb\n; \nis the ith column of B, then Ab\n; \nis the ith column \nof AB. If the columns of B are linearly dependent, \nthen there are scalars \nC\np ... , e\nn \n(not all zero) such \nthat c\n1 \nb\n1 \n+ \n·  ·  · \n+ \nc\nn\nb\nn \n= 0. But then c\n1 \n(Ab\n1\n) \n+ \n·  ·  · \n+ \nc\nn\n(\nAb\nn\n) \n= A(c\n1\nh\n1 \n+ \n·  ·  · \n+ \nc\n\"\nb\n\"\n) =AO= 0, SO the \ncolumns of AB are linearly dependent. \n31. [-� \n� \n�\ni \n0  0  5 \n35. (a)  A\n2 \n= \n[\n-1 \n-1 \nA\n4 \n= \n[\n� \nA\n7 \n= \n[\n_\n� \n33. \nr� \n! \n� \n1 0 1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":121122,"to":121350}}}}],[1760,{"pageContent":"= 0. But then c\n1 \n(Ab\n1\n) \n+ \n·  ·  · \n+ \nc\nn\n(\nAb\nn\n) \n= A(c\n1\nh\n1 \n+ \n·  ·  · \n+ \nc\n\"\nb\n\"\n) =AO= 0, SO the \ncolumns of AB are linearly dependent. \n31. [-� \n� \n�\ni \n0  0  5 \n35. (a)  A\n2 \n= \n[\n-1 \n-1 \nA\n4 \n= \n[\n� \nA\n7 \n= \n[\n_\n� \n33. \nr� \n! \n� \n1 0 1 \n0 1 0 \nl\n]\nA\n3\n=[\n-l \n0 \n, \n0 \n-l\n]\nA\ns\n=[\nl \n-1 \n, \n1 \n�\n] \n-\n�\nl \n-l\n]\nA\n6\n=[\n1 \n0 \n, \n0 \n(b) \n[-1 \nA\n2\n001 \n= \nO \n-\n�\nJ \n37.A\nn \n= \n[\n� \n:\nJ \n[ \nI \n-1 \n1 \n-\n: \nl \n(c) \n[\n; \n0   0 \n-1 \n-1 \n1 1 \n39. (a) \n1 \n-1 \n1 \n-1 \n4 \n8 \n-1 \n1 \n-1 \n1 \n9 \n27 \n�\nl \n,�] \n81","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":121350,"to":121484}}}}],[1761,{"pageContent":"Exercises 3.2 \n1.X=\n[\n! :\nJ \n5. B \n= 2A\n1 \n+ \nA\n2 \n13. Linearly independent \n23. a  = d, c =  0 \n27.a  = d,b  = c =  0 \n15. Linearly independent \n25.  3b = 2c,a  =  d  -c \n29. Let A  = [ a\niJ \nJ \nand B \n= \n[ b\ni\nj \nJ \nbe upper triangular n X n \nmatrices and let i > j. Then, by the definition of an \nupper triangular matrix, \na\n;\n1 \n=  a\ni\n2 \n= \n· · · \n=  a\ni, i\n-\ni \n=  0 \nand \nbi\nj \n=  b\ni\n+\nl,\nj \n= \n· · · \n=  b\nn\nj \n=  0 \nNow let C  = AB. Then \nc\niJ \n=   a\n;\n1 \nb\n1\nj \n+  a\n;\n2\nb\n2\nj \n+ \n... \na\ni, i\n-\n1 \nb\ni\n-\n1,\nJ \n+  a\n;;\nb\ni\nj \n+ a\ni,i\n+\nl\nb\ni\n+\nl,\nj \n+\n· \n.. \n+  a\nin\nb\nn\nj \n=   0 · b\n1\n1 \n+ \n0 · b\n2\n1 \n+ \n· · · \n0 · b\ni\n-\nl,\nJ \n+ \na\n;;\n· 0 \n+ a\ni,\n;\n+\n1 \n·0 +\n· · ·\n+  a\n;\nn\n·O =  0 \nfrom which it follows that C is upper triangular. \n35. (a) A, B symmetric ==?\n(\nA  + B)\nT \n=A\nT \n+  B\nT\n= \nA  +  B =} A  +  B is symmetric \n37. Matrices (b) and (c) are sk  ew-symmetric. \n41. Either A or B (or both) must be the zero matrix. \n43\n.\n(b) \n[\n; � :\nJ \n[\n: ; n \n+ \n[\n� \n-\n� \n=�\nJ \n47. Hint: Use the trace. \nExercises 3.3 \n1.\n[_� \n-\n:J \n3. Not invertible \n5. Not invertible \n[ \na/\n(\na\n2 \n+  b\n2\n) \nb/\n(\na\n2 \n+  b\n2\n)\n] \n9\n· \n-b/\n(\na\n2 \n+ \nb\n2\n) \na/\n(\na\n2 \n+  b\n2\n) \n11. \n[\n-\n!\n]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":121486,"to":121691}}}}],[1762,{"pageContent":"43\n.\n(b) \n[\n; � :\nJ \n[\n: ; n \n+ \n[\n� \n-\n� \n=�\nJ \n47. Hint: Use the trace. \nExercises 3.3 \n1.\n[_� \n-\n:J \n3. Not invertible \n5. Not invertible \n[ \na/\n(\na\n2 \n+  b\n2\n) \nb/\n(\na\n2 \n+  b\n2\n)\n] \n9\n· \n-b/\n(\na\n2 \n+ \nb\n2\n) \na/\n(\na\n2 \n+  b\n2\n) \n11. \n[\n-\n!\n] \nAnswers to Selected Odd-Numbered Exercises \nANS11 \n(c) The method in part (b) uses fewer multiplications. \n17\n. (b) (AB)\n-\n1 \n= A\n-\n1\nB\n-\n1 \nif and only if AB = BA \n21. X =  A\n-\n1\n(\nBA)\n2\nB\n-\n1 \n23. X = \n(AB)\n-\n1\nBA +A \n[\n: \n0 \ni\nl \nu \n0 \n:\n: \n25.E  = \n27.E  = \n1 \n0 0 \n[\ni \n0 \n:\nJ \n31. \n[\n� \n�\n] \n29.E  = \n0 \n33. \n[\n� \n�\n] \n35. \n[\ni \n� \n:\nJ \n37. \n[\ni \nl�c \n:\n: \n39.A=\n[\n-\n� \n�\n]\n[\n� \n-\n�\nl\nA\n-\n1\n=\n[\n� \n-\n�\nJ\n[\n� \n�\n] \n43. (a) If A is invertible, then BA = CA =} (BA\n)\nA\n-\n1 \n= \n(\nCA\n)\nA\n-\n1 \n==? \nB(\nAA\n-\n1\n) \n=  C\n(\nAA\n-\n1\n) \n==?BI= CI ==? \nB \n= C. \n45. Hint: Rewrite A\n2 \n-2A + I= 0 as A\n(\n2I -  A\n) \n= I. \n47. If AB is invertible, then there exists a matrix X such \nthat \n(AB)\nX = I. But then A\n(\nBX\n) \n=  I too, so A is \ninvertible (with inverse BX\n)\n. \n49. \n[\n1 !\n] \n1\n0 \n5 \n53. Not invertible \n[ \n1/a \n0 \n55. -l/a\n2 \nl/a \nl/a\n3 \n-l/a\n2 \n[\n-!� \n-2 \n5 \n-2 \n57. \n5 \n-2 \n9 2 \n-4 \n[ \n1 \n0 \n0 1 \n59. \n-�/d \n0 \n-b/d \n61. Not invertible \n[ \nl/\n(\na\n2 \n+  1\n) \n51. \n2 \na/\n(\na   +  1\n) \nqa*O \nl/a \n-�] \n-a/\n(\na","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":121691,"to":121946}}}}],[1763,{"pageContent":")\n. \n49. \n[\n1 !\n] \n1\n0 \n5 \n53. Not invertible \n[ \n1/a \n0 \n55. -l/a\n2 \nl/a \nl/a\n3 \n-l/a\n2 \n[\n-!� \n-2 \n5 \n-2 \n57. \n5 \n-2 \n9 2 \n-4 \n[ \n1 \n0 \n0 1 \n59. \n-�/d \n0 \n-b/d \n61. Not invertible \n[ \nl/\n(\na\n2 \n+  1\n) \n51. \n2 \na/\n(\na   +  1\n) \nqa*O \nl/a \n-�] \n-a/\n(\na\n2 \n+  l\n)\n] \nl/\n(\na\n2 \n+  1\n) \n0 \nquo \n0 \n1 \n-c/d l/d \n63. [� \n6 \n�\n] \n3 \n6","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":121946,"to":122024}}}}],[1764,{"pageContent":"ANS12 \nAnswers to Selected Odd-Numbered Exercises \n6\n9\n. \n[ \n� \n� \n� � \nl \n71. \n[-\n� \n� \n_\n: \n� l \n-2  -3  1  0 \n0 0 \n-1  -2  0  1 \n1   -1 0 \nExercises 3.4 \n[-3/2] \n3. \n-2 \n-1 \n7. \n[ \n_\n� \n�\n]\n[\n� \n�\n] \n9\n. \n[\n: \n� \n�\n]\n[\n� \n-\n� \n-\n!] \n8  3  1   0    0    3 \n11.\n[ \n� \n� \n� �]\n[\n� \n� \n-\n� \n-\n�\ni \n03 10 00 3    1 \n-1  0  -2  1   0  0    0    1 \n13. \n[\n� \n� \n�\n]\n[\n� \n� \n� \n-2\n5\n1\n] \n0  0  1   0   0   0 \n15. r\n-\n1 \n= \n[ \n� �], u\n-\n1 \n= \n[\n-\nt \n1\nJ\n. \nA \n-\nI \n= \n[\n-\n5\n/\n12  1\n/\n12\n] \n1\n/\n6   1/6 \nrn� \n� \n�\nl \n2\n!\n. \n[\n� \n� \n! m� \n� \n! \n� \nm \n� \nt \n�\nl \n2\n3. \n[\n� \n� \n�JU : �r\n� \n� _,!\n] \n� \n� m� J � �r� -� \ni \n�: \nExercises 3.5 \n1. Subspace \n3. Subspace \n5. Subspace \n7. Not a subspace \n11. bis in col(A), w is not in row(A). \n15. No \n17. {[l  0  -1], [0  1  2\nl\n}isabasis forrow(A); \n{\n[\n:\n]\n. \n[ \n�\n]\n} \ni\n\" \nb\n\"'i\n< \nf\nm \nwl(A \n)\n; \n{ \n[ \n-m \nir n \nb.\n,i\n< \nfor null(A). \n19. {[ 1  0  1 0], [ 0  1 -1  0], [ 0  0  0 1]} is a basis \nfo\nno\nw(A); \n{ \n[ \nn \n[ \n:l \n[ \nJ\n) \ni\n< a b\n\"'i' \nf\nm \nwl(A); \n{ \n[\n-J} i' a b\"i' fornull(A). \n2\n1\n. \n{ [ 1  0 -1], [ 1 \n1 \nl \nl\n} is a basis for row(A); \n{\n[ \n�\n], \n[ \n�\n]\n} \nis a basis for col(A) \n23. { [ 1  1  0 1], [ 0  1 -1  1], [ 0  1 -1  -1 \nl\n} \nis a basis for row(A); \n{ [\n�","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":122026,"to":122248}}}}],[1765,{"pageContent":"{ \n[\n-J} i' a b\"i' fornull(A). \n2\n1\n. \n{ [ 1  0 -1], [ 1 \n1 \nl \nl\n} is a basis for row(A); \n{\n[ \n�\n], \n[ \n�\n]\n} \nis a basis for col(A) \n23. { [ 1  1  0 1], [ 0  1 -1  1], [ 0  1 -1  -1 \nl\n} \nis a basis for row(A); \n{ [\n�\n], \n[\n�], \n[\n�\n]) is a basis for \ncol(A) \nO \nO \n1 \n25. Both {[ 1  0 -1 ], [ 0  1  2 \nl\n} and { [ 1  0 -1 ], \n[ \n1   1  l ] } are linearly independent spanning sets for \nr\now(\nA)\n=\n{[\na \nb \n-a\n+\n2b\nl\n}.\nBo\nth\n{\n[\n�\nl\n[\n�\n]\n} \nand \n{ \n[ \n�\n], \n[ \n�\n] \n} \nare linearly independent spanning \nsets for col(A) = IR\n2\n• \n29. { [ 1  0 0], [ 0  1 0], [ 0  0  l \nl\n} \n31. {[2  -3  l], [l -1  O], [\n4  -4  l]} \n35. rank(A) = 2, nullity(A) =  1 \n37. rank(A) =  3,   nullity(A) =  1 \n39. I\nf \nA is 3X5, then rank(A) :s 3, so there cannot be \nmore than three linearly independent columns. \n41. \nnullity(A) = 2, 3, 4, or  5","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":122248,"to":122336}}}}],[1766,{"pageContent":"43. If \na = -1, then rank(A)  = \nl\n; if \na = 2, then \nrank(A) = 2; for \na \n* -1, 2, rank(A) = 3. \n45. Yes \n47. Yes 49. No \n51. w is in  span(B) if and only if the linear system with \naugmented matrix [B\nI \nw] is consistent, which is true \nin this case, since \nI\nB\nlw\nl \n� [\n� \n_\nH\n]\n---7 \n[\n� \n� \n-\n�\n] \nFrom this reduced row echelon form, it is also clear \nthat \n[w]8 = \n[ \n_\n�J. \n53. rank(A) = 2, nullity(A)  = 1 \n55. rank(A) = 3, nullity(A)  = 1 \n57. Let A\n1\n, ... \n, \nAm be the row vectors of A so   that \nrow(A) =  span(A\n1\n, ... \n, \nAm). If xis in null(A), then, \nsince Ax = 0, we also have Ai · x = 0 for i = 1, ... , m , \nby the row-column definition of matrix multiplication. \nIf r is in row(A), then r is of the form \nr = \nC\n1\nA\n1 \n+ \n... \n+ c\nmAm. Therefore, \nr ·x =  (c\n1\nA\n1 \n+ ·   ·   · + c\nmAm) ·x \n= c\n1\n(\nA\n1 \n·x) + ·   ·   · + \ncm(Am ·x) = 0 \n59. (a) If a set  of columns of AB is linearly independent, \nthen the corresponding columns of B are linearly \nindependent (by an argument similar to that needed","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":122338,"to":122414}}}}],[1767,{"pageContent":"= c\n1\n(\nA\n1 \n·x) + ·   ·   · + \ncm(Am ·x) = 0 \n59. (a) If a set  of columns of AB is linearly independent, \nthen the corresponding columns of B are linearly \nindependent (by an argument similar to that needed \nto prove Exercise 29 in Section 3.1). It follows that \nthe \nma\nximum number k oflinearly independent \ncolumns of AB [i.e., k \n= \nrank(AB)] is not more than \nthe maximum number r oflinearly independent \ncolumnsofB [i.e.,r =  rank(B)].In otherwords, \nrank(AB) :s rank(B). \n61. (a) From Exercise 59(a), rank(UA) :s rank(A) and \nrank(A) = rank((U\n-\n1\nU)A) = rank(U\n-\n1\n(UA)) :s \nrank(UA). Hence, rank(UA) = rank(A). \nExercises 3.6 \n1. T(u) = [\n1\n�\n]\n, T(v)  = \n[\n�\nJ \n11. \n[\n� \n-\n�J \n13.\n[\n� \n-1 \n-\n�\nJ \n15. [F] \n-\n-\n[\n-\n0\n1 \no1\nJ \n17. [D\nJ \n-\n-\n[\n2\n0 \n0\n3\nJ \n19. \n[\nk \n0\nJ stretches or contracts in the x-direction (com-\n0  1 \n[\nl \nO\nJ \nbined with a reflection in the y-axis if k \n< \nO\n)\n; \n0 k \nstretches or contracts in they-direction (combined \nAnswers to Selected Odd-Numbered Exercises ANS13 \nwith a reflection in the x-axis if   k \n<  O)\n; \n[ \n� \n� \nJ is a","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":122414,"to":122507}}}}],[1768,{"pageContent":"< \nO\n)\n; \n0 k \nstretches or contracts in they-direction (combined \nAnswers to Selected Odd-Numbered Exercises ANS13 \nwith a reflection in the x-axis if   k \n<  O)\n; \n[ \n� \n� \nJ is a \nreflection in the line y = x; \n[ \n� \n� J is a she\na\nr in the \nx-direction; \n[\n� \n� \nJ is a she\na\nr in they-direction. For \nexample, \ny \ny \n(\n0\n, \n1) \n(1, 1) \n[\n� \n�J \n(k, 1) \n(k \n+ 1\n, 1) \n� \n(k > 0) \n• \nx \nx \n(0\n, \n0) \n(1, 0) \n(\n0\n, \n0) \n(1, 0) \ny \ny \n(0\n, \nk) \n(1,  k) \n(0\n, \n1) \n(1, 1) \n[\n� \n�\nJ \n� \n(k > \nO) \nx x \n(0\n, \n0\n) \n(1, 0) \n(\n0\n, \n0\n) \n(1, 0) \n[\nv3/2 \n21. \nI \n-1 2 \nl\n/2\nJ \nv3/\n2 \n23. \n[ \n_\n! \n-\nn \n25. [ \n0 \n-1 \n-\n�\nJ \n27. \n[\n-\ni \nfl \n31. [S0 T] \n[\n-\n: \n�\nJ \n33. [\nS\noT] \n[\n� \n6 \n-\n�\nJ \n-2 \nH \n0 \n-\n�\n] \n35. [\nS\no T] \n= \n-1 \n[\n-\nv3\n/2 \n37. \n1/2 \nl\n/2\nJ \nv3/\n2 \n[\n-\nv3\n/2 \n39. \n1/2 \n-\nl\n/2\nJ \n-\nv3\n/2 \n45. In vector form, let the parallel lines be given by \nx = p + td and x' = p\n' \n+ td. Their images are \nT(x) = T(\np \n+ td\n) \n= T(\np\n) + tT\n(\nd\n) \nand T(x') = \nT(\np\n' + td\n) \n= T(\np\n') + tT\n(\nd\n)\n. Suppose T(d) of-0. If \nT(\np\n') - T(\np\n) is parallel to T(d), then the images rep­\nresent the same line; otherwise the images represent \ndistinct parallel lines. On the other hand, if T(d) = 0,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":122507,"to":122701}}}}],[1769,{"pageContent":"ANS14 \nAnswers to Selected Odd-Numbered Exercises \nthen the images represent two distinct points if \nT(\np\n') *  T(\np\n) and single point otherwise. \n47. \ny \n' \n(\n-2\n, \n3\n) \n(\n2\n, \n3\n) \n� \n(\n-2\n, \n-3\n) \n(\n2\n, \n-3\n) \n49. \ny \n(\n-\nt. \n-\n%\n) \n51. \ny \nExercises 3.7 \n[\n0.4\n] \n[\n0.38\n] \n1. \nX\ni \n= \n0.6 \n, \nXz \n= \n0.62 \nx \n5. \nX\n1 = [ ���], \nXz \n= [ ���i \n120 \n115 \n9. (a\n) \np = \n[\n0.662  0.250\n] \n0.338 0.750 \n(c) 42.5% wet, 57.5% dry \n3\n. 64% \n7. \nfs \n(b) 0.353 \n[ \n0.08 0.09 0.11 \nl \n11. (a) P = 0.07 0.11  0.05 \n0.85 0.80 0.84 \n(b) 0.08, 0.1062, 0.1057, 0.1057, 0.1057 \n(c) 10.6% good, 5.5% fair, 83.9% poor \n1\n3\n. The entries of the vector jP are just the column sums \nof the matr  ix P. So Pis stochastic if and only if jP = j. \n15. 4 \n17. 9.375 \n19. Yes, x = \n[\n�\n] \n2\n3\n. No \n27. Productive \n3\n1. x = \n[\n�\n�\n] \n21. No \n25. fo,\nF \nrn\nl \n29. Not productive \n3\n7. \nX\n1 = \n[ \n4\n!\nl \nXz \n= \n[\n1\n�\n�\nl \nx\n3 \n= \n[\n3\n�\n�\n] \n3\n9. x1 = [\n5\n�\n�],\nXz \n= [���],x\n3 \n= [\nl\n��:i \n50 \n35 \n175 \n41. \n(\na\n) \nFor L\nI' \nwe have \nX\n1 \n= \n[\n5\n�\nl \nXz \n= \n[ \n!\n�\nl \nX\n3 \n= \n[ \n2\n�\n� \nl \nX\n4 \n= \n[ \n� \n:\n� \nl \nX\n5 \n= \n[ \n�\n�\n� \nl \n� \n= \n[ \n:\n:\n� \nl \n[\n3200\n] \n[\n2560\n] \n[\n12800\n] \nX7 \n= \n5\n12 \n'\nX\ng \n= \n256\n0 \n, \n� \n= \n2048 \n, \n[\n10240\n] \nX\ni\na \n=","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":122703,"to":122933}}}}],[1770,{"pageContent":"Xz \n= [���],x\n3 \n= [\nl\n��:i \n50 \n35 \n175 \n41. \n(\na\n) \nFor L\nI' \nwe have \nX\n1 \n= \n[\n5\n�\nl \nXz \n= \n[ \n!\n�\nl \nX\n3 \n= \n[ \n2\n�\n� \nl \nX\n4 \n= \n[ \n� \n:\n� \nl \nX\n5 \n= \n[ \n�\n�\n� \nl \n� \n= \n[ \n:\n:\n� \nl \n[\n3200\n] \n[\n2560\n] \n[\n12800\n] \nX7 \n= \n5\n12 \n'\nX\ng \n= \n256\n0 \n, \n� \n= \n2048 \n, \n[\n10240\n] \nX\ni\na \n= \n10240 \n. \n(b) The first population oscillates between two \nstates,while the second approaches a steady state. \n4\n3\n. The population oscillates through a cycle of three \nstates (for the relative population): If 0.1 < s ::::: 1, the \nactual population is growing; ifs =  0.1, the actual \npopulation goes through a cycle of length 3; and if \n0 ::::: s < 0.1, the actual population is declining (and \nwill eventually die out). \n45.\nA \n� \n[\n� \n� \n� \n�\n] \n0  1 1   1 \n1  0     0  0 \n47. \nA\n= \n1  1  0 1  0 \n1  0  1  0  1 \n1  0  0  1 0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":122933,"to":123052}}}}],[1771,{"pageContent":"49. \nV\nz \n51. \nV\n1 \nV\n1 \nV\n5 \nV\nz \nV3 \nV\n4 \nV\n4 \nV3 \n0 0 0 \n[\n; \n1   1 \n!\n] \n1 \n0  0 \n1 \n0 \n0  0 \n5\n3\n.\nA \n= \n55. \nA= \n1  1 \n0  0  0 \n1 \n0 \n1 \n0  0 0 \n0  0 \n0  0 0  0 \n57. \nV\nl \nv\nz \n59. \n61. 2 6\n3\n.3 65. 0 67. 3 \n69. (a) Vertex i is not adjacent to any other vertices. \n71. Ifwe use direct wins only, P\n2 \nis in first place; P\n3\n, P\n4\n, \nand P\n6 \ntie for second place; and P\n1 \nand P\n5 \ntie for third \nplace. Ifwe combine direct and indirect wins, the play­\ners rank as follows: P\n2 \nin first place, followed by P\n6\n, P\n4\n, \nP\n3\n, P\n5\n, and P\n1\n. \n7\n3\n. (a) \nAnn \nDana \nCarla \n0 \n0 \nA= \n0 \n1 \n0 \n0 0 \n0 \n1 \n0  0 \n0 \n0 \n1 0 \n0  0 \n(h) two steps; all of   the off-diagonal entries of the \nsecond row of A + A \n2 \nare nonzero. \n1 \n0 \n1 \n0 \n0 \n( d) If the graph has n vertices, check the \n(\ni, j) entry of \nthe powers A\nk \nfor k =   1, ... ,n - 1. Vertex i is \nAnswers to Selected Odd-Numbered Exercises ANS15 \nconnected to vertex j by a path of length k if and \nonly if (A \nk\n)\ni\nj \n* 0. \n75. (AA \nT\n)\nij \ncounts the number of vertices adjacent to both \nvertex i and vertex j. \n77. Bipartite \n79. Bipartite \nReview \nQ\nuestions \n1. (a) T","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":123054,"to":123191}}}}],[1772,{"pageContent":"only if (A \nk\n)\ni\nj \n* 0. \n75. (AA \nT\n)\nij \ncounts the number of vertices adjacent to both \nvertex i and vertex j. \n77. Bipartite \n79. Bipartite \nReview \nQ\nuestions \n1. (a) T \n(c) F \n(e) T \n(g) T \n3\n. Impossible \n-33 \nI \n] \n[\n1 3\n] \n[ \n4  10\n] \n7\n· \n3 9 \n+ \n10 25 \n1\n�\n6 \n[\no \n-\n9\n] \n9. 2 \n4 \n1   -6 \n(i) T \n11. Because (I -  A)(I +A + A\n2\n) =I - A\n3 \n=I \n-\n0 =I, \n(I - A)\n-\n1 \n=I+ A+ A\n2\n• \n1\n3\n. A basis for row(A) is {[ 1, -2, 0,-1, O], [O, 0, 1, 2, O], \n[O, 0, 0, O, l]); a b\n\"\n'i' fm rn l(A) \ni' \n\\ m [H m) \n(or the standard basis for R\n3\n)\n; and a basis for null(A) is \n2 \n1 \n1 \n0 \n0 ,  -2 \n0 \n1 \n0 0 \n15. An invertible matrix has a trivial (zero) null space. If A \nis invertible, then so is A\nr\n,  and so both A and A\nT \nhave \ntrivial null spaces. If A is not invertible, then A and A\nT \nneed not have the same null space. For example, take \nA = \n[\n� �\n]\n. \n17. Because A has n linearly independent columns, \nrank(A) = n. Hence rank(A \nT\nA) = n by Theorem 3.28. \nBecause A\nT \nA is n X n, this implies that A\nT \nA is invert­\nible, by the Fundamental Theorem of Invertible \nMatrices. AA \nT","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":123191,"to":123298}}}}],[1773,{"pageContent":"rank(A) = n. Hence rank(A \nT\nA) = n by Theorem 3.28. \nBecause A\nT \nA is n X n, this implies that A\nT \nA is invert­\nible, by the Fundamental Theorem of Invertible \nMatrices. AA \nT \nneed not be invertible. For example, \ntake A = \n[\n�\n]\n. \n[-\n1\n/\nS\nVZ \n-\n3\n/\nS\nVZ\n] \n19. \n'h \n'h \n2\n/\n5v2 \n6/ 5v2 \nChapter 4 \nExercises 4.1 \n1. Av = \n[ \n�\n] \n= 3v, \nA \n= 3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":123298,"to":123341}}}}],[1774,{"pageContent":"ANS16 \nAnswers to Selected Odd-Numbered Exercises \n3. Av = \n[\n-\n!\n] \n=  -3v, A =   -3 \n5.\nAF Hl � \n3v, A \n� \n3 \n7. \n[\n�\n] \n9. \n[\n�\n] \nII\n. \n[ \nJ \n13. A \n= \nl,E\n1 \n=\nspan\n([\n�\n]\n)\n; A = \n-l\n,E_\n1 \n=\nspan\n([\n�\n]\n) \n15. A \n= \n0, E\n0 \n= \nspan\n([\n�\n]\n)\n; A \n= \n1, E\n1 \n= \nspan\n([\n�\n]\n) \n17. A \n= \n2,E\n2 \n=\nspan\n([\n�\n]\n)\n; A\n= \n3,E\n3 \n=\nspan\n([\n�\n]\n) \n19. v = \n[\n1\n] \nA= l\n·\nv = \n[\n0\n] \nA= 2 \n0 \n, , \n1 \n, \n[ \n1 \nI \n\\/2\n] \n[\n-\n1 \nI \n\\/2\n] \n21.v = \nl/\\/2 \n,A= 2;v = \nl\n/\\/2 \n,A= 0 \n2\n3. A \n= \n2, E\n2 \n= \nspan\n([\n�\n]\n)\n; A \n= \n3, E\n3 \n= \nspan\n([\n�\n]\n) \ny \n4 \n/ \n3\ny \n2 \n4 \n25. A =  2, E\n2 \n= span\n(\n[\n�\n]\n) \ny \nx \n2x \n-----t---+--B+\n--x \n0 \n2 \n27. A= 1  +   i, E\n1\n+\n; \n= span\n(\n[\n�\n]\n} \nA= 1  -  i, E\n1\n_; \n= \nspan\n(\n[ \n�\nJ\n) \n29. A =  1  + i, E\n1\n+\n; =  span\n(\n[\n�\n]\n)\n; A= 1  -  i, E\n1\n_;\n= \nspan\n(\n[\n_\n�\n]\n) \n31.A=l,2 \nExercises 4.2 \n1. 16 \n9. -12 \n17. 0 \n31. 0 \n39. -8 \n51. (-2)3\nn \n33. A = 4 \n3. 0 \nll. a\n2\nb +   ab\n2 \n25. 2 \n33. -24 \n45. k *   0, 2 \n5. -18 \n13. 4 \n27.  -24 \n35. 8 \n47. -6 \n7. 6 \n15. abdg \n29. 0 \n37. -4 \n49. -� \n53. det(AB) = (det A) (det B) = (det B) (det A) = det(BA) \n55. 0, 1 \n3 \nI \n57. x = \n2»Y \n= \n-\n2 \n59.\nx =   -1,y = O, z =  1 \n[ \nI   I \n2 \n-\n2 \n63.  0 \n1 \n0   0 \nExercises 4.3 \n-1\n] \n-1 \n1 \n61. \n[\ni \n-\ni\nJ \n1. (a)  A\n2 \n- 7 A + 12","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":123343,"to":123603}}}}],[1775,{"pageContent":"53. det(AB) = (det A) (det B) = (det B) (det A) = det(BA) \n55. 0, 1 \n3 \nI \n57. x = \n2»Y \n= \n-\n2 \n59.\nx =   -1,y = O, z =  1 \n[ \nI   I \n2 \n-\n2 \n63.  0 \n1 \n0   0 \nExercises 4.3 \n-1\n] \n-1 \n1 \n61. \n[\ni \n-\ni\nJ \n1. (a)  A\n2 \n- 7 A + 12 \n(b) \nA =   3, 4 \n(c\n) \nE\n3 \n= span\n(\n[\n�\n]\n)\n; E\n4 \n= span\n(\n[\n�\n]\n) \n(d) The algebraic and geometric multiplicities are all 1. \n3. (a)  -A\n3 \n+ 2A\n2 \n+ SA - 6 \n(b)  A = \n-2, 1, 3 \n(d) The algebraic and geometric multiplicities are all 1. \n5. (a) -A\n3\n+A\n2 \n(b)A=O,l \n(\nc\n) \nE\n0 \n� \nspon\n([\n-\nm\nE\n, \n� \nsp�\n([\n�\n]) \n(d) A = 0 has algebraic multiplicity 2 and geometric \nmultiplicity l; A =  1 has algebraic and geometric \nmultiplicity 1. \n7. (a)  -A\n3 \n+ 9A\n2 \n- 27A + 27 \n(b) A =  3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":123603,"to":123695}}}}],[1776,{"pageContent":"(d) A = 3 has algebraic multiplicity 3 and geometric \nmultiplicity 2. \n9. (a) ,\\\n4 \n- 6,\\\n3 \n+ 9,\\\n2 \n+ 4,\\ - 12 \n(b) \n,\\ =   -1, 2, \n3 \n(\nc\n) \nE\n_\n, \n� \n'\nP\n\"\n\"\n( \n[\n-\nrn\n' \nE\n, \n� \n'\nP\n·\n·\n( \n[\n-\n�\n]\n} \nE, � \n'\nP\n\"\n\"\n( \n[\n�\n]\n) \n(d) ,\\ =   -1 and ,\\ = 3 have algebraic and geometric \nmultiplicity l; A = 2 has algebraic multiplicity 2 \nand geometric multiplicity 1. \n11. (a) ,\\\n4 \n- 4A\n3 \n+ 2A\n2 \n+ 4,\\ - 3 \n(b) \n,\\ =   -1, 1, \n3 \n(\nc\n) \nE\n_\n, \n� \n'\nP\n\"\n\"\n( \n[\n�\n]\n} \nE\n, \n� \n'\nP\n�\n( \n[ \n_\n�\n]\nf\n�\n]\n} \nE, � \n'\nP\n�\n( \n[\n�\n]\n) \n(d) ,\\ = \n-\n1 and ,\\ = 3 have algebraic and geometric \nmultiplicity l; A =  1 has algebraic and geometric \nmultiplicity 2. \n[ \nr\n9 \n+ 3·2'\n0\n] \n15. \n-r\n9 \n+ \n3·2'\n0 \n23. (a),\\= -2,E_\n2 \n= span\n(\n[ \n_\n�\n]\n}\nA= 5, E\n5 \n= \nspan\n(\n[\n�\n]\n) \nAnswers to Selected Odd-Numbered Exercises \nANS11 \n(b) (i) ,\\ = -\nL\nE\n-\n1;\n2 \n= span\n(\n[\n_\n�\n]\n}\n,\\= \nk\n,E\n1;5 \n= \nspan\n(\n[\n�\n]\n) \n(iii) A =  0, E\n0 \n= span\n(\n[\n_\n�\n]\n} \nA =   7, \nE\n7 \n= span\n(\n[\n�\n]\n) \n27. n � -\nl\nn\n-A\n' \n- 3A\n' \n+ 4A  -12 \n35. A\n2 \n= 4A -5I,A\n3 \n=  llA -   20I \nA\n4 \n= 24A -   55I \n37. A\n-\n1 \n=  -\n�\nA + \ni\nI A\n-\n2 \n= \n_ \n_i_\nA + \n_!_l_\nI \n5    5 \n' \n25 25 \nExercises 4.4 \n1. The characteristic polynomial of A is ,\\\n2 \n-5,\\ + 1, but","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":123697,"to":123918}}}}],[1777,{"pageContent":"' \n- 3A\n' \n+ 4A  -12 \n35. A\n2 \n= 4A -5I,A\n3 \n=  llA -   20I \nA\n4 \n= 24A -   55I \n37. A\n-\n1 \n=  -\n�\nA + \ni\nI A\n-\n2 \n= \n_ \n_i_\nA + \n_!_l_\nI \n5    5 \n' \n25 25 \nExercises 4.4 \n1. The characteristic polynomial of A is ,\\\n2 \n-5,\\ + 1, but \nthat of B is ,\\ \n2 \n-2A + 1. \n3. The eigenvalues of A are ,\\ = 2 and ,\\ =   4, but those \nof B are A =  1 and A =   4. \n5. A\n1 \n= \n4,\nE\n4 \n= \nspan\n([\n�\n]\n}\n,\\\n2 \n= \n3,E\n3 \n=\nspan\n(\n[\n�\n]\n) \n7. A\n, \n� \n6, E\n, \n� \n'\nP\n\"\n\"\n( \n[\n�]}A, \n� \n-2, \nL, � \n'\nP\n••\n(\n[ \nJ. \nU\nJ\nl \n9. Not diagonalizable \n[ 1    1 \n11.P=   1 \n1 \n1   -2 \n-1\n]    [\n2 \n� \n,D \n= � \n13. Not diagonalizable \n15. P \n= \n[\n� \n! \n� \n-\n�\n]\n,D \n= \n[\nH \n-\n� \n�\n] \n0  0  0    1 \n0  0    0 -2 \n[ \n35839 -69630\n] \n17. \n-11605 \n24234 \n[\n(3\nk \n+ 3(-l)\nk\n)/\n4 \n(3\nk\n+\nl \n-\n3(-l)\nk\n)/\n4\n] \n19\n' \n(3\nk \n- (-1/)/\n4   (3\nk\n+\nl \n+ (-l)\nk\n)/\n4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":123918,"to":124073}}}}],[1778,{"pageContent":"ANS18 \nAnswers \nto Selected \nOdd-N\numbered \nExercises \n21. \n[\n� \n-\n� \n-\n�\nl \n2\n3\n. \n[ \n(5 \n+ \n2\nk+2 \n+ \n(\n-\n3)\nk\n)/10 \n(\n2\nk+\nl \n- 2\n(\n-\n3)\nk)/\n5 \n(\n-\n5 \n+ \n2\nk+2 \n+ \n(\n-\n3l)/10 \n25.\nk \n= \n0 \n29. \nAll real values \nof \nk \n(\n2\nk \n-\n(\n-\n3)\nk\n)/\n5 \n(\n2\nk \n+ \n4\n(\n-\n3)\nk\n)/\n5 \n(\n2\nk \n-\n(\n-\n3)\nk\n)/\n5 \n27. \nk \n= \n0 \n(\n-\n5 \n+ \n2\nk+2 \n+ \n(\n-\n3)\nk\n)/10\n] \n(\n2\nk+\nl \n- 2\n(\n-\n3)\nk)/\n5 \n(5 \n+ \n2\nk+2 \n+ \n(\n-\n3)\nk\n)/10 \n3\n7. \nIf A \n� \nB, then there is \nan invertible \nmatrix \nP such that \nB \n= \np\n-\n'AP. Therefore, \nwe have \ntr(B) \n= \ntr(P\n-\n1\nAP) \n= \ntr(P\n-\n1\n(AP)) \n= \ntr((AP)P\n-\n1\n) \n= \ntr(APP\n-\n1\n) \n= \ntr(AI) \n= \ntr(A) \nusing \nExercise \n45 in Section 3.2. \n3\n9. \np \n= \n[ \n7 \n10 \n-2\n] \n-3 \n41. \np \n� \n[ \n=\n! \nI \n�: \n-\n2 \n3 \n-\n2 \n3 \n-\n2 \n9. \n11. \nk \n0 \n1 \n2 \nx\nk \n[\n�\n] \n[\n2\n:\n] \n[ \n17.692\n] \n5.923 \nY\nk \n[\n�\n] \n[ \n�\n.308\n] \n[ \n�\n.335\n] \nm\nk \n26 \n17.692 \nTherefore, \nA\n1 \n= \n18, v\n1 \n= \n[ \n1 \n]\n. \n0.333 \nk \n0 \n1 \n2 \nx\nk \n[\n�\n] \n[\n�\n] \n[\n7.571\n] \n2.857 \nY\nk \n[\n�\n] \n[\n�\n.286\n] \n[\n�\n.377\n] \nm\nk \n7 \n7.571 \nTherefore,\nA\n1 \n= \n7.827,\nv\n1 \n= \n[\n1 \n]\n. \n0.414 \n3 \n[\n18.018\n] \n6.004 \n[ \n�\n.333\n] \n18.018 \n3 \n[\n7.755\n] \n3.132 \n[\n�\n.404\n] \n7.755 \n51. \n(b) \ndimE\n_\n1 \n= \n1, dimE\n1 \n= \n2, \ndimE\n2 \n= \n3 \nExercises \n4.5 \n1. \n(a) \n[\n1 \n]\n, 6.000 \n2.5 \n(b) \nA\n, \n= \n6 \n3\n. \n(a) \n[\n1 \n]\n, 2.618 \n0.618 \n(b) \nA\n, \n= \n(3 \n+ \nVs)/2 \n= \n2.618 \n5. \n(a) \nm\n5 \n= \nll.001,y\n5 \n= \n[\n-0.333\n]","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":124075,"to":124436}}}}],[1779,{"pageContent":"] \n3.132 \n[\n�\n.404\n] \n7.755 \n51. \n(b) \ndimE\n_\n1 \n= \n1, dimE\n1 \n= \n2, \ndimE\n2 \n= \n3 \nExercises \n4.5 \n1. \n(a) \n[\n1 \n]\n, 6.000 \n2.5 \n(b) \nA\n, \n= \n6 \n3\n. \n(a) \n[\n1 \n]\n, 2.618 \n0.618 \n(b) \nA\n, \n= \n(3 \n+ \nVs)/2 \n= \n2.618 \n5. \n(a) \nm\n5 \n= \nll.001,y\n5 \n= \n[\n-0.333\n] \n1.000 \n7. \n(a) \nm\n, \n� \n10.000, y\n, \n� \nm \n4 \n5 \n[\n17.999\n] \n6.000 \n[\n18.000\n] \n6.000 \n[ \n�\n.333\n] \n[ \n�\n.333\n] \n17.999 \n18.000 \n4 \n5 \n6 \n[ \n7.808\n] \n3.212 \n[\n7.823\n] \n3.234 \n[\n7.827\n] \n3.240 \n[\n�\n.411\n] \n[\n�\n.413\n] \n[ \n�\n.414\n] \n7.808 \n7.823 \n7.827","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":124436,"to":124558}}}}],[1780,{"pageContent":"Answers to Selected Odd-Numbered Exercises ANS19 \n13. k \n0 \n1 \n2 \n3 \n[\n:\n] \n[\n:\n:\n] \n[ \n16809 \nl \n[\n1\n7\n011\n] \nx\nk \n12.238 \n12.371 \n10\n.714 \n10.824 \nYk \nm \n[\n�\n7\n14\n] \n[ \n�\n7\n2\n8 \nl \n[\n�\n7\n2\n7\n] \n0.619 \n0.6\n3\n7 \n0.6\n3\n6 \nm\nk \n21 \n16.809 \n1\n7\n.011 \nThecefoce, A\n, \n= \n1\n7\n, \nv\n, \n= \n[ \n�\n.\n7\n2\n7 \nl \n0.6\n3\n6 \n15. A\n, \n= \n5\n, v\n, \n= \n[ \n� \nl \n0.333 \n17.  k \n0 \n1 \nx\nk \n[\n�\n] \n[\n�\n] \nR\n(x\nk\n) \n7 \n7.755 \nYk \n[\n�\n] \n[ \n�\n.286\n] \n19.  k \n0 \n1 \nx\nk \nm \n[\nm \nR\n(x\nk\n) \n16.333 \n16.998 \nYk \n[\n:\n] \n[\n�\n7\n1\n4\n] \n0.619 \n21.  k \n0 \n1 \n2 \n3 \n4 \n[\n7\n.57\n1\n] \n2.\n857 \n[\n7\n.\n755\n] \n3.132 \n[\n7\n.808\n] \n3.212 \n7\n.823 \n7\n.828 \n7\n.828 \n[\n�\n.377\n] \n[ \n�\n.404\n] \n[\n�\n.411\n] \n2 \n3 \n[\n16809\n] \n[ \n1\n7 \n011 \nl \n12.2\n3\n8 12.371 \n10.714 \n10.824 \n1\n7\n.000 \n1\n7\n.000 \n[ \n� \n7\n28 \nl \n0.6\n3\n7 \n[\n�\n72\n7\n] \n0.6\n3\n6 \n2 \n3 \n4 \n4 \n[ \n16999 \nl \n12.363 \n10.818 \n[\n�\n7\n2\n7 \nl \n0.6\n3\n6 \n16.999 \n5 \n[\n7.8\n23\n] \n3.234 \n7\n.828 \n[\n�\n.413\n] \n4 \n[ \n16999 \nl \n12.\n3\n6\n3 \n10.818 \n1\n7\n.000 \n[\n�\n7\n2\n7\n-\n0.6\n3\n6 \n5 \nx\nk \n[\n�\n] \n[\n!\n] \n[ \n4.8\n] \n3.2 \n[ \n4.667\n] \n2.66\n7 \n[ \n4.57\n1\n] \n2\n.2 86 \n[ \n4.5\n00\n] \n2.000 \nYk \n[\n�\n] \n[\n�\n.8\n] \n[\n�\n.667\n] \n[ \n�\n.571\n] \n[\n�\n.5\noo\n] \nm\nk \n5 \n4.8 \n4.667 \n4.\n571 \nSince A\n1 \n= \nA\n2 \n= \n4, v\n1 \n= \n[ \n1\n]\n, m\nk \nis converging slowly to \nthe exact answer. \n0 \n[ \n�\n.444\n] \n4.500 \n5 \n[ \n1\n7 \n000 \nl \n12.363 \n10.818 \n[\n�\n7\n2\n7 \nl \n0.6\n3\n6 \n1\n7\n.000 \n6 \n[\n7.827\n] \n3.240 \n7\n.828 \n[\n�\n.414\n] \n5 \n[ \n1\n7","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":124560,"to":124945}}}}],[1781,{"pageContent":"] \nm\nk \n5 \n4.8 \n4.667 \n4.\n571 \nSince A\n1 \n= \nA\n2 \n= \n4, v\n1 \n= \n[ \n1\n]\n, m\nk \nis converging slowly to \nthe exact answer. \n0 \n[ \n�\n.444\n] \n4.500 \n5 \n[ \n1\n7 \n000 \nl \n12.363 \n10.818 \n[\n�\n7\n2\n7 \nl \n0.6\n3\n6 \n1\n7\n.000 \n6 \n[\n7.827\n] \n3.240 \n7\n.828 \n[\n�\n.414\n] \n5 \n[ \n1\n7\n000 \nl \n12.\n3\n6\n3 \n10.818 \n1\n7\n.000 \n[\n�\n727\n] \n0.6\n3\n6 \n6 \n7 \n8 \n[ \n4.444\n] \n1.\n77\n8 \n[ \n4.400\n] \n1.600 \n[ \n4.3\n64\n] \n1.455 \n[ \n�\n.400\n] \n[\n�\n.364\n] \n[ \n�\n.333\n] \n4.444 \n4.400 \n4.364","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":124945,"to":125059}}}}],[1782,{"pageContent":"ANS20 Answers to Selected Odd-Numbered Exercises \n2\n3\n. k \n0 \n1 \n2 \n3 \n4 \nm \nm \n[ \n4\n2\n] \n[ \n40\n4\n8\n] \n[ \n4012 \nl \nx\nk \n3.2 \n3.048 \n3.012 \n0.2 \n0.048 \n0.012 \nYk \nm \n[\n�\n· \nl \n[\n�\n762\n] \n[\n�\n753 \nl \n[ \n� \n751 \nl \n0.2 \n0.048 \n0.012 \n0.003 \nm\nk \n5 \n4.2 \n4.048 \n4.012 \nln thi' \"''· A\n, \n� \n,, \n� \n4�dE, \n� \n'P\n�(\n[\nH \n[\nm \nClearly, m\nk \nis converging to 4 and \nYk \nis converging to a \nwdodn th, dgmpooe E,-namdy, \n[ \n�\n] \n+ 0 75 \nm \n25. \nk 0 \n1 \n2 \n3 \n4 \nx\nk \n[\n�\n] \n[\n�\n] \n[ \n=\n�\nJ \n[\n�\n] \n[ \n= \n�\n] \nYk \n[\n�\n] \n[\n�\n] \n[\n�\n] \n[\n�\n] \n[\n�\n] \nm\nk \n-\n1 \n-\n1 \nThe exact eigenvalues are complex \n(\ni and - i\n)\n, so the \npower method cannot possibly converge to either the \ndominant eigenvalue or the dominant eigenvector if we \nstart with a re\na\nl initial iterate. Instead, the power method \noscillates between two sets of real vectors. \n27. k \n0 \n1 \n2 \n3 \n4 \nm \nm \n[ \n2500\n] \n[\n2250\n] \n[\n'\n12\n5\n] \nx\nk \n4.000 4.000 4.000 \n2.\n5\n00 \n2.2\n5\n0 \n2.12\n5 \nYk \nm \n[ \n�\n75\n0\n] \n[ \n�\n625\n] \n[ \n�\n562\n] \n[ \n�\n531\n] \n0.750 \n0.62\n5 \n0.562 \n0.531 \nm\nk \n1 \n4 4 4 4 \n5 \n6 \n7 \n8 \n[ \n4 003 \nl \n[ \n4001 \nl \n[ \n4 000\n] \n[ \n4 000\n] \n3.003 \n3.001 \n3.000 3.000 \n0.003 \n0.001 \n0.000 0.000 \n[\n�\n750 \nl \n0.001 \n[\n�\n75\n0 \nl \n[\n�\n7\n5\n0 \nl \n[ \n� \n750 \n-\n4.003 \n4.001 4.000 4.000","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":125061,"to":125303}}}}],[1783,{"pageContent":"�\n531\n] \n0.750 \n0.62\n5 \n0.562 \n0.531 \nm\nk \n1 \n4 4 4 4 \n5 \n6 \n7 \n8 \n[ \n4 003 \nl \n[ \n4001 \nl \n[ \n4 000\n] \n[ \n4 000\n] \n3.003 \n3.001 \n3.000 3.000 \n0.003 \n0.001 \n0.000 0.000 \n[\n�\n750 \nl \n0.001 \n[\n�\n75\n0 \nl \n[\n�\n7\n5\n0 \nl \n[ \n� \n750 \n-\n4.003 \n4.001 4.000 4.000 \n5 \n[\n�\n] \n[\n�\n] \n5 \n[\n2063\n] \n4.000 \n2.063 \n[\n�\n516\n] \n0.516 \n4","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":125303,"to":125377}}}}],[1784,{"pageContent":"The eigenvalues are A\n1 \n= \n-\n12, A\n2 \n= 4, A\n3 \n= 2, with cor­\nresponding eigenvectors \nSince Xo = \nt\nv\n2 \n+ t\nv\n3\n, the initial vector x\n0 \nhas a zero \ncomponent in the direction of the dominant eigenvector, \nso the power method cannot converge to the dominant \neigenvalue/eigenvector. Instead, it converges to a second \neigenvalue/eigenvector pair, as the calculations show. \n[\n-\n4 \n29. Apply the power method to A \n-\n181 = \n5 \n12\n] \n-15 \n. \nk \nx\nk \nY\nk \nm\nk \n0 \n1 \n2 \n3 \n[\n�\n] \n[\n-\n1\n�\n] \n[ \n15.2\n] \n-\n19 \n[ \n15.2\n] \n-\n19 \n[\n�\n] \n[\n-\n�\n.8\n] \n[\n-\n�\n.8\n] \n[\n-\n�\n.8\n] \n-\n10 \n-\n19 \n-\n19 \nThus, \n-\n19 is the dominant eigenvalue of A \n-\n181, and \nA\n2 \n= \n-\n19 + 18 = \n-\n1 is the  second eigenvalue of A. \n[\n-\n8 \n4 \n8\n] \n31. Applythe power method to A - 171 =   4  -2  -4. \n8 \n-4 \n-8 \n33. \nk \n0 \n1 \n2 \nx\nk \n[\n�\n] \n[ \n0.5\n] \n-\n0.5 \n[ \n-\n0.833\n] \n1.056 \nY\nk \n[\n�\n] \n[ \n_�\n] \n[ \n-\n�\n.789\n] \nm\nk \n1 \n0.5 \n1.056 \nThus, the eigenvalue of A that is smallest in magnitude is \n1\n/(\n- 1) \n= \n-\n1. \nAnswers to Selected Odd-Numbered Exercises \nANS21 \nk \n0 \n1 \n4 \n-\n0.667 \n-\n18 \n2 \n3 \n-\n18 \n-\n18 \n-\n18 \n-\n18 \nIn this case, there is no  dominant eigenvalue. (We \ncould choose either 18 or \n-","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":125379,"to":125556}}}}],[1785,{"pageContent":"1\n/(\n- 1) \n= \n-\n1. \nAnswers to Selected Odd-Numbered Exercises \nANS21 \nk \n0 \n1 \n4 \n-\n0.667 \n-\n18 \n2 \n3 \n-\n18 \n-\n18 \n-\n18 \n-\n18 \nIn this case, there is no  dominant eigenvalue. (We \ncould choose either 18 or \n-\n18 for m\nk\n> k 2 2.) How­\never, the Rayleigh quotient method (Exercises 17-20) \nconverges to \n-\n18. Thus, \n-\n18 is the dominant eigen­\nvalue of A \n-\n17l, and A\n2 \n= \n-\n18 + 17 = \n-\n1 is the \nsecond eigenvalue of A. \n3 \n4 \n5 \n[ \n0.798\n] \n-\n0.997 \n[ \n0.800\n] \n-\n1.000 \n[ \n0.800\n] \n-\n1.000 \n[ \n-\n�\n.801\n] \n[\n-\n�\n.800\n] \n[\n-\n�\n.800\n] \n-\n0.997 \n-\n1.000 \n-\n1.000","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":125556,"to":125642}}}}],[1786,{"pageContent":"ANS22 \nAnswers to Selected Odd-Numbered Exercises \n35. \nk 0 \n1 2 \nu\n-\n[\n-\n05\n00\n] \n[ \n0500] \nx\nk \n0.000 \n0.333 \n0.\n5\n00 \n-\n0.\n5\n00 \n[\nJ \n[\n-\n1000] \n[\n-\n1000] \nYk \n0.000 \n-\n0.667 \n1.000 1.000 \nm\nk \n1 \n-\n0.\n5\n00 \n-\n0.\n5\n00 \nClearly, m\nk \nconverges to \n-\n0.\n5\n, so the smallest eigenvalue \nof A is 1\n/\n(\n-\n0.\n5) \n= \n-\n2. \n37. The calculations are the same as for Exercise 33. \n39. We apply the inverse power method to A \n-\nSI \n= \nk \n[\n-\n1 \n-\n1 \n6 \n0 \n0 \n-\n2 \n0 \n[\n:\n] \n[\n:\n] \n� \nl \n· Taking x\n0 \n= \n-\n1 \n1 2 \n[ \n0200: \n[\n-\n0080] \n-\n0.500 \n-\n0.\n5\n00 \n0.200 \n-\n0.080 \n[\n-\n�400\n-\n-\n0.400 \n[ \n�160] \n0.160 \n-\n0.\n500 \n-\n0.\n5\n00 \n3 \n[ \n0032 l \n-\n0.\n5\n00 \n0.032 \n[\n-\n�064] \n-\n0.064 \n-\n0.500 \nClearly, m\nk \nconverges to \n-\n0.\n5\n, so the eigenvalue of A \nclosest to 5 is 5 + 1/(\n-\n0.5) \n= \n5 \n-\n2 = 3. \n41. 0.732 4\n3. \n-\n0.619 \n47. \nIm \n2 \n-2 \n3 \n4 \n5 \n[ \n0500] \n[ \n0500 l \n[ \n0\n5\n00] \n0.111 \n0.259 \n0.160 \n-\n0.\n5\n00 \n-\n0.\n5\n00 \n-\n0.\n5\n00 \n[\n-\n1000 l \n[\n-\n1000] \n[\n-\n1000] \n-\n0.222 \n-\n0.\n5 18 \n-\n0.321 \n1.000 1.000 \n1.000 \n-\n0.\n5\n00 \n-\n0.\n5\n00 \n-\n0.\n5\n00 \n49. \nIm \n51. Hint: Show that 0 is not contained in any Gerschgorin \ndisk and then apply Theorem 4.16. \n53. Exercise \n5\n2 implies that \nl\n,t\nl \nis less than or equal to all","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":125644,"to":125873}}}}],[1787,{"pageContent":"1.000 \n-\n0.\n5\n00 \n-\n0.\n5\n00 \n-\n0.\n5\n00 \n49. \nIm \n51. Hint: Show that 0 is not contained in any Gerschgorin \ndisk and then apply Theorem 4.16. \n53. Exercise \n5\n2 implies that \nl\n,t\nl \nis less than or equal to all \nof the column sums of A for every eigenvalue \nA\n. But \nfor a stochastic matrix, all column sums are 1. Hence \nI\nA\nI \n:s 1. \nExercises 4.6 \n1. Not regular \n3. Regular \n5. Not regular \n7. L = \n[\ni \ni\nJ \n[ \n0.304 \n0.304 \n0.304 \n-\n9. L \n= \n0.354 \n0.354 \n0.354 \n0.342 \n0.342 0.342 \n11. 1, \n[\n�\n] \n13.\n2\n. \n[\n'\n;\n] \n15. The population is increasing, decreasing, and constant, \nrespectively.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":125873,"to":125938}}}}],[1788,{"pageContent":"17. \nb\n1 \nb\n2\nS1 \nb3\nS1S\n2 \nb\nn -1S1S\n2 \n..• \nSn-\n2 \nb\nn\ns\n1\ns\n2 \n• \n· \n• \ns\n,,_1 \n1 \n0 0 0 0 \np-\nI\n1p = \n0 0 0 0 \n0   0 0 0 0 \n0   0    0 0 \nThe characteristic polynomial of Lis \n(\nA\n\" \n- b,\nA\n\"\n-\n1 \n-\nb\n2\ns\n1\nA\nn\n-\nl \n-  b\n3\ns\n1\ns\n2\nA\n\"\n-\n3 \n-\n·   · •  -\nb,,s\n1\ns\n2 \n• • \n·\ns,,\n_\n, )(-l\nt. \n[ \n0.660 \nl \n19. A \n= \n1.746, p \n= \n0.264 \n0.0\n76 \n0.\n535 \n0.147 \n0.094 \n21. A \n= \n1.092, p \n= \n0.078 \n0.064 \n0.053 \n0.02\n9 \n25. (a) h \n= \n0.082 \n29. 3, \n[ \ni\nJ \n31. 3, \n[\nll \n33. Redu<ibk \n35. Irreducible \n4\n3. 1, 2, 4, 8, 16 \n4\n5. 0, \n1, 1, 0, \n-\n1 \n47. \nx,, \n= \n4\" \n-\n(\n-\n1)\" \n4\n9. y,, \n= \n(\nn \n-\nt\n)2\n\" \n51.b,, \n= \n,\n1\n;;;-[(\nl \n+ \nv3)\n\" \n-\n(1\n-\nv3)\n\"\n] \n2 \nv\n3 \n5\n7. (a) d, \n= \n1, d\n2 \n= \n2, d\n3 \n= \n3, d\n4 \n= \n5, d\n5 \n= \n8 \n(b) d\n,, \n= \nd\n,,\n_\n, \n+ \nd\n,,\n-\n2 \nC \nd \n= \n_\nl \n[\n(\nl \n+ \nVs)n\n+\nI \n_ (\n1 \n-\ny5\n)\nn+\nl\n] \n() \nn \nVs \n2 \n2 \n59. The general solution is x(t) \n= \n-3C\n1\ne\n-\nt \n+ \nC\n2\ne\n4\nt\n, \ny(t) \n= \n2C\n1\ne\n-\nt \n+ \nC\n2\ne\n4\n1\n• The specific solution is \nx(t) \n= \n-\n3e\n-\nt \n+ \n3e\n4\nt\n, y(t) \n= \n2e\n-\nt \n+ \n3e\n4\nt\n. \n61. The general solution is x\n,\n(t) \n= \n(1 \n+ \nV2\n)\nC\n1\ne\n\\/2\nt \n+ \n(1 \n-\nV2\n)\nC\n2\ne\n-\nV2\nt\n, \nx\n2\n(t) \n= \nC\n, e\nV2\nt \n+ \nC\n2\ne\n-\nV2\nt\n. \nThe \nspecific solution isx\n1\n(t) \n= \n(2 \n+ \nV2\n)\ne\n\\/2\nt\n/4 \n+ \n(2 \n-\nV2\n)\ne\n-\nV2\nt\n/4, \nx\n2\n(t) \n= \nV2\ne\nV2\nt\n/4 \n-\nV2\ne\n-\nV2\nt\n/\n4. \n63. Thegeneralsolutionis x(t) \n= \n-\nC\n1 \n+","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":125940,"to":126278}}}}],[1789,{"pageContent":"(t) \n= \n(1 \n+ \nV2\n)\nC\n1\ne\n\\/2\nt \n+ \n(1 \n-\nV2\n)\nC\n2\ne\n-\nV2\nt\n, \nx\n2\n(t) \n= \nC\n, e\nV2\nt \n+ \nC\n2\ne\n-\nV2\nt\n. \nThe \nspecific solution isx\n1\n(t) \n= \n(2 \n+ \nV2\n)\ne\n\\/2\nt\n/4 \n+ \n(2 \n-\nV2\n)\ne\n-\nV2\nt\n/4, \nx\n2\n(t) \n= \nV2\ne\nV2\nt\n/4 \n-\nV2\ne\n-\nV2\nt\n/\n4. \n63. Thegeneralsolutionis x(t) \n= \n-\nC\n1 \n+ \nC\n3\ne\n-\nt\n,\ny(\nt\n) \n= \nC\n1 \n+ \nC\n2\ne\nt \n-  C\n3\ne\n-\nt\n, \nz\n(t) \n= \nC\n1 \n+ \nC\n2\ne\nt\n. The specific \nsolution is x(t) \n= \n2 - e\n-\nt\n, y(t) \n= \n-\n2 \n+ e\nt\n+  e\n-\nt\n, \nz\n(t) \n= \n-\n2 + \ne\nt\n. \nAnswers to Selected Odd-Numbered Exercises ANS23 \n65. (a) \nx(t) \n= \n-\n120\ne\n8\nt\n/\ns \n+ \n5\n20e\n11\nt\nl\n10\n, \ny(t) \n= \n240\ne\n8\nt\n/\ns \n+ \n260\n11\nt\n/\nI\nO. Strain X dies out after approximately \n2.93 days; strain Y continues to grow. \n67. a \n= \n10, b \n= \n20; x(t) \n= \nl\nO\net(cos t + sin t) \n+ \n10, y(t) \n= \nl\nO\net(cos t \n-\nsin t) \n+ \n20. Species Y dies out when \nt \n= \n1.22. \n71. x(t) \n= \nC\n1\ne\n2\nt \n+ \nC\n2\ne\n3\nt \n77\n. (a) \n7\n9. (a) \n81. (a) \n[\n�\nl \n[\n�\nl \n[\n:\nl \n[\n�\n�\n] \n[\n�\nl \n[\n�\nl \n[\n�\nl \n[\n�\n] \n[ \n�\n]\n, \n[ \n-\n�·5], \n[ \n-\n��\ns\ns \nl \n[ \n_\n3\n;\n.\n\\\n2\ns\ns\n] \n83. (a) \n[\n�\nl \n[\n�\n:\n:\nl \n[\n�\n:\n�\n:\nl \n[\n�\n:\n��\n:\n] \n85. r \n= \nv2, e \n= \n45\n°, spiral repeller \n87. r \n= \n2, e \n= \n-\n60°, spiral repeller \n(c) Repeller \n(c) Neither \n(c) Saddle point \n(c) Attractor \n89. P \n= \n[ \n-\n� \n-\n� \nl \nC \n= \n[ \n�\n:\n� \n-\n�\n:\n� \nl \nspiral attractor \n-\n[\n1\n/\n2 \n-\nV3;2\n] \n-\n[ \n1\n/\n2 \n-","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":126278,"to":126613}}}}],[1790,{"pageContent":"= \nv2, e \n= \n45\n°, spiral repeller \n87. r \n= \n2, e \n= \n-\n60°, spiral repeller \n(c) Repeller \n(c) Neither \n(c) Saddle point \n(c) Attractor \n89. P \n= \n[ \n-\n� \n-\n� \nl \nC \n= \n[ \n�\n:\n� \n-\n�\n:\n� \nl \nspiral attractor \n-\n[\n1\n/\n2 \n-\nV3;2\n] \n-\n[ \n1\n/\n2 \n-\nV3;2\n] \n91. P\n-\n1 \nO \n,C\n-\n\\/3\n/\n2    1 /\n2 \n' \norbital center \nReview \nQ\nuestions \n1. (a) F \n3. \n-\n18 \n(c) F \n(e) F \n(g) T (i)  F \n5. Since Ar \n= \n-\nA, we have <let A \n= \ndet(A r) \n= \ndet(\n-\nA) \n= \n(\n-\n1)\n\" \n<let A \n= \n-\n<let A by Theorem 4.7 \nand the fact that n is odd. It follows that <let A \n= \n0. \n7\n. Ax = \n[ \nS \n] \n= \nSx A \n= \n5 \n10 \n, \n9. (a) 4 -  3A \n2 \n- A\n3 \n13. Not similar \n17. 0, 1, or\n-\nl \n15. Not similar \n19. If Ax \n= \nAx, then (A \n2 \n-\nSA \n+ \n2I)x \n= \nA\n2\nx \n-\n5\nAx \n+ 2x \n= \n3\n2\nx \n-\n5(3x) \n+ 2x \n= \n-\n4x.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":126613,"to":126753}}}}],[1791,{"pageContent":"ANS24 \nAnswers to Selected Odd-Numbered Exercises \nChapter 5 \nExercises 5.1 \n1. Orthogonal \n3. Not orthogonal \n5. Orthogonal \n7\n, \n[w]\n3 \n� \n[ \n_\n;\n] \n9. [w]\nB � m 11. Q,thono\nc\nmaJ \n13. \n[\n�\n�\n�\n]\n, \n[\n�:�\n]\n, \n[ \n!\n�\n�\n�\n] \n2\n/3 \n0 \n-\n5\n/3Vs \n15. Orthonormal \n[1\n/\nv2 -1\n/\nv2] \n17.  Orthogonal, \n1 \n/ \nv2    1 \n/ \nv2 \n[\ncos () sin () cos\n2 \n() \n19. Orthogonal, \n-\ncos () \nsin () \n-\nsin\n2 \n() \n-\ncos () sin () \n21. Not orthogonal \nQ\nx·\nQy \nx·\ny \n2\n7. cos(L(Qx, \nQy)) \n= \nll\nQ\nx\nll ll\nQ\nr\nll \nll\nx\nll ll\nr\nll \nsi� () \nl \ncos () \n= cos(L(x, \ny\n)) \nby Theorem 5.6 \n29. Rotation, () = 45° 31. Reflection, y = v3x \n33. (a) A(Ar + Br)B = AArB + ABrB = IB +A\nI\n= \nB+A=A+B \n(b) From part (a), \ndet(A + B) = det (A(A r + Br)B) \n= det A det(A r + Br)det B \n= det A det((A + B)r)det B \n= det A det(A + B)det B \nAssume that det A + det B = 0 (so that det \nB = \n-\ndet A) but that A + Bis invertible. \nThen det(A + B) *  0, so 1 \n= \ndet A det B = \ndet A(\n-\ndet A) = \n-\n(det A)\n2\n. This is impossible, \nso we conclude that A + B cannot be invertible. \nExercises 5.2 \n1. \nw\n� \n= \n{\n[;\n] \n:x \n+ 2\ny \n= \no \n}\n,\nB\n� \n= \n{\n[\n-\n�\n]\n} \n3. \nw\n\" \n� \n{\n[\n;\n]\nx \n� \nt, y \n� \nt, z \n�","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":126755,"to":126915}}}}],[1792,{"pageContent":"= \ndet A det B = \ndet A(\n-\ndet A) = \n-\n(det A)\n2\n. This is impossible, \nso we conclude that A + B cannot be invertible. \nExercises 5.2 \n1. \nw\n� \n= \n{\n[;\n] \n:x \n+ 2\ny \n= \no \n}\n,\nB\n� \n= \n{\n[\n-\n�\n]\n} \n3. \nw\n\" \n� \n{\n[\n;\n]\nx \n� \nt, y \n� \nt, z \n� \n-1). \nB'\n�\n{\n[\nJ\n) \n5. \nw\n\" \n� \n{ \n[\n;\n] \nx \n-\ny + 3z � o\n)\n. \nB\n\" \n� \n{ \n[J \n[\n:\n] \n) \n7. ww(A HI\n! \nO \n1]\n. \n[\nO \n1 \n-\n2]}, null(A)o \n\\\n[ \n�\n]\n) \n9. \nw\nl(A\n) \n{ \n[ \nj} U\nJ \n}\n.\nnull(A\n'\n) \nm\nJ\nrrn \n11 \n{ \n[\n-\n�\n�\nl\n) \n13. \n{ \n[\n-\n]\nf\n]\n} \n15. \n[\nl\nl \n17. \nn \ni \n19. \nv � \n[ \n=\nl\nl \n+ \n[ \n_;\nJ \n21. \nF \n[\n-\ni\nl \n+ \nU\nl \n25. No \nExercises 5.3 \n[ \n1 \n] \n[\n-\n1 \nI \n2\n] \n[ \n1 \nI \nv2\n] \n[\n-\n1 \nI \nv2] \n1. \nv, \n= \n1 \n'\nV\nz \n= \n1\n/2 \n; \nq\n, \n= \n1\n/\\/2 \n'\nqz \n= \n1/\\/2 \n3.\nv\n1\n=\n[\n-\n�\n]\n,\nv\n2 \n=\n[\n�\n]\n,\nv\n3 \n=[\n-\n�]\n;\nq\n,\n= \n[\n-\n�\n�\n�\n]\n, \n-1 \n1 \n1 \n-1\n/v3 \n[\n2/V6\n] \n[ \n0 \nl \nqz \n= \nl/V6 \n, \nq\n3 \n= \n-1\n/\\/2 \nl/V6 \nl/\nv2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":126915,"to":127160}}}}],[1793,{"pageContent":"f \n[\n3\n] [\n-\n?s\ni \n[\n-\n�\n]\n) \nll. \nl \n� \n, \n�1 \n' \n� \n[ \n1\n/\n\\/2 l\n/ v'3 \n1\n/\\/6 \nl \n13. \nQ \n= \n0 \nl\n/v'3 \n-\n2\n/\n\\/2 \n-\n1\n/\n\\/2 l\n/ v'3 \n1\n/\\/6 \n15. \n[\n1;°\n\\/2 \n-�j� \n�\nj�\n]\n[\n� \n�\nj� \n1\n/\n\\/2 \n1\n/\\/6 \n-\n1\n/\\/3 \n0     0 \n17\nR \n� [\n� \n19. A  =AI \n9 \nt\nl \n6 \nI \n3 \n0 \nz \n3 \n21. \nA\n-\nI \n= \n(\nQ\nR\n)\n-\n! \n= \nR\n-\nI\nQ\n-\n1 \n= \nR\n-\nI\nQ\nT \n= \n[\n1\n/\n\\/2 -\n1\n/\\/6 \n-\n1\n/\n2\n\\/3\n] \n0 \n2\n/\\/6 \n-\n1\n/\n2\n\\/3 \n. \n0 \n0 \n3/\n2 \\/3 \n[\n2\n/\n0\n\\/6 \n-\n�\nj� \n�\nj�] \nl\n/\nv'3 \nl\n/\nv'3 -\n1\n/\\/3 \n1\n/\n\\/2\n] \n1\n/\\/6 \n2/v'3 \n23. Let R x= 0. Then Ax= \nQ\nRx = \nQ\nO = 0. Since Ax \nrepresents a linear combination of the columns of A \n(which are linearly independent), we must have x = 0. \nHence, R is invertible, by the Fundamental Theorem. \nExercises 5.4 \n1. \n= \n[\n1\n/\n\\/2 \nQ \n1 /\n\\/2 \n1 /\n\\/2\n] \n[ \n5 \n- l\n/\n\\/2\n'\nD= \n0 \n�\n] \n3 \n-\n[\n2\n/\\/6 \n. \nQ \n-\nl\n/\nv'3 \nl\n/\nv'3\n] \n[\n2 \n-\n2\n/\\/6\n'\nD= \n0 \n-\n�\nJ \n[\n' \n0 \n0 \nl \n[\n' \n5. \nQ \n= \n0 \n1 \nI \nv'2 \n-\n1 \nI \nv'2 \n, D \n= \n0 \n0  1/\n\\/2 \n1/\n\\/2 \n0 \n[\n-\n1/\nVi \n0 \n7. \nQ = \n0 \n1 /\n\\/2 \n0 \n0 \n0 \n1/\n:2\n]\n.\nD \n� \n[\n� \n1 /\n\\/2 \n0 \n-\n1 /\n\\/2 \n0 \n0 \n-\n�\nl \n4 \n0 \n0 \nr \n0 \n[\n'\nM \nI\n/\nVi \n0 \nl \n9 \n-\n1/\n\\/2 \n.\nQ\n-\n1 /\n\\/2 \n0 \n1/\n\\/2 \n, \n0 \n0 \n1 /\n\\/2 \n0 \n-\n1 /\n\\/2 \nAnswers to Selected Odd-Numbered Exercises ANS25 \nD\n� \n[\n� \n� \n� �\n] \nl\nl. \nT\nA \n= \n[\n1\n/\n\\/2 \n1\n/\n\\/2\n] \n[\na \nb\n]. \nQ \nQ \n1/\\/2 \n-\n1/\\/2 \nb \na \n[ \n1/\\/2 \n1/\\/2\n] \n[\na \n+ \nb \n0 \n] \n1","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":127162,"to":127507}}}}],[1794,{"pageContent":"I\n/\nVi \n0 \nl \n9 \n-\n1/\n\\/2 \n.\nQ\n-\n1 /\n\\/2 \n0 \n1/\n\\/2 \n, \n0 \n0 \n1 /\n\\/2 \n0 \n-\n1 /\n\\/2 \nAnswers to Selected Odd-Numbered Exercises ANS25 \nD\n� \n[\n� \n� \n� �\n] \nl\nl. \nT\nA \n= \n[\n1\n/\n\\/2 \n1\n/\n\\/2\n] \n[\na \nb\n]. \nQ \nQ \n1/\\/2 \n-\n1/\\/2 \nb \na \n[ \n1/\\/2 \n1/\\/2\n] \n[\na \n+ \nb \n0 \n] \n1\n/\n\\/2 -\n1\n/\n\\/2 \n= \n0    a \n-\nb \n= D \n13. (a) If A and Bare orthogonally diagonalizable, then \neach is symmetric, by the Spectral Theorem. \nTherefore, A + B is symmetric, by Exercise \n3\n5 in \nSection 3.2, and so is orthogonally diagonalizable, \nby the Spectral Theorem. \n15. If A and Bare orthogonally diagonalizable, then \neach is symmetric, by the Spectral Theorem. Since \nAB = BA , AB is also symmetric, by Exercise \n3\n6 in \nSection 3.2. Hence, AB is orthogonally diagonalizable, \nby the Spectral Theorem. \n17. A  = \n[\ni \nn \n+ \n[ \n_\n! \n-\nn \n19. A \n� [\n1 \n0 \n:\nJ + \n[\n: \n0 \n�\n] \n+ \n[\n: \n0 \nJ \n0 \n2 \n-\n1 \n0 \n2 \n23\n1\n! \n2 \n-\ni\ni \n21. \n[ \n-\ni \n-\ni\nJ \n-\n3 \n� \n3 \nI \n3 \nExercises 5.5 \n1. 2x\n2 \n+ 6xy + 4/ 3. 123 \n5. -5 \n7. \n[\n� \n�\n] \n[\n2/Vs \nl\n3. \nQ \n= \nl\n/Vs \n[\n2\n/\nVs \n15. \nQ \n= \n0 \nl\n/\nVs \n[ \nl\n/v'3 \n17. \nQ \n= \n-\nl\n/v'3 \n-\n1\n/\\/3 \n(y\n'\n)\n2 \n_ \n(\nz'\n)\nz \n9. \n[ \n�\n� \n=\nn \n11. \n[ \n� \n-\n� \n-\n�\ni \n-\n2 \n2 \n2 \nl\n/\nVs\n] \n2 \n2 \n-\n2\n/Vs \n,y\nl \n+ 6y\n2 \n!j�� \n-\n�j�\n]\n,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":127507,"to":127737}}}}],[1795,{"pageContent":"1. 2x\n2 \n+ 6xy + 4/ 3. 123 \n5. -5 \n7. \n[\n� \n�\n] \n[\n2/Vs \nl\n3. \nQ \n= \nl\n/Vs \n[\n2\n/\nVs \n15. \nQ \n= \n0 \nl\n/\nVs \n[ \nl\n/v'3 \n17. \nQ \n= \n-\nl\n/v'3 \n-\n1\n/\\/3 \n(y\n'\n)\n2 \n_ \n(\nz'\n)\nz \n9. \n[ \n�\n� \n=\nn \n11. \n[ \n� \n-\n� \n-\n�\ni \n-\n2 \n2 \n2 \nl\n/\nVs\n] \n2 \n2 \n-\n2\n/Vs \n,y\nl \n+ 6y\n2 \n!j�� \n-\n�j�\n]\n, \n9\ny\ni \n+ \n9\nA \n-  9\ny\n� \n-\n4\n/3\nVs \n2\n/3 \n1 /\n\\/2 \n1\n/\\/6\n] \n0 2/\\/6 \n, 2\n(\nx'\n)\n2 \n+ \n1 /\n\\/2 -\n1\n/\\/6","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":127737,"to":127852}}}}],[1796,{"pageContent":"ANS26 Answers to Selected Odd-Numbered Exercises \n19. Positive definite \n23. Positive definite \n21. Negative definite \n25. Indefinite \n2\n7. For any vector x, we have xr Ax = x\nT\nB\nT\nBx = \n(Bxl(Bx) = \n11\nBx\nll\n2 \n2: 0. IfxrAx = 0, then \n11\nBx\nll\n2 \n= 0, \nso Bx= 0. Since B is invertible, this implies that x = 0. \nTherefore, xr Ax > 0 for all x * 0, and hence A = B\nT\nB \nis positive definite. \n29. (a) Every eigenvalue of cA is of the form \nC\nA for some \neigenvalue A of A. By Theorem \n5\n.24, A > 0, so \nC\nA > 0, since c is positive. Hence, cA is positive \ndefinite, by Theorem \n5\n.24. \n(c) Let x * 0. Then xr Ax > 0 and xrBx > 0, since A \nand \nB are positive definite. But then xr(A + B)x = \nxr Ax + xrBx > 0, so A + B is positive definite. \n31. The maximumvalueofj(x)is 2whenx= ::t::: [-��:�} \n[1 /v2] \nthe minimum value ofj(x) is 0 when x = ::t::: \nl\n/vl \n. \n[1/V3] \n33. The maximum value of f(x) is 4 when x = ::t::: 1/V3 ; \n1/V3 \nthe minimum value of f(x) is 1 when x = \n[ \nl\n/v2] \n[\n-\nl\n/v2] \n::t::: 0 or ::t::: \nl\n/vl \n. \n-\n1 /vl \n0 \n35. Ellipse \n37. Parabola","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":127854,"to":127921}}}}],[1797,{"pageContent":"l\n/vl \n. \n[1/V3] \n33. The maximum value of f(x) is 4 when x = ::t::: 1/V3 ; \n1/V3 \nthe minimum value of f(x) is 1 when x = \n[ \nl\n/v2] \n[\n-\nl\n/v2] \n::t::: 0 or ::t::: \nl\n/vl \n. \n-\n1 /vl \n0 \n35. Ellipse \n37. Parabola \n39. Hyperbola \n4\n1. Circle, x' = x \n-\n2, y\n' \n= y \n-\n2, (x')\n2 \n+ (y ')\n2 \n= 4 \n5 \n4 \n3 \ny \ny\n' \nr \n�--+----+---1----\nx\n' \n1 \n3  4  5 \nI \n4\n3. Hyperbola, x' \n= x, y' \n= y + \nt\n, (x')\n2 \n/ \n4 \n-\n(y'\n)\n2 \n/9 \n= 1 \ny\n,\ny\n' \n3 \n-+--+-+-1--+--+--+--+-+--+ X \n---=j\n----\n•\n�3--\nx\n' \n-3 \n4\n5. Parabola, x' = x \n-\n2, y\n' \n= y + 2, x' = \n-t\n(y ')\n2 \ny  y\n' \n2 \n47\n.Ellipse,(x' )\n2\n/\n4 + (y ')\n2\n/\n12 = 1 \ny \n4\n9. \nHyper\nbola, (x')\n2 \n-\n(y' )\n2 \n= 1 \ny","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":127921,"to":128028}}}}],[1798,{"pageContent":"51. \nEllipse, \n(x\n\"\n)\n2\n/5\n0 \n+ \n(y\n\"\n)\n2\n/\n10 \n= \n1 \n53. \nHyp\ner\nbola, \n(x\n\"\n)\n2 \n-\n(y\n\"\n)\n2 \n= \n1 \n55. Degenerate (two lines) \ny \n57. Degenerate (a point) \ny \n2 \n�1--+--+--e--+--+--+-_. x \n-2 2 \n-2 \n59. Degenerate (two lines) \ny \n61. Hyperboloid of one sheet, (x')\n2 \n-\n(y\n')\n2 \n+ 3(\nz\n')\n2 \n= \n1 \n63. Hyperbolic paraboloid, z = \n-\n(x')\n2 \n+ (y ')\n2 \n65. Hyperbolic paraboloid, x\n' \n= \n-\nV3(y')\n2 \n+ V3(\nz\n')\n2 \n67. Ellipsoid, 3(x\n\"\n)\n2 \n+ (y\n\"\n)\n2 \n+ 2(\nz\"\n)\n2 \n= 4 \nReview \nQ\nuestions \n1. (a) T (c) T \n(e)  F \n(g)  F \n(i)  F \n3. \n[ \n�j� \nl \n-\n11/6 \n5. Verify that \nQ\nr \nQ \n= I. \nAnswers to Selected Odd-Numbered Exercises \nANS21 \n7. Theorem \n5\n.6(c) shows that if V; • v\nj \n= 0, then \nQ\nv\n; \n· Q\nv\nj \n= 0. Theorem \n5\n.6(b) shows that \n{ \nQ\nv\n1\n, .•. , \nQ\nvd consists of unit vectors, because \n{ v\n1\n, .•. , v\nk\n} does. Hence, { \nQ\nv\n1\n, .•. , \nQ\nv\nk\n} is an \northonormal set. \n9. \n{ \n[ \n_\n�\n]\n} \n13. \nrow(A): { [ 1 0 2  3 \n4], [ 0 1 0 2 \nI]} \nr\nnl(A) \n{ \n[\n-\n�\n} \n[t]\n} \n-\n2 \n-\n3 \n-4 \n0 \n-\n2 \n-\n1 \nnull(A): \n1 \n0 \n, \n0 \n0 0 \nnull(A \n'\n) \n{ \n[ \n= \n� \nl \n[\n-\n�\n] \n} \nlS. (a) \n{ \n[] uirm \n17. \n{ \n[\n-\n�\nl \n[\n-\nlJ [ \n_\n)\n]\n} \n19. \nn  -! �l \nChapter 6 \nExercises 6. 1 \n1. Vector space \n0 \n0 \n3.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":128030,"to":128240}}}}],[1799,{"pageContent":"I]} \nr\nnl(A) \n{ \n[\n-\n�\n} \n[t]\n} \n-\n2 \n-\n3 \n-4 \n0 \n-\n2 \n-\n1 \nnull(A): \n1 \n0 \n, \n0 \n0 0 \nnull(A \n'\n) \n{ \n[ \n= \n� \nl \n[\n-\n�\n] \n} \nlS. (a) \n{ \n[] uirm \n17. \n{ \n[\n-\n�\nl \n[\n-\nlJ [ \n_\n)\n]\n} \n19. \nn  -! �l \nChapter 6 \nExercises 6. 1 \n1. Vector space \n0 \n0 \n3. \nNot a vector space; axiom 1 fails. \n5. Not a vector space; axiom 8 fails. \n7. Vector space \n9. Vector space \n11. Vector space \n15. Complex vector space","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":128240,"to":128308}}}}],[1800,{"pageContent":"ANS28 Answers to Selected Odd-Numbered Exercises \n17.  Not a complex vector space; axiom 6 fails. \n19. Not a vector space; axioms 1, 4, and 6 fail. \n21. Not a vector space; the operations of addition and \nmultiplication are not even the same. \n25. Subspace \n2\n7. Not a subspace \n29. Not a subspace 31. Subspace \n33. Subspace \n35. Subspace \n37.  Not a subspace \n4\n1. Subspace \n4\n5. Not a subspace \n39. Subspace \n4\n3. Not a subspace \n47. Take U to be the x-axis and W the y-axis, for example. \nThen \n[ \n�\n] \nand \n[ \n�\n] \nare in U U W, but \n[ \n�\n] \n= \n[ \n�\n] \n+ \n[ \n�\n] \nis not. \n51. No \n53. Yes; s(x) = \n(\n3 + 2t\n)\np\n(\nx\n) \n+ (1 + t)q(\nx\n) \n+ tr\n(\nx\n) \nfor \nany scalar t. \n55. Yes;h(x) = j\n(\nx\n) \n+ g\n(\nx\n) \n57 .No \n59. No \n61. Yes \nExercises 6.2 \n1. Linearly independent \n3. Linearly dependent; \n[\n-\nl \n-\n1 \n[\n� \n�\n]\n-\n2\n[\n_\n� \n�\n] \n5. Linearly independent \n0\n]\n=4\n[\n-\n1 \nl\n]\n+ \n7 \n-\n2 2 \n7. Linearly dependent; 3x + 2x\n2 \n= 7x -  2\n(\n2x -  x\n2\n) \n9. Linearly independent \n11. Linearly dependent; 1 = sin\n2 \nx + cos\n2 \nx \n13. Linearly dependent; ln(x\n2\n) = \n-\n2 ln 2 · 1 + 2 · ln(2x) \n17. (a) Linearly independent","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":128310,"to":128430}}}}],[1801,{"pageContent":"2 \n= 7x -  2\n(\n2x -  x\n2\n) \n9. Linearly independent \n11. Linearly dependent; 1 = sin\n2 \nx + cos\n2 \nx \n13. Linearly dependent; ln(x\n2\n) = \n-\n2 ln 2 · 1 + 2 · ln(2x) \n17. (a) Linearly independent \n(b) Linearly dependent \n19. Basis \n23. Not a basis \n21. Not a basis \n25. Not a basis \n29. \n[p\n(\nx\n) ]\nu \n� \n[\n-\n�\n] \n35. dim V = 2, B = {\nl \n-x, 1 -  x\n2\n} \n37.  dim V = 3, B = \n{ \n[ \n� \n� \nl \n[ \n� \n� \nl \n[ \n� \n�\n]\n} \n39. dimV= 2, B= \n{\n[\n� �\nl\n[\n� \n�\n]\n} \n4\n1. (n\n2 \n-\nn)/\n2 \n4\n3. (a) dim(U X V) = dim U +dim V \n(b) Show that if {w\n1\n, ••• , wJ is a basis for W, then \n{(w\n1\n, w\n1\n)\n, ••• , \n(w\nn\n, w\nn\n)\n} is a basis for �. \n4\n5. {\nl \n+ x, 1 + x + x\n2\n, l} \n47\n·\n{\n[\n� �\nl\n[\n� �\nl\n[\n� \n-\n�\nl\n[\n� \n�\n]\n} \n4\n9. {\nl\n, 1 + x} \n51. {\nl \n-x,x  -\nx\n2\n} \n53. { sin\n2 \nx, cos\n2 \nx} \n59. \n(\na\n) \np\n0(\nx\n) \n= \ni\nx\n2 \n-\n�\nx \n+ \n3,\np\n1\n(\nx\n) \n= \n-x\n2 \n+ \n4x\n-\n3, \nP\n2\n(x\n) \n= \ni\nx\n2 \n-\n�\nx \n+ \n1 \n61. (c) (i) 3x\n2 \n-\n16x + 19 (ii) x\n2 \n-4x + 5 \n63. \n(\np\nn \n_ \nl\n)(p\nn \n_ \np\n)(p\nn \n_ \np\n2\n) \n... \n(\np\nn \n_ \np\nn\n-\n1\n) \nExercises 6.3 \n1. \n[\nx]\ns \n= \n[\n� \nl \n[\nx]\nc \n= \n[ \n_\n:\nJ, \nP\nc+-\nB \n= \n[\nt \n_\nt\nJ\n, P\ns+-\nc \n= \n[\n� \n-\n�\n] \n3. \n[xJ,\n� \nU\nl \n[xJ\n,\n� \n[\n=\n:\nJ\n.\nP\nc�\nu\n� \nH _\n: \nn \nP\ns\n.--\nc\n= \n[\n� \n� \n�\ni \n1  1  1 \n5. \n[p (\nx)\n]\n8=\n[\n-\n�\nl\n[p (\nx)\n]\nc\n=\n[\n-\n�\nl\nP\nc+- s\n= \n[\n-\n� \n�\nl \nP\ns+-\nc \n= \n[\n� \n�\n] \n7. \n[p\n(\nx","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":128430,"to":128734}}}}],[1802,{"pageContent":"Exercises 6.3 \n1. \n[\nx]\ns \n= \n[\n� \nl \n[\nx]\nc \n= \n[ \n_\n:\nJ, \nP\nc+-\nB \n= \n[\nt \n_\nt\nJ\n, P\ns+-\nc \n= \n[\n� \n-\n�\n] \n3. \n[xJ,\n� \nU\nl \n[xJ\n,\n� \n[\n=\n:\nJ\n.\nP\nc�\nu\n� \nH _\n: \nn \nP\ns\n.--\nc\n= \n[\n� \n� \n�\ni \n1  1  1 \n5. \n[p (\nx)\n]\n8=\n[\n-\n�\nl\n[p (\nx)\n]\nc\n=\n[\n-\n�\nl\nP\nc+- s\n= \n[\n-\n� \n�\nl \nP\ns+-\nc \n= \n[\n� \n�\n] \n7. \n[p\n(\nx\n)]\n6 \n� \nHl \n[p\n(\nx\n)]\n, +J\n.\nP\nc�\nB \n� \n[\n: \n0 \nn \nP\nn\n� c\n� \nH \n_\n: \n�\n-","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":128734,"to":128864}}}}],[1803,{"pageContent":"[-� \n[ \n2\n] \n[\n-\n1\n/\n2\n] \n11. \n[j\n(\nx\n) \nls\n= \n_\n5 \n, \n[j\n(\nx\n) \nl e\n= \n5\n1\n2 \n, \nP\nc�s\n=\n[\n� \n-\n�\n�\n�\n]\n,P\ns\n�c\n=\n[\n� \n-\n�\nJ \nl3. (a) \n[ \n(\n3 \n-\n2\\13\n)\n/\n2\n] \n= \n[ \n3.232\n] \n(\n-\n3V3+2\n)/\n2 \n-\n1.598 \n(\nb\n) \n[\n2 + 2V3\n] = \n[\n5.464\n] \n2V3 \n-\n2 1.464 \n15. B = \n{\n[ \n=\n�\nl \n[\n!\n]\n} \n17. \n-\n2 -  8 (x \n-\n1) \n-\n5\n(\nx \n-\n1)\n2 \n19. \n-\n1 +  3\n(\nx + 1) - 3 (\nx + 1)\n2 \n+  (x + 1)\n3 \nExercises 6.4 \n1. Linear transformation \n3. Linear transformation \n5. Linear transformation \n7. Not a linear transformation \n9. Linear transformation \n11. Not a linear transformation \n13. We have \nS(p(x\n) \n+ \nq\n(x\n)) \n= S(\n(\np  + \nq\n)(\nx\n)) \n= x\n((p \n+ \nq\n)(\nx\n)) \nand \n= x\n(p (\nx\n) \n+ \nq\n(x)) \n= xp (x)  +  x\nq\n(x\n) \n= S\n(p (x )) +  S (\nq\n(\nx\n)) \nS\n(\nc\np(\nx\n)) \n= \nS((\nc\np)(\nx\n)) \n= x\n((\nc\np)(\nx\n)) \n= x\n(\nc\np(\nx\n)) \n= cx\np(x) \n= cS(p (\nx\n)) \nTherefore, S is linear. Similarly, \nr\n(\n[\n:\n] \n+ \n[\n�\n]\n) \n= \nr\n[ \na\n+ c\n] \nb+d \n(a \n+  c\n) \n+  ((a +  c\n) \n+ \n(\nb  +  d))\nx \n(a \n+  (a  +  b\n)\nx\n) \n+ \n(\nc  +  ( c  + d)\nx\n) \n= r\n(\n[\n:J\n) \n+ r\n(\n[\n�\n]\n) \nand \nAnswers to Selected Odd-Numbered Exercises ANS29 \nr\n(\nk\n[\n:\n]\n) \n= r\n[\n�\n:\nJ \n= \n(\nk\na) \n+ \n(\nk\na \n+  kb\n)\nx \nk\n(a +(a +  b\n)\nx\n) \n= kT\n(\n[\n:\n]\n) \nTherefore, T is linear. \n[\n-\n7\n] \n[\na\n] \n(\na  + 3b) \n15. T \n9 \n= 5  -14x -8x\n2\n, T \nb \n= \n4 \n-\n(\na \n: \n7b )\nx + \n(a� \nb )\nx","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":128866,"to":129164}}}}],[1804,{"pageContent":"r\n(\nk\n[\n:\n]\n) \n= r\n[\n�\n:\nJ \n= \n(\nk\na) \n+ \n(\nk\na \n+  kb\n)\nx \nk\n(a +(a +  b\n)\nx\n) \n= kT\n(\n[\n:\n]\n) \nTherefore, T is linear. \n[\n-\n7\n] \n[\na\n] \n(\na  + 3b) \n15. T \n9 \n= 5  -14x -8x\n2\n, T \nb \n= \n4 \n-\n(\na \n: \n7b )\nx + \n(a� \nb )\nx\n2 \n17. T\n(\n4 - x + 3x\n2\n) \n= 4 + 3x +  5x\n2\n, T\n(a \n+ bx + cx\n2\n) \n= \n(3\na  -  b  -\nc) \na \n+  ex + \n2 \nx\n2 \n19. \nHint: \nLet \na\n= T\n(\nE\n11\n)\n, b \n= \nT\n(\nE\n1\n2\n)\n, c \n= \nT\n(\nE\n2\n1\n)\n, \nd \n= T\n(\nE\n22\n)\n. \n23. Hint: Consider the effect of T and Don the standard \nbasis for rzf n. \n25. \n(\nS \no \nT) \n[ \n�\n] = \n[ \n� \n-! \nl \n(\nS \no \nT) \n[;\n] = \n[\n2\n; \n( \nT 0 S\n) \n[;\n] \ndoes not make sense. \n27. (S 0 T\n)(p(\nx\n)) \n= \np\n'\n(x + 1), (T \n0 S)(p(\nx\n)) \n= \n(p (x +  l\n)) ' \n= \np\n'\n(x + 1) \n-y \n] \n2x + 2\ny \n· \n29. \n(S \n0 \nT)\n[;\nJ = \ns( \nr\n[;\n]\n) \n= \ns(\n[ \n_\n;\nx\n-\n/\n4\nY\n]\n) \n= \n[4 (x -\ny) \n+  (-3x +  4\ny)] \n= \n[\nx\n] \n3\n(\nx -\ny) \n+  (-\n3x \n+  4\ny) \ny \n(T\no \ns\n)\n[;\nJ = r\n(s[;\n]\n) \n= r\n(\n[!\n:\n: \n�\n]\n) \n= \n[ (4x  + \ny) -  (\n3x + \ny) \n] \n[\nx\n] \n-\n3\n(\n4x\n+ y)\n+4\n( 3x\n+y\n) \n-\ny \nTherefore, S 0 T = I and T 0 S = I, so S and T are \ninverses. \nExercises 6.5 \n1. (a) Only (ii) is in ker(T). \n(\nb\n) \nOnly (iii) is in range(T). \n(c\n) \nker(\nT) \n= \n{ \n[\n� \n�\n]\n}\n, r\nang\ne(T) \n= \n{ \n[\n� �\n]\n} \n3. (a) Only (iii) is in ker(T). \n(h) All of them are in range(T). \n(\nc\n)","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":129164,"to":129442}}}}],[1805,{"pageContent":"inverses. \nExercises 6.5 \n1. (a) Only (ii) is in ker(T). \n(\nb\n) \nOnly (iii) is in range(T). \n(c\n) \nker(\nT) \n= \n{ \n[\n� \n�\n]\n}\n, r\nang\ne(T) \n= \n{ \n[\n� �\n]\n} \n3. (a) Only (iii) is in ker(T). \n(h) All of them are in range(T). \n(\nc\n) \nker(T) = {a +  bx +  cx\n2\n: \na \n= -c, b = -c} = \n{t + tx  -  tx\n2\n}, range(T) = IR\n2","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":129442,"to":129482}}}}],[1806,{"pageContent":"ANS30 Answers to Selected Odd-Numbered Exercises \n5. A basis for ker(T) is \n{ \n[ � \n� \nl \n[ \n� \n�] }\n,\nand a basis \nfor range(T) is \n{ \n[ \n� \n� \nl \n[ � \n�\n]\n}\n; rank(T) = \nnullit\ny(T) = 2 ,  and rank(T) + nullity(T) = 4 = \ndim\nM\n22\n• \n7. A basis for ker(T) is {l +   x   -x\n2\n}  ,  and a basis for \nrange(T) is \n{ \n[\n�\n] , \n[\n�\n] \n}\n; rank(T) = 2 , nullity(T) = 1, \nand rank(T) + nullity(T) \n= \n3 \n=\ndim <!/'\n2\n• \n9. rank(T) = nullity(T) = 2 \n11.rank(T) = nullity(T) = 2 \n13. rank(T) = 1, nullity(T) = 2 \n15. One-to-one and onto \n17. Neither one-to-one nor onto \n19. One-to-one but not onto \n21. lsomocphk\n, \nr\n[ \n� � �\n] \n� \nrn \n23. Not isomorphic \n25. Isomorphic\n, \nT(a +    bi) = \n[\n:\n] \n31. Hint: Define T: Cf;; [O, l] � Cf;; [O, 2]   by letting T(j) be \nthe function whose value at xis (T(j))(x) = j(x\n/\n2) for \nxin [O, 2]. \n33. (a) Let \nv\n1 \nand \nv\n2 \nbe in V and let   (S 0 T)(\nv\n1\n) = \n(S 0 T)(\nv\n2\n). Then S(T(\nv\n1\n)) = S(T(\nv\n2\n))\n, \nso T(\nv\n1\n) = T(\nv\n2\n)\n, \nsince Sis one-to-one. \nBut now \nv\n1 \n= \nv\n2\n, \nsince Tis one-to-one. Hence\n, \nS 0 Tis one-to-one. \n35. (a) By the Rank Theorem\n,","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":129484,"to":129599}}}}],[1807,{"pageContent":"v\n1\n) = \n(S 0 T)(\nv\n2\n). Then S(T(\nv\n1\n)) = S(T(\nv\n2\n))\n, \nso T(\nv\n1\n) = T(\nv\n2\n)\n, \nsince Sis one-to-one. \nBut now \nv\n1 \n= \nv\n2\n, \nsince Tis one-to-one. Hence\n, \nS 0 Tis one-to-one. \n35. (a) By the Rank Theorem\n, \nrank(T) + nullity(T) = \ndim V. If Tis onto\n, \nthen range(T) = W, so \nrank(T) = dim(range(T)) = dim W. Therefore\n, \ndim V + nullity(T) < dim W + nullity(D \n= \nrank(T) + nullity(T) = dim V \nso \nnullity (T) < 0, which is impossible. Therefore\n, \nT cannot be onto. \nExercises 6. 6 \n1. [T\nl\nc\n<-B \n= \n[ \nO \n1\n]\n, \n[T\nl\nc\n<-\ns\n[\n4 \n+ \n2x\n]s \n= \n-1 0 \n[ \n_ \n� \n�\n] \n[ \n�\n] \n= \n[ \n_ \n�\n] \n=  [ \n2    -\n4x] \nc \n=  [ \nT( \n4 \n+    2x) \n] \nc \n3. \n[ T]\nc\n<-B \n= \n[\n� \n� \n�\n]\n, [T]\nc\n<-s\n[a +     bx+ \ncx\n2\n]5 \n= \n0 0 1 \n[\n� \n� \n�\n]\n[\n�\n] \n� \nm \n�[a+ b(x   +     2) + \nc(x   + 2)\n2\n)]c = [T(a +bx+ cx\n2\n)]c \n[ \n1 0 OJ \n2 \n5. [ T]\nc\n<-B \n= \n, [ T]\nc\n<-s\n[a +bx+ \ne\nx \nls\n= \n1 1 1 \no o ol \n0 1 0 \n1 0 0 \n,  [T]\nc\n<-s\n[A]\ns \n= \n0 0 1 \n-1 \n[ 0 \n11. \n[\nTJc\n<-\nB \n= \n� \n[\n-\n� \n-\n�    � \n�]\n[\n�\n]\n=\n[\n�\n=\n�\n]\n= \n1  0  0 -1 c a -d \n0 -1 0 d b    -c \n[[\nc-b d-a\n]] \na-d b-c \nc \n=[\nAB -\nBA ]c \n=  [\nT(A)]c","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":129599,"to":129790}}}}],[1808,{"pageContent":"13. \n(b) \n[DJs\n= \n[\n� \n-\n�\n] \n[\no \n-\n0\n1\n] [\n_\n3\n5\n] \n_\n_ \n(c) [DJs[3 sin x - 5 cos xJs \n= \n1 \n[\n:] \n= \n[ 3 cos x + 5 sin x J s \n= \n[D(3sinx-5cosx)Js \n15. (a) [DJ\ns\n= \n[� \n� \n�\ni \n0  -1 2 \n17. [S 0 TJv+-s \n= \n[\n-\n� \n=\n�\nJ \n19. Invertible\n, \nr-\n1\n(a + bx) \n= \n-b + ax \n21. Invertible\n, \nr\n-\n1\n(\np\n(x)) \n= \np\n(x -2\n) \n23. Invertible\n, \nr\n-\n1\n(a + bx + cx\n2\n) \n= \n(a     -b +  2c\n) \n+ \n(b -2c\n)\nx + cx\n2 \nor \nY\n-\n1\n(\np\n(x)) \n= \np\n(x) -p\n'\n(x) + \np\n\"\n(x) \n25. Not invertible \n27. -3 sin x   -cos x +  C \n29. te\n2\nx cos x - te\n2\nx sin x +  C \n31.C \n= \n{\n[\n-\n�\n]\n, \n[\n-\n�\nn 33. C \n= \n{1 -x ,2 + x} \n35.C\n=\n{l,x} \n[(d\n� \n- d\ni\n)\nj\n(d\n� \n+  d\ni\n) \n37\n· \n[TJ\nt. \n= \n2d\n1\nd\n2\n/(d\nl \n+d\ni\n) \n2d\n1\nd\n2\n/(d\n� \n+  d\ni\n) \n] \n(d\ni \n- d\nl}j\n(d\nl \n+  d\ni\n) \nExercises 6. 7 \n1. y(t) \n= \n2e\n3\n1\n/\ne\n3 \n3. \ny(t) \n= \n((1 \n_ \ne\n4\n)e\n3\nt \n+ \n(e\n3 \n_ \nl)e\n4\nr\n)/(e\n3 \n_ \ne\n4\n) \n5. j(t) \n= \ne \n[ e\n(\n1 \n+\nv'sl\nr\n/\n2 _ \ne\n(\n1\n-\nv's)\n1/\n2 \nJ \n( \n(\nVs\n-\n1\n)\n/\n2\n) \ne\nv's \n- 1 \n7. \ny(t) \n= \ne\n1 \n- (1 - e\n-\n1\n)te\n1 \n9. y(t) \n= \n((k + l)e\nk\nt \n+ (k \n-\nl)e\n-\nk\n1\n)/2k \n11.y(t) = e\n1\ncos(2t) \n13. \n(\na\n) \np(t) \n= \n100e\nl\nn\n(\n16\n)\n1/\n3 \n= \n100e\n0\n9\n2\n4\nt \n(b) 4 5 minutes (c) In 9.968 hours \n15. (a) m(t) \n= \n50e\n-\nc\n1\n, where c \n= \nln2/1590 \n= \n4.\n36 x 10\n- 4; \n32.33 mg remain after 1000 years\n. \n(b) \nAfter 3691.9 years","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":129792,"to":130082}}}}],[1809,{"pageContent":"1\ncos(2t) \n13. \n(\na\n) \np(t) \n= \n100e\nl\nn\n(\n16\n)\n1/\n3 \n= \n100e\n0\n9\n2\n4\nt \n(b) 4 5 minutes (c) In 9.968 hours \n15. (a) m(t) \n= \n50e\n-\nc\n1\n, where c \n= \nln2/1590 \n= \n4.\n36 x 10\n- 4; \n32.33 mg remain after 1000 years\n. \n(b) \nAfter 3691.9 years \n5 -  10 cos(lOVK) \n17. x(t) \n= \nVK \nsin(VKt) + 10 cos(VKt) \nsin(lO K\n) \n19. (b) No \nReview Questions \n1. (a) F (c) T \n(e) F \n(g) F \n(i) T \nAnswers to Selected Odd-Numbered Exercises \nANS31 \n3. Subspace \n5. Subspace \n7. Let c\n1\nA + c\n2\nB \n= \n0. Then c\n1\nA - c\n2\nB \n= \nc\n1\nA \nr \n+ c\n2\nB\nr \n= \n(c\n1\nA \n+ c\n2\nBf \n= \n0. Adding\n, \nwe have 2c\n1\nA    = \n0, so \nc\n1 \n= \n0 because \nA \nis nonzero\n. \nHence c\n2\nB \n= \n0, and so \nc\n2 \n= \n0. Thus,  {\nA\n, \nB} is linearly independent\n. \n9. {1, x\n2\n, x4}, \ndim W \n= \n3 \n11. Linear transformation \n13. Linear transformation \n[\n1  0 -1] \n0  1 -2 \n17. \n0  0 \n1 \n1  0 -1 \n15. n\n2 \n- 1 \n19. S 0 Tis  the zero transformation\n. \nChapter 1 \nExercises 7. 1 \n1. (a) -10 \n(b) \nVi4 \n(c) \\/93 \n3. Any nonzero scalar multiple of \n[ \n�\n] \n5. (a) 1 \n(h) v'i3 \n(c) \nVi4 \n7.  x\n2 \nis one possibility \n9. (a) 1T \n(b) V7i \n(c) V7i \n13. Axiom ( 4 ) fails: u \n= \n[ \n�\n] i= 0, but \n(\nu, u) \n= \n0.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":130082,"to":130242}}}}],[1810,{"pageContent":"1. (a) -10 \n(b) \nVi4 \n(c) \\/93 \n3. Any nonzero scalar multiple of \n[ \n�\n] \n5. (a) 1 \n(h) v'i3 \n(c) \nVi4 \n7.  x\n2 \nis one possibility \n9. (a) 1T \n(b) V7i \n(c) V7i \n13. Axiom ( 4 ) fails: u \n= \n[ \n�\n] i= 0, but \n(\nu, u) \n= \n0. \n15. Axiom ( \n4\n) fails: u \n= \n[ \n�\n] \ni= 0, but \n(\nu, u) \n= \n0. \n17. Axiom (\n4\n) fails: \np\n(x) \n= \n1  -xis not the zero poly­\nnomial\n, \nbut (p(x), p\n(x)\n) \n= \n0. \n19.\nA    = \n[\n� \n:J \n21. \ny \n___,f---+----+�-+--+--x \n-2 \n2 \n25. -8 \n27. \nv'6 \n29. \nll\nu \n+ \nv \n-\nwll\n2 \n= \n(\nu \n+ \nv \n-\nw, \nu \n+ \nv \n-\nw\n) \n= \n(\nu, u) + \n(\nv, v) + \n(\nw, w) \n+ 2\n(\nu, \nv) -2\n(\nu, \nw) -2\n(\nv\n, \nw) \n= \n1 + 3 + 4 +  2 - 10 - 0 \n= \n0","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":130242,"to":130350}}}}],[1811,{"pageContent":"ANS32 \nAnswers to Selected Odd-Numbered Exercises \nTherefore\n, ll\nu + \nv \n-wll \n= \n0\n, \nso\n, \nby axiom (\n4\n)\n, \nu + \nv \n-w \n= \n0 or \nu + \nv \n= \nw\n. \n31. (\nu + \nv\n, \nu \n-\nv\n)    = \n(\nu\n, \nu) \n-(\nu\n, \nv\n) \n+ \n(\nv\n, \nu) \n-(\nv\n, \nv\n) \n= \nll\nu\nll\n2 \n-(\nu\n, \nv\n) \n+ \n(\nu\n, \nv\n) \n-ll\nv\nll\n2 \n= \nll\nu\nll\n2 \n-ll\nv\nll\n2 \n33. \nU\nsing Exercise 32   and a similar identity for \nll\nu \n-\nv\nll\n2\n, \nwe have \nll\nu \n+ \nv\nll\n2 \n+ \nll\nu \n-\nv\nll\n2 \n= \n(\nu \n+ \nv\n, \nu \n+ \nv) \n+ \n(\nu \n-\nv\n, \nu \n-\nv) \n= \nll\nu\nll\n2 \n+ \n2\n(\nu\n, \nv) \n+ \nll\nv\nll\n2 \n+ \nll\nu\nll\n2 \n-2(\nu\n, \nv\n) + \nll\nv\nll\n2 \n= \n2ll\nu\nll\n2 \n+ \n2ll\nv\nll\n2 \nDividing by 2 yields the identity we want\n. \n35. ll\nu + \nv\nii \n= \nll\nu \n-\nv\nii \n9 \nll\nu + \nv\nll\n2 \n= \nll\nu \n-\nv\nll\n2 \n9 \nll\nu\nll\n2 \n+ \n2(\nu\n, \nv\n) \n+ \nll\nv\nll\n2 \n= \nll\nu\nll\n2 \n-2(\nu\n, \nv\n) + \nll\nv\nll\n2 \n9 \n2(\nu\n, \nv\n)  = \n-2(\nu\n, \nv\n)9\n(\nu\n, \nv\n)    = \n0 \n37. \n{ \n[\n�\nl \n[\n�\n]\n} \n39. \n{1, x , x\n2\n} \n41. \n(a) 1/\n\\/2\n, \nV3\nx\n/\nVl\n,\nVs\n(3x\n2 \n-1   )\n/\n2\n\\/2 \n(b) \n\\/7\n(5x3 -3x)\n/\n2Vl \nExercises 7.2 \n1. \nll\nu\nll\nE \n= \nv'42, \nll\nu\nll\n, \n= \n10, ll\nu\nll\nm \n= \n5 \n3. d\nE\n(\nu\n, \nv\n) \n= v70, d\n,\n(\nu\n, \nv\n) \n= \n14, \nd\nm\n(\nu\n, \nv\n) \n= \n6 \n5. ll\nu\nll\nH \n= \n4, ll\nv\nll\nH \n= \n5 \n7.  (a) At most one component of \nv\nis nonzero\n. \n9. Suppose \nll\nv\nll\nm \n= \nI \nvk \nI\n. \nThen \nll\nv\nll\nE \n= \nY\nv\n� \n+ \n... \n+ \nv\n[ \n+ \n... \n+ \nv\n� \n2: \n� \n=\nl\nvk\nl \n= \nll\nv\nll\nm  · \n11. Suppose \nll\nv\nll\nm \n= \nI \nvk \nI","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":130352,"to":130729}}}}],[1812,{"pageContent":"u\n, \nv\n) \n= \n6 \n5. ll\nu\nll\nH \n= \n4, ll\nv\nll\nH \n= \n5 \n7.  (a) At most one component of \nv\nis nonzero\n. \n9. Suppose \nll\nv\nll\nm \n= \nI \nvk \nI\n. \nThen \nll\nv\nll\nE \n= \nY\nv\n� \n+ \n... \n+ \nv\n[ \n+ \n... \n+ \nv\n� \n2: \n� \n=\nl\nvk\nl \n= \nll\nv\nll\nm  · \n11. Suppose \nll\nv\nll\nm \n= \nI \nvk \nI\n. \nThen \nI \nvi \nI \n:s \nI \nvk \nI \nfor \n13. \ni \n= \n1, ... , \nn\n, \nso \nll\nv\nll\ns \n= \nl\nv\n1\nI \n+ \n· · · \n+ \nl\nvn\nl \n:S \nl\nvk\nl \n+ \n· · · \n+ \nl\nvk\nl \n=     n\nl\nvk\nl \n=     n\nll\nv\nll\nm \ny \ny \n� \n1 \n-] \n-1 \n1 \nx \n21. \nll\nA\nll\nF \n= \n\\fi9, \nllA\nll\n1 \n= \n4, \nllA\nll\noo \n= \n6 \n23. \nll\nAl\nl\nF \n= \nv'31, \nll\nAl\nl\n1 \n= \n6\n, \nll\nAl\nl\noo \n= \n6 \n25. \nll\nA\nll\nF \n= \n2\nVU\n, \nll\nA\nll\n1 \n= \n7\n, \nllA\nll\n,, \n= \n7 \n27.\nF \n[\n�\n]\n.y \n� \n[\n-\n:\ni \n29. \nF \nm.y \n� \nrn \n31.\nF \n[\nH\ny\n� \n[ \n=:J \n33. (a) By the definition of an operator norm\n, 11\n1\n11 \n= \nmaxll\nl\nxll \n= \nmaxllxll \n= \n1   . \nllxH \nllxH \n35. cond\n1\n(\nA\n) \n= \ncondoo(\nA\n) \n= \n21; well-conditioned \n37. cond\n1\n(\nA\n) \n= \ncondoo(\nA\n) \n= \n4\n00; ill-conditioned \n39. cond\n1\n(\nA\n) \n= \n77\n, cond00(\nA\n) \n= \n128 ; moderately \nill-conditioned \n41. (a) condoo(\nA\n) \n=\n(max{ \nI \nk\nl \n+ \n1, 2}) \n· \n(\nmax\n{\nl\n�I \n+ \nl�I, \n1\n6\n1}\n) \n43. (a) cond00(\nA\n) \n= \n4 0 \n(b) At most \n4\n00% relative change \n45. \nU\nsing \nExerc\nise \n33\n(a)\n, \nwe \nhave \ncond(\nA\n) \n= \nllA\nll\nllA\n-\n1\n11\n2 \nll\nAA\n-\n1\n11 \n= \n11\n1\n11 \n= \n1\n. \n49.k 2: \n6","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":130729,"to":131054}}}}],[1813,{"pageContent":"A\n) \n=\n(max{ \nI \nk\nl \n+ \n1, 2}) \n· \n(\nmax\n{\nl\n�I \n+ \nl�I, \n1\n6\n1}\n) \n43. (a) cond00(\nA\n) \n= \n4 0 \n(b) At most \n4\n00% relative change \n45. \nU\nsing \nExerc\nise \n33\n(a)\n, \nwe \nhave \ncond(\nA\n) \n= \nllA\nll\nllA\n-\n1\n11\n2 \nll\nAA\n-\n1\n11 \n= \n11\n1\n11 \n= \n1\n. \n49.k 2: \n6 \n51. k 2: 10 \nExercises 7.3 \n1. \nll\ne\nll \n= \nV2 \n= \n1.414 \n5. \nll\ne\nll \n= \n\\/7 \n= \n2\n.\n6\n4\n6 \n3\n. \nll\ne\nll \n= \nV6\n/2 \n= \n1\n.\n2 2\n5 \n7. \ny = \n-\n3 \n+ \n�x\n, ll\ne\nll \n= \n1   .\n225 \n9. \ny \n= \n¥-\n-\n2x\n, \nll\ne\nll \n= \n0\n.  81\n6 \n11. \ny \n= \nto\n+ \nfs\nx, \nll\ne\nll \n= \n0\n. 44\n7 \n13. y \n= \n-\nt  + \n�\nx , ll\ne\nll \n= \n0\n.\n6\n32 \n15. y \n= \n3    -\n¥-\nx \n+ \nx\n2 \n1\n8 \n1\n7 \nI \n2 \n17. y \n= \ns \n-\n1i5X \n-\nz\nX \n-\n5 \n-\n-3 \n[ \nI ] \n[ \n4\n] \n19. \nx   = \nft \n21. \nx \n-\n-\n� \n[ \n4 \n+ \nt \nl \n[\n�] \n-5 \n-\nt \nII \n23. x \n= \n25. H-\n-   5    -\n2t \n4\n2 \nt \nTl","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":131054,"to":131264}}}}],[1814,{"pageContent":"27.\nx \n= \n[ \n_\nl\n] \n29. y = 0.92 + 0.73x \n31. (a) Ifwe let the year 1920 correspond tot = 0, then \ny = 56.6 + 2.9t; 79.9 years \n33. (a)  p(t) = \n1\n5\n0\neQ.\n1\n3\n1\nt \n35. 139 days \n[\n! !\nJ \n[\nz_\nJ \n[\nt t t\nl \n[\n2\n] \n37. \nt  t \n, \ni \n39. \ni  i  i \n, \n� \n41. \nu \n1 \n-\nn \nu\ni \n45. A\n+\n� \ni\ni \nI\ni \n47. A\n+ \n= \n[\ni \n2 \n-\ni\n] \n49. A\n+ \n= \n[\n� \n-\n�\nJ \n-\n3 \nI \n6 \n51. A\n+ \n� \nu \n0 \nI \nJl \n-\n3 \n-1 \n! \n3 \n! \n3 \n53. (a) If A is invertible,  so is Ar, and we have A\n+ \n= \n(A\nT\nA\n)\n-\nI\nA\nT \n= A\n-\nl\n(A\nT\n)\n-\nI\nA\nT \n= A\n-\n1\n. \nExercises 7.4 \n1. 2, 3 \n3. V2,\no \n5. 5 \n7. 2, 3 \n9. \nVs\n, \n2, 0 \nII\n.A=\n[\n� \nO\nJ \n[\nVl  O\nJ \n[\nl/Vl   l/Vl\nJ \n1   0   0 l/Vl \n-1/Vl \n13. A  = \n[\n� \n15. A  = \n[\ni \n17. A \n� \n[\n: \n1\nJ\n[\n3 \nO\nJ\n[\n-1 \nO\nJ \n0  0  2    0 \n-\n1 \n-\nu\n[\n�\nJ\n[l] \no \n'\n]\n[\n3 \no\n]\n[ \n] \n0  0   0  2 ° \n1 \n-1  0   0 \n0 \n1  0 \n19. A  = \n[\nl \nO\nJ\n[\nVs \n0 \n0  1 \n0 \n2 \n�\nr\n:s \nl/Vs \n0 \n1 \n0 \n21. A\n= \nVl\n[\n�\nJ \n[l/Vl \nl/Vl] + \no\n[\n�J \n[ -1/Vl \n1\n/Vl] (Exercise 3) \n1/VS: \n-\n2�Vs \nAnswers to Selected Odd-Numbered Exercises \nANS33 \n23. (Emd\" 7) A \n� \n3 \n[\n:J i \n0 I ] + 2 \n[ \n_\n�J i \n1 \nO] \n33. The line segment [-1, l] \n2 2 \n35. The solid ellipse \nY\ni \n+ \nY\nz \n:::::  1 \n5    4 \n37. (a) \nll\nA\nll\n2 \n= \nVl \n39. (a) \nll\nA\nllz \n= 1.95 \n(b) cond\n2\n(\nA\n) = oo \n(h) cond\n2\n(\nA","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":131266,"to":131525}}}}],[1815,{"pageContent":"23. (Emd\" 7) A \n� \n3 \n[\n:J i \n0 I ] + 2 \n[ \n_\n�J i \n1 \nO] \n33. The line segment [-1, l] \n2 2 \n35. The solid ellipse \nY\ni \n+ \nY\nz \n:::::  1 \n5    4 \n37. (a) \nll\nA\nll\n2 \n= \nVl \n39. (a) \nll\nA\nllz \n= 1.95 \n(b) cond\n2\n(\nA\n) = oo \n(h) cond\n2\n(\nA\n) = 38.11 \n41. A\n+ \n= \n[\ni \n�\nJ \n43.  A\n+ \n� \n[\n! \n45.  A\n+ \n= \n[ \n� \n2\n5 \n� \nl \nx \n= \n[\n0.52\nJ \n25 \n1.04 \n47. A\n+ \n= \nu \n! \ni\n],\nx \n= \n[\n�\nJ \n6 \n! \n6 \n[\nV2 \n61. \n0 \nO\nJ \n[ \nl/Vl \n0   -1\n/Vl \n63. \n[ \n2 \n-1 \n-1 \nJ \n[ \n0 \n3   -1 \nExercises 7.5 \n1. g(x) = \nt \n5. g(x) = \nf6 \n+ \nfi\nx\n2 \n9. g(x) = x -t \n�\nJ \nl/Vl\nJ \nl/Vl \n3. g(x) = �x \n7. {1, x -H \n�\n] \n11. g(x) = (4e \n-\n10\n) \n+ (18 \n- 6e)x \n= \n0.87 + l.69x \n13. g(x) = fa -�x + \n�\nx\n2 \n15. \ng(x) = 39e - 105 + (588 \n- 216e)x + \n(210e -  570)x\n2 \n= \n1.01 + 0.85x + 0.84x\n2 \n21. \n1T \n-\ni\n(cos \nx + \ncos 3x) \n2 \n1T \n9 \n_ \nI \n1  -(-l\n)\nk \n23. a\n0 -\n2, a\nk \n= 0, b\nk \n= \n---­\nk1T \n2( - l\n)\nk \n25. \na\n0 \n= 1T, \na\nk \n= 0, b\nk \n= \n--\nk \nReview Questions \n1. (a) T (c) F \n3. Inner product \n(e) T \n5. \nV3 \n(g) T \n(i)  T","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":131525,"to":131714}}}}],[1816,{"pageContent":"ANS34 \nAnswers to Selected Odd-Numbered Exercises \n9. Not a norm \n11. cond00(\nA\n) \n= \n2\n4\n32 \n13. y  = l.7x 15. \n[\n!\n] \n17. (a) \\/2\n, V2 \n[\n'\nM \n1\n/\\/2 \nm� \n�\n]\n[\n: \n(b) \nA \n= 0 \n0 \n1\n/\\/2 \n-1\n/\\/2 \n(c\n) A\n+ \n= \n[\n! \n0 \n-\n!\nJ \n0 \n�\n] \n19. The singular values of P\nA\nQ are the square roots of the \neigenvalues of(P\nA\nQ)r(P\nA\nQ) = Qr\nA\nrPrP\nA\nQ = \nQr(\nA \nr \nA\n)Q. But Qr(\nA \nr \nA\n)Q is similar to \nA\nr \nA \nbecause \nQr = Q-\n1\n, \nand hence it has the same eigenvalues as \nA \nr\nA\n. Thus, \nP\nA\nQ and \nA \nhave the same singular values.","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":131716,"to":131800}}}}],[1817,{"pageContent":"Index \nA \nAbel,  Niels Henrik, 311, DS \nAbsolute  value,  C3 \nAddition \nclosure under, 192, 429 \nof complex numbers, Cl \nof matrices, 140 \nof polynomials, D2 \nof vectors, 5-6, 9, 42 9 \nAdjacency matrix, 242, 244 \nAdjoint (adjugate) of a matrix, 276 \nAl-Khwarizmi, Abu Ja'far Muhammad ibn \nMusa,  85 \nAlgebraic multiplicity,  294 \nAlgebraic properties of vectors, 10 \nAlgorithm, 85 \nAllocation of resources, 99-101 \nAltitude of a triangle, 33 \nAngle between vectors, 24-26 \nArgand, Jean-Robert, Cl \nArgand plane, Cl \nArgument of a complex number, C4 \nArithmetic mean, 548 \nArithmetic Mean-Geometric Mean Inequality, 548 \nAssociativity, 10, 154, 158, 223, 429 \nAttractor, 350 \nAugmented matrix, 61, 64 \nAxioms \nB \ninner product space, 531 \nvector space, 429 \nBack substitution, 61 \nBalanced chemical equation, 101-102 \nBasis, 198, 446-448 \nchange of, 463-470, 507-509 \ncoordinates with respect to, 208, 448-452 \northogonal, 370, 537 \northonormal, 372, 537 \nstandard, 198, 447 \nBasis step, Bl \nBasis Theorem, 202, 453","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":131802,"to":131843}}}}],[1818,{"pageContent":"Basis, 198, 446-448 \nchange of, 463-470, 507-509 \ncoordinates with respect to, 208, 448-452 \northogonal, 370, 537 \northonormal, 372, 537 \nstandard, 198, 447 \nBasis step, Bl \nBasis Theorem, 202, 453 \nBest Approximation Theorem, 570 \nBest approximation, to a vector, 570-571 \nBinary vector, 14 \nBinet, Jacques, 338 \nBinet's formula, 339, 428 \nBipartite graph, 251, 254 \nBlock, 145 \nBlock multiplication, 148 \nBlock triangular form, 283 \nBunyakovsky,  Viktor Yakovlevitch, 539 \nc \n'€, 435 \nC2, 432 \nC\", 543 \nCarroll, Lewis, 141, 284 \nCassini, Giovanni Domenico, 362 \nCassini's identity, 362 \nCauchy, Augustin-Louis, 273, 280 \nCauchy-Schwarz Inequality,  22, 539-540 \nCayley, Arthur, 300 \nCayley-Hamilton Theorem, 300 \nCentroid of a triangle, 32 \nChange of basis, 463-470 \nCharacteristic equation, 292 \nCharacteristic polynomial, 292 \nCircuit, 242 \nCircumcenter of a triangle, 33 \nClosure \nunder addition, 192, 429 \nunder linear combinations, 192 \nunder scalar multiplication, \n192, 429 \nCodomain, 212 \nCoefficient(s) \nFourier, 615","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":131843,"to":131885}}}}],[1819,{"pageContent":"Circuit, 242 \nCircumcenter of a triangle, 33 \nClosure \nunder addition, 192, 429 \nunder linear combinations, 192 \nunder scalar multiplication, \n192, 429 \nCodomain, 212 \nCoefficient(s) \nFourier, 615 \nof a linear combination, 12, 154 \nof a linear equation, 58 \nmatrix, 64 \nmethod of undetermined, D7 \nof a polynomial, DI \nCofactor, 266 \nCofactor expansion, 266-269 \nColumn matrix, 138 \nColumn-row representation of a matrix \nproduct, 147 \nColumn space, 195 \nColumn vector, 3, 138 \nCommutativity, 10, 19, 154, 429 \nCompanion matrix, 299 \nComplete bipartite graph, 254 \nComplex dot product, 543 \nComplex numbers, CI-CU \nabsolute value of,  C3 \naddition of,  Cl \nargument of,  C4 \nconjugate of, C2 \ndivision of,  C2, CS \nequality of,  Cl \nimaginary part of,  Cl \nmodulus of,  C3 \nmultiplication of,  CI-C2, CS \nnegative of,  C2 \npolar form of,  C3-C6 \npowers of,  C6-C7 \nprincipal argument of,  C4 \nreal part of, C 1 \nroots of,  C7-CS \nComplex plane, Cl \nComplex vector space, 429, 543 \nComponent of a vector, 3 \northogonal to a subspace, 382, 538","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":131885,"to":131930}}}}],[1820,{"pageContent":"powers of,  C6-C7 \nprincipal argument of,  C4 \nreal part of, C 1 \nroots of,  C7-CS \nComplex plane, Cl \nComplex vector space, 429, 543 \nComponent of a vector, 3 \northogonal to a subspace, 382, 538 \nComposition of linear transformations, 219, \n476-478 \nCondensation method, 284-285 \nCondition number, 562, 602 \nConic sections, 415-416 \nConjugate of complex numbers, C2-C3 \nConjugate transpose of a matrix, 544-545 \nConnected graph, 361 \nConservation of flow, 102 \nConsistent linear system, 60 \nConstant polynomial, DI \nConstrained optimization, 413-415, \n547-551 \nConsumption matrix, 236 \nContradiction, proof by,  AS \nContrapositive, proof by,  AS \nConvergence of iterative methods, 125, 316, \n563-566 \nCoordinate grid, 13 \nCoordinate vector, 208, 448-452 \nCoordinates, 207-209 \nCotes, Roger, 569 \nCramer, Gabriel, 274 \nCramer's Rule, 274-275 \nCross product, 48-49, 286-287 \nCrystallographic restriction, 517 \nCurve fitting, 290-291 \nD \n'd!, 435 \nDe Moivre, Abraham, C6 \nDe Moivre's Theorem, C6-C9 \nDegenerate conic, 415, 424","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":131930,"to":131969}}}}],[1821,{"pageContent":"Cramer's Rule, 274-275 \nCross product, 48-49, 286-287 \nCrystallographic restriction, 517 \nCurve fitting, 290-291 \nD \n'd!, 435 \nDe Moivre, Abraham, C6 \nDe Moivre's Theorem, C6-C9 \nDegenerate conic, 415, 424 \nDegree of a polynomial, DI \nDemand vector, 236 \nDescartes, Rene, 3, D9 \n11","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":131969,"to":131981}}}}],[1822,{"pageContent":"12 \nIndex \nDescartes' Rule of Signs, D9-D10 \nDeterminant(s), 165, 263-265 \ncofactor expansion of, 266-269 \nof elementary matrices, 271-272 \ngeometric applications of, 286-291 \nhistory of,  280-281 \nand matrix operations, 272-274 \nof n X n matrices, 265-269 \nproperties of,  269-2 7 4 \nVandermonde, 291 \nDiagonal entries of a matrix, 139 \nDiagonal matrix, 139 \nDiagonalizable linear transformation, 509 \nDiagonalizable matrix, 303 \northogonally, 400 \nunitarily, 546-54 7 \nDiagonalization, 303-309 \northogonal, 400-407 \nDiagonalization Theorem, 307 \nDiagonalizing a quadratic form, 411 \nDiagonally dominant matrix, 128, 324 \nDifference \nof complex numbers, C2 \nof matrices, 140 \nof polynomials, D2 \nof vectors, 8, 433 \nDifferential equation(s), 363, 436, 518 \nboundary conditions for, 523 \nhomogeneous,436,518-525 \ninitial conditions for, 340, 343, \n344,363 \nsolution of,  518 \nsystem oflinear, 340-348 \nDifferential operator, 473 \nDigital image compression, 607-608 \nDigraph, 243 \nDimension, 203, 452-456 \nDirect proof, A7","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":131983,"to":132022}}}}],[1823,{"pageContent":"344,363 \nsolution of,  518 \nsystem oflinear, 340-348 \nDifferential operator, 473 \nDigital image compression, 607-608 \nDigraph, 243 \nDimension, 203, 452-456 \nDirect proof, A7 \nDirection vector, 35, 39 \nDisjoint sets, A4 \nDistance \nHamming, 554 \nfrom a point to a line, 41-43 \nfrom a point to a plane, 43-44 \ntaxicab, 529-531 \nbetween vectors, 23-24, 535 \nDistance functions, 554-555 \nDistributivity, 10, 19, 154, 158, 429 \nDivergence, 127 \nDivision algorithm, D4 \nDodgson, Charles Lutwidge, 281, 284 \nDomain, 212 \nDominant eigenvalue, 311 \nDominant eigenvector, 311 \nDot product, 18-20, 49 \ncomplex, 543 \nweighted, 532 \nDual space, 514 \nDynamical system, 253, 348-355 \ntrajectory of, 349 \nE \nEchelon form of a matrix \nreduced row, 73 \nrow, 65 \nEdge of a graph, 242 \nEigenspace, 256 \nEigenvalue(s), 254 \nalgebraic multiplicity of, 294 \ndominant, 311 \ngeometric multiplicity of,  294 \ninverse power method for computing, \n317-318 \npower method for computing, 311-316 \nshifted inverse power method for computing, \n318-319","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":132022,"to":132066}}}}],[1824,{"pageContent":"dominant, 311 \ngeometric multiplicity of,  294 \ninverse power method for computing, \n317-318 \npower method for computing, 311-316 \nshifted inverse power method for computing, \n318-319 \nshifted power method for computing, \n316-317 \nEigenvector(s), 254 \ndominant, 311 \northogonal, 402 \nElectrical network, 104-107 \nElementary matrix, 170 \nElementary reflector, 397 \nElementary row operations, 66 \nElements of a matrix, 138 \nElimination \nGauss-Jordan, 72-76 \nGaussian, 68-72 \nEmpty set, A2 \nEquality \nof complex numbers, Cl \nof matrices, 139 \nof polynomials, D2 \nof sets, Al-A2 \nof vectors, 4 \nEquation(s) \nlinear, 58 \nnormal, 575 \nsystem oflinear, 59 \nEquilibrium, 50, 107 \nEquivalence relation, 302 \nError vector, 565, 572 \nEuclidean norm, 553 \nEuler, Leonhard, C9 \nEuler's formula, C9-Cll \nEven function, 617 \nExchange matrix, 235 \nExpansion by cofactors, 266-269 \nExponential of a matrix, 346 \nF \nS', 431 \nFactor Theorem, D4 \nFactorization \nLU, 180-186 \nmodified QR, 396-398 \nQR, 392-394 \nFeasible solution, 236 \nFibonacci, 336","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":132066,"to":132115}}}}],[1825,{"pageContent":"Expansion by cofactors, 266-269 \nExponential of a matrix, 346 \nF \nS', 431 \nFactor Theorem, D4 \nFactorization \nLU, 180-186 \nmodified QR, 396-398 \nQR, 392-394 \nFeasible solution, 236 \nFibonacci, 336 \nFibonacci numbers, 335, \n338-339,427 \nField, 429 \nFinite-dimensional vector space, 453 \nFinite linear games, 109-113 \nFloating-point form, 83 \nForce vectors, 50-53 \nFourier approximation, 615 \nFourier coefficients, 615 \nFourier, Jean-Baptiste Joseph, 616 \nFourier series, 617 \nFree variable, 71 \nFrobenius, Georg, 204 \nFrobenius norm, 556 \nFundamental subspaces of a matrix, 380 \nFundamental Theorem of Algebra, DS \nFundamental Theorem of Invertible \nG \nMatrices, 172, 206, 296, 512, \n605-606 \nGalilei, Galileo, 526 \nGalois, Evariste, 311, DS \nGauss, Carl Friedrich, 69, 125, 538, 569, DS \nGauss-Jordan elimination, 72-76 \nGauss-Jordan inverse method, 175-178 \nGauss-Seidel method, 124-131 \nGaussian elimination, 68-72 \nGeneral form of the equation of a line, 34, 36, 41 \nGeneral form of the equation of a plane, 38, 41 \nGeometric mean, 548","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":132115,"to":132155}}}}],[1826,{"pageContent":"Gauss-Seidel method, 124-131 \nGaussian elimination, 68-72 \nGeneral form of the equation of a line, 34, 36, 41 \nGeneral form of the equation of a plane, 38, 41 \nGeometric mean, 548 \nGeometric multiplicity,  294 \nGerschgorin disk, 319 \nGerschgorin Disk Theorem, 321 \nGerschgorin, Semyon Aranovich, 319 \nGerschgorin's theorem, 319-322 \nGibbs, Josiah Willard, 49 \nGlobal Positioning System (GPS), 121-123 \nGoogle, 358 \nGram, Jiirgen Pedersen, 390 \nGram-Schmidt Process, 388-392 \nGraph,242,253-254 \nadjacency matrix of, 242, 244 \nbipartite, 251 \ncomplete, 253 \ncomplete bipartite, 254 \nconnected, 361 \ncycle, 254 \ndirected (digraph), 243 \nedges of, 242 \nk-regular, 361 \npath in a, 242 \nPetersen, 254 \nvertices of, 242 \nGrassmann, Hermann, 429 \nGrassmann's  Identity,  458, 496 \nH \nHalf-life, 520 \nHamilton, William Rowan, 2, 300 \nHamming distance, 554 \nHamming norm, 554 \nHarmonic mean, 551 \nHead of a vector, 3 \nHead-to-tail rule,  6 \nHermitian matrix, 545 \nHilbert, David, 403 \nHoene-Wronski, Josef Maria, 457","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":132155,"to":132195}}}}],[1827,{"pageContent":"Hamming distance, 554 \nHamming norm, 554 \nHarmonic mean, 551 \nHead of a vector, 3 \nHead-to-tail rule,  6 \nHermitian matrix, 545 \nHilbert, David, 403 \nHoene-Wronski, Josef Maria, 457 \nHomogeneous linear differential equations, \n518-525 \nHomogeneous linear system, 76 \nHooke's Law, 524 \nHouseholder, Alston Scott, 397 \nHouseholder matrix, 397 \nHyperplane, 40","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":132195,"to":132209}}}}],[1828,{"pageContent":"I \ni, Cl \nIdempotent matrix, 179 \nIdentity matrix, 139 \nIdentity transformation, 221, 474 \nIll-conditioned linear system, 84 \nIll-conditioned matrix, S61 \nImage, 212 \nImaginary axis, CI \nImaginary conic, 424 \nImaginary part of a complex number, CI \nInconsistent linear system, 60 \nIndefinite matrix, 413 \nquadratic form of, 413 \nIndex of summation, AS \nIndirect proof, A7 \nInduction hypothesis, Bl \nInduction step, Bl \nInfinite-dimensional vector space, 4S3 \nInitial point of a vector, 3 \nInner product, S3 l \nInner product space, S31-S34 \nand Cauchy-Schwarz and Triangle \nInequalities, S39-S40 \ndistance between vectors in, S3S \nlength of vectors in, S3S \northogonal vectors in, S3S \nproperties of,  S3S \nIntegers modulo m,  14-16 \nInterior of a matrix, 284 \nIntersection of sets, A4 \nInverse \nGauss-Jordan method of computing, l 7S-l 78 \nof a linear transformation, 221-222, 478-479 \nof a matrix, 163 \nInverse power method, 317-318 \nshifted, 318-319 \nInvertible linear transformation, 221-222, \n478-479 \nInvertible matrix, 163-170","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":132211,"to":132250}}}}],[1829,{"pageContent":"of a linear transformation, 221-222, 478-479 \nof a matrix, 163 \nInverse power method, 317-318 \nshifted, 318-319 \nInvertible linear transformation, 221-222, \n478-479 \nInvertible matrix, 163-170 \nIrreducible matrix, 33S \nIrreducible polynomial, D7 \nIsometry,  3 7 S \nIsomorphism, 493-49S \nIterative method( s) \nconvergence of, 12S, 316, S63-S66 \nGauss-Seidel method, 124-131 \ninverse power method, 317-318 \nJacobi's method, 124-131 \npower method, 311-316 \nshifted inverse power method, 318-319 \nshifted power method, 316-317 \nJacobi, Carl Gustav,  124 \nJacobi's method, 124-131 \nJordan, Wilhelm, 72 \nK \nKernel, 482 \nKirchhoff's Laws, 104 \nL \nLagrange interpolation formula, 4S9 \nLagrange, Joseph-Louis, 4S8 \nLagrange polynomials, 4S8 \nLaplace Expansion Theorem, 266, 277-280 \nLaplace, Pierre Simon, 267 \nLattice, Sl6 \nLeading entry, 6S \nLeading 1, 73 \nLeading variable, 71 \nLeast squares approximating line, S74 \nLeast squares approximation, S68-S69, S71-S82 \nBest Approximation Theorem and, S70-S71 \nand orthogonal projection, S83-S8S","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":132250,"to":132288}}}}],[1830,{"pageContent":"Leading 1, 73 \nLeading variable, 71 \nLeast squares approximating line, S74 \nLeast squares approximation, S68-S69, S71-S82 \nBest Approximation Theorem and, S70-S71 \nand orthogonal projection, S83-S8S \nand the pseudoinverse of a matrix, S8S-S86 \nvia the QR factorization, S82-S83 \nvia the singular value decomposition, \n603-60S \nLeast squares error, S72 \nLeast squares solution, S 7 4 \nof minimal length, 603-604 \nLeast Squares Theorem, S7S \nLeft singular vectors, S93 \nLegendre, Adrien Marie, S38 \nLegendre polynomials, S38 \nLeibniz, Gottfried Wilhelm von, 281 \nLemma, 271 \nLength \nof a binary vector, 14 \nof an m-ary vector, 16 \nof a path, 242 \nof a vector, 20, S3S \nLeonardo of Pisa, 336 \nLeontief closed model, 108, 23S \nLeontief open model, 108, 236 \nLeontief, Wassily, 107 \nLeslie matrix, 240 \nLeslie model, 239-241, 330-332 \nLine, 34-38 \nof best fit, S74 \nequation(s) of,  34, 36, 41 \nleast squares approximating, S74 \nLinear combination, 12, 1S4, 433 \nLinear dependence,92-93, 1S7, 443,446","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":132288,"to":132323}}}}],[1831,{"pageContent":"Leslie model, 239-241, 330-332 \nLine, 34-38 \nof best fit, S74 \nequation(s) of,  34, 36, 41 \nleast squares approximating, S74 \nLinear combination, 12, 1S4, 433 \nLinear dependence,92-93, 1S7, 443,446 \nLinear economic models, 107-109, 23S-236 \nLinear equation(s), S8, S9. See also Linear \nsystem(s) \nLinear independence, 92-97, 1S7, 443-446 \nLinear recurrence relations, 33S-336 \nLinear system(s), S8-62 \naugmented matrix of,  61, 64 \ncoefficient matrix of, 64 \nconsistent, 60 \ndirect method for solving, 64-79 \nequivalent, 60 \nhomogeneous, 76 \nill-conditioned, 84 \ninconsistent, 60 \niterative methods for solving, 124-131 \nsolution (set) of, S9 \nover !RP, 77-79 \nLinear transformation(s), 213-214, 472-474 \nonto, 488 \ncomposition of, 219, 476-478 \ndiagonalizable, S09 \nidentity,  221, 474 \ninverse of,  221-222, 478-479 \ninvertible, 221-222, 478-479 \nkernel of,  482 \nmatrix of,  216, 497-S03 \nnullity of, 484 \none-to-one, 488 \nproperties of, 47S-476 \nzero, 474 \nIndex \n13 \nLinearly dependent matrices, 1S7","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":132323,"to":132362}}}}],[1832,{"pageContent":"invertible, 221-222, 478-479 \nkernel of,  482 \nmatrix of,  216, 497-S03 \nnullity of, 484 \none-to-one, 488 \nproperties of, 47S-476 \nzero, 474 \nIndex \n13 \nLinearly dependent matrices, 1S7 \nLinearly dependent vectors, 93, 443, 446 \nLinearly independent matrices, 1S7 \nLinearly independent vectors, 93, 443, 446 \nLong range transition matrix, 329 \nLU factorization, 180-186 \nLucas, Edouard, 336, 428 \nM \nm-ary vector, 16 \nMmn• \n430 \nMaclaurin, Colin, 274, 280 \nMagic square, 460-462 \nclassical, 460 \nweight of a, 460 \nMantissa, 83 \nMarkov, Andrei Andreyevich, 230 \nMarkov chain, 230-23S, 32S-330 \nMathematical induction, Bl-B7 \nfirst principle of,  Bl \nsecond principle of,  BS \nMatrix (matrices), 61, 138 \naddition of,  140 \nadjacency,  242, 244 \nadjoint (adjugate), 276 \nassociated with a quadratic form, 409 \naugmented, 61, 64 \nchange-of-basis, 46S \ncharacteristic equation of, 292 \ncharacteristic polynomial of, 292 \ncoefficient, 64 \ncolumn space of, 19S \ncompanion, 299 \ncondition number of,  S62 \nconjugate transpose of, S44-S4S","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":132362,"to":132405}}}}],[1833,{"pageContent":"change-of-basis, 46S \ncharacteristic equation of, 292 \ncharacteristic polynomial of, 292 \ncoefficient, 64 \ncolumn space of, 19S \ncompanion, 299 \ncondition number of,  S62 \nconjugate transpose of, S44-S4S \nconsumption, 236 \ndeterminant of,  16S, 264, 26S-269 \ndiagonal, 139 \ndiagonalizable, 303-309 \ndifference of,  140 \neigenspace of,  2S6 \neigenvalue of, 2S4 \neigenvector of,  2S4 \nelementary, 170 \nelements of,  138 \nentries of,  138 \nequality of,  139 \nexchange, 23S \nexponential of,  346 \nfactorization of, 180 \nfundamental subspaces of,  380 \nHermitian, S4S \nidempotent, 179 \nidentity,  139 \nill-conditioned, S61 \nindefinite, 413 \ninterior, 284 \ninverse of,  163 \ninvertible, 163-170","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":132405,"to":132436}}}}],[1834,{"pageContent":"14 \nIndex \nMatrix (Continued) \nirreducible, 335 \nLeslie, 240 \nof a linear transformation, 216, 497-503 \nmultiplication of,  141-143 \nnegative definite, 413 \nnegative of,  140 \nnegative semidefinite, 413 \nnilpotent, 282 \nnorm of,  555-561 \nnormal, 547 \nnull space of,  197 \nnullity of,  204 \northogonal, 373-376 \northogonally diagonalizable, 400 \npartitioned, 145-149 \npermutation, 187 \npositive, 325 \npositive definite, 413 \npositive semidefinite, 413 \npowers of,  149-150 \nprimitive, 335 \nproductive, 237-238 \nprojection, 218-219, 366, 586 \npseudoinverse of,  585-586, 602-603 \nrank of,  72, 204 \nreduced row echelon form of,  73 \nreducible, 334 \nregular, 325 \nrow echelon form of, 65 \nrow equivalent, 68 \nrow space of,  19 5 \nscalar, 139 \nscalar multiple of,  140 \nsimilar, 301-303 \nsingular values of,  590-591 \nsingular vectors of,  593 \nsize of,  138 \nskew-symmetric, 162 \nsquare, 139 \nstandard, 216 \nstochastic, 232 \nstrictly diagonally dominant, 128 \nsum of,  140 \nsymmetric, 151-152, 160-161 \ntrace of,  162 \ntransition, 231","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":132438,"to":132486}}}}],[1835,{"pageContent":"size of,  138 \nskew-symmetric, 162 \nsquare, 139 \nstandard, 216 \nstochastic, 232 \nstrictly diagonally dominant, 128 \nsum of,  140 \nsymmetric, 151-152, 160-161 \ntrace of,  162 \ntransition, 231 \ntranspose of,  151, 159-160 \nunit lower triangular, 181 \nunitarily diagonalizable, 546-547 \nunitary, 545-546 \nupper triangular, 162 \nzero, 141 \nMatrix-column representation of a matrix \nproduct, 146 \nMatrix factorization, 180. See also Singular value \ndecomposition (SVD) \nand diagonalization, 303-309 \nLU, 180-186 \nmodified QR, 396-398 \nP1 \nLU, 186-187 \nQR, 392-394 \nand Schur's Triangularization Theorem, 408 \nMatrix transformation, 211-216, 472 \nprojection, 218-219, 509-510 \nreflection, 215 \nrotation, 216-218 \nMax norm, 553 \nMean \narithmetic, 548 \ngeometric, 548 \nharmonic, 551 \nquadratic, 550 \nMedian of a triangle, 32 \nMetric, 555 \nMetric space, 555 \nMinimum length least squares solution, 603-604 \nMinor, 264 \nModified QR factorization, 396-398 \nModular arithmetic, 13-16 \nModulus of a complex number, C3 \nMoore, Eliakim Hastings, 602","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":132486,"to":132531}}}}],[1836,{"pageContent":"Metric space, 555 \nMinimum length least squares solution, 603-604 \nMinor, 264 \nModified QR factorization, 396-398 \nModular arithmetic, 13-16 \nModulus of a complex number, C3 \nMoore, Eliakim Hastings, 602 \nMoore-Penrose inverse, 602 \nMuir, Thomas, 281 \nMultiplication \nof complex numbers, CI-C2, CS \nof matrices, 141-143 \nof polynomials, D2-D3 \nscalar, 7-8, 140, 429 \nMultiplicity of an eigenvalue \nalgebraic, 294 \ngeometric, 294 \nN \nNegative \nof a complex  number,  C2 \nof a matrix, 140 \nof a vector, 8, 429 \nNegative definite matrix, 413 \nquadratic form of, 413 \nNegative semidefinite matrix, 413 \nquadratic form of, 413 \nNet reproduction rate, 360 \nNetwork, 102 \nNetwork analysis, 102-103 \nNewton's Second Law of Motion, 524 \nNilpotent matrix, 282 \nNode, 102 \nNondegenerate conic, 415-416 \nNorm of a matrix, 555-561 \n1-, 559 \n2-, 559 \n7-, 559 \ncompatible, 556 \nFrobenius, 556 \noperator, 559 \nNorm of a vector, 20, 535, 552 \n1-, 553 \n2-, 553 \n7-, 553 \nEuclidean, 553 \nHamming, 554 \nmax, 553 \nsum,552 \ntaxicab, 530 \nuniform, 553","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":132531,"to":132580}}}}],[1837,{"pageContent":"1-, 559 \n2-, 559 \n7-, 559 \ncompatible, 556 \nFrobenius, 556 \noperator, 559 \nNorm of a vector, 20, 535, 552 \n1-, 553 \n2-, 553 \n7-, 553 \nEuclidean, 553 \nHamming, 554 \nmax, 553 \nsum,552 \ntaxicab, 530 \nuniform, 553 \nNormal equations, 575 \nNormal form of the equation of a line, 34, 36, 41 \nNormal form of the equation of a plane, 38, 41 \nNormal matrix, 547 \nNormal vector, 34, 38 \nNormalizing a vector, 21 \nNormed linear space, 552 \nNull space, 197 \nNullity \n0 \nof a linear transformation, 484 \nof a matrix, 204 \nOdd function, 617 \nOhm's Law, 104 \nOne-to-one, 488 \nOnto, 488 \nOperator norm, 559 \nOptimization \nconstrained, 413-415 \ngeometric inequalities and, 547-551 \nOrbital center, 355 \nOrdered n-tuple, 9 \nOrdered pair,  3 \nOrdered triple, 8 \nOrthocenter of a triangle, 33 \nOrthogonal basis, 370, 537 \nOrthogonal complement, 378-382 \nOrthogonal Decomposition Theorem, 384-385 \nOrthogonal diagonalization, 400-407 \nOrthogonal matrix, 373-376 \nOrthogonal projection, 382-387, 538 \nleast squares approximation, 583-585","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":132580,"to":132627}}}}],[1838,{"pageContent":"Orthogonal Decomposition Theorem, 384-385 \nOrthogonal diagonalization, 400-407 \nOrthogonal matrix, 373-376 \nOrthogonal projection, 382-387, 538 \nleast squares approximation, 583-585 \nOrthogonal set of vectors, 369-373, 537 \nOrthogonal vectors, 26, 535 \nOrthonormal basis, 372, 537 \nOrthonormal set of vectors, 372, 537 \nOuter product, 147 \nOuter product expansion, 147 \nOuter product form of the SVD, 596 \np \n9/', 431 \n9J>n, \n431 \nParallel vectors, 8 \nParallelogram rule,  6 \nParameter, 36 \nParametric equation \nof a line, 36, 41 \nof a plane, 39, 41 \nPartial fractions, 119 \nPartial pivoting, 84-85 \nPartitioned matrix, 145-149 \nPath(s) \nk-, 243 \nlength of,  242 \nnumber of,  242-245 \nsimple, 242 \nPeano, Giuseppe, 429 \nPenrose conditions, 586 \nPenrose, Roger, 603 \nPermutation matrix, 187 \nPerpendicular bisector, 33","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":132627,"to":132661}}}}],[1839,{"pageContent":"Perron eigenvector, 335 \nPerron-Frobenius Theorem, 332-335 \nPerron, Oskar, 332 \nPerron root, 335 \nPerron's  Theorem, 333 \nPetersen graph, 254 \nPivot, 66 \nPivoting, 66 \npartial, 84-85 \nPlane, 38-41 \nArgand, Cl \ncomplex, Cl \nequation of,  38, 39, 41 \nPolar decomposition, 610 \nPolar form of a complex number, C3-C6 \nP6lya, George, A 7 \nPolynomial, Dl-DlO \ncharacteristic, 292 \nconstant, D 1 \ndegree of,  D 1 \nirreducible, D7 \nLagrange, 458 \nLegendre, 538 \nTaylor, 472 \ntrigonometric, 614 \nzero of, D4 \nPopulation distribution vector, 239 \nPopulation growth, 239-241, 330-332 \nPositive definite matrix, 413 \nquadratic form of, 413 \nPositive matrix, 325 \nPositive semidefinite matrix, 413 \nquadratic form of, 413 \nPower method, 311-316 \ninverse, 317-318 \nshifted, 316-317 \nshifted inverse, 318-319 \nPredator-prey model, 343 \nPrice vector(s), 235 \nPrimitive matrix, 335 \nPrincipal argument of a complex number, C4 \nPrincipal Axes Theorem, 411 \nProbability vector, 231 \nProduct \nof complex numbers, Cl-C2 \nof matrices, 141-143 \nof polynomials, D2-D3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":132663,"to":132709}}}}],[1840,{"pageContent":"Primitive matrix, 335 \nPrincipal argument of a complex number, C4 \nPrincipal Axes Theorem, 411 \nProbability vector, 231 \nProduct \nof complex numbers, Cl-C2 \nof matrices, 141-143 \nof polynomials, D2-D3 \nProduction vector, 236 \nProjection \northogonal, 382-387, 538 \ninto a subspace, 382 \nonto a vector, 27-28 \nProjection form of the Spectral Theorem, 405 \nProjection matrix, 218-219, 366, 586 \nProof \nby contradiction, AS \nby contrapositive, AS \ndirect, A7 \nindirect, A7 \nby mathematical induction, Bl-B7 \nPseudoinverse of a matrix, 585-586, \n602-603 \nPythagoras' Theorem, 26, 537 \nQ \nQR algorithm, 398-399 \nQR factorization, 392-394 \nleast squares and, 582-583 \nmodified, 396-398 \nQuadratic equation(s), D6 \ngraphing, 415-423 \nQuadratic form, 408-416 \nindefinite, 413 \nmatrix associated with, 409 \nnegative definite, 413 \nnegative semidefinite, 413 \npositive definite, 413 \npositive semidefinite, 413 \nQuadratic mean, 550 \nQuadric surface, 420 \nQuotient of complex numbers, \nC2,C5 \nR \nIR, 4 \nIR3, 8 \nIR\", 9-11 \nRacetrack game, 1-3","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":132709,"to":132755}}}}],[1841,{"pageContent":"negative semidefinite, 413 \npositive definite, 413 \npositive semidefinite, 413 \nQuadratic mean, 550 \nQuadric surface, 420 \nQuotient of complex numbers, \nC2,C5 \nR \nIR, 4 \nIR3, 8 \nIR\", 9-11 \nRacetrack game, 1-3 \nRange, 212, 482 \nRank \nof a linear transformation, 484 \nof a matrix, 72, 204 \nsingular value decomposition, 600 \nRank Theorem, 72, 205, 386, 486 \nRanking vector, 356-358 \nRational Roots Theorem, D5 \nRayleigh, Baron, 316 \nRayleigh quotient, 316 \nReal axis, Cl \nReal part of a complex \nnumber, Cl \nRecurrence relation, 336 \nsolution of,  337 \nReduced row echelon form, 73 \nReducible matrix, 334 \nReflection, 215 \nRegular graph, 361 \nRegular matrix, 325 \nRepeller, 352 \nResolving a vector, 51 \nResultants, 50 \nRight singular vectors, 593 \nRobotics, 226-229 \nRoot, for a polynomial equation, D4 \nRoot mean square error, 612 \nRotation, 216-218 \ncenter of,  516 \nRotational symmetry, 516 \nRoundoff error, 62, 83-84 \nRow echelon form, 65 \nRow equivalent matrices, 68 \nRow matrix, 138 \nRow-matrix representation of a matrix \nproduct, 146","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":132755,"to":132802}}}}],[1842,{"pageContent":"center of,  516 \nRotational symmetry, 516 \nRoundoff error, 62, 83-84 \nRow echelon form, 65 \nRow equivalent matrices, 68 \nRow matrix, 138 \nRow-matrix representation of a matrix \nproduct, 146 \nRow reduction, 66 \nRow space, 195 \nRow vector, 3, 138 \ns \nSaddle point, 352 \nScalar, 8 \nScalar matrix, 139 \nScalar multiple, 481 \nIndex \n15 \nScalar multiplication, 7-8, 9, 140, 429 \nclosure under, 192, 429 \nScaling, 314 \nSchmidt, Erhard, 390 \nSchur complement, 283 \nSchur, Issai, 283 \nSchur's Triangularization Theorem, 408 \nSchwarz, Karl Herman Amandus, 539 \nSeidel, Philipp Ludwig, 125 \nSeki Kowa, Takakazu, 280 \nSet(s), Al-A4 \ndisjoint, A4 \nelements of, A 1 \nempty,A2 \nintersection of, A4 \nsubset of, A2 \nunion of, A4 \nShifted inverse power method, 318-319 \nSimilar matrices, 301-303, 508 \nSimple path, 242 \nSingular value decomposition (SVD), 590-599 \napplications of,  599-606 \nand condition number, 602 \nand least squares approximation, 603-605 \nand matrix norms, 600-602 \nouter product form of,  596 \nand polar decomposition, 610","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":132802,"to":132846}}}}],[1843,{"pageContent":"applications of,  599-606 \nand condition number, 602 \nand least squares approximation, 603-605 \nand matrix norms, 600-602 \nouter product form of,  596 \nand polar decomposition, 610 \nand pseudoinverse, 602-603 \nand rank, 600 \nSingular values, 590-591 \nSingular vectors, 593 \nSize of a matrix, 138 \nSkew lines, 76 \nSkew-symmetric matrix, 162 \nSolution \nof a differential equation, 518 \nleast squares, 57 4-582 \nof a linear system, 59 \nminimum length least squares, 603 \nof a recurrence relation, 337 \nof a system of differential equations, 340-342 \nSpan,90, 156, 193, 438 \nSpanning set of vectors, 88-92 \nSpanning sets, 438-441 \nSpectral decomposition, 405 \nSpectral Theorem, 403 \nprojection form of, 405 \nSpectrum, 403 \nSpiral attractor, 355 \nSpiral repeller, 355 \nSquare matrix, 139, 374 \nSquare root of a matrix, 424 \nStandard basis, 198, 447 \nStandard matrix, 216 \nStandard position, 4 \nStandard unit vectors, 22 \nState vector, 231 \nSteady state vector, 233","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":132846,"to":132882}}}}],[1844,{"pageContent":"16 \nIndex \nStochastic matrix, 232 \nStrictly diagonally dominant matrix, 324 \nStrutt, John William, 316 \nSubmatrices, 145 \nSubset, A2 \nSubspace(s), 192, 433-438 \nfundamental, 380 \nspanned by a set of vectors, 192-193, 441 \nsum of,  442 \ntrivial, 43 7 \nzero, 437 \nSubtraction \nof complex numbers, C2 \nof matrices, 140 \nof polynomials, D2 \nof vectors, 8, 433 \nSum \nof complex numbers, Cl \noflinear transformations, 481 \nof matrices, 140 \nof polynomials, D2 \nof subspaces, 442 \nof vectors, 5-6, 9, 439 \nSum norm, 552 \nSummation notation, A4-A7 \nSustainable harvesting policy, 360 \nSylvester, James Joseph, 206, 280 \nSymmetric matrix, 151-152, 160-161 \nSystem oflinear differential equations, 340-348 \nSystem(s) oflinear equations. See Linear \nsystem(s) \nT \nTail of a vector,  3 \nTaussky-Todd, Olga, 320 \nTaxicab circle, 530 \nTaxicab distance, 529-531 \nTaxicab norm, 530 \nTaxicab perpendicular bisector, 530 \nTaxicab pi, 530 \nTaylor polynomial, 472 \nTerminal point of a vector, 3 \nTernary vector, 16 \nTheorem, 10 \nTiling, 515 \nTournament, 244","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":132884,"to":132930}}}}],[1845,{"pageContent":"Taxicab norm, 530 \nTaxicab perpendicular bisector, 530 \nTaxicab pi, 530 \nTaylor polynomial, 472 \nTerminal point of a vector, 3 \nTernary vector, 16 \nTheorem, 10 \nTiling, 515 \nTournament, 244 \nTrace of a matrix, 162 \nTransformation, 212 \nlinear, 213-214, 472-474 \nmatrix, 211-216, 472 \nTransitional matrix, 231 \nTransitional probabilities, 230 \nTranslational symmetry, 516 \nTranspose of a matrix, 151, 159-160 \nTriangle inequality, 22, 540, 552 \nTrigonometric polynomial, 614 \nTriple scalar product identity, 287 \nTrivial subspace, 437 \nTuring, Alan Mathison, 181 \nu \nUniform norm, 553 \nUnion of sets, A4 \nUnit circle, 21 \nUnit lower triangular matrix, 181 \nUnit sphere, 535 \nUnit vector, 21, 535 \nUnitarily diagonalizable matrix, 546-547 \nUnitary matrix, 545-546 \nUpper triangular matrix, 162 \nblock, 283 \nv \nVandermonde, Alexandre-Theophile, 291 \nVandermonde determinant, 291 \nVector(s), 3, 9, 429, 439 \naddition of,  5-6, 9, 439 \nalgebraic properties of,  10 \nangle between, 24-26 \nbinary,  14 \ncolumn, 3, 138 \ncomplex, 429, 432, 543-544","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":132930,"to":132972}}}}],[1846,{"pageContent":"Vandermonde determinant, 291 \nVector(s), 3, 9, 429, 439 \naddition of,  5-6, 9, 439 \nalgebraic properties of,  10 \nangle between, 24-26 \nbinary,  14 \ncolumn, 3, 138 \ncomplex, 429, 432, 543-544 \ncomplex dot product of, 543 \ncomponents of,  3 \ncoordinate, 208, 448-452 \ncross product of, 48-49, 286-287 \ndemand, 236 \ndirection, 35, 39 \ndistance between, 23-24, 535 \ndot product of,  18-20 \nequality of,  3 \nforce, 50-53 \ninner product of,  531 \nlength of, 20, 535 \nlinear combination of,  12, 433 \nlinearly dependent, 92-93, 443, 446 \nlinearly independent, 92-97, 443, 446 \nnorm of, 20, 535, 552 \nnormal, 34, 38 \northogonal, 26, 369-373, 535, 537 \northonormal, 372, 537 \nparallel, 8 \npopulation distribution, 240 \nprice, 235 \nprobability,  231 \nproduction, 236 \nranking, 356-358 \nresultant, 50 \nrow, 3, 138 \nscalar multiplication of, 7-8, 9, 429 \nspan of,  438 \nspanning sets of,  88-92 \nstate, 231 \nsteady-state, 233 \nternary, 16 \nunit, 21, 535 \nzero, 4, 429 \nVector form of the equation of a \nline,36, 41 \nVector form of the equation of a","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":132972,"to":133017}}}}],[1847,{"pageContent":"span of,  438 \nspanning sets of,  88-92 \nstate, 231 \nsteady-state, 233 \nternary, 16 \nunit, 21, 535 \nzero, 4, 429 \nVector form of the equation of a \nline,36, 41 \nVector form of the equation of a \nplane, 39, 41 \nVector space(s), 429 \nbasis for, 446 \ncomplex, 429,  432, 543-544 \ndimension of,  453 \nfinite-dimensional, 453 \ninfinite-dimensional, 453 \nisomorphic, 493-495 \nsubspace of, 433-438 \nover \"ll.\nP\n, 429, 432 \nVenn diagram, A2-A3 \nVenn, John, A2 \nVertex of a graph, 242 \nw \nWeight of a magic square, 460 \nWeighted dot product, 532 \nWell-conditioned matrix, 561 \nWessel, Caspar, Cl \nWey!, Hermann, 429 \nWheatstone bridge circuit, 105-106 \nWilson, Edwin B., 49 \nWronskian, 457 \nx \nx-axis, 3 \nxy-plane, 8 \nxz-plane, 8 \ny \ny-axis, 3 \nyz-plane, 8 \nz \n\"ll.,   14 \n\"ll.,, 14 \n\"ll.�, 14 \n\"ll.m> 16 \n\"ll.�, 16 \nZ-axis, 8 \nZero  matrix, 141 \nZero of a polynomial, D4 \nZero subspace, 437 \nZero transformation, 474 \nZero vector, 4, 429","metadata":{"source":"C:\\Users\\Elija\\Documents\\GitHub\\chatgpt-tutorbot-for-linear-algebra\\docs\\test\\David Poole - Linear Algebra_ A Modern Introduction-Cengage Learning (2014) (2).pdf","pdf_numpages":721,"loc":{"lines":{"from":133017,"to":133069}}}}]]